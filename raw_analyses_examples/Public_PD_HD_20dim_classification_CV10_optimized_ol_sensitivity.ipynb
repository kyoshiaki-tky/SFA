{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24299a06-cc90-4986-ad2f-ec21c911f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39868932-6135-4362-a04b-3f5586697a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ENSG</th>\n",
       "      <th>hgnc_symbol</th>\n",
       "      <th>gene_length</th>\n",
       "      <th>C_0002</th>\n",
       "      <th>C_0003</th>\n",
       "      <th>C_0004</th>\n",
       "      <th>C_0005</th>\n",
       "      <th>C_0006</th>\n",
       "      <th>C_0008</th>\n",
       "      <th>...</th>\n",
       "      <th>H_0740</th>\n",
       "      <th>H_0750</th>\n",
       "      <th>H_0513</th>\n",
       "      <th>H_0601</th>\n",
       "      <th>H_0656</th>\n",
       "      <th>H_0689</th>\n",
       "      <th>H_0709</th>\n",
       "      <th>H_0723</th>\n",
       "      <th>H_1104</th>\n",
       "      <th>H_1105</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>12883</td>\n",
       "      <td>2.703162</td>\n",
       "      <td>4.679815</td>\n",
       "      <td>3.827170</td>\n",
       "      <td>3.940771</td>\n",
       "      <td>3.956139</td>\n",
       "      <td>4.749555</td>\n",
       "      <td>...</td>\n",
       "      <td>4.799810</td>\n",
       "      <td>8.733498</td>\n",
       "      <td>3.666497</td>\n",
       "      <td>4.563176</td>\n",
       "      <td>2.492904</td>\n",
       "      <td>2.949728</td>\n",
       "      <td>3.314066</td>\n",
       "      <td>6.985665</td>\n",
       "      <td>3.947979</td>\n",
       "      <td>3.784485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ENSG00000000005</td>\n",
       "      <td>TNMD</td>\n",
       "      <td>14949</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>0.053272</td>\n",
       "      <td>0.108531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024476</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.083015</td>\n",
       "      <td>0.078651</td>\n",
       "      <td>0.050749</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>0.138079</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>0.064583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ENSG00000000419</td>\n",
       "      <td>DPM1</td>\n",
       "      <td>24273</td>\n",
       "      <td>2.625862</td>\n",
       "      <td>4.387499</td>\n",
       "      <td>4.450224</td>\n",
       "      <td>5.390720</td>\n",
       "      <td>3.707350</td>\n",
       "      <td>5.356800</td>\n",
       "      <td>...</td>\n",
       "      <td>4.100151</td>\n",
       "      <td>6.470481</td>\n",
       "      <td>5.783705</td>\n",
       "      <td>6.878268</td>\n",
       "      <td>3.167153</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>6.380522</td>\n",
       "      <td>8.912018</td>\n",
       "      <td>6.420788</td>\n",
       "      <td>9.267549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ENSG00000000457</td>\n",
       "      <td>SCYL3</td>\n",
       "      <td>44636</td>\n",
       "      <td>0.896318</td>\n",
       "      <td>1.025353</td>\n",
       "      <td>1.568379</td>\n",
       "      <td>1.315815</td>\n",
       "      <td>1.617599</td>\n",
       "      <td>1.350063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610696</td>\n",
       "      <td>0.892573</td>\n",
       "      <td>1.098204</td>\n",
       "      <td>1.589228</td>\n",
       "      <td>0.708180</td>\n",
       "      <td>0.416006</td>\n",
       "      <td>0.628301</td>\n",
       "      <td>1.156094</td>\n",
       "      <td>1.317197</td>\n",
       "      <td>2.011547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ENSG00000000460</td>\n",
       "      <td>C1orf112</td>\n",
       "      <td>192073</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>0.074862</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>0.114637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030003</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.036344</td>\n",
       "      <td>0.091820</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>0.070371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21858</th>\n",
       "      <td>26560</td>\n",
       "      <td>ENSG00000273372</td>\n",
       "      <td>SFTPD-AS1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.680299</td>\n",
       "      <td>0.616103</td>\n",
       "      <td>0.715117</td>\n",
       "      <td>1.061834</td>\n",
       "      <td>1.061811</td>\n",
       "      <td>1.437003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311009</td>\n",
       "      <td>0.202893</td>\n",
       "      <td>0.201662</td>\n",
       "      <td>1.018982</td>\n",
       "      <td>0.050577</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247696</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.096546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21859</th>\n",
       "      <td>26565</td>\n",
       "      <td>ENSG00000273396</td>\n",
       "      <td>LINC01396</td>\n",
       "      <td>6639</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.119952</td>\n",
       "      <td>0.139644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21860</th>\n",
       "      <td>26570</td>\n",
       "      <td>ENSG00000273409</td>\n",
       "      <td>LINC02712</td>\n",
       "      <td>66004</td>\n",
       "      <td>0.019632</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005544</td>\n",
       "      <td>0.027945</td>\n",
       "      <td>0.054055</td>\n",
       "      <td>0.065315</td>\n",
       "      <td>0.015325</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.058509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21861</th>\n",
       "      <td>26571</td>\n",
       "      <td>ENSG00000273415</td>\n",
       "      <td>LINC02725</td>\n",
       "      <td>87913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21862</th>\n",
       "      <td>26591</td>\n",
       "      <td>ENSG00000273492</td>\n",
       "      <td>APP-DT</td>\n",
       "      <td>46510</td>\n",
       "      <td>0.417913</td>\n",
       "      <td>0.416324</td>\n",
       "      <td>0.716177</td>\n",
       "      <td>0.516058</td>\n",
       "      <td>0.388106</td>\n",
       "      <td>0.294017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709995</td>\n",
       "      <td>0.362869</td>\n",
       "      <td>0.411909</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.402350</td>\n",
       "      <td>0.102132</td>\n",
       "      <td>0.890978</td>\n",
       "      <td>0.781096</td>\n",
       "      <td>0.642094</td>\n",
       "      <td>0.332128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21863 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0             ENSG hgnc_symbol  gene_length    C_0002  \\\n",
       "0               1  ENSG00000000003      TSPAN6        12883  2.703162   \n",
       "1               2  ENSG00000000005        TNMD        14949  0.005418   \n",
       "2               3  ENSG00000000419        DPM1        24273  2.625862   \n",
       "3               4  ENSG00000000457       SCYL3        44636  0.896318   \n",
       "4               5  ENSG00000000460    C1orf112       192073  0.033732   \n",
       "...           ...              ...         ...          ...       ...   \n",
       "21858       26560  ENSG00000273372   SFTPD-AS1         5000  0.680299   \n",
       "21859       26565  ENSG00000273396   LINC01396         6639  0.036596   \n",
       "21860       26570  ENSG00000273409   LINC02712        66004  0.019632   \n",
       "21861       26571  ENSG00000273415   LINC02725        87913  0.000000   \n",
       "21862       26591  ENSG00000273492      APP-DT        46510  0.417913   \n",
       "\n",
       "         C_0003    C_0004    C_0005    C_0006    C_0008  ...    H_0740  \\\n",
       "0      4.679815  3.827170  3.940771  3.956139  4.749555  ...  4.799810   \n",
       "1      0.029438  0.088121  0.036995  0.053272  0.108531  ...  0.024476   \n",
       "2      4.387499  4.450224  5.390720  3.707350  5.356800  ...  4.100151   \n",
       "3      1.025353  1.568379  1.315815  1.617599  1.350063  ...  0.610696   \n",
       "4      0.123724  0.065645  0.074862  0.063574  0.114637  ...  0.030003   \n",
       "...         ...       ...       ...       ...       ...  ...       ...   \n",
       "21858  0.616103  0.715117  1.061834  1.061811  1.437003  ...  0.311009   \n",
       "21859  0.066286  0.226767  0.049981  0.119952  0.139644  ...  0.013778   \n",
       "21860  0.040004  0.068428  0.016758  0.036196  0.017558  ...  0.005544   \n",
       "21861  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "21862  0.416324  0.716177  0.516058  0.388106  0.294017  ...  0.709995   \n",
       "\n",
       "         H_0750    H_0513    H_0601    H_0656    H_0689    H_0709    H_0723  \\\n",
       "0      8.733498  3.666497  4.563176  2.492904  2.949728  3.314066  6.985665   \n",
       "1      0.086370  0.083015  0.078651  0.050749  0.014444  0.028001  0.138079   \n",
       "2      6.470481  5.783705  6.878268  3.167153  1.530000  6.380522  8.912018   \n",
       "3      0.892573  1.098204  1.589228  0.708180  0.416006  0.628301  1.156094   \n",
       "4      0.052817  0.036344  0.091820  0.025015  0.042717  0.034868  0.060181   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21858  0.202893  0.201662  1.018982  0.050577  0.647750  0.000000  0.247696   \n",
       "21859  0.027783  0.000000  0.000000  0.038090  0.000000  0.000000  0.000000   \n",
       "21860  0.027945  0.054055  0.065315  0.015325  0.009814  0.000000  0.012509   \n",
       "21861  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21862  0.362869  0.411909  0.564575  0.402350  0.102132  0.890978  0.781096   \n",
       "\n",
       "         H_1104    H_1105  \n",
       "0      3.947979  3.784485  \n",
       "1      0.031214  0.064583  \n",
       "2      6.420788  9.267549  \n",
       "3      1.317197  2.011547  \n",
       "4      0.072882  0.070371  \n",
       "...         ...       ...  \n",
       "21858  0.559947  0.096546  \n",
       "21859  0.000000  0.000000  \n",
       "21860  0.028278  0.058509  \n",
       "21861  0.000000  0.000000  \n",
       "21862  0.642094  0.332128  \n",
       "\n",
       "[21863 rows x 112 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"tpm_data.csv\", header=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4d0d4a-91a4-47b7-bd12-3e9108421e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.621517048846005e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_pre=data.iloc[:,4:]\n",
    "dataX=dataX_pre.T\n",
    "dataX_min=np.min(dataX.values[dataX.values>0])\n",
    "dataX_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b41363e-3fc9-4f6c-8e7e-7d49dd1c4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 0 into minimized number which cannot affect analysis\n",
    "convert_num = 8.621517048846005e-06\n",
    "dataX[dataX== 0] = convert_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de048b5-7083-489e-a08a-8f201531cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX_copy=dataX.copy()\n",
    "dataX_copy_log=dataX_copy.apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06aa8e6b-f410-4b88-82a6-7146c9a2f2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>2.703162</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>2.625862</td>\n",
       "      <td>0.896318</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>1.520210</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>2.390171</td>\n",
       "      <td>1.079804</td>\n",
       "      <td>1.846578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024701</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>12.989309</td>\n",
       "      <td>2.596021</td>\n",
       "      <td>0.680299</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.019632</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.417913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>4.679815</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>4.387499</td>\n",
       "      <td>1.025353</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>2.417254</td>\n",
       "      <td>0.541779</td>\n",
       "      <td>6.715102</td>\n",
       "      <td>2.762668</td>\n",
       "      <td>5.757188</td>\n",
       "      <td>...</td>\n",
       "      <td>3.380591</td>\n",
       "      <td>0.400795</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>42.426011</td>\n",
       "      <td>2.992243</td>\n",
       "      <td>0.616103</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.416324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>3.827170</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>4.450224</td>\n",
       "      <td>1.568379</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>1.855760</td>\n",
       "      <td>0.840779</td>\n",
       "      <td>4.118158</td>\n",
       "      <td>1.709962</td>\n",
       "      <td>3.075835</td>\n",
       "      <td>...</td>\n",
       "      <td>1.955870</td>\n",
       "      <td>0.171392</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>24.410100</td>\n",
       "      <td>2.010758</td>\n",
       "      <td>0.715117</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.716177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>3.940771</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>5.390720</td>\n",
       "      <td>1.315815</td>\n",
       "      <td>0.074862</td>\n",
       "      <td>1.803517</td>\n",
       "      <td>1.254086</td>\n",
       "      <td>5.239934</td>\n",
       "      <td>2.649874</td>\n",
       "      <td>4.998753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698587</td>\n",
       "      <td>0.201471</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.525642</td>\n",
       "      <td>3.975217</td>\n",
       "      <td>1.061834</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.516058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>3.956139</td>\n",
       "      <td>0.053272</td>\n",
       "      <td>3.707350</td>\n",
       "      <td>1.617599</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>1.354761</td>\n",
       "      <td>0.569268</td>\n",
       "      <td>4.474453</td>\n",
       "      <td>2.367638</td>\n",
       "      <td>4.753598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719709</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>33.152522</td>\n",
       "      <td>1.418155</td>\n",
       "      <td>1.061811</td>\n",
       "      <td>0.119952</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.388106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>2.949728</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.416006</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>4.108964</td>\n",
       "      <td>1.871441</td>\n",
       "      <td>2.094295</td>\n",
       "      <td>0.747223</td>\n",
       "      <td>2.215735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.196645</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>9.461733</td>\n",
       "      <td>2.411892</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.102132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>3.314066</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>6.380522</td>\n",
       "      <td>0.628301</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>1.158647</td>\n",
       "      <td>0.860249</td>\n",
       "      <td>3.787714</td>\n",
       "      <td>1.060191</td>\n",
       "      <td>1.706804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945729</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>31.182523</td>\n",
       "      <td>3.252682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.890978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>6.985665</td>\n",
       "      <td>0.138079</td>\n",
       "      <td>8.912018</td>\n",
       "      <td>1.156094</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>1.946203</td>\n",
       "      <td>1.799320</td>\n",
       "      <td>4.370479</td>\n",
       "      <td>1.842778</td>\n",
       "      <td>3.689338</td>\n",
       "      <td>...</td>\n",
       "      <td>2.425103</td>\n",
       "      <td>1.503924</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>57.708965</td>\n",
       "      <td>6.215465</td>\n",
       "      <td>0.247696</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.781096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>3.947979</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>6.420788</td>\n",
       "      <td>1.317197</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>2.361264</td>\n",
       "      <td>1.357402</td>\n",
       "      <td>3.256530</td>\n",
       "      <td>1.497821</td>\n",
       "      <td>3.361442</td>\n",
       "      <td>...</td>\n",
       "      <td>2.319405</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.742013</td>\n",
       "      <td>3.172758</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.642094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>3.784485</td>\n",
       "      <td>0.064583</td>\n",
       "      <td>9.267549</td>\n",
       "      <td>2.011547</td>\n",
       "      <td>0.070371</td>\n",
       "      <td>3.758100</td>\n",
       "      <td>0.637426</td>\n",
       "      <td>7.137567</td>\n",
       "      <td>1.807773</td>\n",
       "      <td>3.100193</td>\n",
       "      <td>...</td>\n",
       "      <td>5.671453</td>\n",
       "      <td>0.439643</td>\n",
       "      <td>0.021656</td>\n",
       "      <td>57.538120</td>\n",
       "      <td>5.157850</td>\n",
       "      <td>0.096546</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.058509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.332128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21862 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002  2.703162  0.005418  2.625862  0.896318  0.033732  1.520210  0.668186   \n",
       "C_0003  4.679815  0.029438  4.387499  1.025353  0.123724  2.417254  0.541779   \n",
       "C_0004  3.827170  0.088121  4.450224  1.568379  0.065645  1.855760  0.840779   \n",
       "C_0005  3.940771  0.036995  5.390720  1.315815  0.074862  1.803517  1.254086   \n",
       "C_0006  3.956139  0.053272  3.707350  1.617599  0.063574  1.354761  0.569268   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689  2.949728  0.014444  1.530000  0.416006  0.042717  4.108964  1.871441   \n",
       "H_0709  3.314066  0.028001  6.380522  0.628301  0.034868  1.158647  0.860249   \n",
       "H_0723  6.985665  0.138079  8.912018  1.156094  0.060181  1.946203  1.799320   \n",
       "H_1104  3.947979  0.031214  6.420788  1.317197  0.072882  2.361264  1.357402   \n",
       "H_1105  3.784485  0.064583  9.267549  2.011547  0.070371  3.758100  0.637426   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002  2.390171  1.079804  1.846578  ...  1.024701  0.000009  0.007266   \n",
       "C_0003  6.715102  2.762668  5.757188  ...  3.380591  0.400795  0.000009   \n",
       "C_0004  4.118158  1.709962  3.075835  ...  1.955870  0.171392  0.008442   \n",
       "C_0005  5.239934  2.649874  4.998753  ...  3.698587  0.201471  0.000009   \n",
       "C_0006  4.474453  2.367638  4.753598  ...  0.719709  0.000009  0.000009   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689  2.094295  0.747223  2.215735  ...  0.487837  0.196645  0.009686   \n",
       "H_0709  3.787714  1.060191  1.706804  ...  0.945729  0.000009  0.000009   \n",
       "H_0723  4.370479  1.842778  3.689338  ...  2.425103  1.503924  0.018520   \n",
       "H_1104  3.256530  1.497821  3.361442  ...  2.319405  0.000009  0.000009   \n",
       "H_1105  7.137567  1.807773  3.100193  ...  5.671453  0.439643  0.021656   \n",
       "\n",
       "            21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002  12.989309  2.596021  0.680299  0.036596  0.019632  0.000009  0.417913  \n",
       "C_0003  42.426011  2.992243  0.616103  0.066286  0.040004  0.000009  0.416324  \n",
       "C_0004  24.410100  2.010758  0.715117  0.226767  0.068428  0.000009  0.716177  \n",
       "C_0005  24.525642  3.975217  1.061834  0.049981  0.016758  0.000009  0.516058  \n",
       "C_0006  33.152522  1.418155  1.061811  0.119952  0.036196  0.000009  0.388106  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0689   9.461733  2.411892  0.647750  0.000009  0.009814  0.000009  0.102132  \n",
       "H_0709  31.182523  3.252682  0.000009  0.000009  0.000009  0.000009  0.890978  \n",
       "H_0723  57.708965  6.215465  0.247696  0.000009  0.012509  0.000009  0.781096  \n",
       "H_1104  24.742013  3.172758  0.559947  0.000009  0.028278  0.000009  0.642094  \n",
       "H_1105  57.538120  5.157850  0.096546  0.000009  0.058509  0.000009  0.332128  \n",
       "\n",
       "[108 rows x 21862 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete a column which has a smallest std (deleting LINC02694)\n",
    "dataX_after_del=dataX_copy.drop(dataX_copy.columns[13609],axis=1)\n",
    "dataX_after_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc3a7ab-76e6-4b05-aea9-0d6f7cfb7ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>2.703162</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>2.625862</td>\n",
       "      <td>0.896318</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>1.520210</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>2.390171</td>\n",
       "      <td>1.079804</td>\n",
       "      <td>1.846578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024701</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>12.989309</td>\n",
       "      <td>2.596021</td>\n",
       "      <td>0.680299</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.019632</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.417913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>4.679815</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>4.387499</td>\n",
       "      <td>1.025353</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>2.417254</td>\n",
       "      <td>0.541779</td>\n",
       "      <td>6.715102</td>\n",
       "      <td>2.762668</td>\n",
       "      <td>5.757188</td>\n",
       "      <td>...</td>\n",
       "      <td>3.380591</td>\n",
       "      <td>0.400795</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>42.426011</td>\n",
       "      <td>2.992243</td>\n",
       "      <td>0.616103</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.416324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>3.827170</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>4.450224</td>\n",
       "      <td>1.568379</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>1.855760</td>\n",
       "      <td>0.840779</td>\n",
       "      <td>4.118158</td>\n",
       "      <td>1.709962</td>\n",
       "      <td>3.075835</td>\n",
       "      <td>...</td>\n",
       "      <td>1.955870</td>\n",
       "      <td>0.171392</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>24.410100</td>\n",
       "      <td>2.010758</td>\n",
       "      <td>0.715117</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.716177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>3.940771</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>5.390720</td>\n",
       "      <td>1.315815</td>\n",
       "      <td>0.074862</td>\n",
       "      <td>1.803517</td>\n",
       "      <td>1.254086</td>\n",
       "      <td>5.239934</td>\n",
       "      <td>2.649874</td>\n",
       "      <td>4.998753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698587</td>\n",
       "      <td>0.201471</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.525642</td>\n",
       "      <td>3.975217</td>\n",
       "      <td>1.061834</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.516058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>3.956139</td>\n",
       "      <td>0.053272</td>\n",
       "      <td>3.707350</td>\n",
       "      <td>1.617599</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>1.354761</td>\n",
       "      <td>0.569268</td>\n",
       "      <td>4.474453</td>\n",
       "      <td>2.367638</td>\n",
       "      <td>4.753598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719709</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>33.152522</td>\n",
       "      <td>1.418155</td>\n",
       "      <td>1.061811</td>\n",
       "      <td>0.119952</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.388106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>2.949728</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.416006</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>4.108964</td>\n",
       "      <td>1.871441</td>\n",
       "      <td>2.094295</td>\n",
       "      <td>0.747223</td>\n",
       "      <td>2.215735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.196645</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>9.461733</td>\n",
       "      <td>2.411892</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.102132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>3.314066</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>6.380522</td>\n",
       "      <td>0.628301</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>1.158647</td>\n",
       "      <td>0.860249</td>\n",
       "      <td>3.787714</td>\n",
       "      <td>1.060191</td>\n",
       "      <td>1.706804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945729</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>31.182523</td>\n",
       "      <td>3.252682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.890978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>6.985665</td>\n",
       "      <td>0.138079</td>\n",
       "      <td>8.912018</td>\n",
       "      <td>1.156094</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>1.946203</td>\n",
       "      <td>1.799320</td>\n",
       "      <td>4.370479</td>\n",
       "      <td>1.842778</td>\n",
       "      <td>3.689338</td>\n",
       "      <td>...</td>\n",
       "      <td>2.425103</td>\n",
       "      <td>1.503924</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>57.708965</td>\n",
       "      <td>6.215465</td>\n",
       "      <td>0.247696</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.781096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>3.947979</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>6.420788</td>\n",
       "      <td>1.317197</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>2.361264</td>\n",
       "      <td>1.357402</td>\n",
       "      <td>3.256530</td>\n",
       "      <td>1.497821</td>\n",
       "      <td>3.361442</td>\n",
       "      <td>...</td>\n",
       "      <td>2.319405</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.742013</td>\n",
       "      <td>3.172758</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.642094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>3.784485</td>\n",
       "      <td>0.064583</td>\n",
       "      <td>9.267549</td>\n",
       "      <td>2.011547</td>\n",
       "      <td>0.070371</td>\n",
       "      <td>3.758100</td>\n",
       "      <td>0.637426</td>\n",
       "      <td>7.137567</td>\n",
       "      <td>1.807773</td>\n",
       "      <td>3.100193</td>\n",
       "      <td>...</td>\n",
       "      <td>5.671453</td>\n",
       "      <td>0.439643</td>\n",
       "      <td>0.021656</td>\n",
       "      <td>57.538120</td>\n",
       "      <td>5.157850</td>\n",
       "      <td>0.096546</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.058509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.332128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002  2.703162  0.005418  2.625862  0.896318  0.033732  1.520210  0.668186   \n",
       "C_0003  4.679815  0.029438  4.387499  1.025353  0.123724  2.417254  0.541779   \n",
       "C_0004  3.827170  0.088121  4.450224  1.568379  0.065645  1.855760  0.840779   \n",
       "C_0005  3.940771  0.036995  5.390720  1.315815  0.074862  1.803517  1.254086   \n",
       "C_0006  3.956139  0.053272  3.707350  1.617599  0.063574  1.354761  0.569268   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689  2.949728  0.014444  1.530000  0.416006  0.042717  4.108964  1.871441   \n",
       "H_0709  3.314066  0.028001  6.380522  0.628301  0.034868  1.158647  0.860249   \n",
       "H_0723  6.985665  0.138079  8.912018  1.156094  0.060181  1.946203  1.799320   \n",
       "H_1104  3.947979  0.031214  6.420788  1.317197  0.072882  2.361264  1.357402   \n",
       "H_1105  3.784485  0.064583  9.267549  2.011547  0.070371  3.758100  0.637426   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002  2.390171  1.079804  1.846578  ...  1.024701  0.000009  0.007266   \n",
       "C_0003  6.715102  2.762668  5.757188  ...  3.380591  0.400795  0.000009   \n",
       "C_0004  4.118158  1.709962  3.075835  ...  1.955870  0.171392  0.008442   \n",
       "C_0005  5.239934  2.649874  4.998753  ...  3.698587  0.201471  0.000009   \n",
       "C_0006  4.474453  2.367638  4.753598  ...  0.719709  0.000009  0.000009   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689  2.094295  0.747223  2.215735  ...  0.487837  0.196645  0.009686   \n",
       "H_0709  3.787714  1.060191  1.706804  ...  0.945729  0.000009  0.000009   \n",
       "H_0723  4.370479  1.842778  3.689338  ...  2.425103  1.503924  0.018520   \n",
       "H_1104  3.256530  1.497821  3.361442  ...  2.319405  0.000009  0.000009   \n",
       "H_1105  7.137567  1.807773  3.100193  ...  5.671453  0.439643  0.021656   \n",
       "\n",
       "            21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002  12.989309  2.596021  0.680299  0.036596  0.019632  0.000009  0.417913  \n",
       "C_0003  42.426011  2.992243  0.616103  0.066286  0.040004  0.000009  0.416324  \n",
       "C_0004  24.410100  2.010758  0.715117  0.226767  0.068428  0.000009  0.716177  \n",
       "C_0005  24.525642  3.975217  1.061834  0.049981  0.016758  0.000009  0.516058  \n",
       "C_0006  33.152522  1.418155  1.061811  0.119952  0.036196  0.000009  0.388106  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0689   9.461733  2.411892  0.647750  0.000009  0.009814  0.000009  0.102132  \n",
       "H_0709  31.182523  3.252682  0.000009  0.000009  0.000009  0.000009  0.890978  \n",
       "H_0723  57.708965  6.215465  0.247696  0.000009  0.012509  0.000009  0.781096  \n",
       "H_1104  24.742013  3.172758  0.559947  0.000009  0.028278  0.000009  0.642094  \n",
       "H_1105  57.538120  5.157850  0.096546  0.000009  0.058509  0.000009  0.332128  \n",
       "\n",
       "[108 rows x 21861 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete a column which has a smallest std\n",
    "dataX_after_del2=dataX_after_del.drop(dataX_copy.columns[19081],axis=1)\n",
    "dataX_after_del2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd76653-736d-438d-a369-92836102a120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>0.994422</td>\n",
       "      <td>-5.218099</td>\n",
       "      <td>0.965409</td>\n",
       "      <td>-0.109460</td>\n",
       "      <td>-3.389303</td>\n",
       "      <td>0.418849</td>\n",
       "      <td>-0.403189</td>\n",
       "      <td>0.871365</td>\n",
       "      <td>0.076779</td>\n",
       "      <td>0.613334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-4.924490</td>\n",
       "      <td>2.564127</td>\n",
       "      <td>0.953980</td>\n",
       "      <td>-0.385223</td>\n",
       "      <td>-3.307803</td>\n",
       "      <td>-3.930581</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.872483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>1.543259</td>\n",
       "      <td>-3.525458</td>\n",
       "      <td>1.478759</td>\n",
       "      <td>0.025037</td>\n",
       "      <td>-2.089705</td>\n",
       "      <td>0.882632</td>\n",
       "      <td>-0.612896</td>\n",
       "      <td>1.904359</td>\n",
       "      <td>1.016197</td>\n",
       "      <td>1.750449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.218051</td>\n",
       "      <td>-0.914304</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.747762</td>\n",
       "      <td>1.096023</td>\n",
       "      <td>-0.484341</td>\n",
       "      <td>-2.713775</td>\n",
       "      <td>-3.218770</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.876291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>1.342126</td>\n",
       "      <td>-2.429044</td>\n",
       "      <td>1.492955</td>\n",
       "      <td>0.450043</td>\n",
       "      <td>-2.723493</td>\n",
       "      <td>0.618294</td>\n",
       "      <td>-0.173427</td>\n",
       "      <td>1.415406</td>\n",
       "      <td>0.536471</td>\n",
       "      <td>1.123576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670835</td>\n",
       "      <td>-1.763801</td>\n",
       "      <td>-4.774493</td>\n",
       "      <td>3.194997</td>\n",
       "      <td>0.698512</td>\n",
       "      <td>-0.335309</td>\n",
       "      <td>-1.483830</td>\n",
       "      <td>-2.681972</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.333828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>1.371376</td>\n",
       "      <td>-3.296972</td>\n",
       "      <td>1.684679</td>\n",
       "      <td>0.274456</td>\n",
       "      <td>-2.592106</td>\n",
       "      <td>0.589739</td>\n",
       "      <td>0.226407</td>\n",
       "      <td>1.656309</td>\n",
       "      <td>0.974512</td>\n",
       "      <td>1.609188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.307951</td>\n",
       "      <td>-1.602108</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.199719</td>\n",
       "      <td>1.380079</td>\n",
       "      <td>0.059998</td>\n",
       "      <td>-2.996114</td>\n",
       "      <td>-4.088895</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.661535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>1.375269</td>\n",
       "      <td>-2.932350</td>\n",
       "      <td>1.310317</td>\n",
       "      <td>0.480943</td>\n",
       "      <td>-2.755552</td>\n",
       "      <td>0.303625</td>\n",
       "      <td>-0.563404</td>\n",
       "      <td>1.498384</td>\n",
       "      <td>0.861893</td>\n",
       "      <td>1.558902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328908</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.501119</td>\n",
       "      <td>0.349357</td>\n",
       "      <td>0.059976</td>\n",
       "      <td>-2.120667</td>\n",
       "      <td>-3.318809</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.946478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>1.081713</td>\n",
       "      <td>-4.237507</td>\n",
       "      <td>0.425267</td>\n",
       "      <td>-0.877056</td>\n",
       "      <td>-3.153152</td>\n",
       "      <td>1.413171</td>\n",
       "      <td>0.626709</td>\n",
       "      <td>0.739217</td>\n",
       "      <td>-0.291392</td>\n",
       "      <td>0.795584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717774</td>\n",
       "      <td>-1.626353</td>\n",
       "      <td>-4.637045</td>\n",
       "      <td>2.247256</td>\n",
       "      <td>0.880411</td>\n",
       "      <td>-0.434250</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-4.623965</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-2.281487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>1.198176</td>\n",
       "      <td>-3.575533</td>\n",
       "      <td>1.853250</td>\n",
       "      <td>-0.464737</td>\n",
       "      <td>-3.356175</td>\n",
       "      <td>0.147253</td>\n",
       "      <td>-0.150534</td>\n",
       "      <td>1.331763</td>\n",
       "      <td>0.058449</td>\n",
       "      <td>0.534623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055800</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.439858</td>\n",
       "      <td>1.179480</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.115436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>1.943860</td>\n",
       "      <td>-1.979933</td>\n",
       "      <td>2.187401</td>\n",
       "      <td>0.145047</td>\n",
       "      <td>-2.810397</td>\n",
       "      <td>0.665880</td>\n",
       "      <td>0.587409</td>\n",
       "      <td>1.474873</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>1.305447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885874</td>\n",
       "      <td>0.408078</td>\n",
       "      <td>-3.988909</td>\n",
       "      <td>4.055413</td>\n",
       "      <td>1.827040</td>\n",
       "      <td>-1.395552</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-4.381294</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.247057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>1.373204</td>\n",
       "      <td>-3.466880</td>\n",
       "      <td>1.859541</td>\n",
       "      <td>0.275506</td>\n",
       "      <td>-2.618914</td>\n",
       "      <td>0.859197</td>\n",
       "      <td>0.305573</td>\n",
       "      <td>1.180662</td>\n",
       "      <td>0.404012</td>\n",
       "      <td>1.212370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841311</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.208503</td>\n",
       "      <td>1.154601</td>\n",
       "      <td>-0.579914</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-3.565657</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.443020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>1.330910</td>\n",
       "      <td>-2.739799</td>\n",
       "      <td>2.226519</td>\n",
       "      <td>0.698904</td>\n",
       "      <td>-2.653973</td>\n",
       "      <td>1.323913</td>\n",
       "      <td>-0.450317</td>\n",
       "      <td>1.965372</td>\n",
       "      <td>0.592096</td>\n",
       "      <td>1.131464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.735445</td>\n",
       "      <td>-0.821792</td>\n",
       "      <td>-3.832485</td>\n",
       "      <td>4.052448</td>\n",
       "      <td>1.640520</td>\n",
       "      <td>-2.337740</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-2.838576</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-1.102234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002  0.994422 -5.218099  0.965409 -0.109460 -3.389303  0.418849 -0.403189   \n",
       "C_0003  1.543259 -3.525458  1.478759  0.025037 -2.089705  0.882632 -0.612896   \n",
       "C_0004  1.342126 -2.429044  1.492955  0.450043 -2.723493  0.618294 -0.173427   \n",
       "C_0005  1.371376 -3.296972  1.684679  0.274456 -2.592106  0.589739  0.226407   \n",
       "C_0006  1.375269 -2.932350  1.310317  0.480943 -2.755552  0.303625 -0.563404   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689  1.081713 -4.237507  0.425267 -0.877056 -3.153152  1.413171  0.626709   \n",
       "H_0709  1.198176 -3.575533  1.853250 -0.464737 -3.356175  0.147253 -0.150534   \n",
       "H_0723  1.943860 -1.979933  2.187401  0.145047 -2.810397  0.665880  0.587409   \n",
       "H_1104  1.373204 -3.466880  1.859541  0.275506 -2.618914  0.859197  0.305573   \n",
       "H_1105  1.330910 -2.739799  2.226519  0.698904 -2.653973  1.323913 -0.450317   \n",
       "\n",
       "           7         8         9      ...     21853      21854      21855  \\\n",
       "C_0002  0.871365  0.076779  0.613334  ...  0.024401 -11.661249  -4.924490   \n",
       "C_0003  1.904359  1.016197  1.750449  ...  1.218051  -0.914304 -11.661249   \n",
       "C_0004  1.415406  0.536471  1.123576  ...  0.670835  -1.763801  -4.774493   \n",
       "C_0005  1.656309  0.974512  1.609188  ...  1.307951  -1.602108 -11.661249   \n",
       "C_0006  1.498384  0.861893  1.558902  ... -0.328908 -11.661249 -11.661249   \n",
       "...          ...       ...       ...  ...       ...        ...        ...   \n",
       "H_0689  0.739217 -0.291392  0.795584  ... -0.717774  -1.626353  -4.637045   \n",
       "H_0709  1.331763  0.058449  0.534623  ... -0.055800 -11.661249 -11.661249   \n",
       "H_0723  1.474873  0.611274  1.305447  ...  0.885874   0.408078  -3.988909   \n",
       "H_1104  1.180662  0.404012  1.212370  ...  0.841311 -11.661249 -11.661249   \n",
       "H_1105  1.965372  0.592096  1.131464  ...  1.735445  -0.821792  -3.832485   \n",
       "\n",
       "           21856     21857      21858      21859      21860      21861  \\\n",
       "C_0002  2.564127  0.953980  -0.385223  -3.307803  -3.930581 -11.661249   \n",
       "C_0003  3.747762  1.096023  -0.484341  -2.713775  -3.218770 -11.661249   \n",
       "C_0004  3.194997  0.698512  -0.335309  -1.483830  -2.681972 -11.661249   \n",
       "C_0005  3.199719  1.380079   0.059998  -2.996114  -4.088895 -11.661249   \n",
       "C_0006  3.501119  0.349357   0.059976  -2.120667  -3.318809 -11.661249   \n",
       "...          ...       ...        ...        ...        ...        ...   \n",
       "H_0689  2.247256  0.880411  -0.434250 -11.661249  -4.623965 -11.661249   \n",
       "H_0709  3.439858  1.179480 -11.661249 -11.661249 -11.661249 -11.661249   \n",
       "H_0723  4.055413  1.827040  -1.395552 -11.661249  -4.381294 -11.661249   \n",
       "H_1104  3.208503  1.154601  -0.579914 -11.661249  -3.565657 -11.661249   \n",
       "H_1105  4.052448  1.640520  -2.337740 -11.661249  -2.838576 -11.661249   \n",
       "\n",
       "           21862  \n",
       "C_0002 -0.872483  \n",
       "C_0003 -0.876291  \n",
       "C_0004 -0.333828  \n",
       "C_0005 -0.661535  \n",
       "C_0006 -0.946478  \n",
       "...          ...  \n",
       "H_0689 -2.281487  \n",
       "H_0709 -0.115436  \n",
       "H_0723 -0.247057  \n",
       "H_1104 -0.443020  \n",
       "H_1105 -1.102234  \n",
       "\n",
       "[108 rows x 21861 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#log conversion of Dataframe\n",
    "dataX_log_del2=dataX_after_del2.apply(np.log)\n",
    "dataX_log_del2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86feb26-3de8-46da-b22f-a7f3db9e1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresToScale=dataX_log_del2.columns\n",
    "sX=pp.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "dataX_log_del2.loc[:, featuresToScale]=sX.fit_transform(dataX_log_del2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d441e13-f7bc-44a8-a428-195cc4360c7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-0.489755</td>\n",
       "      <td>-1.404819</td>\n",
       "      <td>-0.661768</td>\n",
       "      <td>-1.214104</td>\n",
       "      <td>-0.438326</td>\n",
       "      <td>-0.542205</td>\n",
       "      <td>-1.549951</td>\n",
       "      <td>-1.144974</td>\n",
       "      <td>-1.361823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.737597</td>\n",
       "      <td>-1.819190</td>\n",
       "      <td>-0.178338</td>\n",
       "      <td>0.264969</td>\n",
       "      <td>0.721004</td>\n",
       "      <td>-0.094753</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.142859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>-0.094326</td>\n",
       "      <td>0.128294</td>\n",
       "      <td>-0.400562</td>\n",
       "      <td>-0.392906</td>\n",
       "      <td>0.908731</td>\n",
       "      <td>0.454217</td>\n",
       "      <td>-0.792399</td>\n",
       "      <td>0.733360</td>\n",
       "      <td>0.611638</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767006</td>\n",
       "      <td>0.814752</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.719449</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.220818</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.298774</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.149023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>-0.513695</td>\n",
       "      <td>0.528638</td>\n",
       "      <td>-0.372793</td>\n",
       "      <td>0.456696</td>\n",
       "      <td>-0.126533</td>\n",
       "      <td>-0.054497</td>\n",
       "      <td>-0.268084</td>\n",
       "      <td>-0.347413</td>\n",
       "      <td>-0.285398</td>\n",
       "      <td>-0.322790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051075</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.776952</td>\n",
       "      <td>-0.466111</td>\n",
       "      <td>-0.596202</td>\n",
       "      <td>0.287203</td>\n",
       "      <td>1.144874</td>\n",
       "      <td>0.595544</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.728854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>-0.452707</td>\n",
       "      <td>0.211724</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.088081</td>\n",
       "      <td>-0.109452</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.533692</td>\n",
       "      <td>0.666086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884624</td>\n",
       "      <td>0.681302</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.455983</td>\n",
       "      <td>0.518626</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.793437</td>\n",
       "      <td>-0.182278</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.198520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>-0.444591</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>-0.730082</td>\n",
       "      <td>0.518467</td>\n",
       "      <td>-0.178900</td>\n",
       "      <td>-0.660072</td>\n",
       "      <td>-0.733352</td>\n",
       "      <td>-0.164000</td>\n",
       "      <td>0.323106</td>\n",
       "      <td>0.563685</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.256904</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.190454</td>\n",
       "      <td>-1.167309</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.996881</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.262606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0008</th>\n",
       "      <td>-0.063484</td>\n",
       "      <td>0.604704</td>\n",
       "      <td>-0.010074</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.784126</td>\n",
       "      <td>0.523687</td>\n",
       "      <td>0.705062</td>\n",
       "      <td>0.107073</td>\n",
       "      <td>0.650657</td>\n",
       "      <td>0.591910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>0.690349</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>0.357423</td>\n",
       "      <td>0.598058</td>\n",
       "      <td>1.032206</td>\n",
       "      <td>-0.156499</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.711918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0009</th>\n",
       "      <td>-1.014420</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>0.485191</td>\n",
       "      <td>1.511839</td>\n",
       "      <td>2.342554</td>\n",
       "      <td>-0.059718</td>\n",
       "      <td>-0.901242</td>\n",
       "      <td>2.952803</td>\n",
       "      <td>3.037081</td>\n",
       "      <td>1.972919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056384</td>\n",
       "      <td>1.073685</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>2.372708</td>\n",
       "      <td>1.781837</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>1.391432</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-3.818267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0010</th>\n",
       "      <td>-1.071431</td>\n",
       "      <td>0.484398</td>\n",
       "      <td>-0.933368</td>\n",
       "      <td>-0.658817</td>\n",
       "      <td>-0.998986</td>\n",
       "      <td>-0.943378</td>\n",
       "      <td>-1.362364</td>\n",
       "      <td>-0.970939</td>\n",
       "      <td>-0.853908</td>\n",
       "      <td>-0.605820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.555258</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.785608</td>\n",
       "      <td>-0.303165</td>\n",
       "      <td>0.070641</td>\n",
       "      <td>0.277815</td>\n",
       "      <td>0.669304</td>\n",
       "      <td>0.071529</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.462936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0011</th>\n",
       "      <td>-0.921800</td>\n",
       "      <td>0.182561</td>\n",
       "      <td>-2.304816</td>\n",
       "      <td>-1.383902</td>\n",
       "      <td>-1.199928</td>\n",
       "      <td>-0.621190</td>\n",
       "      <td>-1.538195</td>\n",
       "      <td>-1.155816</td>\n",
       "      <td>-0.558486</td>\n",
       "      <td>-0.865617</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.144206</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.399549</td>\n",
       "      <td>-1.752028</td>\n",
       "      <td>-0.121130</td>\n",
       "      <td>0.774877</td>\n",
       "      <td>0.128418</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.916628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0012</th>\n",
       "      <td>0.370489</td>\n",
       "      <td>0.568469</td>\n",
       "      <td>0.573826</td>\n",
       "      <td>1.847973</td>\n",
       "      <td>2.247641</td>\n",
       "      <td>-1.047992</td>\n",
       "      <td>-1.840582</td>\n",
       "      <td>2.485153</td>\n",
       "      <td>2.363211</td>\n",
       "      <td>2.430353</td>\n",
       "      <td>...</td>\n",
       "      <td>1.381163</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.987434</td>\n",
       "      <td>1.425060</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>0.766789</td>\n",
       "      <td>0.848066</td>\n",
       "      <td>0.465206</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.968155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0013</th>\n",
       "      <td>0.237404</td>\n",
       "      <td>0.468118</td>\n",
       "      <td>-0.328469</td>\n",
       "      <td>0.401016</td>\n",
       "      <td>0.664377</td>\n",
       "      <td>-0.110870</td>\n",
       "      <td>1.175788</td>\n",
       "      <td>0.527173</td>\n",
       "      <td>0.230430</td>\n",
       "      <td>0.598487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245687</td>\n",
       "      <td>0.782167</td>\n",
       "      <td>0.773910</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.823008</td>\n",
       "      <td>0.794247</td>\n",
       "      <td>0.658943</td>\n",
       "      <td>0.046881</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.089385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0014</th>\n",
       "      <td>-0.043427</td>\n",
       "      <td>0.164883</td>\n",
       "      <td>-0.241251</td>\n",
       "      <td>-0.231408</td>\n",
       "      <td>-0.153178</td>\n",
       "      <td>0.290115</td>\n",
       "      <td>-0.123583</td>\n",
       "      <td>-0.380659</td>\n",
       "      <td>-0.117571</td>\n",
       "      <td>-0.196329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203878</td>\n",
       "      <td>0.699708</td>\n",
       "      <td>1.132509</td>\n",
       "      <td>-0.447637</td>\n",
       "      <td>-0.504793</td>\n",
       "      <td>0.158031</td>\n",
       "      <td>0.882336</td>\n",
       "      <td>0.306069</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.752933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0015</th>\n",
       "      <td>-0.159249</td>\n",
       "      <td>0.653677</td>\n",
       "      <td>0.067769</td>\n",
       "      <td>0.653778</td>\n",
       "      <td>-0.451082</td>\n",
       "      <td>-1.435228</td>\n",
       "      <td>-1.653990</td>\n",
       "      <td>-0.421961</td>\n",
       "      <td>-0.142727</td>\n",
       "      <td>0.361802</td>\n",
       "      <td>...</td>\n",
       "      <td>1.742628</td>\n",
       "      <td>0.581885</td>\n",
       "      <td>0.973181</td>\n",
       "      <td>-0.168088</td>\n",
       "      <td>-0.960749</td>\n",
       "      <td>0.403107</td>\n",
       "      <td>1.090746</td>\n",
       "      <td>-0.082349</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.371974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0016</th>\n",
       "      <td>-0.313939</td>\n",
       "      <td>0.505943</td>\n",
       "      <td>-0.474605</td>\n",
       "      <td>0.395045</td>\n",
       "      <td>-0.454325</td>\n",
       "      <td>0.768875</td>\n",
       "      <td>1.061524</td>\n",
       "      <td>-0.027705</td>\n",
       "      <td>-0.504156</td>\n",
       "      <td>0.133814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959270</td>\n",
       "      <td>0.802266</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.934674</td>\n",
       "      <td>-0.521819</td>\n",
       "      <td>0.304097</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.469263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0017</th>\n",
       "      <td>0.617328</td>\n",
       "      <td>-2.842400</td>\n",
       "      <td>-0.042863</td>\n",
       "      <td>0.876221</td>\n",
       "      <td>0.948050</td>\n",
       "      <td>-0.556515</td>\n",
       "      <td>-0.586851</td>\n",
       "      <td>1.194970</td>\n",
       "      <td>1.516010</td>\n",
       "      <td>1.081449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927239</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.039801</td>\n",
       "      <td>0.731721</td>\n",
       "      <td>0.243670</td>\n",
       "      <td>0.670608</td>\n",
       "      <td>0.733369</td>\n",
       "      <td>0.427240</td>\n",
       "      <td>2.775695</td>\n",
       "      <td>-0.883431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0018</th>\n",
       "      <td>-0.841823</td>\n",
       "      <td>-2.842400</td>\n",
       "      <td>0.611775</td>\n",
       "      <td>2.342445</td>\n",
       "      <td>2.457968</td>\n",
       "      <td>-1.436767</td>\n",
       "      <td>-1.273134</td>\n",
       "      <td>1.043109</td>\n",
       "      <td>0.310334</td>\n",
       "      <td>2.145601</td>\n",
       "      <td>...</td>\n",
       "      <td>2.039750</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.883744</td>\n",
       "      <td>2.903385</td>\n",
       "      <td>-0.352164</td>\n",
       "      <td>0.753175</td>\n",
       "      <td>0.917304</td>\n",
       "      <td>-0.170010</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.633091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0019</th>\n",
       "      <td>-0.536535</td>\n",
       "      <td>0.282387</td>\n",
       "      <td>-0.669082</td>\n",
       "      <td>-0.261382</td>\n",
       "      <td>-0.587647</td>\n",
       "      <td>-0.113586</td>\n",
       "      <td>0.768471</td>\n",
       "      <td>-0.019680</td>\n",
       "      <td>-0.749318</td>\n",
       "      <td>0.085651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332958</td>\n",
       "      <td>0.683476</td>\n",
       "      <td>0.928697</td>\n",
       "      <td>-1.103343</td>\n",
       "      <td>-0.071915</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.796041</td>\n",
       "      <td>-0.234333</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.359326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0020</th>\n",
       "      <td>-1.286744</td>\n",
       "      <td>0.683392</td>\n",
       "      <td>-0.230312</td>\n",
       "      <td>0.520323</td>\n",
       "      <td>-0.197100</td>\n",
       "      <td>-1.564600</td>\n",
       "      <td>-1.281168</td>\n",
       "      <td>-0.488723</td>\n",
       "      <td>-0.260558</td>\n",
       "      <td>0.360519</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246719</td>\n",
       "      <td>0.644466</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.094803</td>\n",
       "      <td>-0.970501</td>\n",
       "      <td>0.396902</td>\n",
       "      <td>0.910396</td>\n",
       "      <td>0.320131</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.042961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0021</th>\n",
       "      <td>0.526197</td>\n",
       "      <td>-2.842400</td>\n",
       "      <td>-0.390967</td>\n",
       "      <td>0.445794</td>\n",
       "      <td>0.184534</td>\n",
       "      <td>-1.203421</td>\n",
       "      <td>-1.145355</td>\n",
       "      <td>0.687426</td>\n",
       "      <td>0.684889</td>\n",
       "      <td>0.804522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987853</td>\n",
       "      <td>0.836414</td>\n",
       "      <td>1.029127</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>-1.018371</td>\n",
       "      <td>0.330028</td>\n",
       "      <td>0.723915</td>\n",
       "      <td>0.062511</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.227033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0022</th>\n",
       "      <td>-1.362488</td>\n",
       "      <td>0.307889</td>\n",
       "      <td>-0.984365</td>\n",
       "      <td>-1.156695</td>\n",
       "      <td>-1.288870</td>\n",
       "      <td>-0.259354</td>\n",
       "      <td>-0.329153</td>\n",
       "      <td>-1.661139</td>\n",
       "      <td>-1.751085</td>\n",
       "      <td>-1.280175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299670</td>\n",
       "      <td>0.732401</td>\n",
       "      <td>0.706613</td>\n",
       "      <td>-0.743866</td>\n",
       "      <td>-2.689368</td>\n",
       "      <td>0.031758</td>\n",
       "      <td>0.760415</td>\n",
       "      <td>-0.233864</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.712032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0023</th>\n",
       "      <td>-1.226360</td>\n",
       "      <td>-0.380853</td>\n",
       "      <td>-2.068576</td>\n",
       "      <td>-1.459295</td>\n",
       "      <td>-2.173385</td>\n",
       "      <td>-1.454016</td>\n",
       "      <td>-1.745151</td>\n",
       "      <td>-1.424173</td>\n",
       "      <td>-1.924884</td>\n",
       "      <td>-1.365756</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.136616</td>\n",
       "      <td>0.678694</td>\n",
       "      <td>0.997710</td>\n",
       "      <td>-0.693899</td>\n",
       "      <td>-2.348000</td>\n",
       "      <td>-0.091542</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.313074</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.879198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0024</th>\n",
       "      <td>0.218499</td>\n",
       "      <td>0.742546</td>\n",
       "      <td>-0.366158</td>\n",
       "      <td>0.410088</td>\n",
       "      <td>-0.554448</td>\n",
       "      <td>-0.343820</td>\n",
       "      <td>-0.535454</td>\n",
       "      <td>-0.260692</td>\n",
       "      <td>-0.020393</td>\n",
       "      <td>0.194022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.626982</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.048342</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>0.103370</td>\n",
       "      <td>0.875968</td>\n",
       "      <td>0.238226</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.705871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0025</th>\n",
       "      <td>-0.525737</td>\n",
       "      <td>0.242389</td>\n",
       "      <td>0.106966</td>\n",
       "      <td>0.424007</td>\n",
       "      <td>0.286912</td>\n",
       "      <td>-0.878035</td>\n",
       "      <td>-1.127625</td>\n",
       "      <td>0.521370</td>\n",
       "      <td>0.787054</td>\n",
       "      <td>0.509760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186735</td>\n",
       "      <td>0.740892</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.404844</td>\n",
       "      <td>-0.157597</td>\n",
       "      <td>0.652556</td>\n",
       "      <td>1.144599</td>\n",
       "      <td>0.312478</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.480518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0026</th>\n",
       "      <td>-1.102176</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>-1.320566</td>\n",
       "      <td>-0.620084</td>\n",
       "      <td>-1.046110</td>\n",
       "      <td>-0.585962</td>\n",
       "      <td>-0.764696</td>\n",
       "      <td>-0.931632</td>\n",
       "      <td>-0.883488</td>\n",
       "      <td>-0.745137</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.254419</td>\n",
       "      <td>0.617929</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.567240</td>\n",
       "      <td>-1.194215</td>\n",
       "      <td>0.164191</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.037894</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.236674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0029</th>\n",
       "      <td>-1.437548</td>\n",
       "      <td>-0.297698</td>\n",
       "      <td>-1.784748</td>\n",
       "      <td>-1.028572</td>\n",
       "      <td>-1.752626</td>\n",
       "      <td>0.407274</td>\n",
       "      <td>0.008234</td>\n",
       "      <td>-0.784135</td>\n",
       "      <td>-1.550076</td>\n",
       "      <td>-1.090772</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.328883</td>\n",
       "      <td>0.857367</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.160892</td>\n",
       "      <td>-0.607662</td>\n",
       "      <td>-0.239373</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.291046</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.386633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0060</th>\n",
       "      <td>1.016182</td>\n",
       "      <td>0.544289</td>\n",
       "      <td>-0.468397</td>\n",
       "      <td>-0.756485</td>\n",
       "      <td>-0.454883</td>\n",
       "      <td>-0.032562</td>\n",
       "      <td>0.310191</td>\n",
       "      <td>-1.481712</td>\n",
       "      <td>-0.330176</td>\n",
       "      <td>-1.178254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717043</td>\n",
       "      <td>0.766825</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.787432</td>\n",
       "      <td>-1.118817</td>\n",
       "      <td>-0.086020</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.220998</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.530488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0061</th>\n",
       "      <td>-0.473287</td>\n",
       "      <td>0.056637</td>\n",
       "      <td>0.117051</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>0.056609</td>\n",
       "      <td>-0.557902</td>\n",
       "      <td>-0.201168</td>\n",
       "      <td>-0.439586</td>\n",
       "      <td>0.132665</td>\n",
       "      <td>-0.769386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141075</td>\n",
       "      <td>0.875788</td>\n",
       "      <td>1.023826</td>\n",
       "      <td>0.292035</td>\n",
       "      <td>0.637192</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>-0.417092</td>\n",
       "      <td>2.371024</td>\n",
       "      <td>-1.178913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0062</th>\n",
       "      <td>0.745442</td>\n",
       "      <td>0.616147</td>\n",
       "      <td>1.085803</td>\n",
       "      <td>0.833815</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>-0.484330</td>\n",
       "      <td>0.229278</td>\n",
       "      <td>0.747268</td>\n",
       "      <td>0.973920</td>\n",
       "      <td>0.660299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880805</td>\n",
       "      <td>0.883678</td>\n",
       "      <td>0.986659</td>\n",
       "      <td>0.537345</td>\n",
       "      <td>0.138341</td>\n",
       "      <td>0.284933</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.463572</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.086073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0065</th>\n",
       "      <td>4.464034</td>\n",
       "      <td>0.193050</td>\n",
       "      <td>1.630309</td>\n",
       "      <td>1.204508</td>\n",
       "      <td>1.248069</td>\n",
       "      <td>-0.647544</td>\n",
       "      <td>0.895345</td>\n",
       "      <td>2.280033</td>\n",
       "      <td>2.542494</td>\n",
       "      <td>1.493681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648056</td>\n",
       "      <td>1.039465</td>\n",
       "      <td>1.362254</td>\n",
       "      <td>2.228318</td>\n",
       "      <td>1.156093</td>\n",
       "      <td>0.712185</td>\n",
       "      <td>0.806037</td>\n",
       "      <td>1.366535</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.422842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0069</th>\n",
       "      <td>0.348975</td>\n",
       "      <td>0.462294</td>\n",
       "      <td>1.085377</td>\n",
       "      <td>0.647411</td>\n",
       "      <td>-1.216716</td>\n",
       "      <td>-1.487006</td>\n",
       "      <td>-0.768889</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>-0.054809</td>\n",
       "      <td>-0.638935</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.164933</td>\n",
       "      <td>0.779073</td>\n",
       "      <td>1.085613</td>\n",
       "      <td>0.088204</td>\n",
       "      <td>0.281165</td>\n",
       "      <td>-0.110367</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.438022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0071</th>\n",
       "      <td>1.644933</td>\n",
       "      <td>0.098840</td>\n",
       "      <td>0.919072</td>\n",
       "      <td>1.045445</td>\n",
       "      <td>1.664938</td>\n",
       "      <td>-0.048870</td>\n",
       "      <td>0.641201</td>\n",
       "      <td>1.740010</td>\n",
       "      <td>1.594438</td>\n",
       "      <td>1.463288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622412</td>\n",
       "      <td>0.664615</td>\n",
       "      <td>0.796809</td>\n",
       "      <td>1.809818</td>\n",
       "      <td>-0.077944</td>\n",
       "      <td>0.215619</td>\n",
       "      <td>0.934529</td>\n",
       "      <td>0.536589</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.677777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0075</th>\n",
       "      <td>-0.655847</td>\n",
       "      <td>0.101901</td>\n",
       "      <td>-0.216873</td>\n",
       "      <td>-1.024741</td>\n",
       "      <td>-0.981342</td>\n",
       "      <td>0.315110</td>\n",
       "      <td>-0.255794</td>\n",
       "      <td>-0.916104</td>\n",
       "      <td>-0.796130</td>\n",
       "      <td>-1.089311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355271</td>\n",
       "      <td>0.800728</td>\n",
       "      <td>1.039417</td>\n",
       "      <td>-0.704558</td>\n",
       "      <td>-1.319216</td>\n",
       "      <td>-0.153142</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-1.114976</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.983068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0076</th>\n",
       "      <td>-0.310756</td>\n",
       "      <td>0.693401</td>\n",
       "      <td>1.007125</td>\n",
       "      <td>0.964734</td>\n",
       "      <td>-0.202587</td>\n",
       "      <td>-1.844818</td>\n",
       "      <td>-0.896235</td>\n",
       "      <td>-0.697784</td>\n",
       "      <td>0.181026</td>\n",
       "      <td>0.569351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868714</td>\n",
       "      <td>0.862941</td>\n",
       "      <td>0.958617</td>\n",
       "      <td>-0.126775</td>\n",
       "      <td>-1.118319</td>\n",
       "      <td>0.262785</td>\n",
       "      <td>0.916767</td>\n",
       "      <td>0.052875</td>\n",
       "      <td>2.609635</td>\n",
       "      <td>0.057696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0081</th>\n",
       "      <td>0.291229</td>\n",
       "      <td>0.286913</td>\n",
       "      <td>0.724079</td>\n",
       "      <td>0.436671</td>\n",
       "      <td>0.206326</td>\n",
       "      <td>-0.243124</td>\n",
       "      <td>0.625176</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.202261</td>\n",
       "      <td>0.767037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673424</td>\n",
       "      <td>0.630064</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.061468</td>\n",
       "      <td>-1.003477</td>\n",
       "      <td>0.538467</td>\n",
       "      <td>1.011856</td>\n",
       "      <td>0.731500</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.429805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0082</th>\n",
       "      <td>-0.370661</td>\n",
       "      <td>0.739036</td>\n",
       "      <td>1.271797</td>\n",
       "      <td>0.726100</td>\n",
       "      <td>0.766660</td>\n",
       "      <td>-1.076300</td>\n",
       "      <td>-0.679681</td>\n",
       "      <td>1.173660</td>\n",
       "      <td>0.952265</td>\n",
       "      <td>0.059237</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384673</td>\n",
       "      <td>0.901840</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.644238</td>\n",
       "      <td>-0.047370</td>\n",
       "      <td>-0.184204</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>1.046076</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.261577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0083</th>\n",
       "      <td>0.388913</td>\n",
       "      <td>0.232071</td>\n",
       "      <td>1.041605</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.923106</td>\n",
       "      <td>-1.479170</td>\n",
       "      <td>-0.725972</td>\n",
       "      <td>0.561737</td>\n",
       "      <td>1.005341</td>\n",
       "      <td>0.631400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753887</td>\n",
       "      <td>0.656740</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.344238</td>\n",
       "      <td>-0.614887</td>\n",
       "      <td>0.347417</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.130942</td>\n",
       "      <td>3.000866</td>\n",
       "      <td>0.672039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0031</th>\n",
       "      <td>-0.173575</td>\n",
       "      <td>0.559403</td>\n",
       "      <td>1.404774</td>\n",
       "      <td>0.862172</td>\n",
       "      <td>-0.182821</td>\n",
       "      <td>-0.912828</td>\n",
       "      <td>-1.302345</td>\n",
       "      <td>1.019318</td>\n",
       "      <td>0.738750</td>\n",
       "      <td>-0.484735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.864565</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>1.135026</td>\n",
       "      <td>-0.832697</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.263231</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.020141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0032</th>\n",
       "      <td>1.363014</td>\n",
       "      <td>0.852724</td>\n",
       "      <td>1.620287</td>\n",
       "      <td>1.712404</td>\n",
       "      <td>1.276279</td>\n",
       "      <td>0.174185</td>\n",
       "      <td>0.437325</td>\n",
       "      <td>1.204032</td>\n",
       "      <td>1.578419</td>\n",
       "      <td>1.610603</td>\n",
       "      <td>...</td>\n",
       "      <td>1.564273</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.916243</td>\n",
       "      <td>1.594664</td>\n",
       "      <td>-1.017494</td>\n",
       "      <td>0.866281</td>\n",
       "      <td>1.040314</td>\n",
       "      <td>0.346796</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.727455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0033</th>\n",
       "      <td>0.004265</td>\n",
       "      <td>0.846777</td>\n",
       "      <td>1.335345</td>\n",
       "      <td>0.400130</td>\n",
       "      <td>0.469740</td>\n",
       "      <td>-0.250658</td>\n",
       "      <td>-0.528948</td>\n",
       "      <td>0.849011</td>\n",
       "      <td>0.816911</td>\n",
       "      <td>0.121975</td>\n",
       "      <td>...</td>\n",
       "      <td>1.370601</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.024995</td>\n",
       "      <td>0.610686</td>\n",
       "      <td>0.834647</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.510835</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.217325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0034</th>\n",
       "      <td>1.169427</td>\n",
       "      <td>0.556844</td>\n",
       "      <td>0.808415</td>\n",
       "      <td>0.667631</td>\n",
       "      <td>0.581447</td>\n",
       "      <td>0.060103</td>\n",
       "      <td>0.540228</td>\n",
       "      <td>0.516714</td>\n",
       "      <td>0.714247</td>\n",
       "      <td>0.972676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120977</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.938123</td>\n",
       "      <td>-0.844161</td>\n",
       "      <td>0.578631</td>\n",
       "      <td>0.970715</td>\n",
       "      <td>0.564428</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.950002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0035</th>\n",
       "      <td>-0.360567</td>\n",
       "      <td>0.846949</td>\n",
       "      <td>0.979596</td>\n",
       "      <td>0.268736</td>\n",
       "      <td>0.448585</td>\n",
       "      <td>-0.310850</td>\n",
       "      <td>-0.422807</td>\n",
       "      <td>-0.655380</td>\n",
       "      <td>0.036763</td>\n",
       "      <td>-0.277446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262686</td>\n",
       "      <td>0.833449</td>\n",
       "      <td>1.131501</td>\n",
       "      <td>0.188816</td>\n",
       "      <td>-1.644843</td>\n",
       "      <td>0.195078</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.131956</td>\n",
       "      <td>3.609404</td>\n",
       "      <td>1.375492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0036</th>\n",
       "      <td>-0.064681</td>\n",
       "      <td>0.223731</td>\n",
       "      <td>1.326424</td>\n",
       "      <td>1.318422</td>\n",
       "      <td>0.348091</td>\n",
       "      <td>-0.331799</td>\n",
       "      <td>-1.112680</td>\n",
       "      <td>0.753750</td>\n",
       "      <td>0.474937</td>\n",
       "      <td>0.662958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854944</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>1.463038</td>\n",
       "      <td>1.332991</td>\n",
       "      <td>-0.539399</td>\n",
       "      <td>0.473270</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.443274</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.807493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0037</th>\n",
       "      <td>-0.031287</td>\n",
       "      <td>-0.020087</td>\n",
       "      <td>1.275895</td>\n",
       "      <td>0.788813</td>\n",
       "      <td>0.389590</td>\n",
       "      <td>-2.367784</td>\n",
       "      <td>-1.431053</td>\n",
       "      <td>0.742751</td>\n",
       "      <td>0.413208</td>\n",
       "      <td>0.224323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156028</td>\n",
       "      <td>0.870393</td>\n",
       "      <td>0.893215</td>\n",
       "      <td>0.189604</td>\n",
       "      <td>-0.610693</td>\n",
       "      <td>-0.209467</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.945151</td>\n",
       "      <td>2.847851</td>\n",
       "      <td>-0.032311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0038</th>\n",
       "      <td>0.127102</td>\n",
       "      <td>0.735821</td>\n",
       "      <td>0.962326</td>\n",
       "      <td>1.252127</td>\n",
       "      <td>1.027538</td>\n",
       "      <td>-1.468612</td>\n",
       "      <td>-0.540858</td>\n",
       "      <td>1.084620</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>1.476899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902514</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>1.325798</td>\n",
       "      <td>0.474221</td>\n",
       "      <td>0.154646</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.909238</td>\n",
       "      <td>3.778292</td>\n",
       "      <td>1.619436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0039</th>\n",
       "      <td>0.076393</td>\n",
       "      <td>0.369525</td>\n",
       "      <td>0.262198</td>\n",
       "      <td>-0.434926</td>\n",
       "      <td>-0.641064</td>\n",
       "      <td>-0.367527</td>\n",
       "      <td>-0.258453</td>\n",
       "      <td>-0.416765</td>\n",
       "      <td>-0.246040</td>\n",
       "      <td>-0.748481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543181</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.039146</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>-1.086837</td>\n",
       "      <td>-0.351680</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.517124</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.052990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0050</th>\n",
       "      <td>1.048922</td>\n",
       "      <td>0.577184</td>\n",
       "      <td>1.692505</td>\n",
       "      <td>1.645615</td>\n",
       "      <td>0.750238</td>\n",
       "      <td>-0.066897</td>\n",
       "      <td>-0.130413</td>\n",
       "      <td>1.106432</td>\n",
       "      <td>1.015931</td>\n",
       "      <td>1.764417</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333553</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.216004</td>\n",
       "      <td>0.990117</td>\n",
       "      <td>-0.753045</td>\n",
       "      <td>0.618545</td>\n",
       "      <td>1.050514</td>\n",
       "      <td>0.563625</td>\n",
       "      <td>2.764119</td>\n",
       "      <td>1.325121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0053</th>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>1.198284</td>\n",
       "      <td>0.221591</td>\n",
       "      <td>0.145348</td>\n",
       "      <td>-0.039695</td>\n",
       "      <td>0.745301</td>\n",
       "      <td>0.947575</td>\n",
       "      <td>0.749415</td>\n",
       "      <td>-0.464770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274710</td>\n",
       "      <td>0.532431</td>\n",
       "      <td>1.088167</td>\n",
       "      <td>0.843921</td>\n",
       "      <td>1.440228</td>\n",
       "      <td>0.082751</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>1.027156</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.287373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0070</th>\n",
       "      <td>-1.309004</td>\n",
       "      <td>0.213414</td>\n",
       "      <td>-0.418802</td>\n",
       "      <td>-1.097679</td>\n",
       "      <td>-1.892068</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>-1.458774</td>\n",
       "      <td>-2.380129</td>\n",
       "      <td>-1.965949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788725</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-2.301812</td>\n",
       "      <td>-1.832232</td>\n",
       "      <td>-0.425255</td>\n",
       "      <td>0.830336</td>\n",
       "      <td>-1.228546</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.297272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0077</th>\n",
       "      <td>0.308685</td>\n",
       "      <td>0.423766</td>\n",
       "      <td>1.251951</td>\n",
       "      <td>1.631973</td>\n",
       "      <td>1.143059</td>\n",
       "      <td>-1.097219</td>\n",
       "      <td>0.168213</td>\n",
       "      <td>0.915839</td>\n",
       "      <td>1.162923</td>\n",
       "      <td>1.594137</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098347</td>\n",
       "      <td>0.702783</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.351349</td>\n",
       "      <td>0.462246</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>1.047099</td>\n",
       "      <td>0.768713</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.443454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0087</th>\n",
       "      <td>0.262581</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.801761</td>\n",
       "      <td>0.796105</td>\n",
       "      <td>0.392620</td>\n",
       "      <td>-0.952609</td>\n",
       "      <td>0.115493</td>\n",
       "      <td>0.295912</td>\n",
       "      <td>0.798726</td>\n",
       "      <td>0.749829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412925</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.797906</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>-1.457010</td>\n",
       "      <td>0.273234</td>\n",
       "      <td>0.680197</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.079904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002 -1.238669 -0.489755 -1.404819 -0.661768 -1.214104 -0.438326 -0.542205   \n",
       "C_0003 -0.094326  0.128294 -0.400562 -0.392906  0.908731  0.454217 -0.792399   \n",
       "C_0004 -0.513695  0.528638 -0.372793  0.456696 -0.126533 -0.054497 -0.268084   \n",
       "C_0005 -0.452707  0.211724  0.002274  0.105692  0.088081 -0.109452  0.208944   \n",
       "C_0006 -0.444591  0.344861 -0.730082  0.518467 -0.178900 -0.660072 -0.733352   \n",
       "C_0008 -0.063484  0.604704 -0.010074  0.157058  0.784126  0.523687  0.705062   \n",
       "C_0009 -1.014420  0.027920  0.485191  1.511839  2.342554 -0.059718 -0.901242   \n",
       "C_0010 -1.071431  0.484398 -0.933368 -0.658817 -0.998986 -0.943378 -1.362364   \n",
       "C_0011 -0.921800  0.182561 -2.304816 -1.383902 -1.199928 -0.621190 -1.538195   \n",
       "C_0012  0.370489  0.568469  0.573826  1.847973  2.247641 -1.047992 -1.840582   \n",
       "C_0013  0.237404  0.468118 -0.328469  0.401016  0.664377 -0.110870  1.175788   \n",
       "C_0014 -0.043427  0.164883 -0.241251 -0.231408 -0.153178  0.290115 -0.123583   \n",
       "C_0015 -0.159249  0.653677  0.067769  0.653778 -0.451082 -1.435228 -1.653990   \n",
       "C_0016 -0.313939  0.505943 -0.474605  0.395045 -0.454325  0.768875  1.061524   \n",
       "C_0017  0.617328 -2.842400 -0.042863  0.876221  0.948050 -0.556515 -0.586851   \n",
       "C_0018 -0.841823 -2.842400  0.611775  2.342445  2.457968 -1.436767 -1.273134   \n",
       "C_0019 -0.536535  0.282387 -0.669082 -0.261382 -0.587647 -0.113586  0.768471   \n",
       "C_0020 -1.286744  0.683392 -0.230312  0.520323 -0.197100 -1.564600 -1.281168   \n",
       "C_0021  0.526197 -2.842400 -0.390967  0.445794  0.184534 -1.203421 -1.145355   \n",
       "C_0022 -1.362488  0.307889 -0.984365 -1.156695 -1.288870 -0.259354 -0.329153   \n",
       "C_0023 -1.226360 -0.380853 -2.068576 -1.459295 -2.173385 -1.454016 -1.745151   \n",
       "C_0024  0.218499  0.742546 -0.366158  0.410088 -0.554448 -0.343820 -0.535454   \n",
       "C_0025 -0.525737  0.242389  0.106966  0.424007  0.286912 -0.878035 -1.127625   \n",
       "C_0026 -1.102176  0.412127 -1.320566 -0.620084 -1.046110 -0.585962 -0.764696   \n",
       "C_0029 -1.437548 -0.297698 -1.784748 -1.028572 -1.752626  0.407274  0.008234   \n",
       "C_0060  1.016182  0.544289 -0.468397 -0.756485 -0.454883 -0.032562  0.310191   \n",
       "C_0061 -0.473287  0.056637  0.117051 -0.359745  0.056609 -0.557902 -0.201168   \n",
       "C_0062  0.745442  0.616147  1.085803  0.833815  0.022261 -0.484330  0.229278   \n",
       "C_0065  4.464034  0.193050  1.630309  1.204508  1.248069 -0.647544  0.895345   \n",
       "C_0069  0.348975  0.462294  1.085377  0.647411 -1.216716 -1.487006 -0.768889   \n",
       "C_0071  1.644933  0.098840  0.919072  1.045445  1.664938 -0.048870  0.641201   \n",
       "C_0075 -0.655847  0.101901 -0.216873 -1.024741 -0.981342  0.315110 -0.255794   \n",
       "C_0076 -0.310756  0.693401  1.007125  0.964734 -0.202587 -1.844818 -0.896235   \n",
       "C_0081  0.291229  0.286913  0.724079  0.436671  0.206326 -0.243124  0.625176   \n",
       "C_0082 -0.370661  0.739036  1.271797  0.726100  0.766660 -1.076300 -0.679681   \n",
       "C_0083  0.388913  0.232071  1.041605  0.902932  0.923106 -1.479170 -0.725972   \n",
       "C_0031 -0.173575  0.559403  1.404774  0.862172 -0.182821 -0.912828 -1.302345   \n",
       "C_0032  1.363014  0.852724  1.620287  1.712404  1.276279  0.174185  0.437325   \n",
       "C_0033  0.004265  0.846777  1.335345  0.400130  0.469740 -0.250658 -0.528948   \n",
       "C_0034  1.169427  0.556844  0.808415  0.667631  0.581447  0.060103  0.540228   \n",
       "C_0035 -0.360567  0.846949  0.979596  0.268736  0.448585 -0.310850 -0.422807   \n",
       "C_0036 -0.064681  0.223731  1.326424  1.318422  0.348091 -0.331799 -1.112680   \n",
       "C_0037 -0.031287 -0.020087  1.275895  0.788813  0.389590 -2.367784 -1.431053   \n",
       "C_0038  0.127102  0.735821  0.962326  1.252127  1.027538 -1.468612 -0.540858   \n",
       "C_0039  0.076393  0.369525  0.262198 -0.434926 -0.641064 -0.367527 -0.258453   \n",
       "C_0050  1.048922  0.577184  1.692505  1.645615  0.750238 -0.066897 -0.130413   \n",
       "C_0053  0.433121  0.560606  1.198284  0.221591  0.145348 -0.039695  0.745301   \n",
       "C_0070 -1.309004  0.213414 -0.418802 -1.097679 -1.892068  0.063291  0.524590   \n",
       "C_0077  0.308685  0.423766  1.251951  1.631973  1.143059 -1.097219  0.168213   \n",
       "C_0087  0.262581  0.557800  0.801761  0.796105  0.392620 -0.952609  0.115493   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002 -1.549951 -1.144974 -1.361823  ... -0.794664 -1.270403  0.737597   \n",
       "C_0003  0.733360  0.611638  0.953743  ...  0.767006  0.814752 -1.029937   \n",
       "C_0004 -0.347413 -0.285398 -0.322790  ...  0.051075  0.649930  0.776952   \n",
       "C_0005  0.185074  0.533692  0.666086  ...  0.884624  0.681302 -1.029937   \n",
       "C_0006 -0.164000  0.323106  0.563685  ... -1.256904 -1.270403 -1.029937   \n",
       "C_0008  0.107073  0.650657  0.591910  ...  0.626894  0.690349 -1.029937   \n",
       "C_0009  2.952803  3.037081  1.972919  ...  0.056384  1.073685 -1.029937   \n",
       "C_0010 -0.970939 -0.853908 -0.605820  ... -0.555258 -1.270403  0.785608   \n",
       "C_0011 -1.155816 -0.558486 -0.865617  ... -1.144206 -1.270403 -1.029937   \n",
       "C_0012  2.485153  2.363211  2.430353  ...  1.381163  0.671095  0.987434   \n",
       "C_0013  0.527173  0.230430  0.598487  ...  0.245687  0.782167  0.773910   \n",
       "C_0014 -0.380659 -0.117571 -0.196329  ...  0.203878  0.699708  1.132509   \n",
       "C_0015 -0.421961 -0.142727  0.361802  ...  1.742628  0.581885  0.973181   \n",
       "C_0016 -0.027705 -0.504156  0.133814  ...  0.959270  0.802266 -1.029937   \n",
       "C_0017  1.194970  1.516010  1.081449  ...  0.927239 -1.270403  1.039801   \n",
       "C_0018  1.043109  0.310334  2.145601  ...  2.039750 -1.270403  0.883744   \n",
       "C_0019 -0.019680 -0.749318  0.085651  ...  0.332958  0.683476  0.928697   \n",
       "C_0020 -0.488723 -0.260558  0.360519  ...  1.246719  0.644466 -1.029937   \n",
       "C_0021  0.687426  0.684889  0.804522  ...  0.987853  0.836414  1.029127   \n",
       "C_0022 -1.661139 -1.751085 -1.280175  ... -0.299670  0.732401  0.706613   \n",
       "C_0023 -1.424173 -1.924884 -1.365756  ... -1.136616  0.678694  0.997710   \n",
       "C_0024 -0.260692 -0.020393  0.194022  ...  1.626982 -1.270403 -1.029937   \n",
       "C_0025  0.521370  0.787054  0.509760  ... -0.186735  0.740892 -1.029937   \n",
       "C_0026 -0.931632 -0.883488 -0.745137  ... -1.254419  0.617929 -1.029937   \n",
       "C_0029 -0.784135 -1.550076 -1.090772  ... -1.328883  0.857367 -1.029937   \n",
       "C_0060 -1.481712 -0.330176 -1.178254  ... -0.717043  0.766825 -1.029937   \n",
       "C_0061 -0.439586  0.132665 -0.769386  ... -0.141075  0.875788  1.023826   \n",
       "C_0062  0.747268  0.973920  0.660299  ...  0.880805  0.883678  0.986659   \n",
       "C_0065  2.280033  2.542494  1.493681  ...  0.648056  1.039465  1.362254   \n",
       "C_0069  0.030915 -0.054809 -0.638935  ... -1.164933  0.779073  1.085613   \n",
       "C_0071  1.740010  1.594438  1.463288  ...  0.622412  0.664615  0.796809   \n",
       "C_0075 -0.916104 -0.796130 -1.089311  ... -0.355271  0.800728  1.039417   \n",
       "C_0076 -0.697784  0.181026  0.569351  ...  0.868714  0.862941  0.958617   \n",
       "C_0081  0.001589  0.202261  0.767037  ...  0.673424  0.630064 -1.029937   \n",
       "C_0082  1.173660  0.952265  0.059237  ...  1.384673  0.901840 -1.029937   \n",
       "C_0083  0.561737  1.005341  0.631400  ...  0.753887  0.656740 -1.029937   \n",
       "C_0031  1.019318  0.738750 -0.484735  ... -0.864565 -1.270403 -1.029937   \n",
       "C_0032  1.204032  1.578419  1.610603  ...  1.564273 -1.270403  0.916243   \n",
       "C_0033  0.849011  0.816911  0.121975  ...  1.370601 -1.270403  1.024995   \n",
       "C_0034  0.516714  0.714247  0.972676  ... -0.120977 -1.270403 -1.029937   \n",
       "C_0035 -0.655380  0.036763 -0.277446  ...  0.262686  0.833449  1.131501   \n",
       "C_0036  0.753750  0.474937  0.662958  ...  0.854944  0.999951  1.463038   \n",
       "C_0037  0.742751  0.413208  0.224323  ...  0.156028  0.870393  0.893215   \n",
       "C_0038  1.084620  0.898749  1.476899  ...  0.902514 -1.270403 -1.029937   \n",
       "C_0039 -0.416765 -0.246040 -0.748481  ...  0.543181 -1.270403  1.039146   \n",
       "C_0050  1.106432  1.015931  1.764417  ...  1.333553 -1.270403  1.216004   \n",
       "C_0053  0.947575  0.749415 -0.464770  ...  0.274710  0.532431  1.088167   \n",
       "C_0070 -1.458774 -2.380129 -1.965949  ... -0.788725 -1.270403 -1.029937   \n",
       "C_0077  0.915839  1.162923  1.594137  ...  1.098347  0.702783 -1.029937   \n",
       "C_0087  0.295912  0.798726  0.749829  ...  0.412925 -1.270403  0.797906   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002 -1.819190 -0.178338  0.264969  0.721004 -0.094753 -0.349492 -0.142859  \n",
       "C_0003  0.719449  0.054000  0.220818  0.859050  0.298774 -0.349492 -0.149023  \n",
       "C_0004 -0.466111 -0.596202  0.287203  1.144874  0.595544 -0.349492  0.728854  \n",
       "C_0005 -0.455983  0.518626  0.463286  0.793437 -0.182278 -0.349492  0.198520  \n",
       "C_0006  0.190454 -1.167309  0.463277  0.996881  0.243467 -0.349492 -0.262606  \n",
       "C_0008 -0.004865  0.357423  0.598058  1.032206 -0.156499 -0.349492 -0.711918  \n",
       "C_0009  2.372708  1.781837 -4.757775 -1.220236  1.391432 -0.349492 -3.818267  \n",
       "C_0010 -0.303165  0.070641  0.277815  0.669304  0.071529 -0.349492 -0.462936  \n",
       "C_0011 -1.399549 -1.752028 -0.121130  0.774877  0.128418 -0.349492 -0.916628  \n",
       "C_0012  1.425060  0.034088  0.766789  0.848066  0.465206 -0.349492  0.968155  \n",
       "C_0013  0.154829  0.823008  0.794247  0.658943  0.046881 -0.349492 -0.089385  \n",
       "C_0014 -0.447637 -0.504793  0.158031  0.882336  0.306069 -0.349492  0.752933  \n",
       "C_0015 -0.168088 -0.960749  0.403107  1.090746 -0.082349 -0.349492  1.371974  \n",
       "C_0016 -0.934674 -0.521819  0.304097 -1.220236 -0.120013 -0.349492 -0.469263  \n",
       "C_0017  0.731721  0.243670  0.670608  0.733369  0.427240  2.775695 -0.883431  \n",
       "C_0018  2.903385 -0.352164  0.753175  0.917304 -0.170010 -0.349492  1.633091  \n",
       "C_0019 -1.103343 -0.071915  0.273526  0.796041 -0.234333 -0.349492  0.359326  \n",
       "C_0020 -0.094803 -0.970501  0.396902  0.910396  0.320131 -0.349492 -0.042961  \n",
       "C_0021  0.380531 -1.018371  0.330028  0.723915  0.062511 -0.349492 -0.227033  \n",
       "C_0022 -0.743866 -2.689368  0.031758  0.760415 -0.233864 -0.349492 -0.712032  \n",
       "C_0023 -0.693899 -2.348000 -0.091542 -1.220236 -0.313074 -0.349492 -0.879198  \n",
       "C_0024 -0.048342 -0.461180  0.103370  0.875968  0.238226 -0.349492 -0.705871  \n",
       "C_0025  0.404844 -0.157597  0.652556  1.144599  0.312478 -0.349492  0.480518  \n",
       "C_0026 -0.567240 -1.194215  0.164191 -1.220236 -0.037894 -0.349492  0.236674  \n",
       "C_0029 -1.160892 -0.607662 -0.239373 -1.220236  0.291046 -0.349492 -0.386633  \n",
       "C_0060 -0.787432 -1.118817 -0.086020 -1.220236 -0.220998 -0.349492 -0.530488  \n",
       "C_0061  0.292035  0.637192  0.554100  0.558140 -0.417092  2.371024 -1.178913  \n",
       "C_0062  0.537345  0.138341  0.284933 -1.220236  0.463572 -0.349492  0.086073  \n",
       "C_0065  2.228318  1.156093  0.712185  0.806037  1.366535 -0.349492  1.422842  \n",
       "C_0069  0.088204  0.281165 -0.110367 -1.220236  0.255357 -0.349492  1.438022  \n",
       "C_0071  1.809818 -0.077944  0.215619  0.934529  0.536589 -0.349492  0.677777  \n",
       "C_0075 -0.704558 -1.319216 -0.153142 -1.220236 -1.114976 -0.349492 -0.983068  \n",
       "C_0076 -0.126775 -1.118319  0.262785  0.916767  0.052875  2.609635  0.057696  \n",
       "C_0081 -0.061468 -1.003477  0.538467  1.011856  0.731500 -0.349492  0.429805  \n",
       "C_0082  0.644238 -0.047370 -0.184204 -1.220236  1.046076 -0.349492  0.261577  \n",
       "C_0083  0.344238 -0.614887  0.347417 -1.220236  0.130942  3.000866  0.672039  \n",
       "C_0031  1.135026 -0.832697 -4.757775 -1.220236 -0.263231 -0.349492  0.020141  \n",
       "C_0032  1.594664 -1.017494  0.866281  1.040314  0.346796 -0.349492  0.727455  \n",
       "C_0033  0.610686  0.834647 -4.757775 -1.220236  0.510835 -0.349492  1.217325  \n",
       "C_0034  0.938123 -0.844161  0.578631  0.970715  0.564428 -0.349492  0.950002  \n",
       "C_0035  0.188816 -1.644843  0.195078 -1.220236 -0.131956  3.609404  1.375492  \n",
       "C_0036  1.332991 -0.539399  0.473270 -1.220236  0.443274 -0.349492  1.807493  \n",
       "C_0037  0.189604 -0.610693 -0.209467 -1.220236  0.945151  2.847851 -0.032311  \n",
       "C_0038  1.325798  0.474221  0.154646 -1.220236  0.909238  3.778292  1.619436  \n",
       "C_0039  0.081579 -1.086837 -0.351680 -1.220236  0.517124 -0.349492 -1.052990  \n",
       "C_0050  0.990117 -0.753045  0.618545  1.050514  0.563625  2.764119  1.325121  \n",
       "C_0053  0.843921  1.440228  0.082751 -1.220236  1.027156 -0.349492  1.287373  \n",
       "C_0070 -2.301812 -1.832232 -0.425255  0.830336 -1.228546 -0.349492 -0.297272  \n",
       "C_0077  0.351349  0.462246  0.408542  1.047099  0.768713 -0.349492  0.443454  \n",
       "C_0087  0.189569 -1.457010  0.273234  0.680197  0.341713 -0.349492  1.079904  \n",
       "\n",
       "[50 rows x 21861 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_C=dataX_log_del2.filter(like='C', axis=0)\n",
    "dataX_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc0ae04-8f34-4726-9bf6-0d68be2ee005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3372753/805336276.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataX_C['cluster']=1\n",
      "/tmp/ipykernel_3372753/805336276.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataX_P['cluster']=0\n",
      "/tmp/ipykernel_3372753/805336276.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataX_H['cluster']=2\n"
     ]
    }
   ],
   "source": [
    "dataX_C['cluster']=1\n",
    "dataX_P=dataX_log_del2.filter(like='P', axis=0)\n",
    "dataX_P['cluster']=0\n",
    "dataX_H=dataX_log_del2.filter(like='H',axis=0)\n",
    "dataX_H['cluster']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931a7f25-37c2-4fbb-9fd3-ff2c5dca5e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-0.489755</td>\n",
       "      <td>-1.404819</td>\n",
       "      <td>-0.661768</td>\n",
       "      <td>-1.214104</td>\n",
       "      <td>-0.438326</td>\n",
       "      <td>-0.542205</td>\n",
       "      <td>-1.549951</td>\n",
       "      <td>-1.144974</td>\n",
       "      <td>-1.361823</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.737597</td>\n",
       "      <td>-1.819190</td>\n",
       "      <td>-0.178338</td>\n",
       "      <td>0.264969</td>\n",
       "      <td>0.721004</td>\n",
       "      <td>-0.094753</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.142859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>-0.094326</td>\n",
       "      <td>0.128294</td>\n",
       "      <td>-0.400562</td>\n",
       "      <td>-0.392906</td>\n",
       "      <td>0.908731</td>\n",
       "      <td>0.454217</td>\n",
       "      <td>-0.792399</td>\n",
       "      <td>0.733360</td>\n",
       "      <td>0.611638</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814752</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.719449</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.220818</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.298774</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.149023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>-0.513695</td>\n",
       "      <td>0.528638</td>\n",
       "      <td>-0.372793</td>\n",
       "      <td>0.456696</td>\n",
       "      <td>-0.126533</td>\n",
       "      <td>-0.054497</td>\n",
       "      <td>-0.268084</td>\n",
       "      <td>-0.347413</td>\n",
       "      <td>-0.285398</td>\n",
       "      <td>-0.322790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.776952</td>\n",
       "      <td>-0.466111</td>\n",
       "      <td>-0.596202</td>\n",
       "      <td>0.287203</td>\n",
       "      <td>1.144874</td>\n",
       "      <td>0.595544</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.728854</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>-0.452707</td>\n",
       "      <td>0.211724</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.088081</td>\n",
       "      <td>-0.109452</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.533692</td>\n",
       "      <td>0.666086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681302</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.455983</td>\n",
       "      <td>0.518626</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.793437</td>\n",
       "      <td>-0.182278</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.198520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>-0.444591</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>-0.730082</td>\n",
       "      <td>0.518467</td>\n",
       "      <td>-0.178900</td>\n",
       "      <td>-0.660072</td>\n",
       "      <td>-0.733352</td>\n",
       "      <td>-0.164000</td>\n",
       "      <td>0.323106</td>\n",
       "      <td>0.563685</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.190454</td>\n",
       "      <td>-1.167309</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.996881</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.262606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>-1.056665</td>\n",
       "      <td>-0.131702</td>\n",
       "      <td>-2.461487</td>\n",
       "      <td>-2.196218</td>\n",
       "      <td>-0.828361</td>\n",
       "      <td>1.475229</td>\n",
       "      <td>0.686531</td>\n",
       "      <td>-1.842048</td>\n",
       "      <td>-1.833416</td>\n",
       "      <td>-0.990698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676598</td>\n",
       "      <td>0.813014</td>\n",
       "      <td>-2.498809</td>\n",
       "      <td>-0.298672</td>\n",
       "      <td>0.243131</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.478093</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-2.423072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>-0.813836</td>\n",
       "      <td>0.110010</td>\n",
       "      <td>0.332046</td>\n",
       "      <td>-1.371978</td>\n",
       "      <td>-1.159991</td>\n",
       "      <td>-0.961007</td>\n",
       "      <td>-0.240771</td>\n",
       "      <td>-0.532297</td>\n",
       "      <td>-1.179250</td>\n",
       "      <td>-1.522107</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.059062</td>\n",
       "      <td>0.190509</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-4.368677</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.082281</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>0.740941</td>\n",
       "      <td>0.692626</td>\n",
       "      <td>0.985739</td>\n",
       "      <td>-0.153001</td>\n",
       "      <td>-0.268487</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>0.639643</td>\n",
       "      <td>-0.215969</td>\n",
       "      <td>-0.145524</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071325</td>\n",
       "      <td>0.983066</td>\n",
       "      <td>1.379293</td>\n",
       "      <td>1.249711</td>\n",
       "      <td>-0.185067</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.343931</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.869277</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>-0.448897</td>\n",
       "      <td>0.149684</td>\n",
       "      <td>0.344353</td>\n",
       "      <td>0.107790</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>0.409116</td>\n",
       "      <td>0.303394</td>\n",
       "      <td>-0.866286</td>\n",
       "      <td>-0.533084</td>\n",
       "      <td>-0.141975</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.437144</td>\n",
       "      <td>0.149815</td>\n",
       "      <td>0.178247</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>-0.537081</td>\n",
       "      <td>0.415169</td>\n",
       "      <td>1.062265</td>\n",
       "      <td>0.954178</td>\n",
       "      <td>-0.012975</td>\n",
       "      <td>1.303455</td>\n",
       "      <td>-0.598432</td>\n",
       "      <td>0.868221</td>\n",
       "      <td>-0.181386</td>\n",
       "      <td>-0.306728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832702</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>1.372934</td>\n",
       "      <td>0.944623</td>\n",
       "      <td>-0.604752</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.508965</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.514668</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21862 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "C_0002 -1.238669 -0.489755 -1.404819 -0.661768 -1.214104 -0.438326 -0.542205   \n",
       "C_0003 -0.094326  0.128294 -0.400562 -0.392906  0.908731  0.454217 -0.792399   \n",
       "C_0004 -0.513695  0.528638 -0.372793  0.456696 -0.126533 -0.054497 -0.268084   \n",
       "C_0005 -0.452707  0.211724  0.002274  0.105692  0.088081 -0.109452  0.208944   \n",
       "C_0006 -0.444591  0.344861 -0.730082  0.518467 -0.178900 -0.660072 -0.733352   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689 -1.056665 -0.131702 -2.461487 -2.196218 -0.828361  1.475229  0.686531   \n",
       "H_0709 -0.813836  0.110010  0.332046 -1.371978 -1.159991 -0.961007 -0.240771   \n",
       "H_0723  0.740941  0.692626  0.985739 -0.153001 -0.268487  0.037081  0.639643   \n",
       "H_1104 -0.448897  0.149684  0.344353  0.107790  0.044293  0.409116  0.303394   \n",
       "H_1105 -0.537081  0.415169  1.062265  0.954178 -0.012975  1.303455 -0.598432   \n",
       "\n",
       "               7         8         9  ...     21854     21855     21856  \\\n",
       "C_0002 -1.549951 -1.144974 -1.361823  ... -1.270403  0.737597 -1.819190   \n",
       "C_0003  0.733360  0.611638  0.953743  ...  0.814752 -1.029937  0.719449   \n",
       "C_0004 -0.347413 -0.285398 -0.322790  ...  0.649930  0.776952 -0.466111   \n",
       "C_0005  0.185074  0.533692  0.666086  ...  0.681302 -1.029937 -0.455983   \n",
       "C_0006 -0.164000  0.323106  0.563685  ... -1.270403 -1.029937  0.190454   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689 -1.842048 -1.833416 -0.990698  ...  0.676598  0.813014 -2.498809   \n",
       "H_0709 -0.532297 -1.179250 -1.522107  ... -1.270403 -1.029937  0.059062   \n",
       "H_0723 -0.215969 -0.145524  0.047562  ...  1.071325  0.983066  1.379293   \n",
       "H_1104 -0.866286 -0.533084 -0.141975  ... -1.270403 -1.029937 -0.437144   \n",
       "H_1105  0.868221 -0.181386 -0.306728  ...  0.832702  1.024108  1.372934   \n",
       "\n",
       "           21857     21858     21859     21860     21861     21862  cluster  \n",
       "C_0002 -0.178338  0.264969  0.721004 -0.094753 -0.349492 -0.142859        1  \n",
       "C_0003  0.054000  0.220818  0.859050  0.298774 -0.349492 -0.149023        1  \n",
       "C_0004 -0.596202  0.287203  1.144874  0.595544 -0.349492  0.728854        1  \n",
       "C_0005  0.518626  0.463286  0.793437 -0.182278 -0.349492  0.198520        1  \n",
       "C_0006 -1.167309  0.463277  0.996881  0.243467 -0.349492 -0.262606        1  \n",
       "...          ...       ...       ...       ...       ...       ...      ...  \n",
       "H_0689 -0.298672  0.243131 -1.220236 -0.478093 -0.349492 -2.423072        2  \n",
       "H_0709  0.190509 -4.757775 -1.220236 -4.368677 -0.349492  1.082281        2  \n",
       "H_0723  1.249711 -0.185067 -1.220236 -0.343931 -0.349492  0.869277        2  \n",
       "H_1104  0.149815  0.178247 -1.220236  0.106997 -0.349492  0.552147        2  \n",
       "H_1105  0.944623 -0.604752 -1.220236  0.508965 -0.349492 -0.514668        2  \n",
       "\n",
       "[108 rows x 21862 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_cluster=pd.concat([dataX_C, dataX_P, dataX_H], ignore_index=False)\n",
    "dataX_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9b74899-a683-4f54-bb64-e68e5d8a490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-0.489755</td>\n",
       "      <td>-1.404819</td>\n",
       "      <td>-0.661768</td>\n",
       "      <td>-1.214104</td>\n",
       "      <td>-0.438326</td>\n",
       "      <td>-0.542205</td>\n",
       "      <td>-1.549951</td>\n",
       "      <td>-1.144974</td>\n",
       "      <td>-1.361823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.737597</td>\n",
       "      <td>-1.819190</td>\n",
       "      <td>-0.178338</td>\n",
       "      <td>0.264969</td>\n",
       "      <td>0.721004</td>\n",
       "      <td>-0.094753</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.142859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>-0.094326</td>\n",
       "      <td>0.128294</td>\n",
       "      <td>-0.400562</td>\n",
       "      <td>-0.392906</td>\n",
       "      <td>0.908731</td>\n",
       "      <td>0.454217</td>\n",
       "      <td>-0.792399</td>\n",
       "      <td>0.733360</td>\n",
       "      <td>0.611638</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767006</td>\n",
       "      <td>0.814752</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.719449</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.220818</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.298774</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.149023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>-0.513695</td>\n",
       "      <td>0.528638</td>\n",
       "      <td>-0.372793</td>\n",
       "      <td>0.456696</td>\n",
       "      <td>-0.126533</td>\n",
       "      <td>-0.054497</td>\n",
       "      <td>-0.268084</td>\n",
       "      <td>-0.347413</td>\n",
       "      <td>-0.285398</td>\n",
       "      <td>-0.322790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051075</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.776952</td>\n",
       "      <td>-0.466111</td>\n",
       "      <td>-0.596202</td>\n",
       "      <td>0.287203</td>\n",
       "      <td>1.144874</td>\n",
       "      <td>0.595544</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.728854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>-0.452707</td>\n",
       "      <td>0.211724</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.088081</td>\n",
       "      <td>-0.109452</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.533692</td>\n",
       "      <td>0.666086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884624</td>\n",
       "      <td>0.681302</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.455983</td>\n",
       "      <td>0.518626</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.793437</td>\n",
       "      <td>-0.182278</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.198520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>-0.444591</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>-0.730082</td>\n",
       "      <td>0.518467</td>\n",
       "      <td>-0.178900</td>\n",
       "      <td>-0.660072</td>\n",
       "      <td>-0.733352</td>\n",
       "      <td>-0.164000</td>\n",
       "      <td>0.323106</td>\n",
       "      <td>0.563685</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.256904</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.190454</td>\n",
       "      <td>-1.167309</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.996881</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.262606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>-1.056665</td>\n",
       "      <td>-0.131702</td>\n",
       "      <td>-2.461487</td>\n",
       "      <td>-2.196218</td>\n",
       "      <td>-0.828361</td>\n",
       "      <td>1.475229</td>\n",
       "      <td>0.686531</td>\n",
       "      <td>-1.842048</td>\n",
       "      <td>-1.833416</td>\n",
       "      <td>-0.990698</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.765663</td>\n",
       "      <td>0.676598</td>\n",
       "      <td>0.813014</td>\n",
       "      <td>-2.498809</td>\n",
       "      <td>-0.298672</td>\n",
       "      <td>0.243131</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.478093</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-2.423072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>-0.813836</td>\n",
       "      <td>0.110010</td>\n",
       "      <td>0.332046</td>\n",
       "      <td>-1.371978</td>\n",
       "      <td>-1.159991</td>\n",
       "      <td>-0.961007</td>\n",
       "      <td>-0.240771</td>\n",
       "      <td>-0.532297</td>\n",
       "      <td>-1.179250</td>\n",
       "      <td>-1.522107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.899592</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.059062</td>\n",
       "      <td>0.190509</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-4.368677</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.082281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>0.740941</td>\n",
       "      <td>0.692626</td>\n",
       "      <td>0.985739</td>\n",
       "      <td>-0.153001</td>\n",
       "      <td>-0.268487</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>0.639643</td>\n",
       "      <td>-0.215969</td>\n",
       "      <td>-0.145524</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332414</td>\n",
       "      <td>1.071325</td>\n",
       "      <td>0.983066</td>\n",
       "      <td>1.379293</td>\n",
       "      <td>1.249711</td>\n",
       "      <td>-0.185067</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.343931</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.869277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>-0.448897</td>\n",
       "      <td>0.149684</td>\n",
       "      <td>0.344353</td>\n",
       "      <td>0.107790</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>0.409116</td>\n",
       "      <td>0.303394</td>\n",
       "      <td>-0.866286</td>\n",
       "      <td>-0.533084</td>\n",
       "      <td>-0.141975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274111</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.437144</td>\n",
       "      <td>0.149815</td>\n",
       "      <td>0.178247</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.552147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>-0.537081</td>\n",
       "      <td>0.415169</td>\n",
       "      <td>1.062265</td>\n",
       "      <td>0.954178</td>\n",
       "      <td>-0.012975</td>\n",
       "      <td>1.303455</td>\n",
       "      <td>-0.598432</td>\n",
       "      <td>0.868221</td>\n",
       "      <td>-0.181386</td>\n",
       "      <td>-0.306728</td>\n",
       "      <td>...</td>\n",
       "      <td>1.443922</td>\n",
       "      <td>0.832702</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>1.372934</td>\n",
       "      <td>0.944623</td>\n",
       "      <td>-0.604752</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.508965</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.514668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002 -1.238669 -0.489755 -1.404819 -0.661768 -1.214104 -0.438326 -0.542205   \n",
       "C_0003 -0.094326  0.128294 -0.400562 -0.392906  0.908731  0.454217 -0.792399   \n",
       "C_0004 -0.513695  0.528638 -0.372793  0.456696 -0.126533 -0.054497 -0.268084   \n",
       "C_0005 -0.452707  0.211724  0.002274  0.105692  0.088081 -0.109452  0.208944   \n",
       "C_0006 -0.444591  0.344861 -0.730082  0.518467 -0.178900 -0.660072 -0.733352   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689 -1.056665 -0.131702 -2.461487 -2.196218 -0.828361  1.475229  0.686531   \n",
       "H_0709 -0.813836  0.110010  0.332046 -1.371978 -1.159991 -0.961007 -0.240771   \n",
       "H_0723  0.740941  0.692626  0.985739 -0.153001 -0.268487  0.037081  0.639643   \n",
       "H_1104 -0.448897  0.149684  0.344353  0.107790  0.044293  0.409116  0.303394   \n",
       "H_1105 -0.537081  0.415169  1.062265  0.954178 -0.012975  1.303455 -0.598432   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002 -1.549951 -1.144974 -1.361823  ... -0.794664 -1.270403  0.737597   \n",
       "C_0003  0.733360  0.611638  0.953743  ...  0.767006  0.814752 -1.029937   \n",
       "C_0004 -0.347413 -0.285398 -0.322790  ...  0.051075  0.649930  0.776952   \n",
       "C_0005  0.185074  0.533692  0.666086  ...  0.884624  0.681302 -1.029937   \n",
       "C_0006 -0.164000  0.323106  0.563685  ... -1.256904 -1.270403 -1.029937   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689 -1.842048 -1.833416 -0.990698  ... -1.765663  0.676598  0.813014   \n",
       "H_0709 -0.532297 -1.179250 -1.522107  ... -0.899592 -1.270403 -1.029937   \n",
       "H_0723 -0.215969 -0.145524  0.047562  ...  0.332414  1.071325  0.983066   \n",
       "H_1104 -0.866286 -0.533084 -0.141975  ...  0.274111 -1.270403 -1.029937   \n",
       "H_1105  0.868221 -0.181386 -0.306728  ...  1.443922  0.832702  1.024108   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002 -1.819190 -0.178338  0.264969  0.721004 -0.094753 -0.349492 -0.142859  \n",
       "C_0003  0.719449  0.054000  0.220818  0.859050  0.298774 -0.349492 -0.149023  \n",
       "C_0004 -0.466111 -0.596202  0.287203  1.144874  0.595544 -0.349492  0.728854  \n",
       "C_0005 -0.455983  0.518626  0.463286  0.793437 -0.182278 -0.349492  0.198520  \n",
       "C_0006  0.190454 -1.167309  0.463277  0.996881  0.243467 -0.349492 -0.262606  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0689 -2.498809 -0.298672  0.243131 -1.220236 -0.478093 -0.349492 -2.423072  \n",
       "H_0709  0.059062  0.190509 -4.757775 -1.220236 -4.368677 -0.349492  1.082281  \n",
       "H_0723  1.379293  1.249711 -0.185067 -1.220236 -0.343931 -0.349492  0.869277  \n",
       "H_1104 -0.437144  0.149815  0.178247 -1.220236  0.106997 -0.349492  0.552147  \n",
       "H_1105  1.372934  0.944623 -0.604752 -1.220236  0.508965 -0.349492 -0.514668  \n",
       "\n",
       "[108 rows x 21861 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_log_del2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d0c30c8-d3b0-4387-a2dc-69dec6674d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_num=dataX_cluster['cluster']\n",
    "patient_num_array=patient_num.values\n",
    "patient_num_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30e5669f-d76a-4f24-afe2-97ce911afd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(dataX_cluster, test_size=0.05,random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceebda60-7a7a-4fe0-bdb2-249e4a367ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corre_plot(X_train, X_train_pred, X_test, X_test_pred):\n",
    "    x=np.linspace(-2,10)\n",
    "    y=x\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('train correlation')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('test correlation')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('train correlation narrow')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title('test correlation narrow')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e41faf6b-7ace-4e40-8b4d-6c5fdfbdecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'], loc='upper right')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'],loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cfaef88-e50f-4cca-9bd0-7a15b3946e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(df): \n",
    "    n_features = len(df)\n",
    "    df_plot = df.sort_values('importance')\n",
    "    f_importance_plot = df_plot['importance'].values\n",
    "    plt.barh(range(n_features), f_importance_plot, align='center')\n",
    "    cols_plot = df_plot['feature'].values             \n",
    "    plt.yticks(np.arange(n_features), cols_plot)      \n",
    "    plt.xlabel('Feature importance')                  \n",
    "    plt.ylabel('Feature') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "221ea45b-1889-40cf-bb5b-96c4dea8175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:10:45.520516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1f5a5e6-a197-429b-85b4-9756983f44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') memory growth: True\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU') memory growth: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:10:46.676481: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-06-03 19:10:46.677925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2025-06-03 19:10:46.694882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.695697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-06-03 19:10:46.695798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.696569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:05:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-06-03 19:10:46.696670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.697149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GT 710 computeCapability: 3.5\n",
      "coreClock: 0.954GHz coreCount: 1 deviceMemorySize: 978.25MiB deviceMemoryBandwidth: 11.92GiB/s\n",
      "2025-06-03 19:10:46.697224: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-06-03 19:10:46.699900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2025-06-03 19:10:46.700084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2025-06-03 19:10:46.751877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-06-03 19:10:46.752606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-06-03 19:10:46.754954: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-06-03 19:10:46.756419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2025-06-03 19:10:46.761120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2025-06-03 19:10:46.761380: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.762247: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.762973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.763394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.764117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.765026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.765374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1847] Ignoring visible gpu device (device: 2, name: NVIDIA GeForce GT 710, pci bus id: 0000:01:00.0, compute capability: 3.5) with core count: 1. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2025-06-03 19:10:46.765391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "        tf.config.set_soft_device_placement(True)\n",
    "        growth = tf.config.experimental.get_memory_growth(device)\n",
    "        print('{} memory growth: {}'.format(device, growth))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d63c61-8517-4a39-8721-7cec45016136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corre_plot(X_train, X_train_pred, X_test, X_test_pred):\n",
    "    x=np.linspace(-2,10)\n",
    "    y=x\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('train correlation')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('test correlation')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('train correlation narrow')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title('test correlation narrow')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd29a136-ff84-4e51-aa5a-ad879881f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'], loc='upper right')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'],loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c799a028-042c-4bac-b850-34b6a400e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(df): \n",
    "    n_features = len(df)\n",
    "    df_plot = df.sort_values('importance')\n",
    "    f_importance_plot = df_plot['importance'].values\n",
    "    plt.barh(range(n_features), f_importance_plot, align='center')\n",
    "    cols_plot = df_plot['feature'].values             \n",
    "    plt.yticks(np.arange(n_features), cols_plot)      \n",
    "    plt.xlabel('Feature importance')                  \n",
    "    plt.ylabel('Feature') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "645098ee-d101-4acb-9410-d9f15f26c9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:10:46.811029: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-03 19:10:46.812149: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-06-03 19:10:46.934767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.935476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-06-03 19:10:46.935560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.936242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:05:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-06-03 19:10:46.936298: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-06-03 19:10:46.936335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2025-06-03 19:10:46.936367: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2025-06-03 19:10:46.936398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-06-03 19:10:46.936430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-06-03 19:10:46.936462: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-06-03 19:10:46.936494: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2025-06-03 19:10:46.936526: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2025-06-03 19:10:46.936627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.937345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.938081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.938870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:46.939563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2025-06-03 19:10:46.939641: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-06-03 19:10:47.620160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-06-03 19:10:47.620211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2025-06-03 19:10:47.620223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2025-06-03 19:10:47.620232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2025-06-03 19:10:47.620554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:47.621382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:47.622298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:47.623215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:47.624049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10350 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:04:00.0, compute capability: 3.7)\n",
      "2025-06-03 19:10:47.624579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:47.625625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-06-03 19:10:47.626526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10494 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_223\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_914 (Dense)            (None, 100)               2186200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_693 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_915 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_694 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_916 (Dense)            (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "batch_normalization_695 (Bat (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_917 (Dense)            (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "batch_normalization_696 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_918 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_697 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_919 (Dense)            (None, 21861)             2207961   \n",
      "=================================================================\n",
      "Total params: 4,420,161\n",
      "Trainable params: 4,419,321\n",
      "Non-trainable params: 840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model20dims = load_model(\"model20_11_20dim.h5\")\n",
    "model20dims.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f76141bb-ca03-480f-86c5-8db8aa8c96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = X_train.iloc[:, :-1]\n",
    "df_test = X_test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f13fe710-6a14-442b-9b75-dae31f068a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0083</th>\n",
       "      <td>0.388913</td>\n",
       "      <td>0.232071</td>\n",
       "      <td>1.041605</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.923106</td>\n",
       "      <td>-1.479170</td>\n",
       "      <td>-0.725972</td>\n",
       "      <td>0.561737</td>\n",
       "      <td>1.005341</td>\n",
       "      <td>0.631400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753887</td>\n",
       "      <td>0.656740</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.344238</td>\n",
       "      <td>-0.614887</td>\n",
       "      <td>0.347417</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.130942</td>\n",
       "      <td>3.000866</td>\n",
       "      <td>0.672039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0014</th>\n",
       "      <td>-0.491357</td>\n",
       "      <td>-0.208702</td>\n",
       "      <td>-0.949570</td>\n",
       "      <td>-0.080523</td>\n",
       "      <td>0.650120</td>\n",
       "      <td>-0.477062</td>\n",
       "      <td>0.117402</td>\n",
       "      <td>-0.177543</td>\n",
       "      <td>-0.005107</td>\n",
       "      <td>0.238695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227848</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.623604</td>\n",
       "      <td>0.379853</td>\n",
       "      <td>0.394208</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>0.454151</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.281494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0006</th>\n",
       "      <td>1.007862</td>\n",
       "      <td>0.051293</td>\n",
       "      <td>0.647164</td>\n",
       "      <td>-0.520091</td>\n",
       "      <td>0.337481</td>\n",
       "      <td>1.187124</td>\n",
       "      <td>0.750467</td>\n",
       "      <td>-0.115304</td>\n",
       "      <td>-0.631415</td>\n",
       "      <td>-0.789085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340969</td>\n",
       "      <td>0.852506</td>\n",
       "      <td>1.126368</td>\n",
       "      <td>0.267152</td>\n",
       "      <td>0.448367</td>\n",
       "      <td>-0.559285</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.912924</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.014469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0008</th>\n",
       "      <td>-0.063484</td>\n",
       "      <td>0.604704</td>\n",
       "      <td>-0.010074</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.784126</td>\n",
       "      <td>0.523687</td>\n",
       "      <td>0.705062</td>\n",
       "      <td>0.107073</td>\n",
       "      <td>0.650657</td>\n",
       "      <td>0.591910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>0.690349</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>0.357423</td>\n",
       "      <td>0.598058</td>\n",
       "      <td>1.032206</td>\n",
       "      <td>-0.156499</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.711918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0132</th>\n",
       "      <td>-0.379078</td>\n",
       "      <td>-0.063351</td>\n",
       "      <td>-0.422360</td>\n",
       "      <td>-1.034857</td>\n",
       "      <td>-0.855160</td>\n",
       "      <td>0.566042</td>\n",
       "      <td>-0.034787</td>\n",
       "      <td>-0.017356</td>\n",
       "      <td>-0.152592</td>\n",
       "      <td>-1.035450</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.987399</td>\n",
       "      <td>0.712918</td>\n",
       "      <td>1.102537</td>\n",
       "      <td>0.679585</td>\n",
       "      <td>0.828777</td>\n",
       "      <td>-0.162848</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.291017</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.054538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0012</th>\n",
       "      <td>-1.521997</td>\n",
       "      <td>0.067175</td>\n",
       "      <td>-0.349465</td>\n",
       "      <td>-1.227161</td>\n",
       "      <td>-1.159223</td>\n",
       "      <td>0.147176</td>\n",
       "      <td>0.480327</td>\n",
       "      <td>-1.637841</td>\n",
       "      <td>-1.043687</td>\n",
       "      <td>-0.477915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128161</td>\n",
       "      <td>0.782275</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.323493</td>\n",
       "      <td>-0.181305</td>\n",
       "      <td>-0.492980</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-4.368677</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.423462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0015</th>\n",
       "      <td>-1.501659</td>\n",
       "      <td>-0.015775</td>\n",
       "      <td>-2.043978</td>\n",
       "      <td>-1.415894</td>\n",
       "      <td>-0.616190</td>\n",
       "      <td>-1.331602</td>\n",
       "      <td>-1.422446</td>\n",
       "      <td>-1.490010</td>\n",
       "      <td>-1.171014</td>\n",
       "      <td>-0.957152</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.225591</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.174205</td>\n",
       "      <td>0.032423</td>\n",
       "      <td>0.012057</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.143522</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.608102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0087</th>\n",
       "      <td>0.262581</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.801761</td>\n",
       "      <td>0.796105</td>\n",
       "      <td>0.392620</td>\n",
       "      <td>-0.952609</td>\n",
       "      <td>0.115493</td>\n",
       "      <td>0.295912</td>\n",
       "      <td>0.798726</td>\n",
       "      <td>0.749829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412925</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.797906</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>-1.457010</td>\n",
       "      <td>0.273234</td>\n",
       "      <td>0.680197</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.079904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0050</th>\n",
       "      <td>1.048922</td>\n",
       "      <td>0.577184</td>\n",
       "      <td>1.692505</td>\n",
       "      <td>1.645615</td>\n",
       "      <td>0.750238</td>\n",
       "      <td>-0.066897</td>\n",
       "      <td>-0.130413</td>\n",
       "      <td>1.106432</td>\n",
       "      <td>1.015931</td>\n",
       "      <td>1.764417</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333553</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.216004</td>\n",
       "      <td>0.990117</td>\n",
       "      <td>-0.753045</td>\n",
       "      <td>0.618545</td>\n",
       "      <td>1.050514</td>\n",
       "      <td>0.563625</td>\n",
       "      <td>2.764119</td>\n",
       "      <td>1.325121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0657</th>\n",
       "      <td>-0.064258</td>\n",
       "      <td>0.172921</td>\n",
       "      <td>0.519725</td>\n",
       "      <td>-0.133270</td>\n",
       "      <td>-0.175916</td>\n",
       "      <td>1.377655</td>\n",
       "      <td>0.786557</td>\n",
       "      <td>0.458264</td>\n",
       "      <td>-0.163786</td>\n",
       "      <td>-0.114812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179268</td>\n",
       "      <td>0.526197</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.282419</td>\n",
       "      <td>-0.833780</td>\n",
       "      <td>-0.073411</td>\n",
       "      <td>0.835596</td>\n",
       "      <td>0.287229</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.182819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0083  0.388913  0.232071  1.041605  0.902932  0.923106 -1.479170 -0.725972   \n",
       "P_0014 -0.491357 -0.208702 -0.949570 -0.080523  0.650120 -0.477062  0.117402   \n",
       "H_0006  1.007862  0.051293  0.647164 -0.520091  0.337481  1.187124  0.750467   \n",
       "C_0008 -0.063484  0.604704 -0.010074  0.157058  0.784126  0.523687  0.705062   \n",
       "P_0132 -0.379078 -0.063351 -0.422360 -1.034857 -0.855160  0.566042 -0.034787   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0012 -1.521997  0.067175 -0.349465 -1.227161 -1.159223  0.147176  0.480327   \n",
       "P_0015 -1.501659 -0.015775 -2.043978 -1.415894 -0.616190 -1.331602 -1.422446   \n",
       "C_0087  0.262581  0.557800  0.801761  0.796105  0.392620 -0.952609  0.115493   \n",
       "C_0050  1.048922  0.577184  1.692505  1.645615  0.750238 -0.066897 -0.130413   \n",
       "H_0657 -0.064258  0.172921  0.519725 -0.133270 -0.175916  1.377655  0.786557   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0083  0.561737  1.005341  0.631400  ...  0.753887  0.656740 -1.029937   \n",
       "P_0014 -0.177543 -0.005107  0.238695  ... -0.227848 -1.270403 -1.029937   \n",
       "H_0006 -0.115304 -0.631415 -0.789085  ... -0.340969  0.852506  1.126368   \n",
       "C_0008  0.107073  0.650657  0.591910  ...  0.626894  0.690349 -1.029937   \n",
       "P_0132 -0.017356 -0.152592 -1.035450  ... -1.987399  0.712918  1.102537   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0012 -1.637841 -1.043687 -0.477915  ...  0.128161  0.782275 -1.029937   \n",
       "P_0015 -1.490010 -1.171014 -0.957152  ... -1.225591 -1.270403 -1.029937   \n",
       "C_0087  0.295912  0.798726  0.749829  ...  0.412925 -1.270403  0.797906   \n",
       "C_0050  1.106432  1.015931  1.764417  ...  1.333553 -1.270403  1.216004   \n",
       "H_0657  0.458264 -0.163786 -0.114812  ...  0.179268  0.526197 -1.029937   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0083  0.344238 -0.614887  0.347417 -1.220236  0.130942  3.000866  0.672039  \n",
       "P_0014 -0.623604  0.379853  0.394208  0.644572  0.454151 -0.349492 -1.281494  \n",
       "H_0006  0.267152  0.448367 -0.559285 -1.220236  0.912924 -0.349492  1.014469  \n",
       "C_0008 -0.004865  0.357423  0.598058  1.032206 -0.156499 -0.349492 -0.711918  \n",
       "P_0132  0.679585  0.828777 -0.162848 -1.220236  0.291017 -0.349492 -1.054538  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0012 -1.323493 -0.181305 -0.492980 -1.220236 -4.368677 -0.349492 -1.423462  \n",
       "P_0015 -1.174205  0.032423  0.012057 -1.220236 -0.143522 -0.349492 -1.608102  \n",
       "C_0087  0.189569 -1.457010  0.273234  0.680197  0.341713 -0.349492  1.079904  \n",
       "C_0050  0.990117 -0.753045  0.618545  1.050514  0.563625  2.764119  1.325121  \n",
       "H_0657  0.282419 -0.833780 -0.073411  0.835596  0.287229 -0.349492  0.182819  \n",
       "\n",
       "[102 rows x 21861 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12ebc6fb-daec-4fd9-9cb8-4eb1f079a90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P_0063</th>\n",
       "      <td>-0.354154</td>\n",
       "      <td>-0.443490</td>\n",
       "      <td>-0.519781</td>\n",
       "      <td>-0.938225</td>\n",
       "      <td>-0.692398</td>\n",
       "      <td>-1.225452</td>\n",
       "      <td>-0.567080</td>\n",
       "      <td>-0.929789</td>\n",
       "      <td>-0.391042</td>\n",
       "      <td>-0.918295</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.361050</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.770840</td>\n",
       "      <td>-0.650630</td>\n",
       "      <td>0.148355</td>\n",
       "      <td>-0.417226</td>\n",
       "      <td>0.656224</td>\n",
       "      <td>-1.557539</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.420660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0029</th>\n",
       "      <td>-0.495266</td>\n",
       "      <td>0.082892</td>\n",
       "      <td>-1.553595</td>\n",
       "      <td>-1.080348</td>\n",
       "      <td>-0.552315</td>\n",
       "      <td>1.010324</td>\n",
       "      <td>-0.309374</td>\n",
       "      <td>-1.022790</td>\n",
       "      <td>-0.761766</td>\n",
       "      <td>-0.485853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.872063</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.033890</td>\n",
       "      <td>-0.783647</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>0.669075</td>\n",
       "      <td>-0.153178</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.354232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0010</th>\n",
       "      <td>0.947469</td>\n",
       "      <td>0.610187</td>\n",
       "      <td>1.167948</td>\n",
       "      <td>1.184255</td>\n",
       "      <td>0.972135</td>\n",
       "      <td>0.467329</td>\n",
       "      <td>1.196569</td>\n",
       "      <td>0.468982</td>\n",
       "      <td>0.605396</td>\n",
       "      <td>0.755573</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017267</td>\n",
       "      <td>0.936328</td>\n",
       "      <td>1.164238</td>\n",
       "      <td>0.185686</td>\n",
       "      <td>-0.605207</td>\n",
       "      <td>0.286310</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.613593</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.488160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0025</th>\n",
       "      <td>-0.525737</td>\n",
       "      <td>0.242389</td>\n",
       "      <td>0.106966</td>\n",
       "      <td>0.424007</td>\n",
       "      <td>0.286912</td>\n",
       "      <td>-0.878035</td>\n",
       "      <td>-1.127625</td>\n",
       "      <td>0.521370</td>\n",
       "      <td>0.787054</td>\n",
       "      <td>0.509760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186735</td>\n",
       "      <td>0.740892</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.404844</td>\n",
       "      <td>-0.157597</td>\n",
       "      <td>0.652556</td>\n",
       "      <td>1.144599</td>\n",
       "      <td>0.312478</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.480518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0070</th>\n",
       "      <td>-1.309004</td>\n",
       "      <td>0.213414</td>\n",
       "      <td>-0.418802</td>\n",
       "      <td>-1.097679</td>\n",
       "      <td>-1.892068</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>-1.458774</td>\n",
       "      <td>-2.380129</td>\n",
       "      <td>-1.965949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788725</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-2.301812</td>\n",
       "      <td>-1.832232</td>\n",
       "      <td>-0.425255</td>\n",
       "      <td>0.830336</td>\n",
       "      <td>-1.228546</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.297272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0039</th>\n",
       "      <td>0.076393</td>\n",
       "      <td>0.369525</td>\n",
       "      <td>0.262198</td>\n",
       "      <td>-0.434926</td>\n",
       "      <td>-0.641064</td>\n",
       "      <td>-0.367527</td>\n",
       "      <td>-0.258453</td>\n",
       "      <td>-0.416765</td>\n",
       "      <td>-0.246040</td>\n",
       "      <td>-0.748481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543181</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.039146</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>-1.086837</td>\n",
       "      <td>-0.351680</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.517124</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.052990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "P_0063 -0.354154 -0.443490 -0.519781 -0.938225 -0.692398 -1.225452 -0.567080   \n",
       "P_0029 -0.495266  0.082892 -1.553595 -1.080348 -0.552315  1.010324 -0.309374   \n",
       "H_0010  0.947469  0.610187  1.167948  1.184255  0.972135  0.467329  1.196569   \n",
       "C_0025 -0.525737  0.242389  0.106966  0.424007  0.286912 -0.878035 -1.127625   \n",
       "C_0070 -1.309004  0.213414 -0.418802 -1.097679 -1.892068  0.063291  0.524590   \n",
       "C_0039  0.076393  0.369525  0.262198 -0.434926 -0.641064 -0.367527 -0.258453   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "P_0063 -0.929789 -0.391042 -0.918295  ... -1.361050 -1.270403  0.770840   \n",
       "P_0029 -1.022790 -0.761766 -0.485853  ... -0.872063 -1.270403 -1.029937   \n",
       "H_0010  0.468982  0.605396  0.755573  ...  1.017267  0.936328  1.164238   \n",
       "C_0025  0.521370  0.787054  0.509760  ... -0.186735  0.740892 -1.029937   \n",
       "C_0070 -1.458774 -2.380129 -1.965949  ... -0.788725 -1.270403 -1.029937   \n",
       "C_0039 -0.416765 -0.246040 -0.748481  ...  0.543181 -1.270403  1.039146   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "P_0063 -0.650630  0.148355 -0.417226  0.656224 -1.557539 -0.349492 -1.420660  \n",
       "P_0029 -1.033890 -0.783647 -0.293198  0.669075 -0.153178 -0.349492  0.354232  \n",
       "H_0010  0.185686 -0.605207  0.286310 -1.220236  0.613593 -0.349492  1.488160  \n",
       "C_0025  0.404844 -0.157597  0.652556  1.144599  0.312478 -0.349492  0.480518  \n",
       "C_0070 -2.301812 -1.832232 -0.425255  0.830336 -1.228546 -0.349492 -0.297272  \n",
       "C_0039  0.081579 -1.086837 -0.351680 -1.220236  0.517124 -0.349492 -1.052990  \n",
       "\n",
       "[6 rows x 21861 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5436e19-add4-4541-ac2d-b0bdf277ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:10:48.096645: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2025-06-03 19:10:48.097138: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3491720000 Hz\n",
      "2025-06-03 19:10:48.262856: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step\n",
      "pearson's correlation cefficient: 0.9918914483252083\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_all_pred=model20dims.predict(dataX_log_del2,verbose=1)\n",
    "plt.figure(figsize=(2.5,2.5))\n",
    "plt.scatter(dataX_log_del2.to_numpy().reshape(-1),X_all_pred.reshape(-1), s=0.01)\n",
    "plt.plot([-10,10],[-10,10], 'g--',linewidth=1.)\n",
    "plt.title('Autoencoder (20 dimensions)')\n",
    "plt.xlabel('input data')\n",
    "plt.ylabel('output data')\n",
    "print(f\"pearson's correlation cefficient: {np.corrcoef(dataX_log_del2.to_numpy().reshape(-1),X_all_pred.reshape(-1))[0,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "432fa7ae-a400-4945-b074-b58f6040f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_coefficient(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denominator = np.sqrt(np.sum((x - x_mean) ** 2) * np.sum((y - y_mean) ** 2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b52e18e8-9157-443a-a00b-937ce9fa4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9877917766094358\n"
     ]
    }
   ],
   "source": [
    "corr_coef_test = correlation_coefficient(df_test.values, X_test_pred)\n",
    "print(corr_coef_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "457c3599-9208-49d3-b2ff-a76677583c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9920659137328623\n"
     ]
    }
   ],
   "source": [
    "corr_coef_tr = correlation_coefficient(df_train.values, X_train_pred)\n",
    "print(corr_coef_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2e07b29-8a4c-44d2-8eb3-a75bb7026677",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'dense_916'\n",
    "latent_layer = Model(inputs = model20dims.input, outputs = model20dims.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2eed3c05-da5b-469b-8484-0c8158f9f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "latent_layer_pred_train = latent_layer.predict(df_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebf8331a-9d21-4bbf-a044-d828e675cc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_layer_pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84377fd5-e0df-4264-beee-fc5c853b668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "latent_layer_pred_test=latent_layer.predict(df_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ae33d08-53bc-4bc5-8363-d62341af0f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 20)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_layer_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d69f775b-1dfd-4d2c-ac08-d35c527ed69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.894204</td>\n",
       "      <td>4.146495</td>\n",
       "      <td>-10.627995</td>\n",
       "      <td>4.333094</td>\n",
       "      <td>-0.211089</td>\n",
       "      <td>-6.648520</td>\n",
       "      <td>-4.280519</td>\n",
       "      <td>-3.815852</td>\n",
       "      <td>-3.777365</td>\n",
       "      <td>-1.499009</td>\n",
       "      <td>-0.625212</td>\n",
       "      <td>-7.137741</td>\n",
       "      <td>-5.564987</td>\n",
       "      <td>1.921789</td>\n",
       "      <td>-6.913729</td>\n",
       "      <td>7.227826</td>\n",
       "      <td>-7.480397</td>\n",
       "      <td>-3.841025</td>\n",
       "      <td>-2.643135</td>\n",
       "      <td>-4.290059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.944945</td>\n",
       "      <td>-4.172675</td>\n",
       "      <td>2.317636</td>\n",
       "      <td>3.224382</td>\n",
       "      <td>2.160191</td>\n",
       "      <td>3.711342</td>\n",
       "      <td>5.053267</td>\n",
       "      <td>-7.703063</td>\n",
       "      <td>8.844687</td>\n",
       "      <td>-1.244160</td>\n",
       "      <td>0.254470</td>\n",
       "      <td>5.201087</td>\n",
       "      <td>-5.925610</td>\n",
       "      <td>4.695848</td>\n",
       "      <td>3.283216</td>\n",
       "      <td>-3.963865</td>\n",
       "      <td>5.261399</td>\n",
       "      <td>-4.708281</td>\n",
       "      <td>2.653075</td>\n",
       "      <td>-5.063646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.588860</td>\n",
       "      <td>-0.694617</td>\n",
       "      <td>2.670450</td>\n",
       "      <td>-8.996168</td>\n",
       "      <td>-3.310686</td>\n",
       "      <td>-7.067754</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>10.910929</td>\n",
       "      <td>2.443390</td>\n",
       "      <td>-4.109570</td>\n",
       "      <td>7.296700</td>\n",
       "      <td>6.122800</td>\n",
       "      <td>6.156366</td>\n",
       "      <td>-1.021480</td>\n",
       "      <td>3.269602</td>\n",
       "      <td>2.010873</td>\n",
       "      <td>1.207476</td>\n",
       "      <td>5.214314</td>\n",
       "      <td>6.464255</td>\n",
       "      <td>-2.690952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.072881</td>\n",
       "      <td>-4.657294</td>\n",
       "      <td>-6.835857</td>\n",
       "      <td>9.598818</td>\n",
       "      <td>1.465891</td>\n",
       "      <td>-5.823493</td>\n",
       "      <td>-5.777774</td>\n",
       "      <td>-7.312018</td>\n",
       "      <td>-0.630546</td>\n",
       "      <td>-6.401748</td>\n",
       "      <td>-6.745100</td>\n",
       "      <td>3.504016</td>\n",
       "      <td>4.543265</td>\n",
       "      <td>6.947121</td>\n",
       "      <td>8.620768</td>\n",
       "      <td>0.387783</td>\n",
       "      <td>-1.047444</td>\n",
       "      <td>3.723100</td>\n",
       "      <td>5.218976</td>\n",
       "      <td>4.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.568182</td>\n",
       "      <td>9.380668</td>\n",
       "      <td>7.979990</td>\n",
       "      <td>1.130456</td>\n",
       "      <td>5.788285</td>\n",
       "      <td>-3.061255</td>\n",
       "      <td>-3.599283</td>\n",
       "      <td>6.892598</td>\n",
       "      <td>2.539938</td>\n",
       "      <td>2.496101</td>\n",
       "      <td>-4.084094</td>\n",
       "      <td>8.950968</td>\n",
       "      <td>-4.972475</td>\n",
       "      <td>-5.133507</td>\n",
       "      <td>-7.145875</td>\n",
       "      <td>-4.389797</td>\n",
       "      <td>6.115207</td>\n",
       "      <td>-6.742897</td>\n",
       "      <td>5.260499</td>\n",
       "      <td>-5.494330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.891827</td>\n",
       "      <td>2.561099</td>\n",
       "      <td>6.970787</td>\n",
       "      <td>-7.018413</td>\n",
       "      <td>-5.253020</td>\n",
       "      <td>3.758243</td>\n",
       "      <td>3.671204</td>\n",
       "      <td>-10.044440</td>\n",
       "      <td>-7.940752</td>\n",
       "      <td>-0.323961</td>\n",
       "      <td>6.115371</td>\n",
       "      <td>-5.004793</td>\n",
       "      <td>1.852236</td>\n",
       "      <td>5.476236</td>\n",
       "      <td>-3.787934</td>\n",
       "      <td>-4.054970</td>\n",
       "      <td>2.748752</td>\n",
       "      <td>6.475208</td>\n",
       "      <td>0.135383</td>\n",
       "      <td>-7.873878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-7.752234</td>\n",
       "      <td>-4.231459</td>\n",
       "      <td>8.498736</td>\n",
       "      <td>-5.204750</td>\n",
       "      <td>-0.988339</td>\n",
       "      <td>2.461777</td>\n",
       "      <td>-1.641066</td>\n",
       "      <td>-7.762600</td>\n",
       "      <td>-0.770963</td>\n",
       "      <td>6.920038</td>\n",
       "      <td>-2.429935</td>\n",
       "      <td>-3.921720</td>\n",
       "      <td>-4.659022</td>\n",
       "      <td>-4.321660</td>\n",
       "      <td>2.040658</td>\n",
       "      <td>-4.948812</td>\n",
       "      <td>7.810775</td>\n",
       "      <td>-6.515944</td>\n",
       "      <td>4.377778</td>\n",
       "      <td>4.249447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.738795</td>\n",
       "      <td>3.508221</td>\n",
       "      <td>-3.084219</td>\n",
       "      <td>-2.055181</td>\n",
       "      <td>-6.202135</td>\n",
       "      <td>-2.324691</td>\n",
       "      <td>-3.598113</td>\n",
       "      <td>-4.881999</td>\n",
       "      <td>4.343721</td>\n",
       "      <td>3.288866</td>\n",
       "      <td>-8.758722</td>\n",
       "      <td>-8.157871</td>\n",
       "      <td>-1.671575</td>\n",
       "      <td>-0.765159</td>\n",
       "      <td>-4.126219</td>\n",
       "      <td>5.952621</td>\n",
       "      <td>-7.953374</td>\n",
       "      <td>-6.093229</td>\n",
       "      <td>4.663666</td>\n",
       "      <td>4.219165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-1.368667</td>\n",
       "      <td>-3.972085</td>\n",
       "      <td>5.433074</td>\n",
       "      <td>6.601264</td>\n",
       "      <td>-6.426833</td>\n",
       "      <td>1.042758</td>\n",
       "      <td>4.944042</td>\n",
       "      <td>7.671537</td>\n",
       "      <td>-5.333508</td>\n",
       "      <td>-7.128211</td>\n",
       "      <td>-5.145265</td>\n",
       "      <td>1.020387</td>\n",
       "      <td>-3.897650</td>\n",
       "      <td>-7.551147</td>\n",
       "      <td>-4.268405</td>\n",
       "      <td>8.827366</td>\n",
       "      <td>-7.952704</td>\n",
       "      <td>4.574883</td>\n",
       "      <td>-4.473346</td>\n",
       "      <td>8.431083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-6.736793</td>\n",
       "      <td>4.325078</td>\n",
       "      <td>7.136744</td>\n",
       "      <td>-9.854633</td>\n",
       "      <td>-3.301502</td>\n",
       "      <td>7.679328</td>\n",
       "      <td>0.639486</td>\n",
       "      <td>-3.328637</td>\n",
       "      <td>2.995985</td>\n",
       "      <td>-4.804451</td>\n",
       "      <td>-3.153152</td>\n",
       "      <td>-5.169599</td>\n",
       "      <td>2.650948</td>\n",
       "      <td>8.221815</td>\n",
       "      <td>10.039940</td>\n",
       "      <td>5.607178</td>\n",
       "      <td>-3.370453</td>\n",
       "      <td>8.624197</td>\n",
       "      <td>1.544150</td>\n",
       "      <td>-6.974489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1          2         3         4         5         6   \\\n",
       "0   -4.894204  4.146495 -10.627995  4.333094 -0.211089 -6.648520 -4.280519   \n",
       "1   -2.944945 -4.172675   2.317636  3.224382  2.160191  3.711342  5.053267   \n",
       "2    5.588860 -0.694617   2.670450 -8.996168 -3.310686 -7.067754  3.733333   \n",
       "3   -7.072881 -4.657294  -6.835857  9.598818  1.465891 -5.823493 -5.777774   \n",
       "4   -5.568182  9.380668   7.979990  1.130456  5.788285 -3.061255 -3.599283   \n",
       "..        ...       ...        ...       ...       ...       ...       ...   \n",
       "97   3.891827  2.561099   6.970787 -7.018413 -5.253020  3.758243  3.671204   \n",
       "98  -7.752234 -4.231459   8.498736 -5.204750 -0.988339  2.461777 -1.641066   \n",
       "99   5.738795  3.508221  -3.084219 -2.055181 -6.202135 -2.324691 -3.598113   \n",
       "100 -1.368667 -3.972085   5.433074  6.601264 -6.426833  1.042758  4.944042   \n",
       "101 -6.736793  4.325078   7.136744 -9.854633 -3.301502  7.679328  0.639486   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "0    -3.815852 -3.777365 -1.499009 -0.625212 -7.137741 -5.564987  1.921789   \n",
       "1    -7.703063  8.844687 -1.244160  0.254470  5.201087 -5.925610  4.695848   \n",
       "2    10.910929  2.443390 -4.109570  7.296700  6.122800  6.156366 -1.021480   \n",
       "3    -7.312018 -0.630546 -6.401748 -6.745100  3.504016  4.543265  6.947121   \n",
       "4     6.892598  2.539938  2.496101 -4.084094  8.950968 -4.972475 -5.133507   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "97  -10.044440 -7.940752 -0.323961  6.115371 -5.004793  1.852236  5.476236   \n",
       "98   -7.762600 -0.770963  6.920038 -2.429935 -3.921720 -4.659022 -4.321660   \n",
       "99   -4.881999  4.343721  3.288866 -8.758722 -8.157871 -1.671575 -0.765159   \n",
       "100   7.671537 -5.333508 -7.128211 -5.145265  1.020387 -3.897650 -7.551147   \n",
       "101  -3.328637  2.995985 -4.804451 -3.153152 -5.169599  2.650948  8.221815   \n",
       "\n",
       "            14        15        16        17        18        19  \n",
       "0    -6.913729  7.227826 -7.480397 -3.841025 -2.643135 -4.290059  \n",
       "1     3.283216 -3.963865  5.261399 -4.708281  2.653075 -5.063646  \n",
       "2     3.269602  2.010873  1.207476  5.214314  6.464255 -2.690952  \n",
       "3     8.620768  0.387783 -1.047444  3.723100  5.218976  4.335200  \n",
       "4    -7.145875 -4.389797  6.115207 -6.742897  5.260499 -5.494330  \n",
       "..         ...       ...       ...       ...       ...       ...  \n",
       "97   -3.787934 -4.054970  2.748752  6.475208  0.135383 -7.873878  \n",
       "98    2.040658 -4.948812  7.810775 -6.515944  4.377778  4.249447  \n",
       "99   -4.126219  5.952621 -7.953374 -6.093229  4.663666  4.219165  \n",
       "100  -4.268405  8.827366 -7.952704  4.574883 -4.473346  8.431083  \n",
       "101  10.039940  5.607178 -3.370453  8.624197  1.544150 -6.974489  \n",
       "\n",
       "[102 rows x 20 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_20dims=pd.DataFrame(latent_layer_pred_train)\n",
    "X_train_20dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbcce1c4-2df4-4836-bb4f-6067448add23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.376636</td>\n",
       "      <td>6.685627</td>\n",
       "      <td>8.486855</td>\n",
       "      <td>-5.005437</td>\n",
       "      <td>0.986963</td>\n",
       "      <td>-2.510396</td>\n",
       "      <td>0.891873</td>\n",
       "      <td>-5.870406</td>\n",
       "      <td>-4.658065</td>\n",
       "      <td>5.980363</td>\n",
       "      <td>6.565961</td>\n",
       "      <td>-5.318729</td>\n",
       "      <td>-6.247756</td>\n",
       "      <td>-2.401874</td>\n",
       "      <td>-9.539203</td>\n",
       "      <td>4.514901</td>\n",
       "      <td>-6.644996</td>\n",
       "      <td>-1.029745</td>\n",
       "      <td>5.043787</td>\n",
       "      <td>-7.681988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.641945</td>\n",
       "      <td>-7.661404</td>\n",
       "      <td>7.613710</td>\n",
       "      <td>-1.599616</td>\n",
       "      <td>7.090220</td>\n",
       "      <td>6.726690</td>\n",
       "      <td>-2.069273</td>\n",
       "      <td>-2.745076</td>\n",
       "      <td>2.530561</td>\n",
       "      <td>2.660381</td>\n",
       "      <td>7.137661</td>\n",
       "      <td>8.682987</td>\n",
       "      <td>6.308665</td>\n",
       "      <td>5.456786</td>\n",
       "      <td>8.533322</td>\n",
       "      <td>-3.954587</td>\n",
       "      <td>6.808384</td>\n",
       "      <td>-5.964357</td>\n",
       "      <td>-6.926980</td>\n",
       "      <td>-4.553753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.359315</td>\n",
       "      <td>-4.861623</td>\n",
       "      <td>-5.498817</td>\n",
       "      <td>6.374908</td>\n",
       "      <td>1.541136</td>\n",
       "      <td>-6.441969</td>\n",
       "      <td>6.099424</td>\n",
       "      <td>5.147042</td>\n",
       "      <td>-2.580868</td>\n",
       "      <td>-4.977870</td>\n",
       "      <td>6.729605</td>\n",
       "      <td>-3.195096</td>\n",
       "      <td>5.148294</td>\n",
       "      <td>-4.076187</td>\n",
       "      <td>-7.195958</td>\n",
       "      <td>6.914922</td>\n",
       "      <td>-6.155535</td>\n",
       "      <td>4.881458</td>\n",
       "      <td>-7.642689</td>\n",
       "      <td>-8.291637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.756402</td>\n",
       "      <td>0.441134</td>\n",
       "      <td>-6.646461</td>\n",
       "      <td>1.472855</td>\n",
       "      <td>-6.947612</td>\n",
       "      <td>7.541595</td>\n",
       "      <td>4.625729</td>\n",
       "      <td>-6.928267</td>\n",
       "      <td>4.398932</td>\n",
       "      <td>1.071311</td>\n",
       "      <td>-1.451650</td>\n",
       "      <td>-5.603864</td>\n",
       "      <td>4.042096</td>\n",
       "      <td>-0.611265</td>\n",
       "      <td>-1.356958</td>\n",
       "      <td>-5.764071</td>\n",
       "      <td>-6.192120</td>\n",
       "      <td>-8.153080</td>\n",
       "      <td>-6.249497</td>\n",
       "      <td>5.730522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938262</td>\n",
       "      <td>-3.290774</td>\n",
       "      <td>6.039089</td>\n",
       "      <td>-9.971478</td>\n",
       "      <td>4.504210</td>\n",
       "      <td>-7.470418</td>\n",
       "      <td>-7.287096</td>\n",
       "      <td>-3.256257</td>\n",
       "      <td>-2.944622</td>\n",
       "      <td>1.461293</td>\n",
       "      <td>7.112204</td>\n",
       "      <td>5.424419</td>\n",
       "      <td>-4.540638</td>\n",
       "      <td>-0.372598</td>\n",
       "      <td>-7.300920</td>\n",
       "      <td>-0.119995</td>\n",
       "      <td>-5.317137</td>\n",
       "      <td>-6.542215</td>\n",
       "      <td>-6.035981</td>\n",
       "      <td>4.083077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-6.365646</td>\n",
       "      <td>8.686355</td>\n",
       "      <td>-3.822531</td>\n",
       "      <td>-7.310308</td>\n",
       "      <td>-3.612169</td>\n",
       "      <td>-2.193326</td>\n",
       "      <td>-4.453616</td>\n",
       "      <td>-3.896858</td>\n",
       "      <td>2.272247</td>\n",
       "      <td>3.491675</td>\n",
       "      <td>8.014971</td>\n",
       "      <td>-7.702356</td>\n",
       "      <td>1.721457</td>\n",
       "      <td>-6.695879</td>\n",
       "      <td>-4.547217</td>\n",
       "      <td>-7.225779</td>\n",
       "      <td>2.713670</td>\n",
       "      <td>4.677006</td>\n",
       "      <td>1.589107</td>\n",
       "      <td>-5.289326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -6.376636  6.685627  8.486855 -5.005437  0.986963 -2.510396  0.891873   \n",
       "1  3.641945 -7.661404  7.613710 -1.599616  7.090220  6.726690 -2.069273   \n",
       "2  3.359315 -4.861623 -5.498817  6.374908  1.541136 -6.441969  6.099424   \n",
       "3 -0.756402  0.441134 -6.646461  1.472855 -6.947612  7.541595  4.625729   \n",
       "4  0.938262 -3.290774  6.039089 -9.971478  4.504210 -7.470418 -7.287096   \n",
       "5 -6.365646  8.686355 -3.822531 -7.310308 -3.612169 -2.193326 -4.453616   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0 -5.870406 -4.658065  5.980363  6.565961 -5.318729 -6.247756 -2.401874   \n",
       "1 -2.745076  2.530561  2.660381  7.137661  8.682987  6.308665  5.456786   \n",
       "2  5.147042 -2.580868 -4.977870  6.729605 -3.195096  5.148294 -4.076187   \n",
       "3 -6.928267  4.398932  1.071311 -1.451650 -5.603864  4.042096 -0.611265   \n",
       "4 -3.256257 -2.944622  1.461293  7.112204  5.424419 -4.540638 -0.372598   \n",
       "5 -3.896858  2.272247  3.491675  8.014971 -7.702356  1.721457 -6.695879   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0 -9.539203  4.514901 -6.644996 -1.029745  5.043787 -7.681988  \n",
       "1  8.533322 -3.954587  6.808384 -5.964357 -6.926980 -4.553753  \n",
       "2 -7.195958  6.914922 -6.155535  4.881458 -7.642689 -8.291637  \n",
       "3 -1.356958 -5.764071 -6.192120 -8.153080 -6.249497  5.730522  \n",
       "4 -7.300920 -0.119995 -5.317137 -6.542215 -6.035981  4.083077  \n",
       "5 -4.547217 -7.225779  2.713670  4.677006  1.589107 -5.289326  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_20dims=pd.DataFrame(latent_layer_pred_test)\n",
    "X_test_20dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8aae93f1-62b7-40b9-9860-42389058345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20dims['patient number'] = X_train.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c10b056a-bf94-4933-930e-9d3e5aae1891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>patient number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.894204</td>\n",
       "      <td>4.146495</td>\n",
       "      <td>-10.627995</td>\n",
       "      <td>4.333094</td>\n",
       "      <td>-0.211089</td>\n",
       "      <td>-6.648520</td>\n",
       "      <td>-4.280519</td>\n",
       "      <td>-3.815852</td>\n",
       "      <td>-3.777365</td>\n",
       "      <td>-1.499009</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.137741</td>\n",
       "      <td>-5.564987</td>\n",
       "      <td>1.921789</td>\n",
       "      <td>-6.913729</td>\n",
       "      <td>7.227826</td>\n",
       "      <td>-7.480397</td>\n",
       "      <td>-3.841025</td>\n",
       "      <td>-2.643135</td>\n",
       "      <td>-4.290059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.944945</td>\n",
       "      <td>-4.172675</td>\n",
       "      <td>2.317636</td>\n",
       "      <td>3.224382</td>\n",
       "      <td>2.160191</td>\n",
       "      <td>3.711342</td>\n",
       "      <td>5.053267</td>\n",
       "      <td>-7.703063</td>\n",
       "      <td>8.844687</td>\n",
       "      <td>-1.244160</td>\n",
       "      <td>...</td>\n",
       "      <td>5.201087</td>\n",
       "      <td>-5.925610</td>\n",
       "      <td>4.695848</td>\n",
       "      <td>3.283216</td>\n",
       "      <td>-3.963865</td>\n",
       "      <td>5.261399</td>\n",
       "      <td>-4.708281</td>\n",
       "      <td>2.653075</td>\n",
       "      <td>-5.063646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.588860</td>\n",
       "      <td>-0.694617</td>\n",
       "      <td>2.670450</td>\n",
       "      <td>-8.996168</td>\n",
       "      <td>-3.310686</td>\n",
       "      <td>-7.067754</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>10.910929</td>\n",
       "      <td>2.443390</td>\n",
       "      <td>-4.109570</td>\n",
       "      <td>...</td>\n",
       "      <td>6.122800</td>\n",
       "      <td>6.156366</td>\n",
       "      <td>-1.021480</td>\n",
       "      <td>3.269602</td>\n",
       "      <td>2.010873</td>\n",
       "      <td>1.207476</td>\n",
       "      <td>5.214314</td>\n",
       "      <td>6.464255</td>\n",
       "      <td>-2.690952</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.072881</td>\n",
       "      <td>-4.657294</td>\n",
       "      <td>-6.835857</td>\n",
       "      <td>9.598818</td>\n",
       "      <td>1.465891</td>\n",
       "      <td>-5.823493</td>\n",
       "      <td>-5.777774</td>\n",
       "      <td>-7.312018</td>\n",
       "      <td>-0.630546</td>\n",
       "      <td>-6.401748</td>\n",
       "      <td>...</td>\n",
       "      <td>3.504016</td>\n",
       "      <td>4.543265</td>\n",
       "      <td>6.947121</td>\n",
       "      <td>8.620768</td>\n",
       "      <td>0.387783</td>\n",
       "      <td>-1.047444</td>\n",
       "      <td>3.723100</td>\n",
       "      <td>5.218976</td>\n",
       "      <td>4.335200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.568182</td>\n",
       "      <td>9.380668</td>\n",
       "      <td>7.979990</td>\n",
       "      <td>1.130456</td>\n",
       "      <td>5.788285</td>\n",
       "      <td>-3.061255</td>\n",
       "      <td>-3.599283</td>\n",
       "      <td>6.892598</td>\n",
       "      <td>2.539938</td>\n",
       "      <td>2.496101</td>\n",
       "      <td>...</td>\n",
       "      <td>8.950968</td>\n",
       "      <td>-4.972475</td>\n",
       "      <td>-5.133507</td>\n",
       "      <td>-7.145875</td>\n",
       "      <td>-4.389797</td>\n",
       "      <td>6.115207</td>\n",
       "      <td>-6.742897</td>\n",
       "      <td>5.260499</td>\n",
       "      <td>-5.494330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.891827</td>\n",
       "      <td>2.561099</td>\n",
       "      <td>6.970787</td>\n",
       "      <td>-7.018413</td>\n",
       "      <td>-5.253020</td>\n",
       "      <td>3.758243</td>\n",
       "      <td>3.671204</td>\n",
       "      <td>-10.044440</td>\n",
       "      <td>-7.940752</td>\n",
       "      <td>-0.323961</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.004793</td>\n",
       "      <td>1.852236</td>\n",
       "      <td>5.476236</td>\n",
       "      <td>-3.787934</td>\n",
       "      <td>-4.054970</td>\n",
       "      <td>2.748752</td>\n",
       "      <td>6.475208</td>\n",
       "      <td>0.135383</td>\n",
       "      <td>-7.873878</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-7.752234</td>\n",
       "      <td>-4.231459</td>\n",
       "      <td>8.498736</td>\n",
       "      <td>-5.204750</td>\n",
       "      <td>-0.988339</td>\n",
       "      <td>2.461777</td>\n",
       "      <td>-1.641066</td>\n",
       "      <td>-7.762600</td>\n",
       "      <td>-0.770963</td>\n",
       "      <td>6.920038</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.921720</td>\n",
       "      <td>-4.659022</td>\n",
       "      <td>-4.321660</td>\n",
       "      <td>2.040658</td>\n",
       "      <td>-4.948812</td>\n",
       "      <td>7.810775</td>\n",
       "      <td>-6.515944</td>\n",
       "      <td>4.377778</td>\n",
       "      <td>4.249447</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.738795</td>\n",
       "      <td>3.508221</td>\n",
       "      <td>-3.084219</td>\n",
       "      <td>-2.055181</td>\n",
       "      <td>-6.202135</td>\n",
       "      <td>-2.324691</td>\n",
       "      <td>-3.598113</td>\n",
       "      <td>-4.881999</td>\n",
       "      <td>4.343721</td>\n",
       "      <td>3.288866</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.157871</td>\n",
       "      <td>-1.671575</td>\n",
       "      <td>-0.765159</td>\n",
       "      <td>-4.126219</td>\n",
       "      <td>5.952621</td>\n",
       "      <td>-7.953374</td>\n",
       "      <td>-6.093229</td>\n",
       "      <td>4.663666</td>\n",
       "      <td>4.219165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-1.368667</td>\n",
       "      <td>-3.972085</td>\n",
       "      <td>5.433074</td>\n",
       "      <td>6.601264</td>\n",
       "      <td>-6.426833</td>\n",
       "      <td>1.042758</td>\n",
       "      <td>4.944042</td>\n",
       "      <td>7.671537</td>\n",
       "      <td>-5.333508</td>\n",
       "      <td>-7.128211</td>\n",
       "      <td>...</td>\n",
       "      <td>1.020387</td>\n",
       "      <td>-3.897650</td>\n",
       "      <td>-7.551147</td>\n",
       "      <td>-4.268405</td>\n",
       "      <td>8.827366</td>\n",
       "      <td>-7.952704</td>\n",
       "      <td>4.574883</td>\n",
       "      <td>-4.473346</td>\n",
       "      <td>8.431083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-6.736793</td>\n",
       "      <td>4.325078</td>\n",
       "      <td>7.136744</td>\n",
       "      <td>-9.854633</td>\n",
       "      <td>-3.301502</td>\n",
       "      <td>7.679328</td>\n",
       "      <td>0.639486</td>\n",
       "      <td>-3.328637</td>\n",
       "      <td>2.995985</td>\n",
       "      <td>-4.804451</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.169599</td>\n",
       "      <td>2.650948</td>\n",
       "      <td>8.221815</td>\n",
       "      <td>10.039940</td>\n",
       "      <td>5.607178</td>\n",
       "      <td>-3.370453</td>\n",
       "      <td>8.624197</td>\n",
       "      <td>1.544150</td>\n",
       "      <td>-6.974489</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1          2         3         4         5         6  \\\n",
       "0   -4.894204  4.146495 -10.627995  4.333094 -0.211089 -6.648520 -4.280519   \n",
       "1   -2.944945 -4.172675   2.317636  3.224382  2.160191  3.711342  5.053267   \n",
       "2    5.588860 -0.694617   2.670450 -8.996168 -3.310686 -7.067754  3.733333   \n",
       "3   -7.072881 -4.657294  -6.835857  9.598818  1.465891 -5.823493 -5.777774   \n",
       "4   -5.568182  9.380668   7.979990  1.130456  5.788285 -3.061255 -3.599283   \n",
       "..        ...       ...        ...       ...       ...       ...       ...   \n",
       "97   3.891827  2.561099   6.970787 -7.018413 -5.253020  3.758243  3.671204   \n",
       "98  -7.752234 -4.231459   8.498736 -5.204750 -0.988339  2.461777 -1.641066   \n",
       "99   5.738795  3.508221  -3.084219 -2.055181 -6.202135 -2.324691 -3.598113   \n",
       "100 -1.368667 -3.972085   5.433074  6.601264 -6.426833  1.042758  4.944042   \n",
       "101 -6.736793  4.325078   7.136744 -9.854633 -3.301502  7.679328  0.639486   \n",
       "\n",
       "             7         8         9  ...        11        12        13  \\\n",
       "0    -3.815852 -3.777365 -1.499009  ... -7.137741 -5.564987  1.921789   \n",
       "1    -7.703063  8.844687 -1.244160  ...  5.201087 -5.925610  4.695848   \n",
       "2    10.910929  2.443390 -4.109570  ...  6.122800  6.156366 -1.021480   \n",
       "3    -7.312018 -0.630546 -6.401748  ...  3.504016  4.543265  6.947121   \n",
       "4     6.892598  2.539938  2.496101  ...  8.950968 -4.972475 -5.133507   \n",
       "..         ...       ...       ...  ...       ...       ...       ...   \n",
       "97  -10.044440 -7.940752 -0.323961  ... -5.004793  1.852236  5.476236   \n",
       "98   -7.762600 -0.770963  6.920038  ... -3.921720 -4.659022 -4.321660   \n",
       "99   -4.881999  4.343721  3.288866  ... -8.157871 -1.671575 -0.765159   \n",
       "100   7.671537 -5.333508 -7.128211  ...  1.020387 -3.897650 -7.551147   \n",
       "101  -3.328637  2.995985 -4.804451  ... -5.169599  2.650948  8.221815   \n",
       "\n",
       "            14        15        16        17        18        19  \\\n",
       "0    -6.913729  7.227826 -7.480397 -3.841025 -2.643135 -4.290059   \n",
       "1     3.283216 -3.963865  5.261399 -4.708281  2.653075 -5.063646   \n",
       "2     3.269602  2.010873  1.207476  5.214314  6.464255 -2.690952   \n",
       "3     8.620768  0.387783 -1.047444  3.723100  5.218976  4.335200   \n",
       "4    -7.145875 -4.389797  6.115207 -6.742897  5.260499 -5.494330   \n",
       "..         ...       ...       ...       ...       ...       ...   \n",
       "97   -3.787934 -4.054970  2.748752  6.475208  0.135383 -7.873878   \n",
       "98    2.040658 -4.948812  7.810775 -6.515944  4.377778  4.249447   \n",
       "99   -4.126219  5.952621 -7.953374 -6.093229  4.663666  4.219165   \n",
       "100  -4.268405  8.827366 -7.952704  4.574883 -4.473346  8.431083   \n",
       "101  10.039940  5.607178 -3.370453  8.624197  1.544150 -6.974489   \n",
       "\n",
       "     patient number  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 2  \n",
       "3                 1  \n",
       "4                 0  \n",
       "..              ...  \n",
       "97                2  \n",
       "98                0  \n",
       "99                1  \n",
       "100               1  \n",
       "101               2  \n",
       "\n",
       "[102 rows x 21 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_20dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71af9251-7a47-4df6-8cc0-4f7e1ae1d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_20dims['patient number'] = X_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36f4c401-c477-43e5-b651-8b924fd4865c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>patient number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.376636</td>\n",
       "      <td>6.685627</td>\n",
       "      <td>8.486855</td>\n",
       "      <td>-5.005437</td>\n",
       "      <td>0.986963</td>\n",
       "      <td>-2.510396</td>\n",
       "      <td>0.891873</td>\n",
       "      <td>-5.870406</td>\n",
       "      <td>-4.658065</td>\n",
       "      <td>5.980363</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.318729</td>\n",
       "      <td>-6.247756</td>\n",
       "      <td>-2.401874</td>\n",
       "      <td>-9.539203</td>\n",
       "      <td>4.514901</td>\n",
       "      <td>-6.644996</td>\n",
       "      <td>-1.029745</td>\n",
       "      <td>5.043787</td>\n",
       "      <td>-7.681988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.641945</td>\n",
       "      <td>-7.661404</td>\n",
       "      <td>7.613710</td>\n",
       "      <td>-1.599616</td>\n",
       "      <td>7.090220</td>\n",
       "      <td>6.726690</td>\n",
       "      <td>-2.069273</td>\n",
       "      <td>-2.745076</td>\n",
       "      <td>2.530561</td>\n",
       "      <td>2.660381</td>\n",
       "      <td>...</td>\n",
       "      <td>8.682987</td>\n",
       "      <td>6.308665</td>\n",
       "      <td>5.456786</td>\n",
       "      <td>8.533322</td>\n",
       "      <td>-3.954587</td>\n",
       "      <td>6.808384</td>\n",
       "      <td>-5.964357</td>\n",
       "      <td>-6.926980</td>\n",
       "      <td>-4.553753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.359315</td>\n",
       "      <td>-4.861623</td>\n",
       "      <td>-5.498817</td>\n",
       "      <td>6.374908</td>\n",
       "      <td>1.541136</td>\n",
       "      <td>-6.441969</td>\n",
       "      <td>6.099424</td>\n",
       "      <td>5.147042</td>\n",
       "      <td>-2.580868</td>\n",
       "      <td>-4.977870</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.195096</td>\n",
       "      <td>5.148294</td>\n",
       "      <td>-4.076187</td>\n",
       "      <td>-7.195958</td>\n",
       "      <td>6.914922</td>\n",
       "      <td>-6.155535</td>\n",
       "      <td>4.881458</td>\n",
       "      <td>-7.642689</td>\n",
       "      <td>-8.291637</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.756402</td>\n",
       "      <td>0.441134</td>\n",
       "      <td>-6.646461</td>\n",
       "      <td>1.472855</td>\n",
       "      <td>-6.947612</td>\n",
       "      <td>7.541595</td>\n",
       "      <td>4.625729</td>\n",
       "      <td>-6.928267</td>\n",
       "      <td>4.398932</td>\n",
       "      <td>1.071311</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.603864</td>\n",
       "      <td>4.042096</td>\n",
       "      <td>-0.611265</td>\n",
       "      <td>-1.356958</td>\n",
       "      <td>-5.764071</td>\n",
       "      <td>-6.192120</td>\n",
       "      <td>-8.153080</td>\n",
       "      <td>-6.249497</td>\n",
       "      <td>5.730522</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.938262</td>\n",
       "      <td>-3.290774</td>\n",
       "      <td>6.039089</td>\n",
       "      <td>-9.971478</td>\n",
       "      <td>4.504210</td>\n",
       "      <td>-7.470418</td>\n",
       "      <td>-7.287096</td>\n",
       "      <td>-3.256257</td>\n",
       "      <td>-2.944622</td>\n",
       "      <td>1.461293</td>\n",
       "      <td>...</td>\n",
       "      <td>5.424419</td>\n",
       "      <td>-4.540638</td>\n",
       "      <td>-0.372598</td>\n",
       "      <td>-7.300920</td>\n",
       "      <td>-0.119995</td>\n",
       "      <td>-5.317137</td>\n",
       "      <td>-6.542215</td>\n",
       "      <td>-6.035981</td>\n",
       "      <td>4.083077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-6.365646</td>\n",
       "      <td>8.686355</td>\n",
       "      <td>-3.822531</td>\n",
       "      <td>-7.310308</td>\n",
       "      <td>-3.612169</td>\n",
       "      <td>-2.193326</td>\n",
       "      <td>-4.453616</td>\n",
       "      <td>-3.896858</td>\n",
       "      <td>2.272247</td>\n",
       "      <td>3.491675</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.702356</td>\n",
       "      <td>1.721457</td>\n",
       "      <td>-6.695879</td>\n",
       "      <td>-4.547217</td>\n",
       "      <td>-7.225779</td>\n",
       "      <td>2.713670</td>\n",
       "      <td>4.677006</td>\n",
       "      <td>1.589107</td>\n",
       "      <td>-5.289326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -6.376636  6.685627  8.486855 -5.005437  0.986963 -2.510396  0.891873   \n",
       "1  3.641945 -7.661404  7.613710 -1.599616  7.090220  6.726690 -2.069273   \n",
       "2  3.359315 -4.861623 -5.498817  6.374908  1.541136 -6.441969  6.099424   \n",
       "3 -0.756402  0.441134 -6.646461  1.472855 -6.947612  7.541595  4.625729   \n",
       "4  0.938262 -3.290774  6.039089 -9.971478  4.504210 -7.470418 -7.287096   \n",
       "5 -6.365646  8.686355 -3.822531 -7.310308 -3.612169 -2.193326 -4.453616   \n",
       "\n",
       "          7         8         9  ...        11        12        13        14  \\\n",
       "0 -5.870406 -4.658065  5.980363  ... -5.318729 -6.247756 -2.401874 -9.539203   \n",
       "1 -2.745076  2.530561  2.660381  ...  8.682987  6.308665  5.456786  8.533322   \n",
       "2  5.147042 -2.580868 -4.977870  ... -3.195096  5.148294 -4.076187 -7.195958   \n",
       "3 -6.928267  4.398932  1.071311  ... -5.603864  4.042096 -0.611265 -1.356958   \n",
       "4 -3.256257 -2.944622  1.461293  ...  5.424419 -4.540638 -0.372598 -7.300920   \n",
       "5 -3.896858  2.272247  3.491675  ... -7.702356  1.721457 -6.695879 -4.547217   \n",
       "\n",
       "         15        16        17        18        19  patient number  \n",
       "0  4.514901 -6.644996 -1.029745  5.043787 -7.681988               0  \n",
       "1 -3.954587  6.808384 -5.964357 -6.926980 -4.553753               0  \n",
       "2  6.914922 -6.155535  4.881458 -7.642689 -8.291637               2  \n",
       "3 -5.764071 -6.192120 -8.153080 -6.249497  5.730522               1  \n",
       "4 -0.119995 -5.317137 -6.542215 -6.035981  4.083077               1  \n",
       "5 -7.225779  2.713670  4.677006  1.589107 -5.289326               1  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_20dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d909235-1b7c-492d-b476-e6abdf821535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>patient number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.894204</td>\n",
       "      <td>4.146495</td>\n",
       "      <td>-10.627995</td>\n",
       "      <td>4.333094</td>\n",
       "      <td>-0.211089</td>\n",
       "      <td>-6.648520</td>\n",
       "      <td>-4.280519</td>\n",
       "      <td>-3.815852</td>\n",
       "      <td>-3.777365</td>\n",
       "      <td>-1.499009</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.137741</td>\n",
       "      <td>-5.564987</td>\n",
       "      <td>1.921789</td>\n",
       "      <td>-6.913729</td>\n",
       "      <td>7.227826</td>\n",
       "      <td>-7.480397</td>\n",
       "      <td>-3.841025</td>\n",
       "      <td>-2.643135</td>\n",
       "      <td>-4.290059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.944945</td>\n",
       "      <td>-4.172675</td>\n",
       "      <td>2.317636</td>\n",
       "      <td>3.224382</td>\n",
       "      <td>2.160191</td>\n",
       "      <td>3.711342</td>\n",
       "      <td>5.053267</td>\n",
       "      <td>-7.703063</td>\n",
       "      <td>8.844687</td>\n",
       "      <td>-1.244160</td>\n",
       "      <td>...</td>\n",
       "      <td>5.201087</td>\n",
       "      <td>-5.925610</td>\n",
       "      <td>4.695848</td>\n",
       "      <td>3.283216</td>\n",
       "      <td>-3.963865</td>\n",
       "      <td>5.261399</td>\n",
       "      <td>-4.708281</td>\n",
       "      <td>2.653075</td>\n",
       "      <td>-5.063646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.588860</td>\n",
       "      <td>-0.694617</td>\n",
       "      <td>2.670450</td>\n",
       "      <td>-8.996168</td>\n",
       "      <td>-3.310686</td>\n",
       "      <td>-7.067754</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>10.910929</td>\n",
       "      <td>2.443390</td>\n",
       "      <td>-4.109570</td>\n",
       "      <td>...</td>\n",
       "      <td>6.122800</td>\n",
       "      <td>6.156366</td>\n",
       "      <td>-1.021480</td>\n",
       "      <td>3.269602</td>\n",
       "      <td>2.010873</td>\n",
       "      <td>1.207476</td>\n",
       "      <td>5.214314</td>\n",
       "      <td>6.464255</td>\n",
       "      <td>-2.690952</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.072881</td>\n",
       "      <td>-4.657294</td>\n",
       "      <td>-6.835857</td>\n",
       "      <td>9.598818</td>\n",
       "      <td>1.465891</td>\n",
       "      <td>-5.823493</td>\n",
       "      <td>-5.777774</td>\n",
       "      <td>-7.312018</td>\n",
       "      <td>-0.630546</td>\n",
       "      <td>-6.401748</td>\n",
       "      <td>...</td>\n",
       "      <td>3.504016</td>\n",
       "      <td>4.543265</td>\n",
       "      <td>6.947121</td>\n",
       "      <td>8.620768</td>\n",
       "      <td>0.387783</td>\n",
       "      <td>-1.047444</td>\n",
       "      <td>3.723100</td>\n",
       "      <td>5.218976</td>\n",
       "      <td>4.335200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.568182</td>\n",
       "      <td>9.380668</td>\n",
       "      <td>7.979990</td>\n",
       "      <td>1.130456</td>\n",
       "      <td>5.788285</td>\n",
       "      <td>-3.061255</td>\n",
       "      <td>-3.599283</td>\n",
       "      <td>6.892598</td>\n",
       "      <td>2.539938</td>\n",
       "      <td>2.496101</td>\n",
       "      <td>...</td>\n",
       "      <td>8.950968</td>\n",
       "      <td>-4.972475</td>\n",
       "      <td>-5.133507</td>\n",
       "      <td>-7.145875</td>\n",
       "      <td>-4.389797</td>\n",
       "      <td>6.115207</td>\n",
       "      <td>-6.742897</td>\n",
       "      <td>5.260499</td>\n",
       "      <td>-5.494330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>3.641945</td>\n",
       "      <td>-7.661404</td>\n",
       "      <td>7.613710</td>\n",
       "      <td>-1.599616</td>\n",
       "      <td>7.090220</td>\n",
       "      <td>6.726690</td>\n",
       "      <td>-2.069273</td>\n",
       "      <td>-2.745076</td>\n",
       "      <td>2.530561</td>\n",
       "      <td>2.660381</td>\n",
       "      <td>...</td>\n",
       "      <td>8.682987</td>\n",
       "      <td>6.308665</td>\n",
       "      <td>5.456786</td>\n",
       "      <td>8.533322</td>\n",
       "      <td>-3.954587</td>\n",
       "      <td>6.808384</td>\n",
       "      <td>-5.964357</td>\n",
       "      <td>-6.926980</td>\n",
       "      <td>-4.553753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3.359315</td>\n",
       "      <td>-4.861623</td>\n",
       "      <td>-5.498817</td>\n",
       "      <td>6.374908</td>\n",
       "      <td>1.541136</td>\n",
       "      <td>-6.441969</td>\n",
       "      <td>6.099424</td>\n",
       "      <td>5.147042</td>\n",
       "      <td>-2.580868</td>\n",
       "      <td>-4.977870</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.195096</td>\n",
       "      <td>5.148294</td>\n",
       "      <td>-4.076187</td>\n",
       "      <td>-7.195958</td>\n",
       "      <td>6.914922</td>\n",
       "      <td>-6.155535</td>\n",
       "      <td>4.881458</td>\n",
       "      <td>-7.642689</td>\n",
       "      <td>-8.291637</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-0.756402</td>\n",
       "      <td>0.441134</td>\n",
       "      <td>-6.646461</td>\n",
       "      <td>1.472855</td>\n",
       "      <td>-6.947612</td>\n",
       "      <td>7.541595</td>\n",
       "      <td>4.625729</td>\n",
       "      <td>-6.928267</td>\n",
       "      <td>4.398932</td>\n",
       "      <td>1.071311</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.603864</td>\n",
       "      <td>4.042096</td>\n",
       "      <td>-0.611265</td>\n",
       "      <td>-1.356958</td>\n",
       "      <td>-5.764071</td>\n",
       "      <td>-6.192120</td>\n",
       "      <td>-8.153080</td>\n",
       "      <td>-6.249497</td>\n",
       "      <td>5.730522</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.938262</td>\n",
       "      <td>-3.290774</td>\n",
       "      <td>6.039089</td>\n",
       "      <td>-9.971478</td>\n",
       "      <td>4.504210</td>\n",
       "      <td>-7.470418</td>\n",
       "      <td>-7.287096</td>\n",
       "      <td>-3.256257</td>\n",
       "      <td>-2.944622</td>\n",
       "      <td>1.461293</td>\n",
       "      <td>...</td>\n",
       "      <td>5.424419</td>\n",
       "      <td>-4.540638</td>\n",
       "      <td>-0.372598</td>\n",
       "      <td>-7.300920</td>\n",
       "      <td>-0.119995</td>\n",
       "      <td>-5.317137</td>\n",
       "      <td>-6.542215</td>\n",
       "      <td>-6.035981</td>\n",
       "      <td>4.083077</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-6.365646</td>\n",
       "      <td>8.686355</td>\n",
       "      <td>-3.822531</td>\n",
       "      <td>-7.310308</td>\n",
       "      <td>-3.612169</td>\n",
       "      <td>-2.193326</td>\n",
       "      <td>-4.453616</td>\n",
       "      <td>-3.896858</td>\n",
       "      <td>2.272247</td>\n",
       "      <td>3.491675</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.702356</td>\n",
       "      <td>1.721457</td>\n",
       "      <td>-6.695879</td>\n",
       "      <td>-4.547217</td>\n",
       "      <td>-7.225779</td>\n",
       "      <td>2.713670</td>\n",
       "      <td>4.677006</td>\n",
       "      <td>1.589107</td>\n",
       "      <td>-5.289326</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1          2         3         4         5         6  \\\n",
       "0   -4.894204  4.146495 -10.627995  4.333094 -0.211089 -6.648520 -4.280519   \n",
       "1   -2.944945 -4.172675   2.317636  3.224382  2.160191  3.711342  5.053267   \n",
       "2    5.588860 -0.694617   2.670450 -8.996168 -3.310686 -7.067754  3.733333   \n",
       "3   -7.072881 -4.657294  -6.835857  9.598818  1.465891 -5.823493 -5.777774   \n",
       "4   -5.568182  9.380668   7.979990  1.130456  5.788285 -3.061255 -3.599283   \n",
       "..        ...       ...        ...       ...       ...       ...       ...   \n",
       "103  3.641945 -7.661404   7.613710 -1.599616  7.090220  6.726690 -2.069273   \n",
       "104  3.359315 -4.861623  -5.498817  6.374908  1.541136 -6.441969  6.099424   \n",
       "105 -0.756402  0.441134  -6.646461  1.472855 -6.947612  7.541595  4.625729   \n",
       "106  0.938262 -3.290774   6.039089 -9.971478  4.504210 -7.470418 -7.287096   \n",
       "107 -6.365646  8.686355  -3.822531 -7.310308 -3.612169 -2.193326 -4.453616   \n",
       "\n",
       "             7         8         9  ...        11        12        13  \\\n",
       "0    -3.815852 -3.777365 -1.499009  ... -7.137741 -5.564987  1.921789   \n",
       "1    -7.703063  8.844687 -1.244160  ...  5.201087 -5.925610  4.695848   \n",
       "2    10.910929  2.443390 -4.109570  ...  6.122800  6.156366 -1.021480   \n",
       "3    -7.312018 -0.630546 -6.401748  ...  3.504016  4.543265  6.947121   \n",
       "4     6.892598  2.539938  2.496101  ...  8.950968 -4.972475 -5.133507   \n",
       "..         ...       ...       ...  ...       ...       ...       ...   \n",
       "103  -2.745076  2.530561  2.660381  ...  8.682987  6.308665  5.456786   \n",
       "104   5.147042 -2.580868 -4.977870  ... -3.195096  5.148294 -4.076187   \n",
       "105  -6.928267  4.398932  1.071311  ... -5.603864  4.042096 -0.611265   \n",
       "106  -3.256257 -2.944622  1.461293  ...  5.424419 -4.540638 -0.372598   \n",
       "107  -3.896858  2.272247  3.491675  ... -7.702356  1.721457 -6.695879   \n",
       "\n",
       "           14        15        16        17        18        19  \\\n",
       "0   -6.913729  7.227826 -7.480397 -3.841025 -2.643135 -4.290059   \n",
       "1    3.283216 -3.963865  5.261399 -4.708281  2.653075 -5.063646   \n",
       "2    3.269602  2.010873  1.207476  5.214314  6.464255 -2.690952   \n",
       "3    8.620768  0.387783 -1.047444  3.723100  5.218976  4.335200   \n",
       "4   -7.145875 -4.389797  6.115207 -6.742897  5.260499 -5.494330   \n",
       "..        ...       ...       ...       ...       ...       ...   \n",
       "103  8.533322 -3.954587  6.808384 -5.964357 -6.926980 -4.553753   \n",
       "104 -7.195958  6.914922 -6.155535  4.881458 -7.642689 -8.291637   \n",
       "105 -1.356958 -5.764071 -6.192120 -8.153080 -6.249497  5.730522   \n",
       "106 -7.300920 -0.119995 -5.317137 -6.542215 -6.035981  4.083077   \n",
       "107 -4.547217 -7.225779  2.713670  4.677006  1.589107 -5.289326   \n",
       "\n",
       "     patient number  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 2  \n",
       "3                 1  \n",
       "4                 0  \n",
       "..              ...  \n",
       "103               0  \n",
       "104               2  \n",
       "105               1  \n",
       "106               1  \n",
       "107               1  \n",
       "\n",
       "[108 rows x 21 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_20dims=pd.concat([X_train_20dims, X_test_20dims], axis=0, ignore_index=True)\n",
    "df_20dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f700baab-9556-4e09-98b8-c22b7e1c57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "315c4efc-ecad-419a-9936-5bcd8741e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split and standardalization\n",
    "x_gbm = df_20dims.drop('patient number', axis = 1).values\n",
    "y_gbm = df_20dims['patient number'].values\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_gbm)\n",
    "x_gbm = sc.transform(x_gbm)\n",
    "x_train_gbm, x_test_gbm, y_train_gbm, y_test_gbm = train_test_split(x_gbm, y_gbm, test_size=0.05, shuffle = True, random_state=2022, stratify=y_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "477f0a45-5412-4492-a9a3-e6aca2185a11",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-03 19:10:51,935] A new study created in memory with name: no-name-0ee7d3b8-3978-46fb-96db-d648640256ca\n",
      "[I 2025-06-03 19:10:53,458] Trial 0 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 2.6384744630397656, 'lambda_l2': 0.02762937449326426, 'num_leaves': 206, 'feature_fraction': 0.9177831600428838, 'bagging_fraction': 0.42721506984231594, 'bagging_freq': 7, 'min_child_samples': 32, 'learning_rate': 0.004568831473223616}. Best is trial 0 with value: 0.4636363636363637.\n",
      "[I 2025-06-03 19:11:10,419] Trial 1 finished with value: 0.4918181818181818 and parameters: {'lambda_l1': 8.792264477432843e-07, 'lambda_l2': 0.0001746525537886768, 'num_leaves': 48, 'feature_fraction': 0.9268279815768599, 'bagging_fraction': 0.8077929257748757, 'bagging_freq': 3, 'min_child_samples': 26, 'learning_rate': 0.00020761177351523699}. Best is trial 1 with value: 0.4918181818181818.\n",
      "[I 2025-06-03 19:11:21,052] Trial 2 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 3.835882970711389e-06, 'lambda_l2': 2.433593535478488e-06, 'num_leaves': 216, 'feature_fraction': 0.45872816731414096, 'bagging_fraction': 0.47702663320489164, 'bagging_freq': 1, 'min_child_samples': 17, 'learning_rate': 0.00013808900806517167}. Best is trial 1 with value: 0.4918181818181818.\n",
      "[I 2025-06-03 19:11:39,267] Trial 3 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.01888935852031292, 'lambda_l2': 0.9452424527899865, 'num_leaves': 36, 'feature_fraction': 0.8265296397246897, 'bagging_fraction': 0.9682516607261739, 'bagging_freq': 3, 'min_child_samples': 25, 'learning_rate': 2.4709228548437527e-05}. Best is trial 1 with value: 0.4918181818181818.\n",
      "[I 2025-06-03 19:11:48,562] Trial 4 finished with value: 0.6309090909090909 and parameters: {'lambda_l1': 0.00048406340856728506, 'lambda_l2': 0.031492155697935134, 'num_leaves': 99, 'feature_fraction': 0.5068153119264691, 'bagging_fraction': 0.677229117059006, 'bagging_freq': 4, 'min_child_samples': 32, 'learning_rate': 0.004564534172071682}. Best is trial 4 with value: 0.6309090909090909.\n",
      "[I 2025-06-03 19:11:48,789] Trial 5 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.1957189669870168e-07, 'lambda_l2': 0.005985395761886073, 'num_leaves': 5, 'feature_fraction': 0.7734975224965762, 'bagging_fraction': 0.8007398429692429, 'bagging_freq': 4, 'min_child_samples': 50, 'learning_rate': 1.0417627623766103e-05}. Best is trial 4 with value: 0.6309090909090909.\n",
      "[I 2025-06-03 19:11:56,571] Trial 6 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.7678514976984504e-06, 'lambda_l2': 2.2564955140585194e-08, 'num_leaves': 210, 'feature_fraction': 0.43005575328725426, 'bagging_fraction': 0.6074834127721639, 'bagging_freq': 2, 'min_child_samples': 34, 'learning_rate': 5.043939230722334e-05}. Best is trial 4 with value: 0.6309090909090909.\n",
      "[I 2025-06-03 19:11:57,131] Trial 7 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.0014946412794724002, 'lambda_l2': 0.0782630515590642, 'num_leaves': 118, 'feature_fraction': 0.9222494185497001, 'bagging_fraction': 0.7136266499600293, 'bagging_freq': 6, 'min_child_samples': 68, 'learning_rate': 9.471952268527175e-05}. Best is trial 4 with value: 0.6309090909090909.\n",
      "[I 2025-06-03 19:12:01,985] Trial 8 finished with value: 0.6581818181818181 and parameters: {'lambda_l1': 1.1440068402371658e-06, 'lambda_l2': 0.027362842057565465, 'num_leaves': 234, 'feature_fraction': 0.5970763834796056, 'bagging_fraction': 0.7266511563846372, 'bagging_freq': 5, 'min_child_samples': 29, 'learning_rate': 0.010727082038284796}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:02,513] Trial 9 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.06135615968497812, 'lambda_l2': 2.847079691759167e-07, 'num_leaves': 194, 'feature_fraction': 0.9872452876202432, 'bagging_fraction': 0.8842632196260082, 'bagging_freq': 5, 'min_child_samples': 68, 'learning_rate': 0.005808092697364467}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:02,705] Trial 10 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.6083380915325976e-08, 'lambda_l2': 8.486899462876298, 'num_leaves': 163, 'feature_fraction': 0.6533181369734851, 'bagging_fraction': 0.5627479009374544, 'bagging_freq': 7, 'min_child_samples': 100, 'learning_rate': 0.05024453522536326}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:05,907] Trial 11 finished with value: 0.6127272727272727 and parameters: {'lambda_l1': 5.4541275776553894e-05, 'lambda_l2': 0.0005196458936214544, 'num_leaves': 106, 'feature_fraction': 0.5858555767814193, 'bagging_fraction': 0.6805128326582363, 'bagging_freq': 5, 'min_child_samples': 9, 'learning_rate': 0.015020904884293952}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:06,645] Trial 12 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.0001349135579450551, 'lambda_l2': 9.408827132369422e-05, 'num_leaves': 253, 'feature_fraction': 0.5669637868724104, 'bagging_fraction': 0.7021713426116636, 'bagging_freq': 5, 'min_child_samples': 47, 'learning_rate': 0.0011666557541429414}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:07,634] Trial 13 finished with value: 0.6481818181818182 and parameters: {'lambda_l1': 3.57466932917117e-05, 'lambda_l2': 0.16478932955286074, 'num_leaves': 87, 'feature_fraction': 0.5258865925134116, 'bagging_fraction': 0.5835019457804349, 'bagging_freq': 4, 'min_child_samples': 5, 'learning_rate': 0.09437052390357048}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:09,274] Trial 14 finished with value: 0.6218181818181818 and parameters: {'lambda_l1': 1.5299669458706448e-05, 'lambda_l2': 0.46796613944892057, 'num_leaves': 156, 'feature_fraction': 0.6644264756660264, 'bagging_fraction': 0.541803069138404, 'bagging_freq': 3, 'min_child_samples': 5, 'learning_rate': 0.09456766181887705}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:11,243] Trial 15 finished with value: 0.6318181818181818 and parameters: {'lambda_l1': 2.2999610570225892e-07, 'lambda_l2': 0.002381468515275228, 'num_leaves': 64, 'feature_fraction': 0.5590496232885308, 'bagging_fraction': 0.6154134677506883, 'bagging_freq': 6, 'min_child_samples': 16, 'learning_rate': 0.021841375684556023}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:11,947] Trial 16 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.003071027427542306, 'lambda_l2': 9.593787183471372, 'num_leaves': 256, 'feature_fraction': 0.7427169707406762, 'bagging_fraction': 0.7692855497467179, 'bagging_freq': 6, 'min_child_samples': 44, 'learning_rate': 0.024753985364176518}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:12,115] Trial 17 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.6190548004974192e-08, 'lambda_l2': 1.4253373235616457e-05, 'num_leaves': 81, 'feature_fraction': 0.40451187837879005, 'bagging_fraction': 0.8699319468158677, 'bagging_freq': 4, 'min_child_samples': 61, 'learning_rate': 0.0011000931025563778}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:12,284] Trial 18 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.5186270445288382e-05, 'lambda_l2': 0.49000376091313663, 'num_leaves': 145, 'feature_fraction': 0.4973693266480807, 'bagging_fraction': 0.5020925663939685, 'bagging_freq': 2, 'min_child_samples': 85, 'learning_rate': 0.07558602696201672}. Best is trial 8 with value: 0.6581818181818181.\n",
      "[I 2025-06-03 19:12:15,089] Trial 19 finished with value: 0.6681818181818182 and parameters: {'lambda_l1': 2.7311351914174614e-07, 'lambda_l2': 0.0014335721349859238, 'num_leaves': 131, 'feature_fraction': 0.6328016413431138, 'bagging_fraction': 0.6227639062577796, 'bagging_freq': 5, 'min_child_samples': 16, 'learning_rate': 0.010175100821883753}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:19,429] Trial 20 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.4188422212384957e-07, 'lambda_l2': 0.0013051509893827013, 'num_leaves': 184, 'feature_fraction': 0.6278967490960176, 'bagging_fraction': 0.7467498189945041, 'bagging_freq': 5, 'min_child_samples': 40, 'learning_rate': 0.0003978458034063353}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:22,351] Trial 21 finished with value: 0.65 and parameters: {'lambda_l1': 9.686801225626195e-06, 'lambda_l2': 0.007776569445568947, 'num_leaves': 81, 'feature_fraction': 0.5218314188238218, 'bagging_fraction': 0.6371302228457155, 'bagging_freq': 4, 'min_child_samples': 16, 'learning_rate': 0.00982841307817091}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:25,443] Trial 22 finished with value: 0.6590909090909091 and parameters: {'lambda_l1': 3.828835793742807e-06, 'lambda_l2': 0.005527779978041028, 'num_leaves': 134, 'feature_fraction': 0.6187360472296598, 'bagging_fraction': 0.6330511840485122, 'bagging_freq': 6, 'min_child_samples': 18, 'learning_rate': 0.009857449618115664}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:31,322] Trial 23 finished with value: 0.6590909090909092 and parameters: {'lambda_l1': 7.019916464509899e-07, 'lambda_l2': 2.12600465126416e-05, 'num_leaves': 127, 'feature_fraction': 0.7074094576031056, 'bagging_fraction': 0.639234360312117, 'bagging_freq': 6, 'min_child_samples': 22, 'learning_rate': 0.0027284599603301907}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:38,238] Trial 24 finished with value: 0.649090909090909 and parameters: {'lambda_l1': 2.998067372087071e-07, 'lambda_l2': 9.330631538381968e-06, 'num_leaves': 131, 'feature_fraction': 0.6973571855452818, 'bagging_fraction': 0.6479917722840004, 'bagging_freq': 6, 'min_child_samples': 20, 'learning_rate': 0.0019989873164431384}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:44,411] Trial 25 finished with value: 0.6227272727272728 and parameters: {'lambda_l1': 8.038861176090916e-08, 'lambda_l2': 5.194535737471049e-05, 'num_leaves': 136, 'feature_fraction': 0.7509644302229336, 'bagging_fraction': 0.5124280932539067, 'bagging_freq': 7, 'min_child_samples': 12, 'learning_rate': 0.0019621139235958106}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:44,985] Trial 26 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 3.1450908679573562e-06, 'lambda_l2': 4.805161392654577e-07, 'num_leaves': 174, 'feature_fraction': 0.7097785791530787, 'bagging_fraction': 0.4217327145420406, 'bagging_freq': 6, 'min_child_samples': 38, 'learning_rate': 0.002648850444751277}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:54,772] Trial 27 finished with value: 0.5845454545454545 and parameters: {'lambda_l1': 4.175076580674876e-08, 'lambda_l2': 0.0005232846371001818, 'num_leaves': 118, 'feature_fraction': 0.8080841293645005, 'bagging_fraction': 0.5621094861398446, 'bagging_freq': 7, 'min_child_samples': 22, 'learning_rate': 0.00046806844360611173}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:12:55,051] Trial 28 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 5.662752956401365e-07, 'lambda_l2': 3.569860762424621e-05, 'num_leaves': 153, 'feature_fraction': 0.6238872365339616, 'bagging_fraction': 0.6573296535690984, 'bagging_freq': 6, 'min_child_samples': 55, 'learning_rate': 0.03823097726731535}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:02,574] Trial 29 finished with value: 0.6227272727272728 and parameters: {'lambda_l1': 0.25123543691580447, 'lambda_l2': 0.001705692202072301, 'num_leaves': 119, 'feature_fraction': 0.8546448524079225, 'bagging_fraction': 0.6078486985634115, 'bagging_freq': 7, 'min_child_samples': 11, 'learning_rate': 0.004301900118000494}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:03,198] Trial 30 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.00019088612605845168, 'lambda_l2': 4.049748930594299e-06, 'num_leaves': 171, 'feature_fraction': 0.6985665998169983, 'bagging_fraction': 0.4473905131270639, 'bagging_freq': 5, 'min_child_samples': 35, 'learning_rate': 0.007302845614405852}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:06,480] Trial 31 finished with value: 0.65 and parameters: {'lambda_l1': 9.31472106282562e-07, 'lambda_l2': 0.013578685312515363, 'num_leaves': 228, 'feature_fraction': 0.6096546424506781, 'bagging_fraction': 0.7276565986667398, 'bagging_freq': 5, 'min_child_samples': 27, 'learning_rate': 0.012105089902566675}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:13,543] Trial 32 finished with value: 0.65 and parameters: {'lambda_l1': 4.65929454194658e-06, 'lambda_l2': 0.00019012780151997136, 'num_leaves': 141, 'feature_fraction': 0.672594510858733, 'bagging_fraction': 0.7775352582635016, 'bagging_freq': 6, 'min_child_samples': 27, 'learning_rate': 0.0030387626327201913}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:20,885] Trial 33 finished with value: 0.5927272727272725 and parameters: {'lambda_l1': 4.941850303180616, 'lambda_l2': 0.07668722821421854, 'num_leaves': 52, 'feature_fraction': 0.5741754996585036, 'bagging_fraction': 0.8417486308864541, 'bagging_freq': 5, 'min_child_samples': 29, 'learning_rate': 0.02045669526197373}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:26,733] Trial 34 finished with value: 0.6399999999999999 and parameters: {'lambda_l1': 9.76081911469386e-07, 'lambda_l2': 0.029368449496354927, 'num_leaves': 188, 'feature_fraction': 0.609062922157357, 'bagging_fraction': 0.6806429881612004, 'bagging_freq': 6, 'min_child_samples': 20, 'learning_rate': 0.009487366692281602}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:28,576] Trial 35 finished with value: 0.6227272727272727 and parameters: {'lambda_l1': 3.35342409729082e-08, 'lambda_l2': 0.003929765517218694, 'num_leaves': 235, 'feature_fraction': 0.7197261390137878, 'bagging_fraction': 0.6400423636130832, 'bagging_freq': 5, 'min_child_samples': 14, 'learning_rate': 0.03885464390943589}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:33,334] Trial 36 finished with value: 0.6681818181818182 and parameters: {'lambda_l1': 4.932749953646648e-07, 'lambda_l2': 0.0005305982618573938, 'num_leaves': 16, 'feature_fraction': 0.469696578815331, 'bagging_fraction': 0.7382962909382603, 'bagging_freq': 7, 'min_child_samples': 24, 'learning_rate': 0.0051221471446139385}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:45,649] Trial 37 finished with value: 0.5836363636363636 and parameters: {'lambda_l1': 6.24545011131206e-06, 'lambda_l2': 0.00037988559626365244, 'num_leaves': 99, 'feature_fraction': 0.4674331909065913, 'bagging_fraction': 0.9722720615022201, 'bagging_freq': 7, 'min_child_samples': 22, 'learning_rate': 0.0006014625924787312}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:13:51,123] Trial 38 finished with value: 0.639090909090909 and parameters: {'lambda_l1': 4.5171592094322433e-07, 'lambda_l2': 1.5858587399709256e-06, 'num_leaves': 3, 'feature_fraction': 0.8712447021461247, 'bagging_fraction': 0.9276419052365072, 'bagging_freq': 7, 'min_child_samples': 41, 'learning_rate': 0.0037480720593124034}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:00,527] Trial 39 finished with value: 0.6590909090909092 and parameters: {'lambda_l1': 2.8409024872126464e-06, 'lambda_l2': 0.000848716988052028, 'num_leaves': 16, 'feature_fraction': 0.4736201919002686, 'bagging_fraction': 0.5822701240267114, 'bagging_freq': 7, 'min_child_samples': 23, 'learning_rate': 0.001437313841642313}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:02,502] Trial 40 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 7.01345485161338e-08, 'lambda_l2': 1.7365106810225134e-05, 'num_leaves': 26, 'feature_fraction': 0.440688244769322, 'bagging_fraction': 0.5282699070256094, 'bagging_freq': 7, 'min_child_samples': 31, 'learning_rate': 0.00020905807983910948}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:10,911] Trial 41 finished with value: 0.6590909090909092 and parameters: {'lambda_l1': 2.5404696036449534e-06, 'lambda_l2': 0.00118461949381973, 'num_leaves': 21, 'feature_fraction': 0.47982707468285235, 'bagging_fraction': 0.5855653838185831, 'bagging_freq': 6, 'min_child_samples': 23, 'learning_rate': 0.0016341057068612267}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:19,128] Trial 42 finished with value: 0.6581818181818182 and parameters: {'lambda_l1': 3.5463395856060214e-05, 'lambda_l2': 0.0009200046223641859, 'num_leaves': 18, 'feature_fraction': 0.48972992889671974, 'bagging_fraction': 0.5879792524620842, 'bagging_freq': 7, 'min_child_samples': 25, 'learning_rate': 0.0014320762626370976}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:19,797] Trial 43 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.7004750024973212e-06, 'lambda_l2': 0.00013104409942071268, 'num_leaves': 41, 'feature_fraction': 0.46865490751336225, 'bagging_fraction': 0.4880292500853807, 'bagging_freq': 6, 'min_child_samples': 34, 'learning_rate': 0.0006106644200031714}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:27,900] Trial 44 finished with value: 0.5563636363636363 and parameters: {'lambda_l1': 2.439411562638016e-07, 'lambda_l2': 0.0002486889188565911, 'num_leaves': 21, 'feature_fraction': 0.4253318549775531, 'bagging_fraction': 0.4637064610765348, 'bagging_freq': 7, 'min_child_samples': 24, 'learning_rate': 0.0008325527892944995}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:32,167] Trial 45 finished with value: 0.6590909090909092 and parameters: {'lambda_l1': 0.0005422281972875514, 'lambda_l2': 4.067927772961421e-05, 'num_leaves': 62, 'feature_fraction': 0.5454158580141943, 'bagging_fraction': 0.5614284810435498, 'bagging_freq': 6, 'min_child_samples': 11, 'learning_rate': 0.0052744981996760976}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:14:50,017] Trial 46 finished with value: 0.6127272727272727 and parameters: {'lambda_l1': 1.8070465894793022e-06, 'lambda_l2': 0.0006259076656917879, 'num_leaves': 32, 'feature_fraction': 0.45080504286195394, 'bagging_fraction': 0.6868634918633152, 'bagging_freq': 7, 'min_child_samples': 8, 'learning_rate': 0.0017717571660179768}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:15:04,555] Trial 47 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 5.248751265274826e-07, 'lambda_l2': 7.294537976245275e-05, 'num_leaves': 12, 'feature_fraction': 0.40468942529214025, 'bagging_fraction': 0.5918238204077099, 'bagging_freq': 7, 'min_child_samples': 32, 'learning_rate': 0.0002743200397514682}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:15:08,676] Trial 48 finished with value: 0.6399999999999999 and parameters: {'lambda_l1': 6.115739272613076e-05, 'lambda_l2': 0.002674606637854165, 'num_leaves': 60, 'feature_fraction': 0.543209955003897, 'bagging_fraction': 0.7146548251585585, 'bagging_freq': 6, 'min_child_samples': 17, 'learning_rate': 0.0065936074600285435}. Best is trial 19 with value: 0.6681818181818182.\n",
      "[I 2025-06-03 19:15:09,367] Trial 49 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.3286314649279679e-08, 'lambda_l2': 0.011825700729293889, 'num_leaves': 43, 'feature_fraction': 0.7857471789482708, 'bagging_fraction': 0.8209201300165867, 'bagging_freq': 1, 'min_child_samples': 82, 'learning_rate': 7.521547344157565e-05}. Best is trial 19 with value: 0.6681818181818182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.6681818181818182\n",
      "  Params: \n",
      "    lambda_l1: 2.7311351914174614e-07\n",
      "    lambda_l2: 0.0014335721349859238\n",
      "    num_leaves: 131\n",
      "    feature_fraction: 0.6328016413431138\n",
      "    bagging_fraction: 0.6227639062577796\n",
      "    bagging_freq: 5\n",
      "    min_child_samples: 16\n",
      "    learning_rate: 0.010175100821883753\n"
     ]
    }
   ],
   "source": [
    "# parameters optimization using optuna\n",
    "def objective(trial):\n",
    "    # Set LightGBM hyperparameters using Optuna\n",
    "    param = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_class': 3,\n",
    "        'num_iteration': 3000,\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    }\n",
    "\n",
    "    # Set 10-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state = 2022)\n",
    "\n",
    "    # List to save cross-validation results\n",
    "    cv_results = []\n",
    "\n",
    "    # Train and evaluate the model for each fold\n",
    "    for train_idx, valid_idx in cv.split(x_gbm, y_gbm):\n",
    "        X_train, X_valid = x_gbm[train_idx], x_gbm[valid_idx]\n",
    "        y_train, y_valid = y_gbm[train_idx], y_gbm[valid_idx]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(X_train, label = y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, label = y_valid, reference = lgb_train)\n",
    "        \n",
    "        gbm = lgb.train(param,\n",
    "                        lgb_train,\n",
    "                        valid_sets = [lgb_train, lgb_valid],\n",
    "                        early_stopping_rounds = 100,\n",
    "                        verbose_eval = False)\n",
    "        y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n",
    "        y_pred_max = np.argmax(y_pred, axis=1)\n",
    "        cv_results.append(accuracy_score(y_valid, y_pred_max))\n",
    "\n",
    "    # Return the minimum accuracy from cross-validation\n",
    "    return np.mean(cv_results)\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value: {}'.format(trial.value))\n",
    "\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6540cab6-90db-4f76-809f-754566eedb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "# multiclass_log_loss for LGBM \n",
    "\n",
    "class MultiLoglossForLGBM:\n",
    "    \n",
    "    def __init__(self, n_class: int=3, use_softmax: bool=True, epsilon: float=1e-32, grand_truth=np.empty(0)) -> None:\n",
    "        # initialize        \n",
    "        self.name = \"SFC_loss\"\n",
    "        self.grand_truth = grand_truth\n",
    "        self.n_class = n_class\n",
    "        self.prob_func = self._get_prob_value if use_softmax else lambda x: x\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def __call__(self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None) -> float:\n",
    "        #calculate loss function\n",
    "        #get prob value by softmax\n",
    "        prob = self.prob_func(preds)           # <= from logits to probability\n",
    "        #convert labels to 1-hot\n",
    "        labels = self._get_1hot_label(labels)  # <= labels (1D-array) to 1hot\n",
    "        loss_by_sample = np.sum(- np.log(prob) * labels, axis=1)\n",
    "        loss = np.average(loss_by_sample, weight)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _calc_grad_and_hess(\n",
    "        self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None\n",
    "    ) -> Tuple[np.ndarray]:\n",
    "        \"\"\"Calc Grad and Hess\"\"\"\n",
    "        # # get prob value by softmax\n",
    "        prob = self.prob_func(preds)           # <= margin を確率値に直す\n",
    "        # # convert labels to 1-hot\n",
    "        labels = self._get_1hot_label(labels)  # <= labels (1D-array) to 1hot label\n",
    "\n",
    "        grad = prob - labels\n",
    "        hess = prob * (1 - prob)        \n",
    "        if weight is not None:\n",
    "            grad = grad * weight[:, None]\n",
    "            hess = hess * weight[:, None]\n",
    "        return grad, hess\n",
    "    \n",
    "    def return_loss(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[str, float, bool]:\n",
    "        \"\"\"Return Loss for lightgbm\"\"\"\n",
    "        labels = data.get_label()\n",
    "        weight = data.get_weight()\n",
    "        n_example = len(labels)\n",
    "        \n",
    "        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n",
    "        preds = preds.reshape(self.n_class, n_example).T  # <= preds (1D-array) to 2D-array \n",
    "        # # calc loss\n",
    "        loss = self(preds, labels, weight)\n",
    "        \n",
    "        return self.name, loss, False\n",
    "    \n",
    "    def return_grad_and_hess(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[np.ndarray]:\n",
    "        \"\"\"Return Grad and Hess for lightgbm\"\"\"\n",
    "        labels = data.get_label()\n",
    "        weight = data.get_weight()\n",
    "        n_example = len(labels)\n",
    "        \n",
    "        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n",
    "        preds = preds.reshape(self.n_class, n_example).T  # <= preds (1D-array) to 2D-array \n",
    "        # # calc grad and hess.\n",
    "        grad, hess =  self._calc_grad_and_hess(preds, labels, weight)\n",
    "\n",
    "        # # reshape grad, hess: (n_example, n_class) => (n_class, n_example) => (n_class * n_example,) \n",
    "        grad = grad.T.reshape(n_example * self.n_class)   # <= return 1D-array \n",
    "        hess = hess.T.reshape(n_example * self.n_class)   # <= return 1D-array \n",
    "        \n",
    "        return grad, hess\n",
    "    \n",
    "    def softmax(x):\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "    \n",
    "    def _get_prob_value(self, preds: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert Margin(Logit) to Prob by Softmax.\"\"\"\n",
    "        upper = np.exp(preds)\n",
    "        prob = upper / np.sum(upper, axis=1, keepdims=True)\n",
    "        prob = np.clip(prob, self.epsilon, 1 - self.epsilon)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    \n",
    "    def _get_1hot_label(self, labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert labels to 1hot array.\"\"\"\n",
    "        n_example = len(labels)\n",
    "        #make a matrix here\n",
    "        onehot = np.zeros((n_example, self.n_class))\n",
    "        #setting overlap\n",
    "        original_array=self.grand_truth\n",
    "        for index, j in enumerate(labels):\n",
    "            if self.grand_truth.shape[0]==0:\n",
    "                onehot[index, int(j)] =1\n",
    "            else:\n",
    "                onehot[index,:]=original_array[int(j)]\n",
    "        return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98fd4b-bd95-404b-93e3-d944de8eff0f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99 0.01 0.  ]\n",
      " [0.01 0.98 0.01]\n",
      " [0.   0.01 0.99]]\n",
      "-------------------- 0 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.0579\tvalid's multi_logloss: 1.0629\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05439\tvalid's multi_logloss: 1.06299\n",
      "[3]\ttrain's multi_logloss: 1.05121\tvalid's multi_logloss: 1.05944\n",
      "[4]\ttrain's multi_logloss: 1.04779\tvalid's multi_logloss: 1.05866\n",
      "[5]\ttrain's multi_logloss: 1.04472\tvalid's multi_logloss: 1.05578\n",
      "[6]\ttrain's multi_logloss: 1.04058\tvalid's multi_logloss: 1.05157\n",
      "[7]\ttrain's multi_logloss: 1.03664\tvalid's multi_logloss: 1.0463\n",
      "[8]\ttrain's multi_logloss: 1.03318\tvalid's multi_logloss: 1.04168\n",
      "[9]\ttrain's multi_logloss: 1.03007\tvalid's multi_logloss: 1.04009\n",
      "[10]\ttrain's multi_logloss: 1.02676\tvalid's multi_logloss: 1.03542\n",
      "[11]\ttrain's multi_logloss: 1.02308\tvalid's multi_logloss: 1.03143\n",
      "[12]\ttrain's multi_logloss: 1.01918\tvalid's multi_logloss: 1.02992\n",
      "[13]\ttrain's multi_logloss: 1.01504\tvalid's multi_logloss: 1.02854\n",
      "[14]\ttrain's multi_logloss: 1.01109\tvalid's multi_logloss: 1.02578\n",
      "[15]\ttrain's multi_logloss: 1.00723\tvalid's multi_logloss: 1.02445\n",
      "[16]\ttrain's multi_logloss: 1.00308\tvalid's multi_logloss: 1.02301\n",
      "[17]\ttrain's multi_logloss: 0.998978\tvalid's multi_logloss: 1.02082\n",
      "[18]\ttrain's multi_logloss: 0.994621\tvalid's multi_logloss: 1.01662\n",
      "[19]\ttrain's multi_logloss: 0.991628\tvalid's multi_logloss: 1.01514\n",
      "[20]\ttrain's multi_logloss: 0.988213\tvalid's multi_logloss: 1.01474\n",
      "[21]\ttrain's multi_logloss: 0.983597\tvalid's multi_logloss: 1.01214\n",
      "[22]\ttrain's multi_logloss: 0.979265\tvalid's multi_logloss: 1.00906\n",
      "[23]\ttrain's multi_logloss: 0.975583\tvalid's multi_logloss: 1.00693\n",
      "[24]\ttrain's multi_logloss: 0.971409\tvalid's multi_logloss: 1.00395\n",
      "[25]\ttrain's multi_logloss: 0.967355\tvalid's multi_logloss: 1.00337\n",
      "[26]\ttrain's multi_logloss: 0.964013\tvalid's multi_logloss: 1.0041\n",
      "[27]\ttrain's multi_logloss: 0.960996\tvalid's multi_logloss: 1.00487\n",
      "[28]\ttrain's multi_logloss: 0.958557\tvalid's multi_logloss: 1.00534\n",
      "[29]\ttrain's multi_logloss: 0.955595\tvalid's multi_logloss: 1.0038\n",
      "[30]\ttrain's multi_logloss: 0.952302\tvalid's multi_logloss: 1.00323\n",
      "[31]\ttrain's multi_logloss: 0.948963\tvalid's multi_logloss: 1.00197\n",
      "[32]\ttrain's multi_logloss: 0.945821\tvalid's multi_logloss: 1.00041\n",
      "[33]\ttrain's multi_logloss: 0.942543\tvalid's multi_logloss: 0.999012\n",
      "[34]\ttrain's multi_logloss: 0.939633\tvalid's multi_logloss: 0.998956\n",
      "[35]\ttrain's multi_logloss: 0.936527\tvalid's multi_logloss: 0.998513\n",
      "[36]\ttrain's multi_logloss: 0.934454\tvalid's multi_logloss: 0.995385\n",
      "[37]\ttrain's multi_logloss: 0.931559\tvalid's multi_logloss: 0.993605\n",
      "[38]\ttrain's multi_logloss: 0.92903\tvalid's multi_logloss: 0.993018\n",
      "[39]\ttrain's multi_logloss: 0.926117\tvalid's multi_logloss: 0.992179\n",
      "[40]\ttrain's multi_logloss: 0.92308\tvalid's multi_logloss: 0.988406\n",
      "[41]\ttrain's multi_logloss: 0.920382\tvalid's multi_logloss: 0.987102\n",
      "[42]\ttrain's multi_logloss: 0.917751\tvalid's multi_logloss: 0.984449\n",
      "[43]\ttrain's multi_logloss: 0.91538\tvalid's multi_logloss: 0.981602\n",
      "[44]\ttrain's multi_logloss: 0.913094\tvalid's multi_logloss: 0.980349\n",
      "[45]\ttrain's multi_logloss: 0.910641\tvalid's multi_logloss: 0.977224\n",
      "[46]\ttrain's multi_logloss: 0.90835\tvalid's multi_logloss: 0.975122\n",
      "[47]\ttrain's multi_logloss: 0.906056\tvalid's multi_logloss: 0.972502\n",
      "[48]\ttrain's multi_logloss: 0.903568\tvalid's multi_logloss: 0.970092\n",
      "[49]\ttrain's multi_logloss: 0.9016\tvalid's multi_logloss: 0.967911\n",
      "[50]\ttrain's multi_logloss: 0.899419\tvalid's multi_logloss: 0.965395\n",
      "[51]\ttrain's multi_logloss: 0.897106\tvalid's multi_logloss: 0.961877\n",
      "[52]\ttrain's multi_logloss: 0.895378\tvalid's multi_logloss: 0.960752\n",
      "[53]\ttrain's multi_logloss: 0.89281\tvalid's multi_logloss: 0.957778\n",
      "[54]\ttrain's multi_logloss: 0.891222\tvalid's multi_logloss: 0.954708\n",
      "[55]\ttrain's multi_logloss: 0.88854\tvalid's multi_logloss: 0.952641\n",
      "[56]\ttrain's multi_logloss: 0.885554\tvalid's multi_logloss: 0.949665\n",
      "[57]\ttrain's multi_logloss: 0.882758\tvalid's multi_logloss: 0.946503\n",
      "[58]\ttrain's multi_logloss: 0.8803\tvalid's multi_logloss: 0.945226\n",
      "[59]\ttrain's multi_logloss: 0.878007\tvalid's multi_logloss: 0.943186\n",
      "[60]\ttrain's multi_logloss: 0.87583\tvalid's multi_logloss: 0.941093\n",
      "[61]\ttrain's multi_logloss: 0.87237\tvalid's multi_logloss: 0.939739\n",
      "[62]\ttrain's multi_logloss: 0.869196\tvalid's multi_logloss: 0.937481\n",
      "[63]\ttrain's multi_logloss: 0.866017\tvalid's multi_logloss: 0.936446\n",
      "[64]\ttrain's multi_logloss: 0.863179\tvalid's multi_logloss: 0.935822\n",
      "[65]\ttrain's multi_logloss: 0.860811\tvalid's multi_logloss: 0.934029\n",
      "[66]\ttrain's multi_logloss: 0.857881\tvalid's multi_logloss: 0.932538\n",
      "[67]\ttrain's multi_logloss: 0.855349\tvalid's multi_logloss: 0.930641\n",
      "[68]\ttrain's multi_logloss: 0.8531\tvalid's multi_logloss: 0.929392\n",
      "[69]\ttrain's multi_logloss: 0.850894\tvalid's multi_logloss: 0.927116\n",
      "[70]\ttrain's multi_logloss: 0.848521\tvalid's multi_logloss: 0.924989\n",
      "[71]\ttrain's multi_logloss: 0.846159\tvalid's multi_logloss: 0.924343\n",
      "[72]\ttrain's multi_logloss: 0.843582\tvalid's multi_logloss: 0.922267\n",
      "[73]\ttrain's multi_logloss: 0.841402\tvalid's multi_logloss: 0.919753\n",
      "[74]\ttrain's multi_logloss: 0.83901\tvalid's multi_logloss: 0.91855\n",
      "[75]\ttrain's multi_logloss: 0.836418\tvalid's multi_logloss: 0.916826\n",
      "[76]\ttrain's multi_logloss: 0.833779\tvalid's multi_logloss: 0.914866\n",
      "[77]\ttrain's multi_logloss: 0.83143\tvalid's multi_logloss: 0.914541\n",
      "[78]\ttrain's multi_logloss: 0.829692\tvalid's multi_logloss: 0.912999\n",
      "[79]\ttrain's multi_logloss: 0.827705\tvalid's multi_logloss: 0.91378\n",
      "[80]\ttrain's multi_logloss: 0.825592\tvalid's multi_logloss: 0.91199\n",
      "[81]\ttrain's multi_logloss: 0.823028\tvalid's multi_logloss: 0.911867\n",
      "[82]\ttrain's multi_logloss: 0.821565\tvalid's multi_logloss: 0.909973\n",
      "[83]\ttrain's multi_logloss: 0.819354\tvalid's multi_logloss: 0.908563\n",
      "[84]\ttrain's multi_logloss: 0.817499\tvalid's multi_logloss: 0.907926\n",
      "[85]\ttrain's multi_logloss: 0.814994\tvalid's multi_logloss: 0.906789\n",
      "[86]\ttrain's multi_logloss: 0.812945\tvalid's multi_logloss: 0.903877\n",
      "[87]\ttrain's multi_logloss: 0.811413\tvalid's multi_logloss: 0.901873\n",
      "[88]\ttrain's multi_logloss: 0.809802\tvalid's multi_logloss: 0.900401\n",
      "[89]\ttrain's multi_logloss: 0.808198\tvalid's multi_logloss: 0.898786\n",
      "[90]\ttrain's multi_logloss: 0.806414\tvalid's multi_logloss: 0.895884\n",
      "[91]\ttrain's multi_logloss: 0.804026\tvalid's multi_logloss: 0.896066\n",
      "[92]\ttrain's multi_logloss: 0.802151\tvalid's multi_logloss: 0.896407\n",
      "[93]\ttrain's multi_logloss: 0.799626\tvalid's multi_logloss: 0.894688\n",
      "[94]\ttrain's multi_logloss: 0.797306\tvalid's multi_logloss: 0.893467\n",
      "[95]\ttrain's multi_logloss: 0.79475\tvalid's multi_logloss: 0.891451\n",
      "[96]\ttrain's multi_logloss: 0.793373\tvalid's multi_logloss: 0.889957\n",
      "[97]\ttrain's multi_logloss: 0.791912\tvalid's multi_logloss: 0.886679\n",
      "[98]\ttrain's multi_logloss: 0.79008\tvalid's multi_logloss: 0.885334\n",
      "[99]\ttrain's multi_logloss: 0.788024\tvalid's multi_logloss: 0.883249\n",
      "[100]\ttrain's multi_logloss: 0.787016\tvalid's multi_logloss: 0.880495\n",
      "[101]\ttrain's multi_logloss: 0.784292\tvalid's multi_logloss: 0.879784\n",
      "[102]\ttrain's multi_logloss: 0.781464\tvalid's multi_logloss: 0.88025\n",
      "[103]\ttrain's multi_logloss: 0.778846\tvalid's multi_logloss: 0.881658\n",
      "[104]\ttrain's multi_logloss: 0.776514\tvalid's multi_logloss: 0.881081\n",
      "[105]\ttrain's multi_logloss: 0.774087\tvalid's multi_logloss: 0.879198\n",
      "[106]\ttrain's multi_logloss: 0.772601\tvalid's multi_logloss: 0.879208\n",
      "[107]\ttrain's multi_logloss: 0.770882\tvalid's multi_logloss: 0.879522\n",
      "[108]\ttrain's multi_logloss: 0.769611\tvalid's multi_logloss: 0.880698\n",
      "[109]\ttrain's multi_logloss: 0.767875\tvalid's multi_logloss: 0.880049\n",
      "[110]\ttrain's multi_logloss: 0.766173\tvalid's multi_logloss: 0.880027\n",
      "[111]\ttrain's multi_logloss: 0.763767\tvalid's multi_logloss: 0.879424\n",
      "[112]\ttrain's multi_logloss: 0.761605\tvalid's multi_logloss: 0.877795\n",
      "[113]\ttrain's multi_logloss: 0.759635\tvalid's multi_logloss: 0.877212\n",
      "[114]\ttrain's multi_logloss: 0.757302\tvalid's multi_logloss: 0.876065\n",
      "[115]\ttrain's multi_logloss: 0.754976\tvalid's multi_logloss: 0.875683\n",
      "[116]\ttrain's multi_logloss: 0.753134\tvalid's multi_logloss: 0.875929\n",
      "[117]\ttrain's multi_logloss: 0.751113\tvalid's multi_logloss: 0.875732\n",
      "[118]\ttrain's multi_logloss: 0.749558\tvalid's multi_logloss: 0.873911\n",
      "[119]\ttrain's multi_logloss: 0.747577\tvalid's multi_logloss: 0.873486\n",
      "[120]\ttrain's multi_logloss: 0.74636\tvalid's multi_logloss: 0.871438\n",
      "[121]\ttrain's multi_logloss: 0.744303\tvalid's multi_logloss: 0.869717\n",
      "[122]\ttrain's multi_logloss: 0.742659\tvalid's multi_logloss: 0.868906\n",
      "[123]\ttrain's multi_logloss: 0.740996\tvalid's multi_logloss: 0.86721\n",
      "[124]\ttrain's multi_logloss: 0.739183\tvalid's multi_logloss: 0.867606\n",
      "[125]\ttrain's multi_logloss: 0.737304\tvalid's multi_logloss: 0.866933\n",
      "[126]\ttrain's multi_logloss: 0.735505\tvalid's multi_logloss: 0.864263\n",
      "[127]\ttrain's multi_logloss: 0.733794\tvalid's multi_logloss: 0.862016\n",
      "[128]\ttrain's multi_logloss: 0.731896\tvalid's multi_logloss: 0.858929\n",
      "[129]\ttrain's multi_logloss: 0.73046\tvalid's multi_logloss: 0.859436\n",
      "[130]\ttrain's multi_logloss: 0.728795\tvalid's multi_logloss: 0.858875\n",
      "[131]\ttrain's multi_logloss: 0.727347\tvalid's multi_logloss: 0.859376\n",
      "[132]\ttrain's multi_logloss: 0.725082\tvalid's multi_logloss: 0.857007\n",
      "[133]\ttrain's multi_logloss: 0.722821\tvalid's multi_logloss: 0.854133\n",
      "[134]\ttrain's multi_logloss: 0.720755\tvalid's multi_logloss: 0.851387\n",
      "[135]\ttrain's multi_logloss: 0.718715\tvalid's multi_logloss: 0.850383\n",
      "[136]\ttrain's multi_logloss: 0.716616\tvalid's multi_logloss: 0.852448\n",
      "[137]\ttrain's multi_logloss: 0.71482\tvalid's multi_logloss: 0.852381\n",
      "[138]\ttrain's multi_logloss: 0.712958\tvalid's multi_logloss: 0.853657\n",
      "[139]\ttrain's multi_logloss: 0.711039\tvalid's multi_logloss: 0.853422\n",
      "[140]\ttrain's multi_logloss: 0.709683\tvalid's multi_logloss: 0.852092\n",
      "[141]\ttrain's multi_logloss: 0.708683\tvalid's multi_logloss: 0.850292\n",
      "[142]\ttrain's multi_logloss: 0.707197\tvalid's multi_logloss: 0.849534\n",
      "[143]\ttrain's multi_logloss: 0.705979\tvalid's multi_logloss: 0.847393\n",
      "[144]\ttrain's multi_logloss: 0.704423\tvalid's multi_logloss: 0.8466\n",
      "[145]\ttrain's multi_logloss: 0.7033\tvalid's multi_logloss: 0.844539\n",
      "[146]\ttrain's multi_logloss: 0.7017\tvalid's multi_logloss: 0.842727\n",
      "[147]\ttrain's multi_logloss: 0.699615\tvalid's multi_logloss: 0.841692\n",
      "[148]\ttrain's multi_logloss: 0.697811\tvalid's multi_logloss: 0.841506\n",
      "[149]\ttrain's multi_logloss: 0.696733\tvalid's multi_logloss: 0.842545\n",
      "[150]\ttrain's multi_logloss: 0.69546\tvalid's multi_logloss: 0.841617\n",
      "[151]\ttrain's multi_logloss: 0.69374\tvalid's multi_logloss: 0.840366\n",
      "[152]\ttrain's multi_logloss: 0.691885\tvalid's multi_logloss: 0.839631\n",
      "[153]\ttrain's multi_logloss: 0.690052\tvalid's multi_logloss: 0.839444\n",
      "[154]\ttrain's multi_logloss: 0.688569\tvalid's multi_logloss: 0.838535\n",
      "[155]\ttrain's multi_logloss: 0.686616\tvalid's multi_logloss: 0.839019\n",
      "[156]\ttrain's multi_logloss: 0.684881\tvalid's multi_logloss: 0.837549\n",
      "[157]\ttrain's multi_logloss: 0.68271\tvalid's multi_logloss: 0.836324\n",
      "[158]\ttrain's multi_logloss: 0.680773\tvalid's multi_logloss: 0.835045\n",
      "[159]\ttrain's multi_logloss: 0.678919\tvalid's multi_logloss: 0.835806\n",
      "[160]\ttrain's multi_logloss: 0.677237\tvalid's multi_logloss: 0.834805\n",
      "[161]\ttrain's multi_logloss: 0.675899\tvalid's multi_logloss: 0.834981\n",
      "[162]\ttrain's multi_logloss: 0.6747\tvalid's multi_logloss: 0.834391\n",
      "[163]\ttrain's multi_logloss: 0.673928\tvalid's multi_logloss: 0.835153\n",
      "[164]\ttrain's multi_logloss: 0.67309\tvalid's multi_logloss: 0.834646\n",
      "[165]\ttrain's multi_logloss: 0.671933\tvalid's multi_logloss: 0.833966\n",
      "[166]\ttrain's multi_logloss: 0.670584\tvalid's multi_logloss: 0.834439\n",
      "[167]\ttrain's multi_logloss: 0.669289\tvalid's multi_logloss: 0.834005\n",
      "[168]\ttrain's multi_logloss: 0.66805\tvalid's multi_logloss: 0.833831\n",
      "[169]\ttrain's multi_logloss: 0.666759\tvalid's multi_logloss: 0.833175\n",
      "[170]\ttrain's multi_logloss: 0.665409\tvalid's multi_logloss: 0.832396\n",
      "[171]\ttrain's multi_logloss: 0.66421\tvalid's multi_logloss: 0.831114\n",
      "[172]\ttrain's multi_logloss: 0.663172\tvalid's multi_logloss: 0.828824\n",
      "[173]\ttrain's multi_logloss: 0.662163\tvalid's multi_logloss: 0.826576\n",
      "[174]\ttrain's multi_logloss: 0.66096\tvalid's multi_logloss: 0.824476\n",
      "[175]\ttrain's multi_logloss: 0.659994\tvalid's multi_logloss: 0.822292\n",
      "[176]\ttrain's multi_logloss: 0.658193\tvalid's multi_logloss: 0.822843\n",
      "[177]\ttrain's multi_logloss: 0.656569\tvalid's multi_logloss: 0.822153\n",
      "[178]\ttrain's multi_logloss: 0.655265\tvalid's multi_logloss: 0.822236\n",
      "[179]\ttrain's multi_logloss: 0.653843\tvalid's multi_logloss: 0.821257\n",
      "[180]\ttrain's multi_logloss: 0.65221\tvalid's multi_logloss: 0.820623\n",
      "[181]\ttrain's multi_logloss: 0.650861\tvalid's multi_logloss: 0.820095\n",
      "[182]\ttrain's multi_logloss: 0.649225\tvalid's multi_logloss: 0.818212\n",
      "[183]\ttrain's multi_logloss: 0.647942\tvalid's multi_logloss: 0.815505\n",
      "[184]\ttrain's multi_logloss: 0.646512\tvalid's multi_logloss: 0.81576\n",
      "[185]\ttrain's multi_logloss: 0.64499\tvalid's multi_logloss: 0.815673\n",
      "[186]\ttrain's multi_logloss: 0.643212\tvalid's multi_logloss: 0.813569\n",
      "[187]\ttrain's multi_logloss: 0.64164\tvalid's multi_logloss: 0.814692\n",
      "[188]\ttrain's multi_logloss: 0.640057\tvalid's multi_logloss: 0.814738\n",
      "[189]\ttrain's multi_logloss: 0.638464\tvalid's multi_logloss: 0.81534\n",
      "[190]\ttrain's multi_logloss: 0.637129\tvalid's multi_logloss: 0.814531\n",
      "[191]\ttrain's multi_logloss: 0.635696\tvalid's multi_logloss: 0.814153\n",
      "[192]\ttrain's multi_logloss: 0.634354\tvalid's multi_logloss: 0.813218\n",
      "[193]\ttrain's multi_logloss: 0.632583\tvalid's multi_logloss: 0.814562\n",
      "[194]\ttrain's multi_logloss: 0.631109\tvalid's multi_logloss: 0.812683\n",
      "[195]\ttrain's multi_logloss: 0.629788\tvalid's multi_logloss: 0.813564\n",
      "[196]\ttrain's multi_logloss: 0.628037\tvalid's multi_logloss: 0.81113\n",
      "[197]\ttrain's multi_logloss: 0.626346\tvalid's multi_logloss: 0.808081\n",
      "[198]\ttrain's multi_logloss: 0.62465\tvalid's multi_logloss: 0.806035\n",
      "[199]\ttrain's multi_logloss: 0.623284\tvalid's multi_logloss: 0.804901\n",
      "[200]\ttrain's multi_logloss: 0.621766\tvalid's multi_logloss: 0.802741\n",
      "[201]\ttrain's multi_logloss: 0.619706\tvalid's multi_logloss: 0.802655\n",
      "[202]\ttrain's multi_logloss: 0.617944\tvalid's multi_logloss: 0.802999\n",
      "[203]\ttrain's multi_logloss: 0.616006\tvalid's multi_logloss: 0.802663\n",
      "[204]\ttrain's multi_logloss: 0.614245\tvalid's multi_logloss: 0.802885\n",
      "[205]\ttrain's multi_logloss: 0.612342\tvalid's multi_logloss: 0.801882\n",
      "[206]\ttrain's multi_logloss: 0.611219\tvalid's multi_logloss: 0.799883\n",
      "[207]\ttrain's multi_logloss: 0.610077\tvalid's multi_logloss: 0.800704\n",
      "[208]\ttrain's multi_logloss: 0.608958\tvalid's multi_logloss: 0.800896\n",
      "[209]\ttrain's multi_logloss: 0.608093\tvalid's multi_logloss: 0.799695\n",
      "[210]\ttrain's multi_logloss: 0.606848\tvalid's multi_logloss: 0.79848\n",
      "[211]\ttrain's multi_logloss: 0.605932\tvalid's multi_logloss: 0.795567\n",
      "[212]\ttrain's multi_logloss: 0.6049\tvalid's multi_logloss: 0.795418\n",
      "[213]\ttrain's multi_logloss: 0.604076\tvalid's multi_logloss: 0.792799\n",
      "[214]\ttrain's multi_logloss: 0.60317\tvalid's multi_logloss: 0.791689\n",
      "[215]\ttrain's multi_logloss: 0.602286\tvalid's multi_logloss: 0.7905\n",
      "[216]\ttrain's multi_logloss: 0.601108\tvalid's multi_logloss: 0.790469\n",
      "[217]\ttrain's multi_logloss: 0.59996\tvalid's multi_logloss: 0.789648\n",
      "[218]\ttrain's multi_logloss: 0.598729\tvalid's multi_logloss: 0.788848\n",
      "[219]\ttrain's multi_logloss: 0.597512\tvalid's multi_logloss: 0.788079\n",
      "[220]\ttrain's multi_logloss: 0.596601\tvalid's multi_logloss: 0.788125\n",
      "[221]\ttrain's multi_logloss: 0.595014\tvalid's multi_logloss: 0.786532\n",
      "[222]\ttrain's multi_logloss: 0.593393\tvalid's multi_logloss: 0.785573\n",
      "[223]\ttrain's multi_logloss: 0.591661\tvalid's multi_logloss: 0.784749\n",
      "[224]\ttrain's multi_logloss: 0.590254\tvalid's multi_logloss: 0.785206\n",
      "[225]\ttrain's multi_logloss: 0.588951\tvalid's multi_logloss: 0.784774\n",
      "[226]\ttrain's multi_logloss: 0.58758\tvalid's multi_logloss: 0.785021\n",
      "[227]\ttrain's multi_logloss: 0.585982\tvalid's multi_logloss: 0.785713\n",
      "[228]\ttrain's multi_logloss: 0.584647\tvalid's multi_logloss: 0.787834\n",
      "[229]\ttrain's multi_logloss: 0.583433\tvalid's multi_logloss: 0.788895\n",
      "[230]\ttrain's multi_logloss: 0.582106\tvalid's multi_logloss: 0.790123\n",
      "[231]\ttrain's multi_logloss: 0.581047\tvalid's multi_logloss: 0.789643\n",
      "[232]\ttrain's multi_logloss: 0.580131\tvalid's multi_logloss: 0.788577\n",
      "[233]\ttrain's multi_logloss: 0.579418\tvalid's multi_logloss: 0.787796\n",
      "[234]\ttrain's multi_logloss: 0.578626\tvalid's multi_logloss: 0.787309\n",
      "[235]\ttrain's multi_logloss: 0.577699\tvalid's multi_logloss: 0.786666\n",
      "[236]\ttrain's multi_logloss: 0.576745\tvalid's multi_logloss: 0.783496\n",
      "[237]\ttrain's multi_logloss: 0.57594\tvalid's multi_logloss: 0.781724\n",
      "[238]\ttrain's multi_logloss: 0.575158\tvalid's multi_logloss: 0.778305\n",
      "[239]\ttrain's multi_logloss: 0.57428\tvalid's multi_logloss: 0.775092\n",
      "[240]\ttrain's multi_logloss: 0.573448\tvalid's multi_logloss: 0.772103\n",
      "[241]\ttrain's multi_logloss: 0.572017\tvalid's multi_logloss: 0.771167\n",
      "[242]\ttrain's multi_logloss: 0.570764\tvalid's multi_logloss: 0.771722\n",
      "[243]\ttrain's multi_logloss: 0.569665\tvalid's multi_logloss: 0.772205\n",
      "[244]\ttrain's multi_logloss: 0.56841\tvalid's multi_logloss: 0.771835\n",
      "[245]\ttrain's multi_logloss: 0.567245\tvalid's multi_logloss: 0.771062\n",
      "[246]\ttrain's multi_logloss: 0.566027\tvalid's multi_logloss: 0.769086\n",
      "[247]\ttrain's multi_logloss: 0.564588\tvalid's multi_logloss: 0.768854\n",
      "[248]\ttrain's multi_logloss: 0.563234\tvalid's multi_logloss: 0.76832\n",
      "[249]\ttrain's multi_logloss: 0.561863\tvalid's multi_logloss: 0.769257\n",
      "[250]\ttrain's multi_logloss: 0.560452\tvalid's multi_logloss: 0.769558\n",
      "[251]\ttrain's multi_logloss: 0.559265\tvalid's multi_logloss: 0.770083\n",
      "[252]\ttrain's multi_logloss: 0.558216\tvalid's multi_logloss: 0.770805\n",
      "[253]\ttrain's multi_logloss: 0.557102\tvalid's multi_logloss: 0.770648\n",
      "[254]\ttrain's multi_logloss: 0.555847\tvalid's multi_logloss: 0.771569\n",
      "[255]\ttrain's multi_logloss: 0.554884\tvalid's multi_logloss: 0.77131\n",
      "[256]\ttrain's multi_logloss: 0.553854\tvalid's multi_logloss: 0.770392\n",
      "[257]\ttrain's multi_logloss: 0.552765\tvalid's multi_logloss: 0.768628\n",
      "[258]\ttrain's multi_logloss: 0.551647\tvalid's multi_logloss: 0.767162\n",
      "[259]\ttrain's multi_logloss: 0.550822\tvalid's multi_logloss: 0.766217\n",
      "[260]\ttrain's multi_logloss: 0.549621\tvalid's multi_logloss: 0.764225\n",
      "[261]\ttrain's multi_logloss: 0.548592\tvalid's multi_logloss: 0.763475\n",
      "[262]\ttrain's multi_logloss: 0.547544\tvalid's multi_logloss: 0.764987\n",
      "[263]\ttrain's multi_logloss: 0.546626\tvalid's multi_logloss: 0.764878\n",
      "[264]\ttrain's multi_logloss: 0.545747\tvalid's multi_logloss: 0.765438\n",
      "[265]\ttrain's multi_logloss: 0.544669\tvalid's multi_logloss: 0.766931\n",
      "[266]\ttrain's multi_logloss: 0.543635\tvalid's multi_logloss: 0.768092\n",
      "[267]\ttrain's multi_logloss: 0.542427\tvalid's multi_logloss: 0.770219\n",
      "[268]\ttrain's multi_logloss: 0.541004\tvalid's multi_logloss: 0.771811\n",
      "[269]\ttrain's multi_logloss: 0.540009\tvalid's multi_logloss: 0.772676\n",
      "[270]\ttrain's multi_logloss: 0.538821\tvalid's multi_logloss: 0.774116\n",
      "[271]\ttrain's multi_logloss: 0.538088\tvalid's multi_logloss: 0.77455\n",
      "[272]\ttrain's multi_logloss: 0.53715\tvalid's multi_logloss: 0.774727\n",
      "[273]\ttrain's multi_logloss: 0.536246\tvalid's multi_logloss: 0.774247\n",
      "[274]\ttrain's multi_logloss: 0.535245\tvalid's multi_logloss: 0.774563\n",
      "[275]\ttrain's multi_logloss: 0.534312\tvalid's multi_logloss: 0.77417\n",
      "[276]\ttrain's multi_logloss: 0.533203\tvalid's multi_logloss: 0.774014\n",
      "[277]\ttrain's multi_logloss: 0.532137\tvalid's multi_logloss: 0.77394\n",
      "[278]\ttrain's multi_logloss: 0.531338\tvalid's multi_logloss: 0.774398\n",
      "[279]\ttrain's multi_logloss: 0.530323\tvalid's multi_logloss: 0.772795\n",
      "[280]\ttrain's multi_logloss: 0.529554\tvalid's multi_logloss: 0.773312\n",
      "[281]\ttrain's multi_logloss: 0.528664\tvalid's multi_logloss: 0.772665\n",
      "[282]\ttrain's multi_logloss: 0.527326\tvalid's multi_logloss: 0.772139\n",
      "[283]\ttrain's multi_logloss: 0.526101\tvalid's multi_logloss: 0.771803\n",
      "[284]\ttrain's multi_logloss: 0.525343\tvalid's multi_logloss: 0.770989\n",
      "[285]\ttrain's multi_logloss: 0.524786\tvalid's multi_logloss: 0.769927\n",
      "[286]\ttrain's multi_logloss: 0.523722\tvalid's multi_logloss: 0.769885\n",
      "[287]\ttrain's multi_logloss: 0.522389\tvalid's multi_logloss: 0.771125\n",
      "[288]\ttrain's multi_logloss: 0.521351\tvalid's multi_logloss: 0.768836\n",
      "[289]\ttrain's multi_logloss: 0.51989\tvalid's multi_logloss: 0.770388\n",
      "[290]\ttrain's multi_logloss: 0.518753\tvalid's multi_logloss: 0.769754\n",
      "[291]\ttrain's multi_logloss: 0.517577\tvalid's multi_logloss: 0.769171\n",
      "[292]\ttrain's multi_logloss: 0.516508\tvalid's multi_logloss: 0.768341\n",
      "[293]\ttrain's multi_logloss: 0.5153\tvalid's multi_logloss: 0.76775\n",
      "[294]\ttrain's multi_logloss: 0.514362\tvalid's multi_logloss: 0.766776\n",
      "[295]\ttrain's multi_logloss: 0.513216\tvalid's multi_logloss: 0.766136\n",
      "[296]\ttrain's multi_logloss: 0.512698\tvalid's multi_logloss: 0.767399\n",
      "[297]\ttrain's multi_logloss: 0.512248\tvalid's multi_logloss: 0.767328\n",
      "[298]\ttrain's multi_logloss: 0.511574\tvalid's multi_logloss: 0.768305\n",
      "[299]\ttrain's multi_logloss: 0.51122\tvalid's multi_logloss: 0.769367\n",
      "[300]\ttrain's multi_logloss: 0.510828\tvalid's multi_logloss: 0.767874\n",
      "[301]\ttrain's multi_logloss: 0.510136\tvalid's multi_logloss: 0.766379\n",
      "[302]\ttrain's multi_logloss: 0.509037\tvalid's multi_logloss: 0.765961\n",
      "[303]\ttrain's multi_logloss: 0.50821\tvalid's multi_logloss: 0.76524\n",
      "[304]\ttrain's multi_logloss: 0.507665\tvalid's multi_logloss: 0.764596\n",
      "[305]\ttrain's multi_logloss: 0.507043\tvalid's multi_logloss: 0.763701\n",
      "[306]\ttrain's multi_logloss: 0.505838\tvalid's multi_logloss: 0.761526\n",
      "[307]\ttrain's multi_logloss: 0.504523\tvalid's multi_logloss: 0.759558\n",
      "[308]\ttrain's multi_logloss: 0.503086\tvalid's multi_logloss: 0.757754\n",
      "[309]\ttrain's multi_logloss: 0.501749\tvalid's multi_logloss: 0.756204\n",
      "[310]\ttrain's multi_logloss: 0.500921\tvalid's multi_logloss: 0.755372\n",
      "[311]\ttrain's multi_logloss: 0.500179\tvalid's multi_logloss: 0.756425\n",
      "[312]\ttrain's multi_logloss: 0.499256\tvalid's multi_logloss: 0.757029\n",
      "[313]\ttrain's multi_logloss: 0.498442\tvalid's multi_logloss: 0.757637\n",
      "[314]\ttrain's multi_logloss: 0.497593\tvalid's multi_logloss: 0.759429\n",
      "[315]\ttrain's multi_logloss: 0.496807\tvalid's multi_logloss: 0.759374\n",
      "[316]\ttrain's multi_logloss: 0.496144\tvalid's multi_logloss: 0.759991\n",
      "[317]\ttrain's multi_logloss: 0.495669\tvalid's multi_logloss: 0.761344\n",
      "[318]\ttrain's multi_logloss: 0.495224\tvalid's multi_logloss: 0.761503\n",
      "[319]\ttrain's multi_logloss: 0.494636\tvalid's multi_logloss: 0.762687\n",
      "[320]\ttrain's multi_logloss: 0.494213\tvalid's multi_logloss: 0.763238\n",
      "[321]\ttrain's multi_logloss: 0.492914\tvalid's multi_logloss: 0.764432\n",
      "[322]\ttrain's multi_logloss: 0.491805\tvalid's multi_logloss: 0.764642\n",
      "[323]\ttrain's multi_logloss: 0.490675\tvalid's multi_logloss: 0.764627\n",
      "[324]\ttrain's multi_logloss: 0.489356\tvalid's multi_logloss: 0.765649\n",
      "[325]\ttrain's multi_logloss: 0.488027\tvalid's multi_logloss: 0.766539\n",
      "[326]\ttrain's multi_logloss: 0.487259\tvalid's multi_logloss: 0.765321\n",
      "[327]\ttrain's multi_logloss: 0.486413\tvalid's multi_logloss: 0.763444\n",
      "[328]\ttrain's multi_logloss: 0.485239\tvalid's multi_logloss: 0.764338\n",
      "[329]\ttrain's multi_logloss: 0.48418\tvalid's multi_logloss: 0.76404\n",
      "[330]\ttrain's multi_logloss: 0.483317\tvalid's multi_logloss: 0.763982\n",
      "[331]\ttrain's multi_logloss: 0.482556\tvalid's multi_logloss: 0.764363\n",
      "[332]\ttrain's multi_logloss: 0.481606\tvalid's multi_logloss: 0.764815\n",
      "[333]\ttrain's multi_logloss: 0.480642\tvalid's multi_logloss: 0.765502\n",
      "[334]\ttrain's multi_logloss: 0.47975\tvalid's multi_logloss: 0.765414\n",
      "[335]\ttrain's multi_logloss: 0.478896\tvalid's multi_logloss: 0.766089\n",
      "[336]\ttrain's multi_logloss: 0.478285\tvalid's multi_logloss: 0.766538\n",
      "[337]\ttrain's multi_logloss: 0.477537\tvalid's multi_logloss: 0.766933\n",
      "[338]\ttrain's multi_logloss: 0.477052\tvalid's multi_logloss: 0.766634\n",
      "[339]\ttrain's multi_logloss: 0.476192\tvalid's multi_logloss: 0.767825\n",
      "[340]\ttrain's multi_logloss: 0.475353\tvalid's multi_logloss: 0.769021\n",
      "[341]\ttrain's multi_logloss: 0.474801\tvalid's multi_logloss: 0.769203\n",
      "[342]\ttrain's multi_logloss: 0.473965\tvalid's multi_logloss: 0.769014\n",
      "[343]\ttrain's multi_logloss: 0.473404\tvalid's multi_logloss: 0.768264\n",
      "[344]\ttrain's multi_logloss: 0.472683\tvalid's multi_logloss: 0.767774\n",
      "[345]\ttrain's multi_logloss: 0.471897\tvalid's multi_logloss: 0.768724\n",
      "[346]\ttrain's multi_logloss: 0.470769\tvalid's multi_logloss: 0.769561\n",
      "[347]\ttrain's multi_logloss: 0.46993\tvalid's multi_logloss: 0.769199\n",
      "[348]\ttrain's multi_logloss: 0.469053\tvalid's multi_logloss: 0.769082\n",
      "[349]\ttrain's multi_logloss: 0.468052\tvalid's multi_logloss: 0.769417\n",
      "[350]\ttrain's multi_logloss: 0.467211\tvalid's multi_logloss: 0.769325\n",
      "[351]\ttrain's multi_logloss: 0.466301\tvalid's multi_logloss: 0.770135\n",
      "[352]\ttrain's multi_logloss: 0.465338\tvalid's multi_logloss: 0.771284\n",
      "[353]\ttrain's multi_logloss: 0.464411\tvalid's multi_logloss: 0.77222\n",
      "[354]\ttrain's multi_logloss: 0.463512\tvalid's multi_logloss: 0.773116\n",
      "[355]\ttrain's multi_logloss: 0.462848\tvalid's multi_logloss: 0.773772\n",
      "[356]\ttrain's multi_logloss: 0.462218\tvalid's multi_logloss: 0.774296\n",
      "[357]\ttrain's multi_logloss: 0.461231\tvalid's multi_logloss: 0.776633\n",
      "[358]\ttrain's multi_logloss: 0.460351\tvalid's multi_logloss: 0.776677\n",
      "[359]\ttrain's multi_logloss: 0.459494\tvalid's multi_logloss: 0.778887\n",
      "[360]\ttrain's multi_logloss: 0.458707\tvalid's multi_logloss: 0.780127\n",
      "[361]\ttrain's multi_logloss: 0.457485\tvalid's multi_logloss: 0.780843\n",
      "[362]\ttrain's multi_logloss: 0.456414\tvalid's multi_logloss: 0.780132\n",
      "[363]\ttrain's multi_logloss: 0.455197\tvalid's multi_logloss: 0.7804\n",
      "[364]\ttrain's multi_logloss: 0.454062\tvalid's multi_logloss: 0.780603\n",
      "[365]\ttrain's multi_logloss: 0.453127\tvalid's multi_logloss: 0.781444\n",
      "[366]\ttrain's multi_logloss: 0.452204\tvalid's multi_logloss: 0.781306\n",
      "[367]\ttrain's multi_logloss: 0.451295\tvalid's multi_logloss: 0.7816\n",
      "[368]\ttrain's multi_logloss: 0.450457\tvalid's multi_logloss: 0.782007\n",
      "[369]\ttrain's multi_logloss: 0.449614\tvalid's multi_logloss: 0.782682\n",
      "[370]\ttrain's multi_logloss: 0.448706\tvalid's multi_logloss: 0.783463\n",
      "[371]\ttrain's multi_logloss: 0.447759\tvalid's multi_logloss: 0.78074\n",
      "[372]\ttrain's multi_logloss: 0.446861\tvalid's multi_logloss: 0.781308\n",
      "[373]\ttrain's multi_logloss: 0.445949\tvalid's multi_logloss: 0.781583\n",
      "[374]\ttrain's multi_logloss: 0.444916\tvalid's multi_logloss: 0.781345\n",
      "[375]\ttrain's multi_logloss: 0.444143\tvalid's multi_logloss: 0.781622\n",
      "[376]\ttrain's multi_logloss: 0.443545\tvalid's multi_logloss: 0.782398\n",
      "[377]\ttrain's multi_logloss: 0.442813\tvalid's multi_logloss: 0.783125\n",
      "[378]\ttrain's multi_logloss: 0.442202\tvalid's multi_logloss: 0.782603\n",
      "[379]\ttrain's multi_logloss: 0.441597\tvalid's multi_logloss: 0.782675\n",
      "[380]\ttrain's multi_logloss: 0.441108\tvalid's multi_logloss: 0.782345\n",
      "[381]\ttrain's multi_logloss: 0.44031\tvalid's multi_logloss: 0.78381\n",
      "[382]\ttrain's multi_logloss: 0.439526\tvalid's multi_logloss: 0.784697\n",
      "[383]\ttrain's multi_logloss: 0.438859\tvalid's multi_logloss: 0.785818\n",
      "[384]\ttrain's multi_logloss: 0.438153\tvalid's multi_logloss: 0.787315\n",
      "[385]\ttrain's multi_logloss: 0.437308\tvalid's multi_logloss: 0.785923\n",
      "[386]\ttrain's multi_logloss: 0.436663\tvalid's multi_logloss: 0.786811\n",
      "[387]\ttrain's multi_logloss: 0.435834\tvalid's multi_logloss: 0.785457\n",
      "[388]\ttrain's multi_logloss: 0.435149\tvalid's multi_logloss: 0.784285\n",
      "[389]\ttrain's multi_logloss: 0.434497\tvalid's multi_logloss: 0.783385\n",
      "[390]\ttrain's multi_logloss: 0.433973\tvalid's multi_logloss: 0.783101\n",
      "[391]\ttrain's multi_logloss: 0.433492\tvalid's multi_logloss: 0.782612\n",
      "[392]\ttrain's multi_logloss: 0.432885\tvalid's multi_logloss: 0.782884\n",
      "[393]\ttrain's multi_logloss: 0.432292\tvalid's multi_logloss: 0.782596\n",
      "[394]\ttrain's multi_logloss: 0.431526\tvalid's multi_logloss: 0.783453\n",
      "[395]\ttrain's multi_logloss: 0.43102\tvalid's multi_logloss: 0.782706\n",
      "[396]\ttrain's multi_logloss: 0.430226\tvalid's multi_logloss: 0.783392\n",
      "[397]\ttrain's multi_logloss: 0.429514\tvalid's multi_logloss: 0.782643\n",
      "[398]\ttrain's multi_logloss: 0.42883\tvalid's multi_logloss: 0.78149\n",
      "[399]\ttrain's multi_logloss: 0.428186\tvalid's multi_logloss: 0.780252\n",
      "[400]\ttrain's multi_logloss: 0.427373\tvalid's multi_logloss: 0.779604\n",
      "[401]\ttrain's multi_logloss: 0.426822\tvalid's multi_logloss: 0.780453\n",
      "[402]\ttrain's multi_logloss: 0.426272\tvalid's multi_logloss: 0.78121\n",
      "[403]\ttrain's multi_logloss: 0.425716\tvalid's multi_logloss: 0.781587\n",
      "[404]\ttrain's multi_logloss: 0.425172\tvalid's multi_logloss: 0.781992\n",
      "[405]\ttrain's multi_logloss: 0.424639\tvalid's multi_logloss: 0.782405\n",
      "[406]\ttrain's multi_logloss: 0.423663\tvalid's multi_logloss: 0.783362\n",
      "[407]\ttrain's multi_logloss: 0.422695\tvalid's multi_logloss: 0.784609\n",
      "[408]\ttrain's multi_logloss: 0.421874\tvalid's multi_logloss: 0.785965\n",
      "[409]\ttrain's multi_logloss: 0.421241\tvalid's multi_logloss: 0.78489\n",
      "[410]\ttrain's multi_logloss: 0.420517\tvalid's multi_logloss: 0.785735\n",
      "Early stopping, best iteration is:\n",
      "[310]\ttrain's multi_logloss: 0.500921\tvalid's multi_logloss: 0.755372\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09221\tvalid's SFC_loss: 1.09638\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08557\tvalid's SFC_loss: 1.09418\n",
      "[3]\ttrain's SFC_loss: 1.07943\tvalid's SFC_loss: 1.08929\n",
      "[4]\ttrain's SFC_loss: 1.07313\tvalid's SFC_loss: 1.08564\n",
      "[5]\ttrain's SFC_loss: 1.06714\tvalid's SFC_loss: 1.08191\n",
      "[6]\ttrain's SFC_loss: 1.06266\tvalid's SFC_loss: 1.07807\n",
      "[7]\ttrain's SFC_loss: 1.05559\tvalid's SFC_loss: 1.07166\n",
      "[8]\ttrain's SFC_loss: 1.0491\tvalid's SFC_loss: 1.06466\n",
      "[9]\ttrain's SFC_loss: 1.0419\tvalid's SFC_loss: 1.0593\n",
      "[10]\ttrain's SFC_loss: 1.03511\tvalid's SFC_loss: 1.05185\n",
      "[11]\ttrain's SFC_loss: 1.0289\tvalid's SFC_loss: 1.05159\n",
      "[12]\ttrain's SFC_loss: 1.02345\tvalid's SFC_loss: 1.0469\n",
      "[13]\ttrain's SFC_loss: 1.01689\tvalid's SFC_loss: 1.04376\n",
      "[14]\ttrain's SFC_loss: 1.01054\tvalid's SFC_loss: 1.04069\n",
      "[15]\ttrain's SFC_loss: 1.00479\tvalid's SFC_loss: 1.039\n",
      "[16]\ttrain's SFC_loss: 0.99887\tvalid's SFC_loss: 1.03558\n",
      "[17]\ttrain's SFC_loss: 0.993471\tvalid's SFC_loss: 1.03281\n",
      "[18]\ttrain's SFC_loss: 0.988042\tvalid's SFC_loss: 1.02907\n",
      "[19]\ttrain's SFC_loss: 0.982409\tvalid's SFC_loss: 1.02454\n",
      "[20]\ttrain's SFC_loss: 0.977419\tvalid's SFC_loss: 1.02143\n",
      "[21]\ttrain's SFC_loss: 0.971949\tvalid's SFC_loss: 1.01485\n",
      "[22]\ttrain's SFC_loss: 0.965932\tvalid's SFC_loss: 1.01127\n",
      "[23]\ttrain's SFC_loss: 0.959927\tvalid's SFC_loss: 1.00797\n",
      "[24]\ttrain's SFC_loss: 0.954743\tvalid's SFC_loss: 1.00447\n",
      "[25]\ttrain's SFC_loss: 0.948796\tvalid's SFC_loss: 1.00129\n",
      "[26]\ttrain's SFC_loss: 0.943956\tvalid's SFC_loss: 1.00078\n",
      "[27]\ttrain's SFC_loss: 0.940031\tvalid's SFC_loss: 0.998106\n",
      "[28]\ttrain's SFC_loss: 0.935671\tvalid's SFC_loss: 0.996398\n",
      "[29]\ttrain's SFC_loss: 0.931027\tvalid's SFC_loss: 0.996554\n",
      "[30]\ttrain's SFC_loss: 0.926231\tvalid's SFC_loss: 0.994423\n",
      "[31]\ttrain's SFC_loss: 0.921995\tvalid's SFC_loss: 0.992776\n",
      "[32]\ttrain's SFC_loss: 0.917043\tvalid's SFC_loss: 0.98938\n",
      "[33]\ttrain's SFC_loss: 0.912615\tvalid's SFC_loss: 0.988714\n",
      "[34]\ttrain's SFC_loss: 0.908202\tvalid's SFC_loss: 0.98415\n",
      "[35]\ttrain's SFC_loss: 0.904233\tvalid's SFC_loss: 0.982728\n",
      "[36]\ttrain's SFC_loss: 0.900651\tvalid's SFC_loss: 0.976142\n",
      "[37]\ttrain's SFC_loss: 0.896823\tvalid's SFC_loss: 0.972655\n",
      "[38]\ttrain's SFC_loss: 0.892545\tvalid's SFC_loss: 0.967029\n",
      "[39]\ttrain's SFC_loss: 0.888775\tvalid's SFC_loss: 0.964283\n",
      "[40]\ttrain's SFC_loss: 0.884915\tvalid's SFC_loss: 0.961661\n",
      "[41]\ttrain's SFC_loss: 0.88202\tvalid's SFC_loss: 0.957542\n",
      "[42]\ttrain's SFC_loss: 0.878842\tvalid's SFC_loss: 0.957364\n",
      "[43]\ttrain's SFC_loss: 0.875559\tvalid's SFC_loss: 0.956152\n",
      "[44]\ttrain's SFC_loss: 0.872867\tvalid's SFC_loss: 0.953816\n",
      "[45]\ttrain's SFC_loss: 0.869359\tvalid's SFC_loss: 0.951777\n",
      "[46]\ttrain's SFC_loss: 0.866562\tvalid's SFC_loss: 0.949114\n",
      "[47]\ttrain's SFC_loss: 0.863646\tvalid's SFC_loss: 0.945583\n",
      "[48]\ttrain's SFC_loss: 0.860841\tvalid's SFC_loss: 0.942729\n",
      "[49]\ttrain's SFC_loss: 0.858149\tvalid's SFC_loss: 0.939094\n",
      "[50]\ttrain's SFC_loss: 0.855668\tvalid's SFC_loss: 0.935526\n",
      "[51]\ttrain's SFC_loss: 0.852554\tvalid's SFC_loss: 0.933452\n",
      "[52]\ttrain's SFC_loss: 0.849\tvalid's SFC_loss: 0.931035\n",
      "[53]\ttrain's SFC_loss: 0.845331\tvalid's SFC_loss: 0.930326\n",
      "[54]\ttrain's SFC_loss: 0.841807\tvalid's SFC_loss: 0.930234\n",
      "[55]\ttrain's SFC_loss: 0.838942\tvalid's SFC_loss: 0.927855\n",
      "[56]\ttrain's SFC_loss: 0.835642\tvalid's SFC_loss: 0.92531\n",
      "[57]\ttrain's SFC_loss: 0.832892\tvalid's SFC_loss: 0.922274\n",
      "[58]\ttrain's SFC_loss: 0.83\tvalid's SFC_loss: 0.918821\n",
      "[59]\ttrain's SFC_loss: 0.827192\tvalid's SFC_loss: 0.915781\n",
      "[60]\ttrain's SFC_loss: 0.823954\tvalid's SFC_loss: 0.913968\n",
      "[61]\ttrain's SFC_loss: 0.819748\tvalid's SFC_loss: 0.912355\n",
      "[62]\ttrain's SFC_loss: 0.816808\tvalid's SFC_loss: 0.912632\n",
      "[63]\ttrain's SFC_loss: 0.812786\tvalid's SFC_loss: 0.911126\n",
      "[64]\ttrain's SFC_loss: 0.808856\tvalid's SFC_loss: 0.909416\n",
      "[65]\ttrain's SFC_loss: 0.805037\tvalid's SFC_loss: 0.908041\n",
      "[66]\ttrain's SFC_loss: 0.802297\tvalid's SFC_loss: 0.903165\n",
      "[67]\ttrain's SFC_loss: 0.798676\tvalid's SFC_loss: 0.901111\n",
      "[68]\ttrain's SFC_loss: 0.79575\tvalid's SFC_loss: 0.899678\n",
      "[69]\ttrain's SFC_loss: 0.792468\tvalid's SFC_loss: 0.896946\n",
      "[70]\ttrain's SFC_loss: 0.789104\tvalid's SFC_loss: 0.895192\n",
      "[71]\ttrain's SFC_loss: 0.786433\tvalid's SFC_loss: 0.895845\n",
      "[72]\ttrain's SFC_loss: 0.784243\tvalid's SFC_loss: 0.893473\n",
      "[73]\ttrain's SFC_loss: 0.781563\tvalid's SFC_loss: 0.89458\n",
      "[74]\ttrain's SFC_loss: 0.779073\tvalid's SFC_loss: 0.894307\n",
      "[75]\ttrain's SFC_loss: 0.775787\tvalid's SFC_loss: 0.893383\n",
      "[76]\ttrain's SFC_loss: 0.773749\tvalid's SFC_loss: 0.894888\n",
      "[77]\ttrain's SFC_loss: 0.771166\tvalid's SFC_loss: 0.892355\n",
      "[78]\ttrain's SFC_loss: 0.768937\tvalid's SFC_loss: 0.893699\n",
      "[79]\ttrain's SFC_loss: 0.766815\tvalid's SFC_loss: 0.89283\n",
      "[80]\ttrain's SFC_loss: 0.764809\tvalid's SFC_loss: 0.893431\n",
      "[81]\ttrain's SFC_loss: 0.762152\tvalid's SFC_loss: 0.892606\n",
      "[82]\ttrain's SFC_loss: 0.759044\tvalid's SFC_loss: 0.891202\n",
      "[83]\ttrain's SFC_loss: 0.75574\tvalid's SFC_loss: 0.892453\n",
      "[84]\ttrain's SFC_loss: 0.753485\tvalid's SFC_loss: 0.891186\n",
      "[85]\ttrain's SFC_loss: 0.750556\tvalid's SFC_loss: 0.889674\n",
      "[86]\ttrain's SFC_loss: 0.748229\tvalid's SFC_loss: 0.886763\n",
      "[87]\ttrain's SFC_loss: 0.746614\tvalid's SFC_loss: 0.883674\n",
      "[88]\ttrain's SFC_loss: 0.744797\tvalid's SFC_loss: 0.881726\n",
      "[89]\ttrain's SFC_loss: 0.742884\tvalid's SFC_loss: 0.87892\n",
      "[90]\ttrain's SFC_loss: 0.741309\tvalid's SFC_loss: 0.876931\n",
      "[91]\ttrain's SFC_loss: 0.738319\tvalid's SFC_loss: 0.874494\n",
      "[92]\ttrain's SFC_loss: 0.735041\tvalid's SFC_loss: 0.872617\n",
      "[93]\ttrain's SFC_loss: 0.732291\tvalid's SFC_loss: 0.870369\n",
      "[94]\ttrain's SFC_loss: 0.729411\tvalid's SFC_loss: 0.868762\n",
      "[95]\ttrain's SFC_loss: 0.726389\tvalid's SFC_loss: 0.866745\n",
      "[96]\ttrain's SFC_loss: 0.724471\tvalid's SFC_loss: 0.86261\n",
      "[97]\ttrain's SFC_loss: 0.722698\tvalid's SFC_loss: 0.858703\n",
      "[98]\ttrain's SFC_loss: 0.720892\tvalid's SFC_loss: 0.855961\n",
      "[99]\ttrain's SFC_loss: 0.719093\tvalid's SFC_loss: 0.854006\n",
      "[100]\ttrain's SFC_loss: 0.7169\tvalid's SFC_loss: 0.852582\n",
      "[101]\ttrain's SFC_loss: 0.714634\tvalid's SFC_loss: 0.852381\n",
      "[102]\ttrain's SFC_loss: 0.71136\tvalid's SFC_loss: 0.854441\n",
      "[103]\ttrain's SFC_loss: 0.708318\tvalid's SFC_loss: 0.854405\n",
      "[104]\ttrain's SFC_loss: 0.706149\tvalid's SFC_loss: 0.856576\n",
      "[105]\ttrain's SFC_loss: 0.704337\tvalid's SFC_loss: 0.856145\n",
      "[106]\ttrain's SFC_loss: 0.702176\tvalid's SFC_loss: 0.857159\n",
      "[107]\ttrain's SFC_loss: 0.700414\tvalid's SFC_loss: 0.857492\n",
      "[108]\ttrain's SFC_loss: 0.698599\tvalid's SFC_loss: 0.859605\n",
      "[109]\ttrain's SFC_loss: 0.696579\tvalid's SFC_loss: 0.860657\n",
      "[110]\ttrain's SFC_loss: 0.694916\tvalid's SFC_loss: 0.864231\n",
      "[111]\ttrain's SFC_loss: 0.693024\tvalid's SFC_loss: 0.862594\n",
      "[112]\ttrain's SFC_loss: 0.69062\tvalid's SFC_loss: 0.863758\n",
      "[113]\ttrain's SFC_loss: 0.688721\tvalid's SFC_loss: 0.862512\n",
      "[114]\ttrain's SFC_loss: 0.68645\tvalid's SFC_loss: 0.860895\n",
      "[115]\ttrain's SFC_loss: 0.684872\tvalid's SFC_loss: 0.860948\n",
      "[116]\ttrain's SFC_loss: 0.682647\tvalid's SFC_loss: 0.860543\n",
      "[117]\ttrain's SFC_loss: 0.680465\tvalid's SFC_loss: 0.859974\n",
      "[118]\ttrain's SFC_loss: 0.678335\tvalid's SFC_loss: 0.860101\n",
      "[119]\ttrain's SFC_loss: 0.676543\tvalid's SFC_loss: 0.860395\n",
      "[120]\ttrain's SFC_loss: 0.675165\tvalid's SFC_loss: 0.858333\n",
      "[121]\ttrain's SFC_loss: 0.672677\tvalid's SFC_loss: 0.857565\n",
      "[122]\ttrain's SFC_loss: 0.670201\tvalid's SFC_loss: 0.858533\n",
      "[123]\ttrain's SFC_loss: 0.667685\tvalid's SFC_loss: 0.859758\n",
      "[124]\ttrain's SFC_loss: 0.665428\tvalid's SFC_loss: 0.859427\n",
      "[125]\ttrain's SFC_loss: 0.663512\tvalid's SFC_loss: 0.857956\n",
      "[126]\ttrain's SFC_loss: 0.661852\tvalid's SFC_loss: 0.856971\n",
      "[127]\ttrain's SFC_loss: 0.65962\tvalid's SFC_loss: 0.85522\n",
      "[128]\ttrain's SFC_loss: 0.657364\tvalid's SFC_loss: 0.854373\n",
      "[129]\ttrain's SFC_loss: 0.656399\tvalid's SFC_loss: 0.855851\n",
      "[130]\ttrain's SFC_loss: 0.655184\tvalid's SFC_loss: 0.856704\n",
      "[131]\ttrain's SFC_loss: 0.652913\tvalid's SFC_loss: 0.855825\n",
      "[132]\ttrain's SFC_loss: 0.650155\tvalid's SFC_loss: 0.853886\n",
      "[133]\ttrain's SFC_loss: 0.647831\tvalid's SFC_loss: 0.851506\n",
      "[134]\ttrain's SFC_loss: 0.645566\tvalid's SFC_loss: 0.850567\n",
      "[135]\ttrain's SFC_loss: 0.642924\tvalid's SFC_loss: 0.850288\n",
      "[136]\ttrain's SFC_loss: 0.641471\tvalid's SFC_loss: 0.849541\n",
      "[137]\ttrain's SFC_loss: 0.64008\tvalid's SFC_loss: 0.848845\n",
      "[138]\ttrain's SFC_loss: 0.637929\tvalid's SFC_loss: 0.847886\n",
      "[139]\ttrain's SFC_loss: 0.635976\tvalid's SFC_loss: 0.844961\n",
      "[140]\ttrain's SFC_loss: 0.634035\tvalid's SFC_loss: 0.844043\n",
      "[141]\ttrain's SFC_loss: 0.632169\tvalid's SFC_loss: 0.843147\n",
      "[142]\ttrain's SFC_loss: 0.631269\tvalid's SFC_loss: 0.842958\n",
      "[143]\ttrain's SFC_loss: 0.629672\tvalid's SFC_loss: 0.842438\n",
      "[144]\ttrain's SFC_loss: 0.628074\tvalid's SFC_loss: 0.842007\n",
      "[145]\ttrain's SFC_loss: 0.626401\tvalid's SFC_loss: 0.841623\n",
      "[146]\ttrain's SFC_loss: 0.6246\tvalid's SFC_loss: 0.841406\n",
      "[147]\ttrain's SFC_loss: 0.622632\tvalid's SFC_loss: 0.840668\n",
      "[148]\ttrain's SFC_loss: 0.621213\tvalid's SFC_loss: 0.839717\n",
      "[149]\ttrain's SFC_loss: 0.620215\tvalid's SFC_loss: 0.8388\n",
      "[150]\ttrain's SFC_loss: 0.618792\tvalid's SFC_loss: 0.838138\n",
      "[151]\ttrain's SFC_loss: 0.616722\tvalid's SFC_loss: 0.837901\n",
      "[152]\ttrain's SFC_loss: 0.615015\tvalid's SFC_loss: 0.838348\n",
      "[153]\ttrain's SFC_loss: 0.613138\tvalid's SFC_loss: 0.840276\n",
      "[154]\ttrain's SFC_loss: 0.611312\tvalid's SFC_loss: 0.842217\n",
      "[155]\ttrain's SFC_loss: 0.609666\tvalid's SFC_loss: 0.842657\n",
      "[156]\ttrain's SFC_loss: 0.607613\tvalid's SFC_loss: 0.843464\n",
      "[157]\ttrain's SFC_loss: 0.605648\tvalid's SFC_loss: 0.84441\n",
      "[158]\ttrain's SFC_loss: 0.603749\tvalid's SFC_loss: 0.846979\n",
      "[159]\ttrain's SFC_loss: 0.601689\tvalid's SFC_loss: 0.845531\n",
      "[160]\ttrain's SFC_loss: 0.600035\tvalid's SFC_loss: 0.843887\n",
      "[161]\ttrain's SFC_loss: 0.599052\tvalid's SFC_loss: 0.843046\n",
      "[162]\ttrain's SFC_loss: 0.598287\tvalid's SFC_loss: 0.842422\n",
      "[163]\ttrain's SFC_loss: 0.597637\tvalid's SFC_loss: 0.841393\n",
      "[164]\ttrain's SFC_loss: 0.596636\tvalid's SFC_loss: 0.841497\n",
      "[165]\ttrain's SFC_loss: 0.596125\tvalid's SFC_loss: 0.841898\n",
      "[166]\ttrain's SFC_loss: 0.594697\tvalid's SFC_loss: 0.842018\n",
      "[167]\ttrain's SFC_loss: 0.593529\tvalid's SFC_loss: 0.841279\n",
      "[168]\ttrain's SFC_loss: 0.592253\tvalid's SFC_loss: 0.842031\n",
      "[169]\ttrain's SFC_loss: 0.590515\tvalid's SFC_loss: 0.840737\n",
      "[170]\ttrain's SFC_loss: 0.588934\tvalid's SFC_loss: 0.842548\n",
      "[171]\ttrain's SFC_loss: 0.587249\tvalid's SFC_loss: 0.840833\n",
      "[172]\ttrain's SFC_loss: 0.586181\tvalid's SFC_loss: 0.838666\n",
      "[173]\ttrain's SFC_loss: 0.584974\tvalid's SFC_loss: 0.836227\n",
      "[174]\ttrain's SFC_loss: 0.583896\tvalid's SFC_loss: 0.834644\n",
      "[175]\ttrain's SFC_loss: 0.582549\tvalid's SFC_loss: 0.834096\n",
      "[176]\ttrain's SFC_loss: 0.580923\tvalid's SFC_loss: 0.835939\n",
      "[177]\ttrain's SFC_loss: 0.579662\tvalid's SFC_loss: 0.835011\n",
      "[178]\ttrain's SFC_loss: 0.57796\tvalid's SFC_loss: 0.836712\n",
      "[179]\ttrain's SFC_loss: 0.576274\tvalid's SFC_loss: 0.834975\n",
      "[180]\ttrain's SFC_loss: 0.575018\tvalid's SFC_loss: 0.834423\n",
      "[181]\ttrain's SFC_loss: 0.573411\tvalid's SFC_loss: 0.833251\n",
      "[182]\ttrain's SFC_loss: 0.571898\tvalid's SFC_loss: 0.834245\n",
      "[183]\ttrain's SFC_loss: 0.57011\tvalid's SFC_loss: 0.833835\n",
      "[184]\ttrain's SFC_loss: 0.568588\tvalid's SFC_loss: 0.833488\n",
      "[185]\ttrain's SFC_loss: 0.567355\tvalid's SFC_loss: 0.832229\n",
      "[186]\ttrain's SFC_loss: 0.565371\tvalid's SFC_loss: 0.832116\n",
      "[187]\ttrain's SFC_loss: 0.563542\tvalid's SFC_loss: 0.831421\n",
      "[188]\ttrain's SFC_loss: 0.561507\tvalid's SFC_loss: 0.834681\n",
      "[189]\ttrain's SFC_loss: 0.559907\tvalid's SFC_loss: 0.835784\n",
      "[190]\ttrain's SFC_loss: 0.558135\tvalid's SFC_loss: 0.835132\n",
      "[191]\ttrain's SFC_loss: 0.556754\tvalid's SFC_loss: 0.835027\n",
      "[192]\ttrain's SFC_loss: 0.555656\tvalid's SFC_loss: 0.834993\n",
      "[193]\ttrain's SFC_loss: 0.554677\tvalid's SFC_loss: 0.831891\n",
      "[194]\ttrain's SFC_loss: 0.553944\tvalid's SFC_loss: 0.831143\n",
      "[195]\ttrain's SFC_loss: 0.552721\tvalid's SFC_loss: 0.830342\n",
      "[196]\ttrain's SFC_loss: 0.551281\tvalid's SFC_loss: 0.831659\n",
      "[197]\ttrain's SFC_loss: 0.549683\tvalid's SFC_loss: 0.828394\n",
      "[198]\ttrain's SFC_loss: 0.548169\tvalid's SFC_loss: 0.828974\n",
      "[199]\ttrain's SFC_loss: 0.546486\tvalid's SFC_loss: 0.826688\n",
      "[200]\ttrain's SFC_loss: 0.544951\tvalid's SFC_loss: 0.829843\n",
      "[201]\ttrain's SFC_loss: 0.542951\tvalid's SFC_loss: 0.829092\n",
      "[202]\ttrain's SFC_loss: 0.540824\tvalid's SFC_loss: 0.828377\n",
      "[203]\ttrain's SFC_loss: 0.538837\tvalid's SFC_loss: 0.8286\n",
      "[204]\ttrain's SFC_loss: 0.536654\tvalid's SFC_loss: 0.83139\n",
      "[205]\ttrain's SFC_loss: 0.534691\tvalid's SFC_loss: 0.832023\n",
      "[206]\ttrain's SFC_loss: 0.533799\tvalid's SFC_loss: 0.830118\n",
      "[207]\ttrain's SFC_loss: 0.532987\tvalid's SFC_loss: 0.83013\n",
      "[208]\ttrain's SFC_loss: 0.53234\tvalid's SFC_loss: 0.828823\n",
      "[209]\ttrain's SFC_loss: 0.531576\tvalid's SFC_loss: 0.830238\n",
      "[210]\ttrain's SFC_loss: 0.530834\tvalid's SFC_loss: 0.829854\n",
      "[211]\ttrain's SFC_loss: 0.529857\tvalid's SFC_loss: 0.826679\n",
      "[212]\ttrain's SFC_loss: 0.528628\tvalid's SFC_loss: 0.825517\n",
      "[213]\ttrain's SFC_loss: 0.527509\tvalid's SFC_loss: 0.823415\n",
      "[214]\ttrain's SFC_loss: 0.526559\tvalid's SFC_loss: 0.822389\n",
      "[215]\ttrain's SFC_loss: 0.525551\tvalid's SFC_loss: 0.8202\n",
      "[216]\ttrain's SFC_loss: 0.524054\tvalid's SFC_loss: 0.819959\n",
      "[217]\ttrain's SFC_loss: 0.522789\tvalid's SFC_loss: 0.820622\n",
      "[218]\ttrain's SFC_loss: 0.521513\tvalid's SFC_loss: 0.821725\n",
      "[219]\ttrain's SFC_loss: 0.519926\tvalid's SFC_loss: 0.818308\n",
      "[220]\ttrain's SFC_loss: 0.518422\tvalid's SFC_loss: 0.814896\n",
      "[221]\ttrain's SFC_loss: 0.516711\tvalid's SFC_loss: 0.815468\n",
      "[222]\ttrain's SFC_loss: 0.515196\tvalid's SFC_loss: 0.815729\n",
      "[223]\ttrain's SFC_loss: 0.51347\tvalid's SFC_loss: 0.813968\n",
      "[224]\ttrain's SFC_loss: 0.511636\tvalid's SFC_loss: 0.812724\n",
      "[225]\ttrain's SFC_loss: 0.509594\tvalid's SFC_loss: 0.812077\n",
      "[226]\ttrain's SFC_loss: 0.508446\tvalid's SFC_loss: 0.813419\n",
      "[227]\ttrain's SFC_loss: 0.507057\tvalid's SFC_loss: 0.813537\n",
      "[228]\ttrain's SFC_loss: 0.505898\tvalid's SFC_loss: 0.814971\n",
      "[229]\ttrain's SFC_loss: 0.504757\tvalid's SFC_loss: 0.816346\n",
      "[230]\ttrain's SFC_loss: 0.503343\tvalid's SFC_loss: 0.81644\n",
      "[231]\ttrain's SFC_loss: 0.502685\tvalid's SFC_loss: 0.815825\n",
      "[232]\ttrain's SFC_loss: 0.502001\tvalid's SFC_loss: 0.815259\n",
      "[233]\ttrain's SFC_loss: 0.501298\tvalid's SFC_loss: 0.814374\n",
      "[234]\ttrain's SFC_loss: 0.500788\tvalid's SFC_loss: 0.813967\n",
      "[235]\ttrain's SFC_loss: 0.500126\tvalid's SFC_loss: 0.812568\n",
      "[236]\ttrain's SFC_loss: 0.498844\tvalid's SFC_loss: 0.807737\n",
      "[237]\ttrain's SFC_loss: 0.498253\tvalid's SFC_loss: 0.80637\n",
      "[238]\ttrain's SFC_loss: 0.497201\tvalid's SFC_loss: 0.80281\n",
      "[239]\ttrain's SFC_loss: 0.495952\tvalid's SFC_loss: 0.797894\n",
      "[240]\ttrain's SFC_loss: 0.494709\tvalid's SFC_loss: 0.796089\n",
      "[241]\ttrain's SFC_loss: 0.493515\tvalid's SFC_loss: 0.796598\n",
      "[242]\ttrain's SFC_loss: 0.492466\tvalid's SFC_loss: 0.799566\n",
      "[243]\ttrain's SFC_loss: 0.491546\tvalid's SFC_loss: 0.800828\n",
      "[244]\ttrain's SFC_loss: 0.490222\tvalid's SFC_loss: 0.803326\n",
      "[245]\ttrain's SFC_loss: 0.488822\tvalid's SFC_loss: 0.804681\n",
      "[246]\ttrain's SFC_loss: 0.487779\tvalid's SFC_loss: 0.806994\n",
      "[247]\ttrain's SFC_loss: 0.48669\tvalid's SFC_loss: 0.808518\n",
      "[248]\ttrain's SFC_loss: 0.485295\tvalid's SFC_loss: 0.810072\n",
      "[249]\ttrain's SFC_loss: 0.484091\tvalid's SFC_loss: 0.811471\n",
      "[250]\ttrain's SFC_loss: 0.482947\tvalid's SFC_loss: 0.81265\n",
      "[251]\ttrain's SFC_loss: 0.482009\tvalid's SFC_loss: 0.814005\n",
      "[252]\ttrain's SFC_loss: 0.480955\tvalid's SFC_loss: 0.81552\n",
      "[253]\ttrain's SFC_loss: 0.479857\tvalid's SFC_loss: 0.816423\n",
      "[254]\ttrain's SFC_loss: 0.478608\tvalid's SFC_loss: 0.817863\n",
      "[255]\ttrain's SFC_loss: 0.477549\tvalid's SFC_loss: 0.818735\n",
      "[256]\ttrain's SFC_loss: 0.476434\tvalid's SFC_loss: 0.818144\n",
      "[257]\ttrain's SFC_loss: 0.475471\tvalid's SFC_loss: 0.815952\n",
      "[258]\ttrain's SFC_loss: 0.474322\tvalid's SFC_loss: 0.814045\n",
      "[259]\ttrain's SFC_loss: 0.473483\tvalid's SFC_loss: 0.812077\n",
      "[260]\ttrain's SFC_loss: 0.472631\tvalid's SFC_loss: 0.811346\n",
      "[261]\ttrain's SFC_loss: 0.471368\tvalid's SFC_loss: 0.81379\n",
      "[262]\ttrain's SFC_loss: 0.470521\tvalid's SFC_loss: 0.815073\n",
      "[263]\ttrain's SFC_loss: 0.46954\tvalid's SFC_loss: 0.815152\n",
      "[264]\ttrain's SFC_loss: 0.468705\tvalid's SFC_loss: 0.81702\n",
      "[265]\ttrain's SFC_loss: 0.46773\tvalid's SFC_loss: 0.819829\n",
      "[266]\ttrain's SFC_loss: 0.466272\tvalid's SFC_loss: 0.821621\n",
      "[267]\ttrain's SFC_loss: 0.46511\tvalid's SFC_loss: 0.82298\n",
      "[268]\ttrain's SFC_loss: 0.46416\tvalid's SFC_loss: 0.824025\n",
      "[269]\ttrain's SFC_loss: 0.463123\tvalid's SFC_loss: 0.823603\n",
      "[270]\ttrain's SFC_loss: 0.462198\tvalid's SFC_loss: 0.822761\n",
      "[271]\ttrain's SFC_loss: 0.461217\tvalid's SFC_loss: 0.82336\n",
      "[272]\ttrain's SFC_loss: 0.460366\tvalid's SFC_loss: 0.822707\n",
      "[273]\ttrain's SFC_loss: 0.459466\tvalid's SFC_loss: 0.82294\n",
      "[274]\ttrain's SFC_loss: 0.458109\tvalid's SFC_loss: 0.824886\n",
      "[275]\ttrain's SFC_loss: 0.457289\tvalid's SFC_loss: 0.825261\n",
      "[276]\ttrain's SFC_loss: 0.456165\tvalid's SFC_loss: 0.823913\n",
      "[277]\ttrain's SFC_loss: 0.454898\tvalid's SFC_loss: 0.824363\n",
      "[278]\ttrain's SFC_loss: 0.45399\tvalid's SFC_loss: 0.825949\n",
      "[279]\ttrain's SFC_loss: 0.452904\tvalid's SFC_loss: 0.828022\n",
      "[280]\ttrain's SFC_loss: 0.452063\tvalid's SFC_loss: 0.829173\n",
      "[281]\ttrain's SFC_loss: 0.451052\tvalid's SFC_loss: 0.829783\n",
      "[282]\ttrain's SFC_loss: 0.450062\tvalid's SFC_loss: 0.829451\n",
      "[283]\ttrain's SFC_loss: 0.449043\tvalid's SFC_loss: 0.829131\n",
      "[284]\ttrain's SFC_loss: 0.448001\tvalid's SFC_loss: 0.828119\n",
      "[285]\ttrain's SFC_loss: 0.447014\tvalid's SFC_loss: 0.8281\n",
      "[286]\ttrain's SFC_loss: 0.445695\tvalid's SFC_loss: 0.831641\n",
      "[287]\ttrain's SFC_loss: 0.444523\tvalid's SFC_loss: 0.830913\n",
      "[288]\ttrain's SFC_loss: 0.443365\tvalid's SFC_loss: 0.830321\n",
      "[289]\ttrain's SFC_loss: 0.442294\tvalid's SFC_loss: 0.827269\n",
      "[290]\ttrain's SFC_loss: 0.441236\tvalid's SFC_loss: 0.824361\n",
      "[291]\ttrain's SFC_loss: 0.440448\tvalid's SFC_loss: 0.823086\n",
      "[292]\ttrain's SFC_loss: 0.439465\tvalid's SFC_loss: 0.823534\n",
      "[293]\ttrain's SFC_loss: 0.438292\tvalid's SFC_loss: 0.822602\n",
      "[294]\ttrain's SFC_loss: 0.437115\tvalid's SFC_loss: 0.822315\n",
      "[295]\ttrain's SFC_loss: 0.436034\tvalid's SFC_loss: 0.821528\n",
      "[296]\ttrain's SFC_loss: 0.435677\tvalid's SFC_loss: 0.821924\n",
      "[297]\ttrain's SFC_loss: 0.435155\tvalid's SFC_loss: 0.822761\n",
      "[298]\ttrain's SFC_loss: 0.434859\tvalid's SFC_loss: 0.822994\n",
      "[299]\ttrain's SFC_loss: 0.434296\tvalid's SFC_loss: 0.82338\n",
      "[300]\ttrain's SFC_loss: 0.434106\tvalid's SFC_loss: 0.823489\n",
      "[301]\ttrain's SFC_loss: 0.433153\tvalid's SFC_loss: 0.821538\n",
      "[302]\ttrain's SFC_loss: 0.432358\tvalid's SFC_loss: 0.818897\n",
      "[303]\ttrain's SFC_loss: 0.431391\tvalid's SFC_loss: 0.818071\n",
      "[304]\ttrain's SFC_loss: 0.430457\tvalid's SFC_loss: 0.817281\n",
      "[305]\ttrain's SFC_loss: 0.429734\tvalid's SFC_loss: 0.820181\n",
      "[306]\ttrain's SFC_loss: 0.427997\tvalid's SFC_loss: 0.817776\n",
      "[307]\ttrain's SFC_loss: 0.426662\tvalid's SFC_loss: 0.817392\n",
      "[308]\ttrain's SFC_loss: 0.425441\tvalid's SFC_loss: 0.817076\n",
      "[309]\ttrain's SFC_loss: 0.424205\tvalid's SFC_loss: 0.81671\n",
      "[310]\ttrain's SFC_loss: 0.423005\tvalid's SFC_loss: 0.816365\n",
      "[311]\ttrain's SFC_loss: 0.421997\tvalid's SFC_loss: 0.816357\n",
      "[312]\ttrain's SFC_loss: 0.421329\tvalid's SFC_loss: 0.815855\n",
      "[313]\ttrain's SFC_loss: 0.420263\tvalid's SFC_loss: 0.817042\n",
      "[314]\ttrain's SFC_loss: 0.419487\tvalid's SFC_loss: 0.817428\n",
      "[315]\ttrain's SFC_loss: 0.418671\tvalid's SFC_loss: 0.817601\n",
      "[316]\ttrain's SFC_loss: 0.418091\tvalid's SFC_loss: 0.818581\n",
      "[317]\ttrain's SFC_loss: 0.417523\tvalid's SFC_loss: 0.818626\n",
      "[318]\ttrain's SFC_loss: 0.417009\tvalid's SFC_loss: 0.820374\n",
      "[319]\ttrain's SFC_loss: 0.416513\tvalid's SFC_loss: 0.8212\n",
      "[320]\ttrain's SFC_loss: 0.416143\tvalid's SFC_loss: 0.822944\n",
      "[321]\ttrain's SFC_loss: 0.414862\tvalid's SFC_loss: 0.825428\n",
      "[322]\ttrain's SFC_loss: 0.414038\tvalid's SFC_loss: 0.826731\n",
      "[323]\ttrain's SFC_loss: 0.412827\tvalid's SFC_loss: 0.829025\n",
      "[324]\ttrain's SFC_loss: 0.411754\tvalid's SFC_loss: 0.831759\n",
      "[325]\ttrain's SFC_loss: 0.410771\tvalid's SFC_loss: 0.832782\n",
      "[326]\ttrain's SFC_loss: 0.409683\tvalid's SFC_loss: 0.834686\n",
      "[327]\ttrain's SFC_loss: 0.408797\tvalid's SFC_loss: 0.837393\n",
      "[328]\ttrain's SFC_loss: 0.408039\tvalid's SFC_loss: 0.83851\n",
      "[329]\ttrain's SFC_loss: 0.40731\tvalid's SFC_loss: 0.837914\n",
      "[330]\ttrain's SFC_loss: 0.406544\tvalid's SFC_loss: 0.835903\n",
      "[331]\ttrain's SFC_loss: 0.405606\tvalid's SFC_loss: 0.836325\n",
      "[332]\ttrain's SFC_loss: 0.404578\tvalid's SFC_loss: 0.835783\n",
      "[333]\ttrain's SFC_loss: 0.403468\tvalid's SFC_loss: 0.83687\n",
      "[334]\ttrain's SFC_loss: 0.402446\tvalid's SFC_loss: 0.838937\n",
      "[335]\ttrain's SFC_loss: 0.401246\tvalid's SFC_loss: 0.839101\n",
      "[336]\ttrain's SFC_loss: 0.400202\tvalid's SFC_loss: 0.840332\n",
      "[337]\ttrain's SFC_loss: 0.399513\tvalid's SFC_loss: 0.839491\n",
      "[338]\ttrain's SFC_loss: 0.39911\tvalid's SFC_loss: 0.839451\n",
      "[339]\ttrain's SFC_loss: 0.398726\tvalid's SFC_loss: 0.839432\n",
      "[340]\ttrain's SFC_loss: 0.398361\tvalid's SFC_loss: 0.839436\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttrain's SFC_loss: 0.494709\tvalid's SFC_loss: 0.796089\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1    0.033925\n",
      "1    feature2   -0.008011\n",
      "2    feature3   -0.085604\n",
      "3    feature4   -0.032797\n",
      "4    feature5    0.038004\n",
      "5    feature6    0.064685\n",
      "6    feature7   -0.072203\n",
      "7    feature8    0.067892\n",
      "8    feature9    0.034341\n",
      "9   feature10   -0.055417\n",
      "10  feature11   -0.008558\n",
      "11  feature12    0.000295\n",
      "12  feature13   -0.006481\n",
      "13  feature14   -0.095640\n",
      "14  feature15   -0.042086\n",
      "15  feature16    0.039895\n",
      "16  feature17   -0.001994\n",
      "17  feature18    0.088913\n",
      "18  feature19    0.132049\n",
      "19  feature20   -0.091208\n",
      "-------------------- 1 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05752\tvalid's multi_logloss: 1.06822\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05287\tvalid's multi_logloss: 1.06819\n",
      "[3]\ttrain's multi_logloss: 1.04903\tvalid's multi_logloss: 1.07082\n",
      "[4]\ttrain's multi_logloss: 1.04616\tvalid's multi_logloss: 1.07013\n",
      "[5]\ttrain's multi_logloss: 1.04316\tvalid's multi_logloss: 1.07106\n",
      "[6]\ttrain's multi_logloss: 1.0392\tvalid's multi_logloss: 1.07126\n",
      "[7]\ttrain's multi_logloss: 1.03484\tvalid's multi_logloss: 1.06883\n",
      "[8]\ttrain's multi_logloss: 1.03115\tvalid's multi_logloss: 1.06742\n",
      "[9]\ttrain's multi_logloss: 1.02726\tvalid's multi_logloss: 1.06741\n",
      "[10]\ttrain's multi_logloss: 1.02368\tvalid's multi_logloss: 1.06605\n",
      "[11]\ttrain's multi_logloss: 1.01949\tvalid's multi_logloss: 1.06932\n",
      "[12]\ttrain's multi_logloss: 1.01521\tvalid's multi_logloss: 1.07176\n",
      "[13]\ttrain's multi_logloss: 1.01102\tvalid's multi_logloss: 1.07287\n",
      "[14]\ttrain's multi_logloss: 1.00759\tvalid's multi_logloss: 1.0743\n",
      "[15]\ttrain's multi_logloss: 1.00376\tvalid's multi_logloss: 1.0764\n",
      "[16]\ttrain's multi_logloss: 0.999494\tvalid's multi_logloss: 1.07693\n",
      "[17]\ttrain's multi_logloss: 0.995748\tvalid's multi_logloss: 1.07575\n",
      "[18]\ttrain's multi_logloss: 0.992977\tvalid's multi_logloss: 1.07865\n",
      "[19]\ttrain's multi_logloss: 0.989451\tvalid's multi_logloss: 1.07992\n",
      "[20]\ttrain's multi_logloss: 0.986557\tvalid's multi_logloss: 1.08067\n",
      "[21]\ttrain's multi_logloss: 0.98322\tvalid's multi_logloss: 1.07837\n",
      "[22]\ttrain's multi_logloss: 0.979104\tvalid's multi_logloss: 1.07886\n",
      "[23]\ttrain's multi_logloss: 0.97503\tvalid's multi_logloss: 1.07932\n",
      "[24]\ttrain's multi_logloss: 0.971167\tvalid's multi_logloss: 1.08004\n",
      "[25]\ttrain's multi_logloss: 0.967401\tvalid's multi_logloss: 1.07982\n",
      "[26]\ttrain's multi_logloss: 0.964099\tvalid's multi_logloss: 1.0766\n",
      "[27]\ttrain's multi_logloss: 0.960165\tvalid's multi_logloss: 1.07823\n",
      "[28]\ttrain's multi_logloss: 0.956576\tvalid's multi_logloss: 1.07916\n",
      "[29]\ttrain's multi_logloss: 0.953212\tvalid's multi_logloss: 1.07591\n",
      "[30]\ttrain's multi_logloss: 0.949151\tvalid's multi_logloss: 1.07552\n",
      "[31]\ttrain's multi_logloss: 0.946259\tvalid's multi_logloss: 1.07616\n",
      "[32]\ttrain's multi_logloss: 0.942633\tvalid's multi_logloss: 1.07799\n",
      "[33]\ttrain's multi_logloss: 0.939507\tvalid's multi_logloss: 1.07817\n",
      "[34]\ttrain's multi_logloss: 0.936302\tvalid's multi_logloss: 1.07975\n",
      "[35]\ttrain's multi_logloss: 0.933116\tvalid's multi_logloss: 1.07992\n",
      "[36]\ttrain's multi_logloss: 0.92936\tvalid's multi_logloss: 1.08007\n",
      "[37]\ttrain's multi_logloss: 0.92574\tvalid's multi_logloss: 1.08135\n",
      "[38]\ttrain's multi_logloss: 0.921431\tvalid's multi_logloss: 1.08226\n",
      "[39]\ttrain's multi_logloss: 0.917243\tvalid's multi_logloss: 1.08323\n",
      "[40]\ttrain's multi_logloss: 0.913222\tvalid's multi_logloss: 1.08534\n",
      "[41]\ttrain's multi_logloss: 0.910331\tvalid's multi_logloss: 1.08459\n",
      "[42]\ttrain's multi_logloss: 0.908176\tvalid's multi_logloss: 1.08365\n",
      "[43]\ttrain's multi_logloss: 0.905869\tvalid's multi_logloss: 1.08347\n",
      "[44]\ttrain's multi_logloss: 0.903422\tvalid's multi_logloss: 1.08212\n",
      "[45]\ttrain's multi_logloss: 0.90018\tvalid's multi_logloss: 1.08395\n",
      "[46]\ttrain's multi_logloss: 0.897705\tvalid's multi_logloss: 1.08374\n",
      "[47]\ttrain's multi_logloss: 0.895481\tvalid's multi_logloss: 1.08393\n",
      "[48]\ttrain's multi_logloss: 0.893254\tvalid's multi_logloss: 1.08517\n",
      "[49]\ttrain's multi_logloss: 0.891468\tvalid's multi_logloss: 1.08574\n",
      "[50]\ttrain's multi_logloss: 0.889359\tvalid's multi_logloss: 1.08596\n",
      "[51]\ttrain's multi_logloss: 0.886357\tvalid's multi_logloss: 1.08797\n",
      "[52]\ttrain's multi_logloss: 0.883816\tvalid's multi_logloss: 1.08865\n",
      "[53]\ttrain's multi_logloss: 0.88121\tvalid's multi_logloss: 1.08918\n",
      "[54]\ttrain's multi_logloss: 0.879446\tvalid's multi_logloss: 1.09015\n",
      "[55]\ttrain's multi_logloss: 0.876605\tvalid's multi_logloss: 1.09223\n",
      "[56]\ttrain's multi_logloss: 0.87401\tvalid's multi_logloss: 1.09236\n",
      "[57]\ttrain's multi_logloss: 0.87175\tvalid's multi_logloss: 1.09298\n",
      "[58]\ttrain's multi_logloss: 0.869379\tvalid's multi_logloss: 1.09181\n",
      "[59]\ttrain's multi_logloss: 0.866569\tvalid's multi_logloss: 1.0924\n",
      "[60]\ttrain's multi_logloss: 0.863941\tvalid's multi_logloss: 1.09346\n",
      "[61]\ttrain's multi_logloss: 0.860957\tvalid's multi_logloss: 1.09482\n",
      "[62]\ttrain's multi_logloss: 0.857935\tvalid's multi_logloss: 1.09025\n",
      "[63]\ttrain's multi_logloss: 0.855278\tvalid's multi_logloss: 1.0891\n",
      "[64]\ttrain's multi_logloss: 0.852745\tvalid's multi_logloss: 1.08859\n",
      "[65]\ttrain's multi_logloss: 0.850256\tvalid's multi_logloss: 1.08839\n",
      "[66]\ttrain's multi_logloss: 0.847666\tvalid's multi_logloss: 1.08991\n",
      "[67]\ttrain's multi_logloss: 0.844613\tvalid's multi_logloss: 1.09299\n",
      "[68]\ttrain's multi_logloss: 0.84184\tvalid's multi_logloss: 1.09627\n",
      "[69]\ttrain's multi_logloss: 0.839058\tvalid's multi_logloss: 1.09925\n",
      "[70]\ttrain's multi_logloss: 0.836327\tvalid's multi_logloss: 1.10227\n",
      "[71]\ttrain's multi_logloss: 0.834503\tvalid's multi_logloss: 1.1036\n",
      "[72]\ttrain's multi_logloss: 0.832718\tvalid's multi_logloss: 1.10541\n",
      "[73]\ttrain's multi_logloss: 0.830708\tvalid's multi_logloss: 1.10692\n",
      "[74]\ttrain's multi_logloss: 0.828924\tvalid's multi_logloss: 1.10785\n",
      "[75]\ttrain's multi_logloss: 0.827572\tvalid's multi_logloss: 1.10726\n",
      "[76]\ttrain's multi_logloss: 0.82461\tvalid's multi_logloss: 1.10882\n",
      "[77]\ttrain's multi_logloss: 0.82186\tvalid's multi_logloss: 1.10994\n",
      "[78]\ttrain's multi_logloss: 0.819277\tvalid's multi_logloss: 1.10958\n",
      "[79]\ttrain's multi_logloss: 0.816624\tvalid's multi_logloss: 1.10767\n",
      "[80]\ttrain's multi_logloss: 0.813904\tvalid's multi_logloss: 1.10772\n",
      "[81]\ttrain's multi_logloss: 0.81192\tvalid's multi_logloss: 1.10455\n",
      "[82]\ttrain's multi_logloss: 0.81001\tvalid's multi_logloss: 1.1031\n",
      "[83]\ttrain's multi_logloss: 0.807815\tvalid's multi_logloss: 1.10271\n",
      "[84]\ttrain's multi_logloss: 0.805705\tvalid's multi_logloss: 1.10277\n",
      "[85]\ttrain's multi_logloss: 0.803289\tvalid's multi_logloss: 1.10261\n",
      "[86]\ttrain's multi_logloss: 0.800513\tvalid's multi_logloss: 1.10247\n",
      "[87]\ttrain's multi_logloss: 0.798326\tvalid's multi_logloss: 1.1026\n",
      "[88]\ttrain's multi_logloss: 0.795909\tvalid's multi_logloss: 1.10186\n",
      "[89]\ttrain's multi_logloss: 0.793261\tvalid's multi_logloss: 1.10183\n",
      "[90]\ttrain's multi_logloss: 0.791081\tvalid's multi_logloss: 1.10238\n",
      "[91]\ttrain's multi_logloss: 0.788564\tvalid's multi_logloss: 1.10031\n",
      "[92]\ttrain's multi_logloss: 0.786784\tvalid's multi_logloss: 1.09961\n",
      "[93]\ttrain's multi_logloss: 0.783887\tvalid's multi_logloss: 1.0993\n",
      "[94]\ttrain's multi_logloss: 0.781478\tvalid's multi_logloss: 1.09807\n",
      "[95]\ttrain's multi_logloss: 0.778946\tvalid's multi_logloss: 1.09646\n",
      "[96]\ttrain's multi_logloss: 0.776473\tvalid's multi_logloss: 1.09783\n",
      "[97]\ttrain's multi_logloss: 0.774023\tvalid's multi_logloss: 1.10046\n",
      "[98]\ttrain's multi_logloss: 0.771841\tvalid's multi_logloss: 1.10302\n",
      "[99]\ttrain's multi_logloss: 0.769463\tvalid's multi_logloss: 1.10567\n",
      "[100]\ttrain's multi_logloss: 0.767103\tvalid's multi_logloss: 1.10761\n",
      "[101]\ttrain's multi_logloss: 0.765173\tvalid's multi_logloss: 1.10813\n",
      "[102]\ttrain's multi_logloss: 0.762747\tvalid's multi_logloss: 1.11008\n",
      "[103]\ttrain's multi_logloss: 0.760848\tvalid's multi_logloss: 1.11459\n",
      "[104]\ttrain's multi_logloss: 0.758658\tvalid's multi_logloss: 1.11317\n",
      "[105]\ttrain's multi_logloss: 0.756919\tvalid's multi_logloss: 1.11311\n",
      "[106]\ttrain's multi_logloss: 0.755543\tvalid's multi_logloss: 1.11438\n",
      "[107]\ttrain's multi_logloss: 0.753673\tvalid's multi_logloss: 1.11367\n",
      "[108]\ttrain's multi_logloss: 0.75186\tvalid's multi_logloss: 1.1128\n",
      "[109]\ttrain's multi_logloss: 0.750164\tvalid's multi_logloss: 1.11283\n",
      "[110]\ttrain's multi_logloss: 0.748357\tvalid's multi_logloss: 1.11138\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttrain's multi_logloss: 1.02368\tvalid's multi_logloss: 1.06605\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.45454545454545453\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09252\tvalid's SFC_loss: 1.09792\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08604\tvalid's SFC_loss: 1.09874\n",
      "[3]\ttrain's SFC_loss: 1.0787\tvalid's SFC_loss: 1.0972\n",
      "[4]\ttrain's SFC_loss: 1.07325\tvalid's SFC_loss: 1.09698\n",
      "[5]\ttrain's SFC_loss: 1.06847\tvalid's SFC_loss: 1.09584\n",
      "[6]\ttrain's SFC_loss: 1.06319\tvalid's SFC_loss: 1.09753\n",
      "[7]\ttrain's SFC_loss: 1.05646\tvalid's SFC_loss: 1.0923\n",
      "[8]\ttrain's SFC_loss: 1.05009\tvalid's SFC_loss: 1.09366\n",
      "[9]\ttrain's SFC_loss: 1.04416\tvalid's SFC_loss: 1.09082\n",
      "[10]\ttrain's SFC_loss: 1.03787\tvalid's SFC_loss: 1.09144\n",
      "[11]\ttrain's SFC_loss: 1.03224\tvalid's SFC_loss: 1.09268\n",
      "[12]\ttrain's SFC_loss: 1.02576\tvalid's SFC_loss: 1.0972\n",
      "[13]\ttrain's SFC_loss: 1.01876\tvalid's SFC_loss: 1.10133\n",
      "[14]\ttrain's SFC_loss: 1.01315\tvalid's SFC_loss: 1.10402\n",
      "[15]\ttrain's SFC_loss: 1.00718\tvalid's SFC_loss: 1.10654\n",
      "[16]\ttrain's SFC_loss: 1.00201\tvalid's SFC_loss: 1.10715\n",
      "[17]\ttrain's SFC_loss: 0.996983\tvalid's SFC_loss: 1.10816\n",
      "[18]\ttrain's SFC_loss: 0.992656\tvalid's SFC_loss: 1.1107\n",
      "[19]\ttrain's SFC_loss: 0.988695\tvalid's SFC_loss: 1.11279\n",
      "[20]\ttrain's SFC_loss: 0.985235\tvalid's SFC_loss: 1.11601\n",
      "[21]\ttrain's SFC_loss: 0.980388\tvalid's SFC_loss: 1.11161\n",
      "[22]\ttrain's SFC_loss: 0.976321\tvalid's SFC_loss: 1.10923\n",
      "[23]\ttrain's SFC_loss: 0.971316\tvalid's SFC_loss: 1.10594\n",
      "[24]\ttrain's SFC_loss: 0.96606\tvalid's SFC_loss: 1.10428\n",
      "[25]\ttrain's SFC_loss: 0.959943\tvalid's SFC_loss: 1.10695\n",
      "[26]\ttrain's SFC_loss: 0.95448\tvalid's SFC_loss: 1.10761\n",
      "[27]\ttrain's SFC_loss: 0.948854\tvalid's SFC_loss: 1.10925\n",
      "[28]\ttrain's SFC_loss: 0.943958\tvalid's SFC_loss: 1.10957\n",
      "[29]\ttrain's SFC_loss: 0.938831\tvalid's SFC_loss: 1.10963\n",
      "[30]\ttrain's SFC_loss: 0.933017\tvalid's SFC_loss: 1.11586\n",
      "[31]\ttrain's SFC_loss: 0.928327\tvalid's SFC_loss: 1.11674\n",
      "[32]\ttrain's SFC_loss: 0.923495\tvalid's SFC_loss: 1.11768\n",
      "[33]\ttrain's SFC_loss: 0.918932\tvalid's SFC_loss: 1.11837\n",
      "[34]\ttrain's SFC_loss: 0.914145\tvalid's SFC_loss: 1.12047\n",
      "[35]\ttrain's SFC_loss: 0.909831\tvalid's SFC_loss: 1.1169\n",
      "[36]\ttrain's SFC_loss: 0.905029\tvalid's SFC_loss: 1.11172\n",
      "[37]\ttrain's SFC_loss: 0.900486\tvalid's SFC_loss: 1.11437\n",
      "[38]\ttrain's SFC_loss: 0.894993\tvalid's SFC_loss: 1.11545\n",
      "[39]\ttrain's SFC_loss: 0.889725\tvalid's SFC_loss: 1.11682\n",
      "[40]\ttrain's SFC_loss: 0.884945\tvalid's SFC_loss: 1.11789\n",
      "[41]\ttrain's SFC_loss: 0.881858\tvalid's SFC_loss: 1.11535\n",
      "[42]\ttrain's SFC_loss: 0.878489\tvalid's SFC_loss: 1.11414\n",
      "[43]\ttrain's SFC_loss: 0.874618\tvalid's SFC_loss: 1.11284\n",
      "[44]\ttrain's SFC_loss: 0.87214\tvalid's SFC_loss: 1.11141\n",
      "[45]\ttrain's SFC_loss: 0.868925\tvalid's SFC_loss: 1.11149\n",
      "[46]\ttrain's SFC_loss: 0.865944\tvalid's SFC_loss: 1.11299\n",
      "[47]\ttrain's SFC_loss: 0.862228\tvalid's SFC_loss: 1.1081\n",
      "[48]\ttrain's SFC_loss: 0.859146\tvalid's SFC_loss: 1.10782\n",
      "[49]\ttrain's SFC_loss: 0.856724\tvalid's SFC_loss: 1.10756\n",
      "[50]\ttrain's SFC_loss: 0.853431\tvalid's SFC_loss: 1.10563\n",
      "[51]\ttrain's SFC_loss: 0.850267\tvalid's SFC_loss: 1.10501\n",
      "[52]\ttrain's SFC_loss: 0.846965\tvalid's SFC_loss: 1.1071\n",
      "[53]\ttrain's SFC_loss: 0.843281\tvalid's SFC_loss: 1.11033\n",
      "[54]\ttrain's SFC_loss: 0.839909\tvalid's SFC_loss: 1.11391\n",
      "[55]\ttrain's SFC_loss: 0.836884\tvalid's SFC_loss: 1.11605\n",
      "[56]\ttrain's SFC_loss: 0.833725\tvalid's SFC_loss: 1.11452\n",
      "[57]\ttrain's SFC_loss: 0.830482\tvalid's SFC_loss: 1.11464\n",
      "[58]\ttrain's SFC_loss: 0.827139\tvalid's SFC_loss: 1.11574\n",
      "[59]\ttrain's SFC_loss: 0.823841\tvalid's SFC_loss: 1.11633\n",
      "[60]\ttrain's SFC_loss: 0.820569\tvalid's SFC_loss: 1.11688\n",
      "[61]\ttrain's SFC_loss: 0.817193\tvalid's SFC_loss: 1.11437\n",
      "[62]\ttrain's SFC_loss: 0.814\tvalid's SFC_loss: 1.11633\n",
      "[63]\ttrain's SFC_loss: 0.81051\tvalid's SFC_loss: 1.11584\n",
      "[64]\ttrain's SFC_loss: 0.806645\tvalid's SFC_loss: 1.11111\n",
      "[65]\ttrain's SFC_loss: 0.803304\tvalid's SFC_loss: 1.11101\n",
      "[66]\ttrain's SFC_loss: 0.80071\tvalid's SFC_loss: 1.10982\n",
      "[67]\ttrain's SFC_loss: 0.796821\tvalid's SFC_loss: 1.11285\n",
      "[68]\ttrain's SFC_loss: 0.793567\tvalid's SFC_loss: 1.1164\n",
      "[69]\ttrain's SFC_loss: 0.790245\tvalid's SFC_loss: 1.12082\n",
      "[70]\ttrain's SFC_loss: 0.786758\tvalid's SFC_loss: 1.12576\n",
      "[71]\ttrain's SFC_loss: 0.783448\tvalid's SFC_loss: 1.12688\n",
      "[72]\ttrain's SFC_loss: 0.781057\tvalid's SFC_loss: 1.1277\n",
      "[73]\ttrain's SFC_loss: 0.778823\tvalid's SFC_loss: 1.12842\n",
      "[74]\ttrain's SFC_loss: 0.77607\tvalid's SFC_loss: 1.12793\n",
      "[75]\ttrain's SFC_loss: 0.773596\tvalid's SFC_loss: 1.12705\n",
      "[76]\ttrain's SFC_loss: 0.769907\tvalid's SFC_loss: 1.12936\n",
      "[77]\ttrain's SFC_loss: 0.767096\tvalid's SFC_loss: 1.12946\n",
      "[78]\ttrain's SFC_loss: 0.763517\tvalid's SFC_loss: 1.12905\n",
      "[79]\ttrain's SFC_loss: 0.760848\tvalid's SFC_loss: 1.1284\n",
      "[80]\ttrain's SFC_loss: 0.757631\tvalid's SFC_loss: 1.13105\n",
      "[81]\ttrain's SFC_loss: 0.755031\tvalid's SFC_loss: 1.13073\n",
      "[82]\ttrain's SFC_loss: 0.752263\tvalid's SFC_loss: 1.12862\n",
      "[83]\ttrain's SFC_loss: 0.749566\tvalid's SFC_loss: 1.12602\n",
      "[84]\ttrain's SFC_loss: 0.746989\tvalid's SFC_loss: 1.12708\n",
      "[85]\ttrain's SFC_loss: 0.744283\tvalid's SFC_loss: 1.12718\n",
      "[86]\ttrain's SFC_loss: 0.741494\tvalid's SFC_loss: 1.1267\n",
      "[87]\ttrain's SFC_loss: 0.739357\tvalid's SFC_loss: 1.12657\n",
      "[88]\ttrain's SFC_loss: 0.736881\tvalid's SFC_loss: 1.12472\n",
      "[89]\ttrain's SFC_loss: 0.734402\tvalid's SFC_loss: 1.12474\n",
      "[90]\ttrain's SFC_loss: 0.732008\tvalid's SFC_loss: 1.12513\n",
      "[91]\ttrain's SFC_loss: 0.728668\tvalid's SFC_loss: 1.12528\n",
      "[92]\ttrain's SFC_loss: 0.725465\tvalid's SFC_loss: 1.12959\n",
      "[93]\ttrain's SFC_loss: 0.722357\tvalid's SFC_loss: 1.13111\n",
      "[94]\ttrain's SFC_loss: 0.719346\tvalid's SFC_loss: 1.13214\n",
      "[95]\ttrain's SFC_loss: 0.716271\tvalid's SFC_loss: 1.13217\n",
      "[96]\ttrain's SFC_loss: 0.713584\tvalid's SFC_loss: 1.13416\n",
      "[97]\ttrain's SFC_loss: 0.711325\tvalid's SFC_loss: 1.13747\n",
      "[98]\ttrain's SFC_loss: 0.70854\tvalid's SFC_loss: 1.13712\n",
      "[99]\ttrain's SFC_loss: 0.706043\tvalid's SFC_loss: 1.1392\n",
      "[100]\ttrain's SFC_loss: 0.70352\tvalid's SFC_loss: 1.14247\n",
      "[101]\ttrain's SFC_loss: 0.701064\tvalid's SFC_loss: 1.14446\n",
      "[102]\ttrain's SFC_loss: 0.698251\tvalid's SFC_loss: 1.14479\n",
      "[103]\ttrain's SFC_loss: 0.695595\tvalid's SFC_loss: 1.14228\n",
      "[104]\ttrain's SFC_loss: 0.693694\tvalid's SFC_loss: 1.13892\n",
      "[105]\ttrain's SFC_loss: 0.691379\tvalid's SFC_loss: 1.13788\n",
      "[106]\ttrain's SFC_loss: 0.688787\tvalid's SFC_loss: 1.13631\n",
      "[107]\ttrain's SFC_loss: 0.686419\tvalid's SFC_loss: 1.13421\n",
      "[108]\ttrain's SFC_loss: 0.683798\tvalid's SFC_loss: 1.13204\n",
      "[109]\ttrain's SFC_loss: 0.681475\tvalid's SFC_loss: 1.12963\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttrain's SFC_loss: 1.04416\tvalid's SFC_loss: 1.09082\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.36363636363636365\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1   -0.004448\n",
      "1    feature2    0.045740\n",
      "2    feature3   -0.149556\n",
      "3    feature4    0.011228\n",
      "4    feature5    0.011013\n",
      "5    feature6   -0.048259\n",
      "6    feature7    0.000000\n",
      "7    feature8    0.000000\n",
      "8    feature9   -0.014714\n",
      "9   feature10    0.032317\n",
      "10  feature11    0.015148\n",
      "11  feature12   -0.150870\n",
      "12  feature13    0.000000\n",
      "13  feature14   -0.033362\n",
      "14  feature15    0.306172\n",
      "15  feature16    0.000000\n",
      "16  feature17    0.048177\n",
      "17  feature18   -0.065676\n",
      "18  feature19    0.030205\n",
      "19  feature20   -0.033115\n",
      "-------------------- 2 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05821\tvalid's multi_logloss: 1.06583\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05435\tvalid's multi_logloss: 1.06476\n",
      "[3]\ttrain's multi_logloss: 1.05074\tvalid's multi_logloss: 1.0632\n",
      "[4]\ttrain's multi_logloss: 1.0475\tvalid's multi_logloss: 1.06317\n",
      "[5]\ttrain's multi_logloss: 1.04375\tvalid's multi_logloss: 1.06206\n",
      "[6]\ttrain's multi_logloss: 1.04022\tvalid's multi_logloss: 1.05857\n",
      "[7]\ttrain's multi_logloss: 1.03656\tvalid's multi_logloss: 1.05698\n",
      "[8]\ttrain's multi_logloss: 1.03304\tvalid's multi_logloss: 1.05394\n",
      "[9]\ttrain's multi_logloss: 1.03008\tvalid's multi_logloss: 1.05374\n",
      "[10]\ttrain's multi_logloss: 1.02663\tvalid's multi_logloss: 1.05076\n",
      "[11]\ttrain's multi_logloss: 1.02235\tvalid's multi_logloss: 1.04816\n",
      "[12]\ttrain's multi_logloss: 1.01839\tvalid's multi_logloss: 1.04543\n",
      "[13]\ttrain's multi_logloss: 1.01433\tvalid's multi_logloss: 1.04263\n",
      "[14]\ttrain's multi_logloss: 1.01141\tvalid's multi_logloss: 1.0397\n",
      "[15]\ttrain's multi_logloss: 1.00789\tvalid's multi_logloss: 1.03857\n",
      "[16]\ttrain's multi_logloss: 1.00411\tvalid's multi_logloss: 1.0349\n",
      "[17]\ttrain's multi_logloss: 1.00063\tvalid's multi_logloss: 1.03277\n",
      "[18]\ttrain's multi_logloss: 0.997194\tvalid's multi_logloss: 1.03086\n",
      "[19]\ttrain's multi_logloss: 0.993408\tvalid's multi_logloss: 1.02865\n",
      "[20]\ttrain's multi_logloss: 0.989867\tvalid's multi_logloss: 1.02872\n",
      "[21]\ttrain's multi_logloss: 0.986148\tvalid's multi_logloss: 1.02593\n",
      "[22]\ttrain's multi_logloss: 0.982238\tvalid's multi_logloss: 1.0259\n",
      "[23]\ttrain's multi_logloss: 0.978404\tvalid's multi_logloss: 1.0241\n",
      "[24]\ttrain's multi_logloss: 0.97498\tvalid's multi_logloss: 1.02375\n",
      "[25]\ttrain's multi_logloss: 0.971406\tvalid's multi_logloss: 1.02352\n",
      "[26]\ttrain's multi_logloss: 0.967916\tvalid's multi_logloss: 1.02213\n",
      "[27]\ttrain's multi_logloss: 0.964105\tvalid's multi_logloss: 1.02235\n",
      "[28]\ttrain's multi_logloss: 0.960534\tvalid's multi_logloss: 1.02068\n",
      "[29]\ttrain's multi_logloss: 0.957904\tvalid's multi_logloss: 1.01989\n",
      "[30]\ttrain's multi_logloss: 0.954286\tvalid's multi_logloss: 1.01751\n",
      "[31]\ttrain's multi_logloss: 0.950897\tvalid's multi_logloss: 1.01457\n",
      "[32]\ttrain's multi_logloss: 0.947803\tvalid's multi_logloss: 1.01189\n",
      "[33]\ttrain's multi_logloss: 0.944358\tvalid's multi_logloss: 1.00971\n",
      "[34]\ttrain's multi_logloss: 0.941741\tvalid's multi_logloss: 1.00688\n",
      "[35]\ttrain's multi_logloss: 0.939019\tvalid's multi_logloss: 1.00302\n",
      "[36]\ttrain's multi_logloss: 0.936302\tvalid's multi_logloss: 1.00101\n",
      "[37]\ttrain's multi_logloss: 0.933589\tvalid's multi_logloss: 0.999894\n",
      "[38]\ttrain's multi_logloss: 0.930789\tvalid's multi_logloss: 0.999418\n",
      "[39]\ttrain's multi_logloss: 0.928312\tvalid's multi_logloss: 0.998241\n",
      "[40]\ttrain's multi_logloss: 0.925649\tvalid's multi_logloss: 0.99582\n",
      "[41]\ttrain's multi_logloss: 0.922004\tvalid's multi_logloss: 0.996214\n",
      "[42]\ttrain's multi_logloss: 0.918788\tvalid's multi_logloss: 0.998225\n",
      "[43]\ttrain's multi_logloss: 0.915238\tvalid's multi_logloss: 0.998868\n",
      "[44]\ttrain's multi_logloss: 0.911726\tvalid's multi_logloss: 0.99692\n",
      "[45]\ttrain's multi_logloss: 0.90836\tvalid's multi_logloss: 0.99863\n",
      "[46]\ttrain's multi_logloss: 0.906014\tvalid's multi_logloss: 0.9958\n",
      "[47]\ttrain's multi_logloss: 0.903186\tvalid's multi_logloss: 0.994427\n",
      "[48]\ttrain's multi_logloss: 0.900544\tvalid's multi_logloss: 0.991956\n",
      "[49]\ttrain's multi_logloss: 0.898143\tvalid's multi_logloss: 0.989876\n",
      "[50]\ttrain's multi_logloss: 0.895921\tvalid's multi_logloss: 0.987253\n",
      "[51]\ttrain's multi_logloss: 0.893434\tvalid's multi_logloss: 0.987081\n",
      "[52]\ttrain's multi_logloss: 0.89161\tvalid's multi_logloss: 0.987716\n",
      "[53]\ttrain's multi_logloss: 0.889153\tvalid's multi_logloss: 0.987574\n",
      "[54]\ttrain's multi_logloss: 0.887069\tvalid's multi_logloss: 0.987163\n",
      "[55]\ttrain's multi_logloss: 0.884883\tvalid's multi_logloss: 0.987453\n",
      "[56]\ttrain's multi_logloss: 0.882031\tvalid's multi_logloss: 0.986452\n",
      "[57]\ttrain's multi_logloss: 0.879231\tvalid's multi_logloss: 0.985483\n",
      "[58]\ttrain's multi_logloss: 0.876658\tvalid's multi_logloss: 0.985955\n",
      "[59]\ttrain's multi_logloss: 0.874284\tvalid's multi_logloss: 0.985645\n",
      "[60]\ttrain's multi_logloss: 0.87209\tvalid's multi_logloss: 0.985589\n",
      "[61]\ttrain's multi_logloss: 0.868483\tvalid's multi_logloss: 0.983312\n",
      "[62]\ttrain's multi_logloss: 0.866141\tvalid's multi_logloss: 0.984043\n",
      "[63]\ttrain's multi_logloss: 0.863276\tvalid's multi_logloss: 0.982931\n",
      "[64]\ttrain's multi_logloss: 0.860942\tvalid's multi_logloss: 0.98249\n",
      "[65]\ttrain's multi_logloss: 0.858726\tvalid's multi_logloss: 0.984453\n",
      "[66]\ttrain's multi_logloss: 0.855913\tvalid's multi_logloss: 0.984725\n",
      "[67]\ttrain's multi_logloss: 0.853179\tvalid's multi_logloss: 0.983154\n",
      "[68]\ttrain's multi_logloss: 0.850685\tvalid's multi_logloss: 0.983009\n",
      "[69]\ttrain's multi_logloss: 0.848098\tvalid's multi_logloss: 0.98259\n",
      "[70]\ttrain's multi_logloss: 0.846023\tvalid's multi_logloss: 0.981156\n",
      "[71]\ttrain's multi_logloss: 0.84354\tvalid's multi_logloss: 0.979629\n",
      "[72]\ttrain's multi_logloss: 0.840993\tvalid's multi_logloss: 0.977823\n",
      "[73]\ttrain's multi_logloss: 0.838588\tvalid's multi_logloss: 0.978158\n",
      "[74]\ttrain's multi_logloss: 0.836313\tvalid's multi_logloss: 0.977273\n",
      "[75]\ttrain's multi_logloss: 0.833518\tvalid's multi_logloss: 0.974885\n",
      "[76]\ttrain's multi_logloss: 0.831096\tvalid's multi_logloss: 0.975308\n",
      "[77]\ttrain's multi_logloss: 0.828831\tvalid's multi_logloss: 0.975263\n",
      "[78]\ttrain's multi_logloss: 0.826226\tvalid's multi_logloss: 0.97344\n",
      "[79]\ttrain's multi_logloss: 0.824287\tvalid's multi_logloss: 0.972654\n",
      "[80]\ttrain's multi_logloss: 0.822455\tvalid's multi_logloss: 0.971862\n",
      "[81]\ttrain's multi_logloss: 0.820982\tvalid's multi_logloss: 0.969121\n",
      "[82]\ttrain's multi_logloss: 0.819382\tvalid's multi_logloss: 0.966572\n",
      "[83]\ttrain's multi_logloss: 0.817251\tvalid's multi_logloss: 0.963115\n",
      "[84]\ttrain's multi_logloss: 0.815166\tvalid's multi_logloss: 0.959706\n",
      "[85]\ttrain's multi_logloss: 0.813205\tvalid's multi_logloss: 0.954233\n",
      "[86]\ttrain's multi_logloss: 0.810949\tvalid's multi_logloss: 0.950807\n",
      "[87]\ttrain's multi_logloss: 0.809046\tvalid's multi_logloss: 0.950258\n",
      "[88]\ttrain's multi_logloss: 0.807246\tvalid's multi_logloss: 0.947709\n",
      "[89]\ttrain's multi_logloss: 0.805482\tvalid's multi_logloss: 0.945339\n",
      "[90]\ttrain's multi_logloss: 0.803517\tvalid's multi_logloss: 0.94277\n",
      "[91]\ttrain's multi_logloss: 0.801307\tvalid's multi_logloss: 0.941946\n",
      "[92]\ttrain's multi_logloss: 0.799196\tvalid's multi_logloss: 0.941552\n",
      "[93]\ttrain's multi_logloss: 0.79724\tvalid's multi_logloss: 0.94138\n",
      "[94]\ttrain's multi_logloss: 0.795177\tvalid's multi_logloss: 0.941794\n",
      "[95]\ttrain's multi_logloss: 0.793553\tvalid's multi_logloss: 0.940713\n",
      "[96]\ttrain's multi_logloss: 0.791988\tvalid's multi_logloss: 0.941588\n",
      "[97]\ttrain's multi_logloss: 0.790159\tvalid's multi_logloss: 0.941938\n",
      "[98]\ttrain's multi_logloss: 0.788863\tvalid's multi_logloss: 0.942432\n",
      "[99]\ttrain's multi_logloss: 0.787411\tvalid's multi_logloss: 0.942022\n",
      "[100]\ttrain's multi_logloss: 0.785785\tvalid's multi_logloss: 0.942205\n",
      "[101]\ttrain's multi_logloss: 0.783303\tvalid's multi_logloss: 0.940883\n",
      "[102]\ttrain's multi_logloss: 0.780735\tvalid's multi_logloss: 0.940174\n",
      "[103]\ttrain's multi_logloss: 0.778597\tvalid's multi_logloss: 0.938953\n",
      "[104]\ttrain's multi_logloss: 0.776232\tvalid's multi_logloss: 0.936772\n",
      "[105]\ttrain's multi_logloss: 0.77367\tvalid's multi_logloss: 0.936529\n",
      "[106]\ttrain's multi_logloss: 0.772367\tvalid's multi_logloss: 0.937778\n",
      "[107]\ttrain's multi_logloss: 0.770891\tvalid's multi_logloss: 0.938066\n",
      "[108]\ttrain's multi_logloss: 0.769441\tvalid's multi_logloss: 0.936414\n",
      "[109]\ttrain's multi_logloss: 0.767727\tvalid's multi_logloss: 0.936077\n",
      "[110]\ttrain's multi_logloss: 0.766023\tvalid's multi_logloss: 0.935798\n",
      "[111]\ttrain's multi_logloss: 0.764636\tvalid's multi_logloss: 0.936162\n",
      "[112]\ttrain's multi_logloss: 0.763345\tvalid's multi_logloss: 0.9356\n",
      "[113]\ttrain's multi_logloss: 0.762029\tvalid's multi_logloss: 0.936012\n",
      "[114]\ttrain's multi_logloss: 0.760297\tvalid's multi_logloss: 0.935763\n",
      "[115]\ttrain's multi_logloss: 0.758794\tvalid's multi_logloss: 0.935623\n",
      "[116]\ttrain's multi_logloss: 0.757002\tvalid's multi_logloss: 0.933493\n",
      "[117]\ttrain's multi_logloss: 0.755\tvalid's multi_logloss: 0.933162\n",
      "[118]\ttrain's multi_logloss: 0.753192\tvalid's multi_logloss: 0.933922\n",
      "[119]\ttrain's multi_logloss: 0.751408\tvalid's multi_logloss: 0.932502\n",
      "[120]\ttrain's multi_logloss: 0.750189\tvalid's multi_logloss: 0.932549\n",
      "[121]\ttrain's multi_logloss: 0.748247\tvalid's multi_logloss: 0.932093\n",
      "[122]\ttrain's multi_logloss: 0.746764\tvalid's multi_logloss: 0.931042\n",
      "[123]\ttrain's multi_logloss: 0.744699\tvalid's multi_logloss: 0.931369\n",
      "[124]\ttrain's multi_logloss: 0.742835\tvalid's multi_logloss: 0.93097\n",
      "[125]\ttrain's multi_logloss: 0.741013\tvalid's multi_logloss: 0.930604\n",
      "[126]\ttrain's multi_logloss: 0.739961\tvalid's multi_logloss: 0.930012\n",
      "[127]\ttrain's multi_logloss: 0.738407\tvalid's multi_logloss: 0.929416\n",
      "[128]\ttrain's multi_logloss: 0.737067\tvalid's multi_logloss: 0.928518\n",
      "[129]\ttrain's multi_logloss: 0.735709\tvalid's multi_logloss: 0.928419\n",
      "[130]\ttrain's multi_logloss: 0.734167\tvalid's multi_logloss: 0.926886\n",
      "[131]\ttrain's multi_logloss: 0.732698\tvalid's multi_logloss: 0.925862\n",
      "[132]\ttrain's multi_logloss: 0.730139\tvalid's multi_logloss: 0.925527\n",
      "[133]\ttrain's multi_logloss: 0.727624\tvalid's multi_logloss: 0.925219\n",
      "[134]\ttrain's multi_logloss: 0.725152\tvalid's multi_logloss: 0.924937\n",
      "[135]\ttrain's multi_logloss: 0.723036\tvalid's multi_logloss: 0.922804\n",
      "[136]\ttrain's multi_logloss: 0.721448\tvalid's multi_logloss: 0.922459\n",
      "[137]\ttrain's multi_logloss: 0.719317\tvalid's multi_logloss: 0.921535\n",
      "[138]\ttrain's multi_logloss: 0.717793\tvalid's multi_logloss: 0.920081\n",
      "[139]\ttrain's multi_logloss: 0.716074\tvalid's multi_logloss: 0.91919\n",
      "[140]\ttrain's multi_logloss: 0.714479\tvalid's multi_logloss: 0.918418\n",
      "[141]\ttrain's multi_logloss: 0.712447\tvalid's multi_logloss: 0.917218\n",
      "[142]\ttrain's multi_logloss: 0.710713\tvalid's multi_logloss: 0.915762\n",
      "[143]\ttrain's multi_logloss: 0.709195\tvalid's multi_logloss: 0.915356\n",
      "[144]\ttrain's multi_logloss: 0.707565\tvalid's multi_logloss: 0.914368\n",
      "[145]\ttrain's multi_logloss: 0.706142\tvalid's multi_logloss: 0.914778\n",
      "[146]\ttrain's multi_logloss: 0.704638\tvalid's multi_logloss: 0.914778\n",
      "[147]\ttrain's multi_logloss: 0.703091\tvalid's multi_logloss: 0.914821\n",
      "[148]\ttrain's multi_logloss: 0.701631\tvalid's multi_logloss: 0.913855\n",
      "[149]\ttrain's multi_logloss: 0.700451\tvalid's multi_logloss: 0.915003\n",
      "[150]\ttrain's multi_logloss: 0.699028\tvalid's multi_logloss: 0.914867\n",
      "[151]\ttrain's multi_logloss: 0.697465\tvalid's multi_logloss: 0.912863\n",
      "[152]\ttrain's multi_logloss: 0.696057\tvalid's multi_logloss: 0.912338\n",
      "[153]\ttrain's multi_logloss: 0.69453\tvalid's multi_logloss: 0.910152\n",
      "[154]\ttrain's multi_logloss: 0.693017\tvalid's multi_logloss: 0.908652\n",
      "[155]\ttrain's multi_logloss: 0.691293\tvalid's multi_logloss: 0.906184\n",
      "[156]\ttrain's multi_logloss: 0.69019\tvalid's multi_logloss: 0.904443\n",
      "[157]\ttrain's multi_logloss: 0.688647\tvalid's multi_logloss: 0.904233\n",
      "[158]\ttrain's multi_logloss: 0.687051\tvalid's multi_logloss: 0.903401\n",
      "[159]\ttrain's multi_logloss: 0.685349\tvalid's multi_logloss: 0.903737\n",
      "[160]\ttrain's multi_logloss: 0.683747\tvalid's multi_logloss: 0.903157\n",
      "[161]\ttrain's multi_logloss: 0.682457\tvalid's multi_logloss: 0.903967\n",
      "[162]\ttrain's multi_logloss: 0.681194\tvalid's multi_logloss: 0.904795\n",
      "[163]\ttrain's multi_logloss: 0.679855\tvalid's multi_logloss: 0.905252\n",
      "[164]\ttrain's multi_logloss: 0.678413\tvalid's multi_logloss: 0.90673\n",
      "[165]\ttrain's multi_logloss: 0.677212\tvalid's multi_logloss: 0.908062\n",
      "[166]\ttrain's multi_logloss: 0.675497\tvalid's multi_logloss: 0.905021\n",
      "[167]\ttrain's multi_logloss: 0.673767\tvalid's multi_logloss: 0.902867\n",
      "[168]\ttrain's multi_logloss: 0.672306\tvalid's multi_logloss: 0.900657\n",
      "[169]\ttrain's multi_logloss: 0.670812\tvalid's multi_logloss: 0.897791\n",
      "[170]\ttrain's multi_logloss: 0.669443\tvalid's multi_logloss: 0.895618\n",
      "[171]\ttrain's multi_logloss: 0.667745\tvalid's multi_logloss: 0.894348\n",
      "[172]\ttrain's multi_logloss: 0.666312\tvalid's multi_logloss: 0.894096\n",
      "[173]\ttrain's multi_logloss: 0.664593\tvalid's multi_logloss: 0.892268\n",
      "[174]\ttrain's multi_logloss: 0.66293\tvalid's multi_logloss: 0.890427\n",
      "[175]\ttrain's multi_logloss: 0.661299\tvalid's multi_logloss: 0.888619\n",
      "[176]\ttrain's multi_logloss: 0.660015\tvalid's multi_logloss: 0.887752\n",
      "[177]\ttrain's multi_logloss: 0.658253\tvalid's multi_logloss: 0.887518\n",
      "[178]\ttrain's multi_logloss: 0.657061\tvalid's multi_logloss: 0.888393\n",
      "[179]\ttrain's multi_logloss: 0.655438\tvalid's multi_logloss: 0.886626\n",
      "[180]\ttrain's multi_logloss: 0.654112\tvalid's multi_logloss: 0.888195\n",
      "[181]\ttrain's multi_logloss: 0.652743\tvalid's multi_logloss: 0.887937\n",
      "[182]\ttrain's multi_logloss: 0.651426\tvalid's multi_logloss: 0.887192\n",
      "[183]\ttrain's multi_logloss: 0.649861\tvalid's multi_logloss: 0.88656\n",
      "[184]\ttrain's multi_logloss: 0.648511\tvalid's multi_logloss: 0.886543\n",
      "[185]\ttrain's multi_logloss: 0.647002\tvalid's multi_logloss: 0.886122\n",
      "[186]\ttrain's multi_logloss: 0.6452\tvalid's multi_logloss: 0.886403\n",
      "[187]\ttrain's multi_logloss: 0.643849\tvalid's multi_logloss: 0.886529\n",
      "[188]\ttrain's multi_logloss: 0.642531\tvalid's multi_logloss: 0.887236\n",
      "[189]\ttrain's multi_logloss: 0.640821\tvalid's multi_logloss: 0.88653\n",
      "[190]\ttrain's multi_logloss: 0.63917\tvalid's multi_logloss: 0.886334\n",
      "[191]\ttrain's multi_logloss: 0.638168\tvalid's multi_logloss: 0.886066\n",
      "[192]\ttrain's multi_logloss: 0.637223\tvalid's multi_logloss: 0.88582\n",
      "[193]\ttrain's multi_logloss: 0.636177\tvalid's multi_logloss: 0.884889\n",
      "[194]\ttrain's multi_logloss: 0.634771\tvalid's multi_logloss: 0.886037\n",
      "[195]\ttrain's multi_logloss: 0.633834\tvalid's multi_logloss: 0.885826\n",
      "[196]\ttrain's multi_logloss: 0.632424\tvalid's multi_logloss: 0.885928\n",
      "[197]\ttrain's multi_logloss: 0.630993\tvalid's multi_logloss: 0.884589\n",
      "[198]\ttrain's multi_logloss: 0.629625\tvalid's multi_logloss: 0.884519\n",
      "[199]\ttrain's multi_logloss: 0.62835\tvalid's multi_logloss: 0.883504\n",
      "[200]\ttrain's multi_logloss: 0.626872\tvalid's multi_logloss: 0.88376\n",
      "[201]\ttrain's multi_logloss: 0.625167\tvalid's multi_logloss: 0.883364\n",
      "[202]\ttrain's multi_logloss: 0.623308\tvalid's multi_logloss: 0.883856\n",
      "[203]\ttrain's multi_logloss: 0.621254\tvalid's multi_logloss: 0.882939\n",
      "[204]\ttrain's multi_logloss: 0.619878\tvalid's multi_logloss: 0.882019\n",
      "[205]\ttrain's multi_logloss: 0.617888\tvalid's multi_logloss: 0.881078\n",
      "[206]\ttrain's multi_logloss: 0.617033\tvalid's multi_logloss: 0.881377\n",
      "[207]\ttrain's multi_logloss: 0.616344\tvalid's multi_logloss: 0.88212\n",
      "[208]\ttrain's multi_logloss: 0.615437\tvalid's multi_logloss: 0.882619\n",
      "[209]\ttrain's multi_logloss: 0.614538\tvalid's multi_logloss: 0.883121\n",
      "[210]\ttrain's multi_logloss: 0.613885\tvalid's multi_logloss: 0.883884\n",
      "[211]\ttrain's multi_logloss: 0.612642\tvalid's multi_logloss: 0.882071\n",
      "[212]\ttrain's multi_logloss: 0.61137\tvalid's multi_logloss: 0.88112\n",
      "[213]\ttrain's multi_logloss: 0.61022\tvalid's multi_logloss: 0.882001\n",
      "[214]\ttrain's multi_logloss: 0.609142\tvalid's multi_logloss: 0.882052\n",
      "[215]\ttrain's multi_logloss: 0.607916\tvalid's multi_logloss: 0.882603\n",
      "[216]\ttrain's multi_logloss: 0.606649\tvalid's multi_logloss: 0.881342\n",
      "[217]\ttrain's multi_logloss: 0.605419\tvalid's multi_logloss: 0.881294\n",
      "[218]\ttrain's multi_logloss: 0.604215\tvalid's multi_logloss: 0.880691\n",
      "[219]\ttrain's multi_logloss: 0.603092\tvalid's multi_logloss: 0.879615\n",
      "[220]\ttrain's multi_logloss: 0.601818\tvalid's multi_logloss: 0.879752\n",
      "[221]\ttrain's multi_logloss: 0.60026\tvalid's multi_logloss: 0.877955\n",
      "[222]\ttrain's multi_logloss: 0.59853\tvalid's multi_logloss: 0.877275\n",
      "[223]\ttrain's multi_logloss: 0.596932\tvalid's multi_logloss: 0.876719\n",
      "[224]\ttrain's multi_logloss: 0.595288\tvalid's multi_logloss: 0.875592\n",
      "[225]\ttrain's multi_logloss: 0.593781\tvalid's multi_logloss: 0.874605\n",
      "[226]\ttrain's multi_logloss: 0.592509\tvalid's multi_logloss: 0.873558\n",
      "[227]\ttrain's multi_logloss: 0.590965\tvalid's multi_logloss: 0.873116\n",
      "[228]\ttrain's multi_logloss: 0.58967\tvalid's multi_logloss: 0.87109\n",
      "[229]\ttrain's multi_logloss: 0.588807\tvalid's multi_logloss: 0.869582\n",
      "[230]\ttrain's multi_logloss: 0.587661\tvalid's multi_logloss: 0.867812\n",
      "[231]\ttrain's multi_logloss: 0.58648\tvalid's multi_logloss: 0.867495\n",
      "[232]\ttrain's multi_logloss: 0.585178\tvalid's multi_logloss: 0.866603\n",
      "[233]\ttrain's multi_logloss: 0.584235\tvalid's multi_logloss: 0.865654\n",
      "[234]\ttrain's multi_logloss: 0.583223\tvalid's multi_logloss: 0.865246\n",
      "[235]\ttrain's multi_logloss: 0.58213\tvalid's multi_logloss: 0.864194\n",
      "[236]\ttrain's multi_logloss: 0.580915\tvalid's multi_logloss: 0.863199\n",
      "[237]\ttrain's multi_logloss: 0.58004\tvalid's multi_logloss: 0.862559\n",
      "[238]\ttrain's multi_logloss: 0.57911\tvalid's multi_logloss: 0.86198\n",
      "[239]\ttrain's multi_logloss: 0.577988\tvalid's multi_logloss: 0.862075\n",
      "[240]\ttrain's multi_logloss: 0.57696\tvalid's multi_logloss: 0.862607\n",
      "[241]\ttrain's multi_logloss: 0.576238\tvalid's multi_logloss: 0.863056\n",
      "[242]\ttrain's multi_logloss: 0.575482\tvalid's multi_logloss: 0.862733\n",
      "[243]\ttrain's multi_logloss: 0.574264\tvalid's multi_logloss: 0.862715\n",
      "[244]\ttrain's multi_logloss: 0.573182\tvalid's multi_logloss: 0.862265\n",
      "[245]\ttrain's multi_logloss: 0.571825\tvalid's multi_logloss: 0.861556\n",
      "[246]\ttrain's multi_logloss: 0.570681\tvalid's multi_logloss: 0.861766\n",
      "[247]\ttrain's multi_logloss: 0.569609\tvalid's multi_logloss: 0.859355\n",
      "[248]\ttrain's multi_logloss: 0.568483\tvalid's multi_logloss: 0.859697\n",
      "[249]\ttrain's multi_logloss: 0.567492\tvalid's multi_logloss: 0.857951\n",
      "[250]\ttrain's multi_logloss: 0.566441\tvalid's multi_logloss: 0.85617\n",
      "[251]\ttrain's multi_logloss: 0.565115\tvalid's multi_logloss: 0.856103\n",
      "[252]\ttrain's multi_logloss: 0.563717\tvalid's multi_logloss: 0.855722\n",
      "[253]\ttrain's multi_logloss: 0.562616\tvalid's multi_logloss: 0.855325\n",
      "[254]\ttrain's multi_logloss: 0.561479\tvalid's multi_logloss: 0.854224\n",
      "[255]\ttrain's multi_logloss: 0.560134\tvalid's multi_logloss: 0.854243\n",
      "[256]\ttrain's multi_logloss: 0.559222\tvalid's multi_logloss: 0.854442\n",
      "[257]\ttrain's multi_logloss: 0.558255\tvalid's multi_logloss: 0.853809\n",
      "[258]\ttrain's multi_logloss: 0.55738\tvalid's multi_logloss: 0.854027\n",
      "[259]\ttrain's multi_logloss: 0.556698\tvalid's multi_logloss: 0.852877\n",
      "[260]\ttrain's multi_logloss: 0.555943\tvalid's multi_logloss: 0.852642\n",
      "[261]\ttrain's multi_logloss: 0.554995\tvalid's multi_logloss: 0.852229\n",
      "[262]\ttrain's multi_logloss: 0.554045\tvalid's multi_logloss: 0.851529\n",
      "[263]\ttrain's multi_logloss: 0.5534\tvalid's multi_logloss: 0.851709\n",
      "[264]\ttrain's multi_logloss: 0.552253\tvalid's multi_logloss: 0.851773\n",
      "[265]\ttrain's multi_logloss: 0.5512\tvalid's multi_logloss: 0.851649\n",
      "[266]\ttrain's multi_logloss: 0.549994\tvalid's multi_logloss: 0.85014\n",
      "[267]\ttrain's multi_logloss: 0.549046\tvalid's multi_logloss: 0.850245\n",
      "[268]\ttrain's multi_logloss: 0.547663\tvalid's multi_logloss: 0.849641\n",
      "[269]\ttrain's multi_logloss: 0.546202\tvalid's multi_logloss: 0.84902\n",
      "[270]\ttrain's multi_logloss: 0.545008\tvalid's multi_logloss: 0.847906\n",
      "[271]\ttrain's multi_logloss: 0.544087\tvalid's multi_logloss: 0.846082\n",
      "[272]\ttrain's multi_logloss: 0.54316\tvalid's multi_logloss: 0.845837\n",
      "[273]\ttrain's multi_logloss: 0.542326\tvalid's multi_logloss: 0.845921\n",
      "[274]\ttrain's multi_logloss: 0.541436\tvalid's multi_logloss: 0.846033\n",
      "[275]\ttrain's multi_logloss: 0.540568\tvalid's multi_logloss: 0.846162\n",
      "[276]\ttrain's multi_logloss: 0.539582\tvalid's multi_logloss: 0.84567\n",
      "[277]\ttrain's multi_logloss: 0.538662\tvalid's multi_logloss: 0.84549\n",
      "[278]\ttrain's multi_logloss: 0.537544\tvalid's multi_logloss: 0.846542\n",
      "[279]\ttrain's multi_logloss: 0.536498\tvalid's multi_logloss: 0.846227\n",
      "[280]\ttrain's multi_logloss: 0.535514\tvalid's multi_logloss: 0.846198\n",
      "[281]\ttrain's multi_logloss: 0.534579\tvalid's multi_logloss: 0.845828\n",
      "[282]\ttrain's multi_logloss: 0.533553\tvalid's multi_logloss: 0.847266\n",
      "[283]\ttrain's multi_logloss: 0.53241\tvalid's multi_logloss: 0.847613\n",
      "[284]\ttrain's multi_logloss: 0.531393\tvalid's multi_logloss: 0.846903\n",
      "[285]\ttrain's multi_logloss: 0.530469\tvalid's multi_logloss: 0.845034\n",
      "[286]\ttrain's multi_logloss: 0.529928\tvalid's multi_logloss: 0.845079\n",
      "[287]\ttrain's multi_logloss: 0.529407\tvalid's multi_logloss: 0.845145\n",
      "[288]\ttrain's multi_logloss: 0.528567\tvalid's multi_logloss: 0.844519\n",
      "[289]\ttrain's multi_logloss: 0.52784\tvalid's multi_logloss: 0.845418\n",
      "[290]\ttrain's multi_logloss: 0.526784\tvalid's multi_logloss: 0.84509\n",
      "[291]\ttrain's multi_logloss: 0.526074\tvalid's multi_logloss: 0.845695\n",
      "[292]\ttrain's multi_logloss: 0.525339\tvalid's multi_logloss: 0.846241\n",
      "[293]\ttrain's multi_logloss: 0.524628\tvalid's multi_logloss: 0.846477\n",
      "[294]\ttrain's multi_logloss: 0.523872\tvalid's multi_logloss: 0.846666\n",
      "[295]\ttrain's multi_logloss: 0.523142\tvalid's multi_logloss: 0.846846\n",
      "[296]\ttrain's multi_logloss: 0.522556\tvalid's multi_logloss: 0.847712\n",
      "[297]\ttrain's multi_logloss: 0.521876\tvalid's multi_logloss: 0.848262\n",
      "[298]\ttrain's multi_logloss: 0.521086\tvalid's multi_logloss: 0.849044\n",
      "[299]\ttrain's multi_logloss: 0.52032\tvalid's multi_logloss: 0.849847\n",
      "[300]\ttrain's multi_logloss: 0.519751\tvalid's multi_logloss: 0.850825\n",
      "[301]\ttrain's multi_logloss: 0.518612\tvalid's multi_logloss: 0.850666\n",
      "[302]\ttrain's multi_logloss: 0.517887\tvalid's multi_logloss: 0.850719\n",
      "[303]\ttrain's multi_logloss: 0.517178\tvalid's multi_logloss: 0.850785\n",
      "[304]\ttrain's multi_logloss: 0.516472\tvalid's multi_logloss: 0.850038\n",
      "[305]\ttrain's multi_logloss: 0.515693\tvalid's multi_logloss: 0.850837\n",
      "[306]\ttrain's multi_logloss: 0.514485\tvalid's multi_logloss: 0.849341\n",
      "[307]\ttrain's multi_logloss: 0.513304\tvalid's multi_logloss: 0.847876\n",
      "[308]\ttrain's multi_logloss: 0.511909\tvalid's multi_logloss: 0.846978\n",
      "[309]\ttrain's multi_logloss: 0.510724\tvalid's multi_logloss: 0.845336\n",
      "[310]\ttrain's multi_logloss: 0.50954\tvalid's multi_logloss: 0.845511\n",
      "[311]\ttrain's multi_logloss: 0.508465\tvalid's multi_logloss: 0.845477\n",
      "[312]\ttrain's multi_logloss: 0.507312\tvalid's multi_logloss: 0.845525\n",
      "[313]\ttrain's multi_logloss: 0.50613\tvalid's multi_logloss: 0.845479\n",
      "[314]\ttrain's multi_logloss: 0.505259\tvalid's multi_logloss: 0.846901\n",
      "[315]\ttrain's multi_logloss: 0.504136\tvalid's multi_logloss: 0.846278\n",
      "[316]\ttrain's multi_logloss: 0.503158\tvalid's multi_logloss: 0.845782\n",
      "[317]\ttrain's multi_logloss: 0.50208\tvalid's multi_logloss: 0.844723\n",
      "[318]\ttrain's multi_logloss: 0.501056\tvalid's multi_logloss: 0.843917\n",
      "[319]\ttrain's multi_logloss: 0.500293\tvalid's multi_logloss: 0.843207\n",
      "[320]\ttrain's multi_logloss: 0.499415\tvalid's multi_logloss: 0.841839\n",
      "[321]\ttrain's multi_logloss: 0.498345\tvalid's multi_logloss: 0.840483\n",
      "[322]\ttrain's multi_logloss: 0.497412\tvalid's multi_logloss: 0.840098\n",
      "[323]\ttrain's multi_logloss: 0.496483\tvalid's multi_logloss: 0.839753\n",
      "[324]\ttrain's multi_logloss: 0.495676\tvalid's multi_logloss: 0.839777\n",
      "[325]\ttrain's multi_logloss: 0.494722\tvalid's multi_logloss: 0.838744\n",
      "[326]\ttrain's multi_logloss: 0.49391\tvalid's multi_logloss: 0.837916\n",
      "[327]\ttrain's multi_logloss: 0.49294\tvalid's multi_logloss: 0.838063\n",
      "[328]\ttrain's multi_logloss: 0.491989\tvalid's multi_logloss: 0.838222\n",
      "[329]\ttrain's multi_logloss: 0.49122\tvalid's multi_logloss: 0.83858\n",
      "[330]\ttrain's multi_logloss: 0.490368\tvalid's multi_logloss: 0.839124\n",
      "[331]\ttrain's multi_logloss: 0.489708\tvalid's multi_logloss: 0.840489\n",
      "[332]\ttrain's multi_logloss: 0.488874\tvalid's multi_logloss: 0.84049\n",
      "[333]\ttrain's multi_logloss: 0.487891\tvalid's multi_logloss: 0.838985\n",
      "[334]\ttrain's multi_logloss: 0.486462\tvalid's multi_logloss: 0.838793\n",
      "[335]\ttrain's multi_logloss: 0.485807\tvalid's multi_logloss: 0.838379\n",
      "[336]\ttrain's multi_logloss: 0.484862\tvalid's multi_logloss: 0.83714\n",
      "[337]\ttrain's multi_logloss: 0.484231\tvalid's multi_logloss: 0.836186\n",
      "[338]\ttrain's multi_logloss: 0.483359\tvalid's multi_logloss: 0.834942\n",
      "[339]\ttrain's multi_logloss: 0.482676\tvalid's multi_logloss: 0.834377\n",
      "[340]\ttrain's multi_logloss: 0.481823\tvalid's multi_logloss: 0.832713\n",
      "[341]\ttrain's multi_logloss: 0.481016\tvalid's multi_logloss: 0.833321\n",
      "[342]\ttrain's multi_logloss: 0.480209\tvalid's multi_logloss: 0.833477\n",
      "[343]\ttrain's multi_logloss: 0.479448\tvalid's multi_logloss: 0.8338\n",
      "[344]\ttrain's multi_logloss: 0.478783\tvalid's multi_logloss: 0.835439\n",
      "[345]\ttrain's multi_logloss: 0.478014\tvalid's multi_logloss: 0.836306\n",
      "[346]\ttrain's multi_logloss: 0.477089\tvalid's multi_logloss: 0.835646\n",
      "[347]\ttrain's multi_logloss: 0.476447\tvalid's multi_logloss: 0.835884\n",
      "[348]\ttrain's multi_logloss: 0.475589\tvalid's multi_logloss: 0.834675\n",
      "[349]\ttrain's multi_logloss: 0.474858\tvalid's multi_logloss: 0.834095\n",
      "[350]\ttrain's multi_logloss: 0.473906\tvalid's multi_logloss: 0.834111\n",
      "[351]\ttrain's multi_logloss: 0.473096\tvalid's multi_logloss: 0.832995\n",
      "[352]\ttrain's multi_logloss: 0.472364\tvalid's multi_logloss: 0.830824\n",
      "[353]\ttrain's multi_logloss: 0.471579\tvalid's multi_logloss: 0.829327\n",
      "[354]\ttrain's multi_logloss: 0.47076\tvalid's multi_logloss: 0.827224\n",
      "[355]\ttrain's multi_logloss: 0.469999\tvalid's multi_logloss: 0.826104\n",
      "[356]\ttrain's multi_logloss: 0.46935\tvalid's multi_logloss: 0.826131\n",
      "[357]\ttrain's multi_logloss: 0.468601\tvalid's multi_logloss: 0.825691\n",
      "[358]\ttrain's multi_logloss: 0.467879\tvalid's multi_logloss: 0.825713\n",
      "[359]\ttrain's multi_logloss: 0.467246\tvalid's multi_logloss: 0.825615\n",
      "[360]\ttrain's multi_logloss: 0.466717\tvalid's multi_logloss: 0.825643\n",
      "[361]\ttrain's multi_logloss: 0.465343\tvalid's multi_logloss: 0.825675\n",
      "[362]\ttrain's multi_logloss: 0.46436\tvalid's multi_logloss: 0.825568\n",
      "[363]\ttrain's multi_logloss: 0.463021\tvalid's multi_logloss: 0.825627\n",
      "[364]\ttrain's multi_logloss: 0.461978\tvalid's multi_logloss: 0.826388\n",
      "[365]\ttrain's multi_logloss: 0.460993\tvalid's multi_logloss: 0.825698\n",
      "[366]\ttrain's multi_logloss: 0.460148\tvalid's multi_logloss: 0.827035\n",
      "[367]\ttrain's multi_logloss: 0.459446\tvalid's multi_logloss: 0.827738\n",
      "[368]\ttrain's multi_logloss: 0.458754\tvalid's multi_logloss: 0.828204\n",
      "[369]\ttrain's multi_logloss: 0.458071\tvalid's multi_logloss: 0.829099\n",
      "[370]\ttrain's multi_logloss: 0.45726\tvalid's multi_logloss: 0.830168\n",
      "[371]\ttrain's multi_logloss: 0.45623\tvalid's multi_logloss: 0.829553\n",
      "[372]\ttrain's multi_logloss: 0.455087\tvalid's multi_logloss: 0.828931\n",
      "[373]\ttrain's multi_logloss: 0.453988\tvalid's multi_logloss: 0.829368\n",
      "[374]\ttrain's multi_logloss: 0.452954\tvalid's multi_logloss: 0.830083\n",
      "[375]\ttrain's multi_logloss: 0.451952\tvalid's multi_logloss: 0.829288\n",
      "[376]\ttrain's multi_logloss: 0.451533\tvalid's multi_logloss: 0.82803\n",
      "[377]\ttrain's multi_logloss: 0.451093\tvalid's multi_logloss: 0.827325\n",
      "[378]\ttrain's multi_logloss: 0.450057\tvalid's multi_logloss: 0.826707\n",
      "[379]\ttrain's multi_logloss: 0.449291\tvalid's multi_logloss: 0.826774\n",
      "[380]\ttrain's multi_logloss: 0.448513\tvalid's multi_logloss: 0.827505\n",
      "[381]\ttrain's multi_logloss: 0.447737\tvalid's multi_logloss: 0.826563\n",
      "[382]\ttrain's multi_logloss: 0.447171\tvalid's multi_logloss: 0.824218\n",
      "[383]\ttrain's multi_logloss: 0.446626\tvalid's multi_logloss: 0.823286\n",
      "[384]\ttrain's multi_logloss: 0.445987\tvalid's multi_logloss: 0.822658\n",
      "[385]\ttrain's multi_logloss: 0.445467\tvalid's multi_logloss: 0.821755\n",
      "[386]\ttrain's multi_logloss: 0.444947\tvalid's multi_logloss: 0.821999\n",
      "[387]\ttrain's multi_logloss: 0.444265\tvalid's multi_logloss: 0.822797\n",
      "[388]\ttrain's multi_logloss: 0.443473\tvalid's multi_logloss: 0.822732\n",
      "[389]\ttrain's multi_logloss: 0.442787\tvalid's multi_logloss: 0.822665\n",
      "[390]\ttrain's multi_logloss: 0.442214\tvalid's multi_logloss: 0.82312\n",
      "[391]\ttrain's multi_logloss: 0.44105\tvalid's multi_logloss: 0.82289\n",
      "[392]\ttrain's multi_logloss: 0.44006\tvalid's multi_logloss: 0.822325\n",
      "[393]\ttrain's multi_logloss: 0.439242\tvalid's multi_logloss: 0.823059\n",
      "[394]\ttrain's multi_logloss: 0.438305\tvalid's multi_logloss: 0.823608\n",
      "[395]\ttrain's multi_logloss: 0.437532\tvalid's multi_logloss: 0.823748\n",
      "[396]\ttrain's multi_logloss: 0.437218\tvalid's multi_logloss: 0.82459\n",
      "[397]\ttrain's multi_logloss: 0.436845\tvalid's multi_logloss: 0.82509\n",
      "[398]\ttrain's multi_logloss: 0.436545\tvalid's multi_logloss: 0.824955\n",
      "[399]\ttrain's multi_logloss: 0.436264\tvalid's multi_logloss: 0.825683\n",
      "[400]\ttrain's multi_logloss: 0.435973\tvalid's multi_logloss: 0.826548\n",
      "[401]\ttrain's multi_logloss: 0.435223\tvalid's multi_logloss: 0.824294\n",
      "[402]\ttrain's multi_logloss: 0.434661\tvalid's multi_logloss: 0.822662\n",
      "[403]\ttrain's multi_logloss: 0.434116\tvalid's multi_logloss: 0.821057\n",
      "[404]\ttrain's multi_logloss: 0.433586\tvalid's multi_logloss: 0.819477\n",
      "[405]\ttrain's multi_logloss: 0.433156\tvalid's multi_logloss: 0.817917\n",
      "[406]\ttrain's multi_logloss: 0.432524\tvalid's multi_logloss: 0.816663\n",
      "[407]\ttrain's multi_logloss: 0.432054\tvalid's multi_logloss: 0.814934\n",
      "[408]\ttrain's multi_logloss: 0.431273\tvalid's multi_logloss: 0.815155\n",
      "[409]\ttrain's multi_logloss: 0.430673\tvalid's multi_logloss: 0.813714\n",
      "[410]\ttrain's multi_logloss: 0.430225\tvalid's multi_logloss: 0.812023\n",
      "[411]\ttrain's multi_logloss: 0.429496\tvalid's multi_logloss: 0.811468\n",
      "[412]\ttrain's multi_logloss: 0.428602\tvalid's multi_logloss: 0.811634\n",
      "[413]\ttrain's multi_logloss: 0.428052\tvalid's multi_logloss: 0.812658\n",
      "[414]\ttrain's multi_logloss: 0.427377\tvalid's multi_logloss: 0.812405\n",
      "[415]\ttrain's multi_logloss: 0.426699\tvalid's multi_logloss: 0.812941\n",
      "[416]\ttrain's multi_logloss: 0.426003\tvalid's multi_logloss: 0.81232\n",
      "[417]\ttrain's multi_logloss: 0.425419\tvalid's multi_logloss: 0.811516\n",
      "[418]\ttrain's multi_logloss: 0.424915\tvalid's multi_logloss: 0.811362\n",
      "[419]\ttrain's multi_logloss: 0.424302\tvalid's multi_logloss: 0.810814\n",
      "[420]\ttrain's multi_logloss: 0.423733\tvalid's multi_logloss: 0.81107\n",
      "[421]\ttrain's multi_logloss: 0.423257\tvalid's multi_logloss: 0.811431\n",
      "[422]\ttrain's multi_logloss: 0.422732\tvalid's multi_logloss: 0.810516\n",
      "[423]\ttrain's multi_logloss: 0.422278\tvalid's multi_logloss: 0.810904\n",
      "[424]\ttrain's multi_logloss: 0.421585\tvalid's multi_logloss: 0.811046\n",
      "[425]\ttrain's multi_logloss: 0.420696\tvalid's multi_logloss: 0.810499\n",
      "[426]\ttrain's multi_logloss: 0.419692\tvalid's multi_logloss: 0.811122\n",
      "[427]\ttrain's multi_logloss: 0.418715\tvalid's multi_logloss: 0.811478\n",
      "[428]\ttrain's multi_logloss: 0.417886\tvalid's multi_logloss: 0.811942\n",
      "[429]\ttrain's multi_logloss: 0.417177\tvalid's multi_logloss: 0.812344\n",
      "[430]\ttrain's multi_logloss: 0.416315\tvalid's multi_logloss: 0.812551\n",
      "[431]\ttrain's multi_logloss: 0.415499\tvalid's multi_logloss: 0.812913\n",
      "[432]\ttrain's multi_logloss: 0.414531\tvalid's multi_logloss: 0.813834\n",
      "[433]\ttrain's multi_logloss: 0.413525\tvalid's multi_logloss: 0.813481\n",
      "[434]\ttrain's multi_logloss: 0.412801\tvalid's multi_logloss: 0.811491\n",
      "[435]\ttrain's multi_logloss: 0.411881\tvalid's multi_logloss: 0.812444\n",
      "[436]\ttrain's multi_logloss: 0.411364\tvalid's multi_logloss: 0.812477\n",
      "[437]\ttrain's multi_logloss: 0.410849\tvalid's multi_logloss: 0.811262\n",
      "[438]\ttrain's multi_logloss: 0.410349\tvalid's multi_logloss: 0.810065\n",
      "[439]\ttrain's multi_logloss: 0.409811\tvalid's multi_logloss: 0.809346\n",
      "[440]\ttrain's multi_logloss: 0.409379\tvalid's multi_logloss: 0.808935\n",
      "[441]\ttrain's multi_logloss: 0.408809\tvalid's multi_logloss: 0.807257\n",
      "[442]\ttrain's multi_logloss: 0.408139\tvalid's multi_logloss: 0.806355\n",
      "[443]\ttrain's multi_logloss: 0.407563\tvalid's multi_logloss: 0.8064\n",
      "[444]\ttrain's multi_logloss: 0.407016\tvalid's multi_logloss: 0.806223\n",
      "[445]\ttrain's multi_logloss: 0.406422\tvalid's multi_logloss: 0.804107\n",
      "[446]\ttrain's multi_logloss: 0.405475\tvalid's multi_logloss: 0.803525\n",
      "[447]\ttrain's multi_logloss: 0.40496\tvalid's multi_logloss: 0.803488\n",
      "[448]\ttrain's multi_logloss: 0.404054\tvalid's multi_logloss: 0.802935\n",
      "[449]\ttrain's multi_logloss: 0.403473\tvalid's multi_logloss: 0.802522\n",
      "[450]\ttrain's multi_logloss: 0.402952\tvalid's multi_logloss: 0.801446\n",
      "[451]\ttrain's multi_logloss: 0.402179\tvalid's multi_logloss: 0.80099\n",
      "[452]\ttrain's multi_logloss: 0.401438\tvalid's multi_logloss: 0.799887\n",
      "[453]\ttrain's multi_logloss: 0.400744\tvalid's multi_logloss: 0.800726\n",
      "[454]\ttrain's multi_logloss: 0.400136\tvalid's multi_logloss: 0.800112\n",
      "[455]\ttrain's multi_logloss: 0.399456\tvalid's multi_logloss: 0.800361\n",
      "[456]\ttrain's multi_logloss: 0.398634\tvalid's multi_logloss: 0.800456\n",
      "[457]\ttrain's multi_logloss: 0.397998\tvalid's multi_logloss: 0.799876\n",
      "[458]\ttrain's multi_logloss: 0.397414\tvalid's multi_logloss: 0.800227\n",
      "[459]\ttrain's multi_logloss: 0.396842\tvalid's multi_logloss: 0.800815\n",
      "[460]\ttrain's multi_logloss: 0.396272\tvalid's multi_logloss: 0.800493\n",
      "[461]\ttrain's multi_logloss: 0.395525\tvalid's multi_logloss: 0.801422\n",
      "[462]\ttrain's multi_logloss: 0.394954\tvalid's multi_logloss: 0.801695\n",
      "[463]\ttrain's multi_logloss: 0.394279\tvalid's multi_logloss: 0.801627\n",
      "[464]\ttrain's multi_logloss: 0.393602\tvalid's multi_logloss: 0.802558\n",
      "[465]\ttrain's multi_logloss: 0.393082\tvalid's multi_logloss: 0.801951\n",
      "[466]\ttrain's multi_logloss: 0.392672\tvalid's multi_logloss: 0.800009\n",
      "[467]\ttrain's multi_logloss: 0.392324\tvalid's multi_logloss: 0.799302\n",
      "[468]\ttrain's multi_logloss: 0.391739\tvalid's multi_logloss: 0.797509\n",
      "[469]\ttrain's multi_logloss: 0.391296\tvalid's multi_logloss: 0.796226\n",
      "[470]\ttrain's multi_logloss: 0.390861\tvalid's multi_logloss: 0.794764\n",
      "[471]\ttrain's multi_logloss: 0.390236\tvalid's multi_logloss: 0.795654\n",
      "[472]\ttrain's multi_logloss: 0.389503\tvalid's multi_logloss: 0.796189\n",
      "[473]\ttrain's multi_logloss: 0.388827\tvalid's multi_logloss: 0.796907\n",
      "[474]\ttrain's multi_logloss: 0.38817\tvalid's multi_logloss: 0.796999\n",
      "[475]\ttrain's multi_logloss: 0.387608\tvalid's multi_logloss: 0.796292\n",
      "[476]\ttrain's multi_logloss: 0.387106\tvalid's multi_logloss: 0.795965\n",
      "[477]\ttrain's multi_logloss: 0.38671\tvalid's multi_logloss: 0.795925\n",
      "[478]\ttrain's multi_logloss: 0.386349\tvalid's multi_logloss: 0.795678\n",
      "[479]\ttrain's multi_logloss: 0.385973\tvalid's multi_logloss: 0.795631\n",
      "[480]\ttrain's multi_logloss: 0.385615\tvalid's multi_logloss: 0.795283\n",
      "[481]\ttrain's multi_logloss: 0.384841\tvalid's multi_logloss: 0.795744\n",
      "[482]\ttrain's multi_logloss: 0.384053\tvalid's multi_logloss: 0.796478\n",
      "[483]\ttrain's multi_logloss: 0.383237\tvalid's multi_logloss: 0.796937\n",
      "[484]\ttrain's multi_logloss: 0.382369\tvalid's multi_logloss: 0.796862\n",
      "[485]\ttrain's multi_logloss: 0.381562\tvalid's multi_logloss: 0.797714\n",
      "[486]\ttrain's multi_logloss: 0.380794\tvalid's multi_logloss: 0.797551\n",
      "[487]\ttrain's multi_logloss: 0.380639\tvalid's multi_logloss: 0.797848\n",
      "[488]\ttrain's multi_logloss: 0.380225\tvalid's multi_logloss: 0.797674\n",
      "[489]\ttrain's multi_logloss: 0.3796\tvalid's multi_logloss: 0.797267\n",
      "[490]\ttrain's multi_logloss: 0.379156\tvalid's multi_logloss: 0.797335\n",
      "[491]\ttrain's multi_logloss: 0.378783\tvalid's multi_logloss: 0.796832\n",
      "[492]\ttrain's multi_logloss: 0.378482\tvalid's multi_logloss: 0.796625\n",
      "[493]\ttrain's multi_logloss: 0.378015\tvalid's multi_logloss: 0.794879\n",
      "[494]\ttrain's multi_logloss: 0.377345\tvalid's multi_logloss: 0.794143\n",
      "[495]\ttrain's multi_logloss: 0.376987\tvalid's multi_logloss: 0.792804\n",
      "[496]\ttrain's multi_logloss: 0.376114\tvalid's multi_logloss: 0.792292\n",
      "[497]\ttrain's multi_logloss: 0.375382\tvalid's multi_logloss: 0.791828\n",
      "[498]\ttrain's multi_logloss: 0.374708\tvalid's multi_logloss: 0.791739\n",
      "[499]\ttrain's multi_logloss: 0.374141\tvalid's multi_logloss: 0.791318\n",
      "[500]\ttrain's multi_logloss: 0.373514\tvalid's multi_logloss: 0.791182\n",
      "[501]\ttrain's multi_logloss: 0.372936\tvalid's multi_logloss: 0.791587\n",
      "[502]\ttrain's multi_logloss: 0.372637\tvalid's multi_logloss: 0.792988\n",
      "[503]\ttrain's multi_logloss: 0.372199\tvalid's multi_logloss: 0.793446\n",
      "[504]\ttrain's multi_logloss: 0.371699\tvalid's multi_logloss: 0.794683\n",
      "[505]\ttrain's multi_logloss: 0.371319\tvalid's multi_logloss: 0.795874\n",
      "[506]\ttrain's multi_logloss: 0.370548\tvalid's multi_logloss: 0.794486\n",
      "[507]\ttrain's multi_logloss: 0.369909\tvalid's multi_logloss: 0.794669\n",
      "[508]\ttrain's multi_logloss: 0.369209\tvalid's multi_logloss: 0.794979\n",
      "[509]\ttrain's multi_logloss: 0.368782\tvalid's multi_logloss: 0.795335\n",
      "[510]\ttrain's multi_logloss: 0.368119\tvalid's multi_logloss: 0.794684\n",
      "[511]\ttrain's multi_logloss: 0.367618\tvalid's multi_logloss: 0.793555\n",
      "[512]\ttrain's multi_logloss: 0.36697\tvalid's multi_logloss: 0.793496\n",
      "[513]\ttrain's multi_logloss: 0.366347\tvalid's multi_logloss: 0.793969\n",
      "[514]\ttrain's multi_logloss: 0.365971\tvalid's multi_logloss: 0.793589\n",
      "[515]\ttrain's multi_logloss: 0.365431\tvalid's multi_logloss: 0.793651\n",
      "[516]\ttrain's multi_logloss: 0.364909\tvalid's multi_logloss: 0.793636\n",
      "[517]\ttrain's multi_logloss: 0.364065\tvalid's multi_logloss: 0.792869\n",
      "[518]\ttrain's multi_logloss: 0.363437\tvalid's multi_logloss: 0.793209\n",
      "[519]\ttrain's multi_logloss: 0.362684\tvalid's multi_logloss: 0.792633\n",
      "[520]\ttrain's multi_logloss: 0.361982\tvalid's multi_logloss: 0.792653\n",
      "[521]\ttrain's multi_logloss: 0.36131\tvalid's multi_logloss: 0.792358\n",
      "[522]\ttrain's multi_logloss: 0.360806\tvalid's multi_logloss: 0.791724\n",
      "[523]\ttrain's multi_logloss: 0.360302\tvalid's multi_logloss: 0.791127\n",
      "[524]\ttrain's multi_logloss: 0.359619\tvalid's multi_logloss: 0.791682\n",
      "[525]\ttrain's multi_logloss: 0.359145\tvalid's multi_logloss: 0.790858\n",
      "[526]\ttrain's multi_logloss: 0.358647\tvalid's multi_logloss: 0.791335\n",
      "[527]\ttrain's multi_logloss: 0.358127\tvalid's multi_logloss: 0.790584\n",
      "[528]\ttrain's multi_logloss: 0.357561\tvalid's multi_logloss: 0.790324\n",
      "[529]\ttrain's multi_logloss: 0.356918\tvalid's multi_logloss: 0.7888\n",
      "[530]\ttrain's multi_logloss: 0.356394\tvalid's multi_logloss: 0.788794\n",
      "[531]\ttrain's multi_logloss: 0.355959\tvalid's multi_logloss: 0.790323\n",
      "[532]\ttrain's multi_logloss: 0.355557\tvalid's multi_logloss: 0.79199\n",
      "[533]\ttrain's multi_logloss: 0.355212\tvalid's multi_logloss: 0.793309\n",
      "[534]\ttrain's multi_logloss: 0.354715\tvalid's multi_logloss: 0.79377\n",
      "[535]\ttrain's multi_logloss: 0.354331\tvalid's multi_logloss: 0.794176\n",
      "[536]\ttrain's multi_logloss: 0.353635\tvalid's multi_logloss: 0.793806\n",
      "[537]\ttrain's multi_logloss: 0.35305\tvalid's multi_logloss: 0.793002\n",
      "[538]\ttrain's multi_logloss: 0.352353\tvalid's multi_logloss: 0.791607\n",
      "[539]\ttrain's multi_logloss: 0.351709\tvalid's multi_logloss: 0.790072\n",
      "[540]\ttrain's multi_logloss: 0.351252\tvalid's multi_logloss: 0.789685\n",
      "[541]\ttrain's multi_logloss: 0.350877\tvalid's multi_logloss: 0.790219\n",
      "[542]\ttrain's multi_logloss: 0.350451\tvalid's multi_logloss: 0.78956\n",
      "[543]\ttrain's multi_logloss: 0.350042\tvalid's multi_logloss: 0.788611\n",
      "[544]\ttrain's multi_logloss: 0.349657\tvalid's multi_logloss: 0.788242\n",
      "[545]\ttrain's multi_logloss: 0.349408\tvalid's multi_logloss: 0.788071\n",
      "[546]\ttrain's multi_logloss: 0.348692\tvalid's multi_logloss: 0.787675\n",
      "[547]\ttrain's multi_logloss: 0.347948\tvalid's multi_logloss: 0.787246\n",
      "[548]\ttrain's multi_logloss: 0.347146\tvalid's multi_logloss: 0.786265\n",
      "[549]\ttrain's multi_logloss: 0.346436\tvalid's multi_logloss: 0.786441\n",
      "[550]\ttrain's multi_logloss: 0.345649\tvalid's multi_logloss: 0.786339\n",
      "[551]\ttrain's multi_logloss: 0.345068\tvalid's multi_logloss: 0.785243\n",
      "[552]\ttrain's multi_logloss: 0.344618\tvalid's multi_logloss: 0.78323\n",
      "[553]\ttrain's multi_logloss: 0.34418\tvalid's multi_logloss: 0.782534\n",
      "[554]\ttrain's multi_logloss: 0.343708\tvalid's multi_logloss: 0.781145\n",
      "[555]\ttrain's multi_logloss: 0.343391\tvalid's multi_logloss: 0.780366\n",
      "[556]\ttrain's multi_logloss: 0.34282\tvalid's multi_logloss: 0.780346\n",
      "[557]\ttrain's multi_logloss: 0.342094\tvalid's multi_logloss: 0.781465\n",
      "[558]\ttrain's multi_logloss: 0.341549\tvalid's multi_logloss: 0.781494\n",
      "[559]\ttrain's multi_logloss: 0.340867\tvalid's multi_logloss: 0.782301\n",
      "[560]\ttrain's multi_logloss: 0.340274\tvalid's multi_logloss: 0.782495\n",
      "[561]\ttrain's multi_logloss: 0.339707\tvalid's multi_logloss: 0.78158\n",
      "[562]\ttrain's multi_logloss: 0.339098\tvalid's multi_logloss: 0.779616\n",
      "[563]\ttrain's multi_logloss: 0.338623\tvalid's multi_logloss: 0.779362\n",
      "[564]\ttrain's multi_logloss: 0.33803\tvalid's multi_logloss: 0.777203\n",
      "[565]\ttrain's multi_logloss: 0.3375\tvalid's multi_logloss: 0.776552\n",
      "[566]\ttrain's multi_logloss: 0.337278\tvalid's multi_logloss: 0.776081\n",
      "[567]\ttrain's multi_logloss: 0.33682\tvalid's multi_logloss: 0.77568\n",
      "[568]\ttrain's multi_logloss: 0.336452\tvalid's multi_logloss: 0.775618\n",
      "[569]\ttrain's multi_logloss: 0.336098\tvalid's multi_logloss: 0.7749\n",
      "[570]\ttrain's multi_logloss: 0.335747\tvalid's multi_logloss: 0.774862\n",
      "[571]\ttrain's multi_logloss: 0.335163\tvalid's multi_logloss: 0.775672\n",
      "[572]\ttrain's multi_logloss: 0.334626\tvalid's multi_logloss: 0.775763\n",
      "[573]\ttrain's multi_logloss: 0.334002\tvalid's multi_logloss: 0.776367\n",
      "[574]\ttrain's multi_logloss: 0.333462\tvalid's multi_logloss: 0.776169\n",
      "[575]\ttrain's multi_logloss: 0.332938\tvalid's multi_logloss: 0.775437\n",
      "[576]\ttrain's multi_logloss: 0.332406\tvalid's multi_logloss: 0.775584\n",
      "[577]\ttrain's multi_logloss: 0.331716\tvalid's multi_logloss: 0.776325\n",
      "[578]\ttrain's multi_logloss: 0.331091\tvalid's multi_logloss: 0.776404\n",
      "[579]\ttrain's multi_logloss: 0.330641\tvalid's multi_logloss: 0.775963\n",
      "[580]\ttrain's multi_logloss: 0.330089\tvalid's multi_logloss: 0.775098\n",
      "[581]\ttrain's multi_logloss: 0.329645\tvalid's multi_logloss: 0.776503\n",
      "[582]\ttrain's multi_logloss: 0.329307\tvalid's multi_logloss: 0.776575\n",
      "[583]\ttrain's multi_logloss: 0.328922\tvalid's multi_logloss: 0.776813\n",
      "[584]\ttrain's multi_logloss: 0.328676\tvalid's multi_logloss: 0.777357\n",
      "[585]\ttrain's multi_logloss: 0.328157\tvalid's multi_logloss: 0.777473\n",
      "[586]\ttrain's multi_logloss: 0.327747\tvalid's multi_logloss: 0.778515\n",
      "[587]\ttrain's multi_logloss: 0.327244\tvalid's multi_logloss: 0.778882\n",
      "[588]\ttrain's multi_logloss: 0.326792\tvalid's multi_logloss: 0.780608\n",
      "[589]\ttrain's multi_logloss: 0.326143\tvalid's multi_logloss: 0.779542\n",
      "[590]\ttrain's multi_logloss: 0.325362\tvalid's multi_logloss: 0.777945\n",
      "[591]\ttrain's multi_logloss: 0.324948\tvalid's multi_logloss: 0.778316\n",
      "[592]\ttrain's multi_logloss: 0.324587\tvalid's multi_logloss: 0.777476\n",
      "[593]\ttrain's multi_logloss: 0.324075\tvalid's multi_logloss: 0.778229\n",
      "[594]\ttrain's multi_logloss: 0.323452\tvalid's multi_logloss: 0.777699\n",
      "[595]\ttrain's multi_logloss: 0.32299\tvalid's multi_logloss: 0.777619\n",
      "[596]\ttrain's multi_logloss: 0.322426\tvalid's multi_logloss: 0.777125\n",
      "[597]\ttrain's multi_logloss: 0.321846\tvalid's multi_logloss: 0.776541\n",
      "[598]\ttrain's multi_logloss: 0.321272\tvalid's multi_logloss: 0.77657\n",
      "[599]\ttrain's multi_logloss: 0.320729\tvalid's multi_logloss: 0.776673\n",
      "[600]\ttrain's multi_logloss: 0.320064\tvalid's multi_logloss: 0.775718\n",
      "[601]\ttrain's multi_logloss: 0.319482\tvalid's multi_logloss: 0.775029\n",
      "[602]\ttrain's multi_logloss: 0.318905\tvalid's multi_logloss: 0.775245\n",
      "[603]\ttrain's multi_logloss: 0.31825\tvalid's multi_logloss: 0.775254\n",
      "[604]\ttrain's multi_logloss: 0.317708\tvalid's multi_logloss: 0.775796\n",
      "[605]\ttrain's multi_logloss: 0.317059\tvalid's multi_logloss: 0.7767\n",
      "[606]\ttrain's multi_logloss: 0.316456\tvalid's multi_logloss: 0.776444\n",
      "[607]\ttrain's multi_logloss: 0.315904\tvalid's multi_logloss: 0.775425\n",
      "[608]\ttrain's multi_logloss: 0.315305\tvalid's multi_logloss: 0.774836\n",
      "[609]\ttrain's multi_logloss: 0.314676\tvalid's multi_logloss: 0.77364\n",
      "[610]\ttrain's multi_logloss: 0.314069\tvalid's multi_logloss: 0.773933\n",
      "[611]\ttrain's multi_logloss: 0.313663\tvalid's multi_logloss: 0.773465\n",
      "[612]\ttrain's multi_logloss: 0.313348\tvalid's multi_logloss: 0.773746\n",
      "[613]\ttrain's multi_logloss: 0.31308\tvalid's multi_logloss: 0.773426\n",
      "[614]\ttrain's multi_logloss: 0.312848\tvalid's multi_logloss: 0.773651\n",
      "[615]\ttrain's multi_logloss: 0.312497\tvalid's multi_logloss: 0.77394\n",
      "[616]\ttrain's multi_logloss: 0.311997\tvalid's multi_logloss: 0.774189\n",
      "[617]\ttrain's multi_logloss: 0.311546\tvalid's multi_logloss: 0.773005\n",
      "[618]\ttrain's multi_logloss: 0.311119\tvalid's multi_logloss: 0.772205\n",
      "[619]\ttrain's multi_logloss: 0.310574\tvalid's multi_logloss: 0.77157\n",
      "[620]\ttrain's multi_logloss: 0.310106\tvalid's multi_logloss: 0.771199\n",
      "[621]\ttrain's multi_logloss: 0.309646\tvalid's multi_logloss: 0.770324\n",
      "[622]\ttrain's multi_logloss: 0.309224\tvalid's multi_logloss: 0.770298\n",
      "[623]\ttrain's multi_logloss: 0.308687\tvalid's multi_logloss: 0.771167\n",
      "[624]\ttrain's multi_logloss: 0.30815\tvalid's multi_logloss: 0.770863\n",
      "[625]\ttrain's multi_logloss: 0.307698\tvalid's multi_logloss: 0.771075\n",
      "[626]\ttrain's multi_logloss: 0.307286\tvalid's multi_logloss: 0.77174\n",
      "[627]\ttrain's multi_logloss: 0.306719\tvalid's multi_logloss: 0.773148\n",
      "[628]\ttrain's multi_logloss: 0.306232\tvalid's multi_logloss: 0.773841\n",
      "[629]\ttrain's multi_logloss: 0.305901\tvalid's multi_logloss: 0.7728\n",
      "[630]\ttrain's multi_logloss: 0.305409\tvalid's multi_logloss: 0.773224\n",
      "[631]\ttrain's multi_logloss: 0.304795\tvalid's multi_logloss: 0.773434\n",
      "[632]\ttrain's multi_logloss: 0.304144\tvalid's multi_logloss: 0.772915\n",
      "[633]\ttrain's multi_logloss: 0.30361\tvalid's multi_logloss: 0.772325\n",
      "[634]\ttrain's multi_logloss: 0.30303\tvalid's multi_logloss: 0.772589\n",
      "[635]\ttrain's multi_logloss: 0.302522\tvalid's multi_logloss: 0.77282\n",
      "[636]\ttrain's multi_logloss: 0.301876\tvalid's multi_logloss: 0.772234\n",
      "[637]\ttrain's multi_logloss: 0.301377\tvalid's multi_logloss: 0.771673\n",
      "[638]\ttrain's multi_logloss: 0.300869\tvalid's multi_logloss: 0.772148\n",
      "[639]\ttrain's multi_logloss: 0.300423\tvalid's multi_logloss: 0.770761\n",
      "[640]\ttrain's multi_logloss: 0.299848\tvalid's multi_logloss: 0.770003\n",
      "[641]\ttrain's multi_logloss: 0.299439\tvalid's multi_logloss: 0.77078\n",
      "[642]\ttrain's multi_logloss: 0.298867\tvalid's multi_logloss: 0.77145\n",
      "[643]\ttrain's multi_logloss: 0.298452\tvalid's multi_logloss: 0.772824\n",
      "[644]\ttrain's multi_logloss: 0.297985\tvalid's multi_logloss: 0.773771\n",
      "[645]\ttrain's multi_logloss: 0.297503\tvalid's multi_logloss: 0.774841\n",
      "[646]\ttrain's multi_logloss: 0.296985\tvalid's multi_logloss: 0.774436\n",
      "[647]\ttrain's multi_logloss: 0.29631\tvalid's multi_logloss: 0.775373\n",
      "[648]\ttrain's multi_logloss: 0.29576\tvalid's multi_logloss: 0.775837\n",
      "[649]\ttrain's multi_logloss: 0.295395\tvalid's multi_logloss: 0.776598\n",
      "[650]\ttrain's multi_logloss: 0.294813\tvalid's multi_logloss: 0.777955\n",
      "[651]\ttrain's multi_logloss: 0.294205\tvalid's multi_logloss: 0.777536\n",
      "[652]\ttrain's multi_logloss: 0.293864\tvalid's multi_logloss: 0.77707\n",
      "[653]\ttrain's multi_logloss: 0.293483\tvalid's multi_logloss: 0.776809\n",
      "[654]\ttrain's multi_logloss: 0.293041\tvalid's multi_logloss: 0.776\n",
      "[655]\ttrain's multi_logloss: 0.292568\tvalid's multi_logloss: 0.776196\n",
      "[656]\ttrain's multi_logloss: 0.29229\tvalid's multi_logloss: 0.775718\n",
      "[657]\ttrain's multi_logloss: 0.292085\tvalid's multi_logloss: 0.774938\n",
      "[658]\ttrain's multi_logloss: 0.291745\tvalid's multi_logloss: 0.774338\n",
      "[659]\ttrain's multi_logloss: 0.291488\tvalid's multi_logloss: 0.773432\n",
      "[660]\ttrain's multi_logloss: 0.29126\tvalid's multi_logloss: 0.772631\n",
      "[661]\ttrain's multi_logloss: 0.290736\tvalid's multi_logloss: 0.774829\n",
      "[662]\ttrain's multi_logloss: 0.290177\tvalid's multi_logloss: 0.777548\n",
      "[663]\ttrain's multi_logloss: 0.289689\tvalid's multi_logloss: 0.779042\n",
      "[664]\ttrain's multi_logloss: 0.289173\tvalid's multi_logloss: 0.780596\n",
      "[665]\ttrain's multi_logloss: 0.28859\tvalid's multi_logloss: 0.782721\n",
      "[666]\ttrain's multi_logloss: 0.288123\tvalid's multi_logloss: 0.782777\n",
      "[667]\ttrain's multi_logloss: 0.287623\tvalid's multi_logloss: 0.782707\n",
      "[668]\ttrain's multi_logloss: 0.287036\tvalid's multi_logloss: 0.783154\n",
      "[669]\ttrain's multi_logloss: 0.286555\tvalid's multi_logloss: 0.783106\n",
      "[670]\ttrain's multi_logloss: 0.286086\tvalid's multi_logloss: 0.783762\n",
      "[671]\ttrain's multi_logloss: 0.285737\tvalid's multi_logloss: 0.783628\n",
      "[672]\ttrain's multi_logloss: 0.285399\tvalid's multi_logloss: 0.785438\n",
      "[673]\ttrain's multi_logloss: 0.285133\tvalid's multi_logloss: 0.784939\n",
      "[674]\ttrain's multi_logloss: 0.284757\tvalid's multi_logloss: 0.785986\n",
      "[675]\ttrain's multi_logloss: 0.284397\tvalid's multi_logloss: 0.787413\n",
      "[676]\ttrain's multi_logloss: 0.283775\tvalid's multi_logloss: 0.786856\n",
      "[677]\ttrain's multi_logloss: 0.283278\tvalid's multi_logloss: 0.787807\n",
      "[678]\ttrain's multi_logloss: 0.282719\tvalid's multi_logloss: 0.787236\n",
      "[679]\ttrain's multi_logloss: 0.282347\tvalid's multi_logloss: 0.788191\n",
      "[680]\ttrain's multi_logloss: 0.281868\tvalid's multi_logloss: 0.787712\n",
      "[681]\ttrain's multi_logloss: 0.281239\tvalid's multi_logloss: 0.786101\n",
      "[682]\ttrain's multi_logloss: 0.280432\tvalid's multi_logloss: 0.785073\n",
      "[683]\ttrain's multi_logloss: 0.279798\tvalid's multi_logloss: 0.785169\n",
      "[684]\ttrain's multi_logloss: 0.279162\tvalid's multi_logloss: 0.78402\n",
      "[685]\ttrain's multi_logloss: 0.278599\tvalid's multi_logloss: 0.78364\n",
      "[686]\ttrain's multi_logloss: 0.278182\tvalid's multi_logloss: 0.78269\n",
      "[687]\ttrain's multi_logloss: 0.277681\tvalid's multi_logloss: 0.78132\n",
      "[688]\ttrain's multi_logloss: 0.277255\tvalid's multi_logloss: 0.782122\n",
      "[689]\ttrain's multi_logloss: 0.276702\tvalid's multi_logloss: 0.781248\n",
      "[690]\ttrain's multi_logloss: 0.276327\tvalid's multi_logloss: 0.780733\n",
      "[691]\ttrain's multi_logloss: 0.27588\tvalid's multi_logloss: 0.781793\n",
      "[692]\ttrain's multi_logloss: 0.275409\tvalid's multi_logloss: 0.781625\n",
      "[693]\ttrain's multi_logloss: 0.27501\tvalid's multi_logloss: 0.781743\n",
      "[694]\ttrain's multi_logloss: 0.274652\tvalid's multi_logloss: 0.78194\n",
      "[695]\ttrain's multi_logloss: 0.274175\tvalid's multi_logloss: 0.78276\n",
      "[696]\ttrain's multi_logloss: 0.27385\tvalid's multi_logloss: 0.783288\n",
      "[697]\ttrain's multi_logloss: 0.273487\tvalid's multi_logloss: 0.785139\n",
      "[698]\ttrain's multi_logloss: 0.273255\tvalid's multi_logloss: 0.785448\n",
      "[699]\ttrain's multi_logloss: 0.272951\tvalid's multi_logloss: 0.786348\n",
      "[700]\ttrain's multi_logloss: 0.272669\tvalid's multi_logloss: 0.787745\n",
      "[701]\ttrain's multi_logloss: 0.272261\tvalid's multi_logloss: 0.787438\n",
      "[702]\ttrain's multi_logloss: 0.271917\tvalid's multi_logloss: 0.786632\n",
      "[703]\ttrain's multi_logloss: 0.271582\tvalid's multi_logloss: 0.785489\n",
      "[704]\ttrain's multi_logloss: 0.271156\tvalid's multi_logloss: 0.785053\n",
      "[705]\ttrain's multi_logloss: 0.270719\tvalid's multi_logloss: 0.783984\n",
      "[706]\ttrain's multi_logloss: 0.270226\tvalid's multi_logloss: 0.78381\n",
      "[707]\ttrain's multi_logloss: 0.269792\tvalid's multi_logloss: 0.782489\n",
      "[708]\ttrain's multi_logloss: 0.269153\tvalid's multi_logloss: 0.782504\n",
      "[709]\ttrain's multi_logloss: 0.26864\tvalid's multi_logloss: 0.781922\n",
      "[710]\ttrain's multi_logloss: 0.268069\tvalid's multi_logloss: 0.780973\n",
      "[711]\ttrain's multi_logloss: 0.267744\tvalid's multi_logloss: 0.781029\n",
      "[712]\ttrain's multi_logloss: 0.267431\tvalid's multi_logloss: 0.78053\n",
      "[713]\ttrain's multi_logloss: 0.267039\tvalid's multi_logloss: 0.779643\n",
      "[714]\ttrain's multi_logloss: 0.266763\tvalid's multi_logloss: 0.779558\n",
      "[715]\ttrain's multi_logloss: 0.266416\tvalid's multi_logloss: 0.780141\n",
      "[716]\ttrain's multi_logloss: 0.265958\tvalid's multi_logloss: 0.7799\n",
      "[717]\ttrain's multi_logloss: 0.265594\tvalid's multi_logloss: 0.779396\n",
      "[718]\ttrain's multi_logloss: 0.265243\tvalid's multi_logloss: 0.779708\n",
      "[719]\ttrain's multi_logloss: 0.264866\tvalid's multi_logloss: 0.77908\n",
      "[720]\ttrain's multi_logloss: 0.264431\tvalid's multi_logloss: 0.778774\n",
      "[721]\ttrain's multi_logloss: 0.263991\tvalid's multi_logloss: 0.778271\n",
      "[722]\ttrain's multi_logloss: 0.26366\tvalid's multi_logloss: 0.77732\n",
      "[723]\ttrain's multi_logloss: 0.263415\tvalid's multi_logloss: 0.777074\n",
      "[724]\ttrain's multi_logloss: 0.262887\tvalid's multi_logloss: 0.776451\n",
      "[725]\ttrain's multi_logloss: 0.262568\tvalid's multi_logloss: 0.775842\n",
      "[726]\ttrain's multi_logloss: 0.262136\tvalid's multi_logloss: 0.775411\n",
      "[727]\ttrain's multi_logloss: 0.261772\tvalid's multi_logloss: 0.774817\n",
      "[728]\ttrain's multi_logloss: 0.261337\tvalid's multi_logloss: 0.773931\n",
      "[729]\ttrain's multi_logloss: 0.260804\tvalid's multi_logloss: 0.77328\n",
      "[730]\ttrain's multi_logloss: 0.260465\tvalid's multi_logloss: 0.772665\n",
      "[731]\ttrain's multi_logloss: 0.260032\tvalid's multi_logloss: 0.770205\n",
      "[732]\ttrain's multi_logloss: 0.259567\tvalid's multi_logloss: 0.770656\n",
      "[733]\ttrain's multi_logloss: 0.259135\tvalid's multi_logloss: 0.770832\n",
      "[734]\ttrain's multi_logloss: 0.258729\tvalid's multi_logloss: 0.770466\n",
      "[735]\ttrain's multi_logloss: 0.258225\tvalid's multi_logloss: 0.770607\n",
      "[736]\ttrain's multi_logloss: 0.257782\tvalid's multi_logloss: 0.77107\n",
      "[737]\ttrain's multi_logloss: 0.257379\tvalid's multi_logloss: 0.771924\n",
      "[738]\ttrain's multi_logloss: 0.256996\tvalid's multi_logloss: 0.772656\n",
      "[739]\ttrain's multi_logloss: 0.256607\tvalid's multi_logloss: 0.773338\n",
      "[740]\ttrain's multi_logloss: 0.256257\tvalid's multi_logloss: 0.773548\n",
      "Early stopping, best iteration is:\n",
      "[640]\ttrain's multi_logloss: 0.299848\tvalid's multi_logloss: 0.770003\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09195\tvalid's SFC_loss: 1.09678\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08644\tvalid's SFC_loss: 1.09475\n",
      "[3]\ttrain's SFC_loss: 1.08048\tvalid's SFC_loss: 1.09243\n",
      "[4]\ttrain's SFC_loss: 1.07555\tvalid's SFC_loss: 1.0924\n",
      "[5]\ttrain's SFC_loss: 1.07001\tvalid's SFC_loss: 1.0898\n",
      "[6]\ttrain's SFC_loss: 1.06552\tvalid's SFC_loss: 1.087\n",
      "[7]\ttrain's SFC_loss: 1.05959\tvalid's SFC_loss: 1.08182\n",
      "[8]\ttrain's SFC_loss: 1.05423\tvalid's SFC_loss: 1.07568\n",
      "[9]\ttrain's SFC_loss: 1.04898\tvalid's SFC_loss: 1.07141\n",
      "[10]\ttrain's SFC_loss: 1.04355\tvalid's SFC_loss: 1.06594\n",
      "[11]\ttrain's SFC_loss: 1.03863\tvalid's SFC_loss: 1.0612\n",
      "[12]\ttrain's SFC_loss: 1.03336\tvalid's SFC_loss: 1.05691\n",
      "[13]\ttrain's SFC_loss: 1.0274\tvalid's SFC_loss: 1.05312\n",
      "[14]\ttrain's SFC_loss: 1.02191\tvalid's SFC_loss: 1.04882\n",
      "[15]\ttrain's SFC_loss: 1.01647\tvalid's SFC_loss: 1.04625\n",
      "[16]\ttrain's SFC_loss: 1.01083\tvalid's SFC_loss: 1.04319\n",
      "[17]\ttrain's SFC_loss: 1.0047\tvalid's SFC_loss: 1.03901\n",
      "[18]\ttrain's SFC_loss: 1.00023\tvalid's SFC_loss: 1.03999\n",
      "[19]\ttrain's SFC_loss: 0.994128\tvalid's SFC_loss: 1.03697\n",
      "[20]\ttrain's SFC_loss: 0.989225\tvalid's SFC_loss: 1.03376\n",
      "[21]\ttrain's SFC_loss: 0.984443\tvalid's SFC_loss: 1.03233\n",
      "[22]\ttrain's SFC_loss: 0.978615\tvalid's SFC_loss: 1.0296\n",
      "[23]\ttrain's SFC_loss: 0.973001\tvalid's SFC_loss: 1.02945\n",
      "[24]\ttrain's SFC_loss: 0.967598\tvalid's SFC_loss: 1.02642\n",
      "[25]\ttrain's SFC_loss: 0.961944\tvalid's SFC_loss: 1.0261\n",
      "[26]\ttrain's SFC_loss: 0.955541\tvalid's SFC_loss: 1.02371\n",
      "[27]\ttrain's SFC_loss: 0.950289\tvalid's SFC_loss: 1.02216\n",
      "[28]\ttrain's SFC_loss: 0.943924\tvalid's SFC_loss: 1.01932\n",
      "[29]\ttrain's SFC_loss: 0.93998\tvalid's SFC_loss: 1.01895\n",
      "[30]\ttrain's SFC_loss: 0.935651\tvalid's SFC_loss: 1.01941\n",
      "[31]\ttrain's SFC_loss: 0.931688\tvalid's SFC_loss: 1.01572\n",
      "[32]\ttrain's SFC_loss: 0.927725\tvalid's SFC_loss: 1.01098\n",
      "[33]\ttrain's SFC_loss: 0.923707\tvalid's SFC_loss: 1.00695\n",
      "[34]\ttrain's SFC_loss: 0.919994\tvalid's SFC_loss: 1.00186\n",
      "[35]\ttrain's SFC_loss: 0.916041\tvalid's SFC_loss: 0.998595\n",
      "[36]\ttrain's SFC_loss: 0.911768\tvalid's SFC_loss: 0.994806\n",
      "[37]\ttrain's SFC_loss: 0.908213\tvalid's SFC_loss: 0.991475\n",
      "[38]\ttrain's SFC_loss: 0.904161\tvalid's SFC_loss: 0.987929\n",
      "[39]\ttrain's SFC_loss: 0.90088\tvalid's SFC_loss: 0.986802\n",
      "[40]\ttrain's SFC_loss: 0.897569\tvalid's SFC_loss: 0.98462\n",
      "[41]\ttrain's SFC_loss: 0.893063\tvalid's SFC_loss: 0.986146\n",
      "[42]\ttrain's SFC_loss: 0.88833\tvalid's SFC_loss: 0.98523\n",
      "[43]\ttrain's SFC_loss: 0.883741\tvalid's SFC_loss: 0.988124\n",
      "[44]\ttrain's SFC_loss: 0.880305\tvalid's SFC_loss: 0.989513\n",
      "[45]\ttrain's SFC_loss: 0.875347\tvalid's SFC_loss: 0.985725\n",
      "[46]\ttrain's SFC_loss: 0.871866\tvalid's SFC_loss: 0.983989\n",
      "[47]\ttrain's SFC_loss: 0.86819\tvalid's SFC_loss: 0.982237\n",
      "[48]\ttrain's SFC_loss: 0.864749\tvalid's SFC_loss: 0.980414\n",
      "[49]\ttrain's SFC_loss: 0.861793\tvalid's SFC_loss: 0.977521\n",
      "[50]\ttrain's SFC_loss: 0.857603\tvalid's SFC_loss: 0.97368\n",
      "[51]\ttrain's SFC_loss: 0.85438\tvalid's SFC_loss: 0.972315\n",
      "[52]\ttrain's SFC_loss: 0.851001\tvalid's SFC_loss: 0.971888\n",
      "[53]\ttrain's SFC_loss: 0.847762\tvalid's SFC_loss: 0.9699\n",
      "[54]\ttrain's SFC_loss: 0.844555\tvalid's SFC_loss: 0.970004\n",
      "[55]\ttrain's SFC_loss: 0.841203\tvalid's SFC_loss: 0.969329\n",
      "[56]\ttrain's SFC_loss: 0.837702\tvalid's SFC_loss: 0.969695\n",
      "[57]\ttrain's SFC_loss: 0.834952\tvalid's SFC_loss: 0.970701\n",
      "[58]\ttrain's SFC_loss: 0.83211\tvalid's SFC_loss: 0.970287\n",
      "[59]\ttrain's SFC_loss: 0.82902\tvalid's SFC_loss: 0.971118\n",
      "[60]\ttrain's SFC_loss: 0.82629\tvalid's SFC_loss: 0.971846\n",
      "[61]\ttrain's SFC_loss: 0.822385\tvalid's SFC_loss: 0.970942\n",
      "[62]\ttrain's SFC_loss: 0.819042\tvalid's SFC_loss: 0.969485\n",
      "[63]\ttrain's SFC_loss: 0.814549\tvalid's SFC_loss: 0.968806\n",
      "[64]\ttrain's SFC_loss: 0.81074\tvalid's SFC_loss: 0.966299\n",
      "[65]\ttrain's SFC_loss: 0.806662\tvalid's SFC_loss: 0.965005\n",
      "[66]\ttrain's SFC_loss: 0.803231\tvalid's SFC_loss: 0.966745\n",
      "[67]\ttrain's SFC_loss: 0.799969\tvalid's SFC_loss: 0.965669\n",
      "[68]\ttrain's SFC_loss: 0.796979\tvalid's SFC_loss: 0.96677\n",
      "[69]\ttrain's SFC_loss: 0.793752\tvalid's SFC_loss: 0.96756\n",
      "[70]\ttrain's SFC_loss: 0.790428\tvalid's SFC_loss: 0.965892\n",
      "[71]\ttrain's SFC_loss: 0.787399\tvalid's SFC_loss: 0.965523\n",
      "[72]\ttrain's SFC_loss: 0.784528\tvalid's SFC_loss: 0.962971\n",
      "[73]\ttrain's SFC_loss: 0.781366\tvalid's SFC_loss: 0.961175\n",
      "[74]\ttrain's SFC_loss: 0.778615\tvalid's SFC_loss: 0.958704\n",
      "[75]\ttrain's SFC_loss: 0.776505\tvalid's SFC_loss: 0.956401\n",
      "[76]\ttrain's SFC_loss: 0.773294\tvalid's SFC_loss: 0.954113\n",
      "[77]\ttrain's SFC_loss: 0.771386\tvalid's SFC_loss: 0.954037\n",
      "[78]\ttrain's SFC_loss: 0.768019\tvalid's SFC_loss: 0.953612\n",
      "[79]\ttrain's SFC_loss: 0.765157\tvalid's SFC_loss: 0.951342\n",
      "[80]\ttrain's SFC_loss: 0.762251\tvalid's SFC_loss: 0.948173\n",
      "[81]\ttrain's SFC_loss: 0.759798\tvalid's SFC_loss: 0.94132\n",
      "[82]\ttrain's SFC_loss: 0.757514\tvalid's SFC_loss: 0.935754\n",
      "[83]\ttrain's SFC_loss: 0.755634\tvalid's SFC_loss: 0.935333\n",
      "[84]\ttrain's SFC_loss: 0.753367\tvalid's SFC_loss: 0.930995\n",
      "[85]\ttrain's SFC_loss: 0.750886\tvalid's SFC_loss: 0.926973\n",
      "[86]\ttrain's SFC_loss: 0.748553\tvalid's SFC_loss: 0.927426\n",
      "[87]\ttrain's SFC_loss: 0.746564\tvalid's SFC_loss: 0.925945\n",
      "[88]\ttrain's SFC_loss: 0.744665\tvalid's SFC_loss: 0.92343\n",
      "[89]\ttrain's SFC_loss: 0.742489\tvalid's SFC_loss: 0.922038\n",
      "[90]\ttrain's SFC_loss: 0.740213\tvalid's SFC_loss: 0.918255\n",
      "[91]\ttrain's SFC_loss: 0.737671\tvalid's SFC_loss: 0.918751\n",
      "[92]\ttrain's SFC_loss: 0.735036\tvalid's SFC_loss: 0.918333\n",
      "[93]\ttrain's SFC_loss: 0.732318\tvalid's SFC_loss: 0.916602\n",
      "[94]\ttrain's SFC_loss: 0.72982\tvalid's SFC_loss: 0.915833\n",
      "[95]\ttrain's SFC_loss: 0.727235\tvalid's SFC_loss: 0.912967\n",
      "[96]\ttrain's SFC_loss: 0.725354\tvalid's SFC_loss: 0.91349\n",
      "[97]\ttrain's SFC_loss: 0.723499\tvalid's SFC_loss: 0.914344\n",
      "[98]\ttrain's SFC_loss: 0.721872\tvalid's SFC_loss: 0.915195\n",
      "[99]\ttrain's SFC_loss: 0.720298\tvalid's SFC_loss: 0.916063\n",
      "[100]\ttrain's SFC_loss: 0.71835\tvalid's SFC_loss: 0.914783\n",
      "[101]\ttrain's SFC_loss: 0.716316\tvalid's SFC_loss: 0.912956\n",
      "[102]\ttrain's SFC_loss: 0.713323\tvalid's SFC_loss: 0.911718\n",
      "[103]\ttrain's SFC_loss: 0.711019\tvalid's SFC_loss: 0.910773\n",
      "[104]\ttrain's SFC_loss: 0.708957\tvalid's SFC_loss: 0.910108\n",
      "[105]\ttrain's SFC_loss: 0.707312\tvalid's SFC_loss: 0.911012\n",
      "[106]\ttrain's SFC_loss: 0.705525\tvalid's SFC_loss: 0.910156\n",
      "[107]\ttrain's SFC_loss: 0.70395\tvalid's SFC_loss: 0.910986\n",
      "[108]\ttrain's SFC_loss: 0.702393\tvalid's SFC_loss: 0.910345\n",
      "[109]\ttrain's SFC_loss: 0.701058\tvalid's SFC_loss: 0.909223\n",
      "[110]\ttrain's SFC_loss: 0.699194\tvalid's SFC_loss: 0.908772\n",
      "[111]\ttrain's SFC_loss: 0.697617\tvalid's SFC_loss: 0.907958\n",
      "[112]\ttrain's SFC_loss: 0.69626\tvalid's SFC_loss: 0.909013\n",
      "[113]\ttrain's SFC_loss: 0.694636\tvalid's SFC_loss: 0.909164\n",
      "[114]\ttrain's SFC_loss: 0.69297\tvalid's SFC_loss: 0.909406\n",
      "[115]\ttrain's SFC_loss: 0.690952\tvalid's SFC_loss: 0.909741\n",
      "[116]\ttrain's SFC_loss: 0.689095\tvalid's SFC_loss: 0.908407\n",
      "[117]\ttrain's SFC_loss: 0.687428\tvalid's SFC_loss: 0.906058\n",
      "[118]\ttrain's SFC_loss: 0.685506\tvalid's SFC_loss: 0.90457\n",
      "[119]\ttrain's SFC_loss: 0.683535\tvalid's SFC_loss: 0.904977\n",
      "[120]\ttrain's SFC_loss: 0.682045\tvalid's SFC_loss: 0.903008\n",
      "[121]\ttrain's SFC_loss: 0.679959\tvalid's SFC_loss: 0.903867\n",
      "[122]\ttrain's SFC_loss: 0.677619\tvalid's SFC_loss: 0.90325\n",
      "[123]\ttrain's SFC_loss: 0.6759\tvalid's SFC_loss: 0.905719\n",
      "[124]\ttrain's SFC_loss: 0.673624\tvalid's SFC_loss: 0.906056\n",
      "[125]\ttrain's SFC_loss: 0.671626\tvalid's SFC_loss: 0.905188\n",
      "[126]\ttrain's SFC_loss: 0.670155\tvalid's SFC_loss: 0.903769\n",
      "[127]\ttrain's SFC_loss: 0.668913\tvalid's SFC_loss: 0.90286\n",
      "[128]\ttrain's SFC_loss: 0.667661\tvalid's SFC_loss: 0.90251\n",
      "[129]\ttrain's SFC_loss: 0.666397\tvalid's SFC_loss: 0.902599\n",
      "[130]\ttrain's SFC_loss: 0.665168\tvalid's SFC_loss: 0.902719\n",
      "[131]\ttrain's SFC_loss: 0.66275\tvalid's SFC_loss: 0.901733\n",
      "[132]\ttrain's SFC_loss: 0.660479\tvalid's SFC_loss: 0.90129\n",
      "[133]\ttrain's SFC_loss: 0.658204\tvalid's SFC_loss: 0.901341\n",
      "[134]\ttrain's SFC_loss: 0.656153\tvalid's SFC_loss: 0.900134\n",
      "[135]\ttrain's SFC_loss: 0.653915\tvalid's SFC_loss: 0.900228\n",
      "[136]\ttrain's SFC_loss: 0.652392\tvalid's SFC_loss: 0.898769\n",
      "[137]\ttrain's SFC_loss: 0.650897\tvalid's SFC_loss: 0.897251\n",
      "[138]\ttrain's SFC_loss: 0.649269\tvalid's SFC_loss: 0.896478\n",
      "[139]\ttrain's SFC_loss: 0.647227\tvalid's SFC_loss: 0.895134\n",
      "[140]\ttrain's SFC_loss: 0.645267\tvalid's SFC_loss: 0.893751\n",
      "[141]\ttrain's SFC_loss: 0.643618\tvalid's SFC_loss: 0.892698\n",
      "[142]\ttrain's SFC_loss: 0.642116\tvalid's SFC_loss: 0.890686\n",
      "[143]\ttrain's SFC_loss: 0.64065\tvalid's SFC_loss: 0.888693\n",
      "[144]\ttrain's SFC_loss: 0.639076\tvalid's SFC_loss: 0.886847\n",
      "[145]\ttrain's SFC_loss: 0.637608\tvalid's SFC_loss: 0.885547\n",
      "[146]\ttrain's SFC_loss: 0.635803\tvalid's SFC_loss: 0.887621\n",
      "[147]\ttrain's SFC_loss: 0.634282\tvalid's SFC_loss: 0.888431\n",
      "[148]\ttrain's SFC_loss: 0.63246\tvalid's SFC_loss: 0.889391\n",
      "[149]\ttrain's SFC_loss: 0.630712\tvalid's SFC_loss: 0.889403\n",
      "[150]\ttrain's SFC_loss: 0.629106\tvalid's SFC_loss: 0.891132\n",
      "[151]\ttrain's SFC_loss: 0.627094\tvalid's SFC_loss: 0.887552\n",
      "[152]\ttrain's SFC_loss: 0.625576\tvalid's SFC_loss: 0.883626\n",
      "[153]\ttrain's SFC_loss: 0.623651\tvalid's SFC_loss: 0.882068\n",
      "[154]\ttrain's SFC_loss: 0.622004\tvalid's SFC_loss: 0.879952\n",
      "[155]\ttrain's SFC_loss: 0.620404\tvalid's SFC_loss: 0.87714\n",
      "[156]\ttrain's SFC_loss: 0.618536\tvalid's SFC_loss: 0.87629\n",
      "[157]\ttrain's SFC_loss: 0.617074\tvalid's SFC_loss: 0.875888\n",
      "[158]\ttrain's SFC_loss: 0.615348\tvalid's SFC_loss: 0.877849\n",
      "[159]\ttrain's SFC_loss: 0.613754\tvalid's SFC_loss: 0.8783\n",
      "[160]\ttrain's SFC_loss: 0.611945\tvalid's SFC_loss: 0.878875\n",
      "[161]\ttrain's SFC_loss: 0.610589\tvalid's SFC_loss: 0.879048\n",
      "[162]\ttrain's SFC_loss: 0.609182\tvalid's SFC_loss: 0.880745\n",
      "[163]\ttrain's SFC_loss: 0.607801\tvalid's SFC_loss: 0.880906\n",
      "[164]\ttrain's SFC_loss: 0.60651\tvalid's SFC_loss: 0.882654\n",
      "[165]\ttrain's SFC_loss: 0.605116\tvalid's SFC_loss: 0.885067\n",
      "[166]\ttrain's SFC_loss: 0.603641\tvalid's SFC_loss: 0.88205\n",
      "[167]\ttrain's SFC_loss: 0.6019\tvalid's SFC_loss: 0.882015\n",
      "[168]\ttrain's SFC_loss: 0.600002\tvalid's SFC_loss: 0.879296\n",
      "[169]\ttrain's SFC_loss: 0.598727\tvalid's SFC_loss: 0.875563\n",
      "[170]\ttrain's SFC_loss: 0.597175\tvalid's SFC_loss: 0.87519\n",
      "[171]\ttrain's SFC_loss: 0.595649\tvalid's SFC_loss: 0.872682\n",
      "[172]\ttrain's SFC_loss: 0.593971\tvalid's SFC_loss: 0.870207\n",
      "[173]\ttrain's SFC_loss: 0.592468\tvalid's SFC_loss: 0.867688\n",
      "[174]\ttrain's SFC_loss: 0.590938\tvalid's SFC_loss: 0.869252\n",
      "[175]\ttrain's SFC_loss: 0.589853\tvalid's SFC_loss: 0.86785\n",
      "[176]\ttrain's SFC_loss: 0.588402\tvalid's SFC_loss: 0.869529\n",
      "[177]\ttrain's SFC_loss: 0.587057\tvalid's SFC_loss: 0.870842\n",
      "[178]\ttrain's SFC_loss: 0.585814\tvalid's SFC_loss: 0.872592\n",
      "[179]\ttrain's SFC_loss: 0.584258\tvalid's SFC_loss: 0.870928\n",
      "[180]\ttrain's SFC_loss: 0.582522\tvalid's SFC_loss: 0.869561\n",
      "[181]\ttrain's SFC_loss: 0.580995\tvalid's SFC_loss: 0.868417\n",
      "[182]\ttrain's SFC_loss: 0.579693\tvalid's SFC_loss: 0.867009\n",
      "[183]\ttrain's SFC_loss: 0.578047\tvalid's SFC_loss: 0.866395\n",
      "[184]\ttrain's SFC_loss: 0.576882\tvalid's SFC_loss: 0.864918\n",
      "[185]\ttrain's SFC_loss: 0.575638\tvalid's SFC_loss: 0.862745\n",
      "[186]\ttrain's SFC_loss: 0.574103\tvalid's SFC_loss: 0.862923\n",
      "[187]\ttrain's SFC_loss: 0.572296\tvalid's SFC_loss: 0.862029\n",
      "[188]\ttrain's SFC_loss: 0.570587\tvalid's SFC_loss: 0.858778\n",
      "[189]\ttrain's SFC_loss: 0.56847\tvalid's SFC_loss: 0.857521\n",
      "[190]\ttrain's SFC_loss: 0.566797\tvalid's SFC_loss: 0.85662\n",
      "[191]\ttrain's SFC_loss: 0.56589\tvalid's SFC_loss: 0.857466\n",
      "[192]\ttrain's SFC_loss: 0.564926\tvalid's SFC_loss: 0.859079\n",
      "[193]\ttrain's SFC_loss: 0.563785\tvalid's SFC_loss: 0.861195\n",
      "[194]\ttrain's SFC_loss: 0.562829\tvalid's SFC_loss: 0.860352\n",
      "[195]\ttrain's SFC_loss: 0.561863\tvalid's SFC_loss: 0.859258\n",
      "[196]\ttrain's SFC_loss: 0.560203\tvalid's SFC_loss: 0.859134\n",
      "[197]\ttrain's SFC_loss: 0.558795\tvalid's SFC_loss: 0.858182\n",
      "[198]\ttrain's SFC_loss: 0.557343\tvalid's SFC_loss: 0.85704\n",
      "[199]\ttrain's SFC_loss: 0.556073\tvalid's SFC_loss: 0.855953\n",
      "[200]\ttrain's SFC_loss: 0.554795\tvalid's SFC_loss: 0.854727\n",
      "[201]\ttrain's SFC_loss: 0.55298\tvalid's SFC_loss: 0.854229\n",
      "[202]\ttrain's SFC_loss: 0.550691\tvalid's SFC_loss: 0.853916\n",
      "[203]\ttrain's SFC_loss: 0.548843\tvalid's SFC_loss: 0.851787\n",
      "[204]\ttrain's SFC_loss: 0.547234\tvalid's SFC_loss: 0.852079\n",
      "[205]\ttrain's SFC_loss: 0.545386\tvalid's SFC_loss: 0.850132\n",
      "[206]\ttrain's SFC_loss: 0.544588\tvalid's SFC_loss: 0.85197\n",
      "[207]\ttrain's SFC_loss: 0.543844\tvalid's SFC_loss: 0.854576\n",
      "[208]\ttrain's SFC_loss: 0.54334\tvalid's SFC_loss: 0.854839\n",
      "[209]\ttrain's SFC_loss: 0.542345\tvalid's SFC_loss: 0.854133\n",
      "[210]\ttrain's SFC_loss: 0.541582\tvalid's SFC_loss: 0.855684\n",
      "[211]\ttrain's SFC_loss: 0.540245\tvalid's SFC_loss: 0.854288\n",
      "[212]\ttrain's SFC_loss: 0.539144\tvalid's SFC_loss: 0.85458\n",
      "[213]\ttrain's SFC_loss: 0.53777\tvalid's SFC_loss: 0.85444\n",
      "[214]\ttrain's SFC_loss: 0.536567\tvalid's SFC_loss: 0.854859\n",
      "[215]\ttrain's SFC_loss: 0.535647\tvalid's SFC_loss: 0.854736\n",
      "[216]\ttrain's SFC_loss: 0.534152\tvalid's SFC_loss: 0.854595\n",
      "[217]\ttrain's SFC_loss: 0.532559\tvalid's SFC_loss: 0.854142\n",
      "[218]\ttrain's SFC_loss: 0.531342\tvalid's SFC_loss: 0.854074\n",
      "[219]\ttrain's SFC_loss: 0.529967\tvalid's SFC_loss: 0.85405\n",
      "[220]\ttrain's SFC_loss: 0.528056\tvalid's SFC_loss: 0.855648\n",
      "[221]\ttrain's SFC_loss: 0.526298\tvalid's SFC_loss: 0.853755\n",
      "[222]\ttrain's SFC_loss: 0.524727\tvalid's SFC_loss: 0.851258\n",
      "[223]\ttrain's SFC_loss: 0.523157\tvalid's SFC_loss: 0.850274\n",
      "[224]\ttrain's SFC_loss: 0.521433\tvalid's SFC_loss: 0.848255\n",
      "[225]\ttrain's SFC_loss: 0.519856\tvalid's SFC_loss: 0.848872\n",
      "[226]\ttrain's SFC_loss: 0.518871\tvalid's SFC_loss: 0.84658\n",
      "[227]\ttrain's SFC_loss: 0.51779\tvalid's SFC_loss: 0.844123\n",
      "[228]\ttrain's SFC_loss: 0.51676\tvalid's SFC_loss: 0.844132\n",
      "[229]\ttrain's SFC_loss: 0.515306\tvalid's SFC_loss: 0.843724\n",
      "[230]\ttrain's SFC_loss: 0.5142\tvalid's SFC_loss: 0.841688\n",
      "[231]\ttrain's SFC_loss: 0.512864\tvalid's SFC_loss: 0.840652\n",
      "[232]\ttrain's SFC_loss: 0.511626\tvalid's SFC_loss: 0.839903\n",
      "[233]\ttrain's SFC_loss: 0.510316\tvalid's SFC_loss: 0.839759\n",
      "[234]\ttrain's SFC_loss: 0.509148\tvalid's SFC_loss: 0.839095\n",
      "[235]\ttrain's SFC_loss: 0.508141\tvalid's SFC_loss: 0.838836\n",
      "[236]\ttrain's SFC_loss: 0.506779\tvalid's SFC_loss: 0.838513\n",
      "[237]\ttrain's SFC_loss: 0.505417\tvalid's SFC_loss: 0.839368\n",
      "[238]\ttrain's SFC_loss: 0.504453\tvalid's SFC_loss: 0.839503\n",
      "[239]\ttrain's SFC_loss: 0.503368\tvalid's SFC_loss: 0.838892\n",
      "[240]\ttrain's SFC_loss: 0.502558\tvalid's SFC_loss: 0.840666\n",
      "[241]\ttrain's SFC_loss: 0.50132\tvalid's SFC_loss: 0.840537\n",
      "[242]\ttrain's SFC_loss: 0.500349\tvalid's SFC_loss: 0.840362\n",
      "[243]\ttrain's SFC_loss: 0.499363\tvalid's SFC_loss: 0.840105\n",
      "[244]\ttrain's SFC_loss: 0.498443\tvalid's SFC_loss: 0.841411\n",
      "[245]\ttrain's SFC_loss: 0.497304\tvalid's SFC_loss: 0.840226\n",
      "[246]\ttrain's SFC_loss: 0.496099\tvalid's SFC_loss: 0.837139\n",
      "[247]\ttrain's SFC_loss: 0.495111\tvalid's SFC_loss: 0.83517\n",
      "[248]\ttrain's SFC_loss: 0.494087\tvalid's SFC_loss: 0.832336\n",
      "[249]\ttrain's SFC_loss: 0.493157\tvalid's SFC_loss: 0.830426\n",
      "[250]\ttrain's SFC_loss: 0.492494\tvalid's SFC_loss: 0.828839\n",
      "[251]\ttrain's SFC_loss: 0.491206\tvalid's SFC_loss: 0.828694\n",
      "[252]\ttrain's SFC_loss: 0.489987\tvalid's SFC_loss: 0.828025\n",
      "[253]\ttrain's SFC_loss: 0.48872\tvalid's SFC_loss: 0.827597\n",
      "[254]\ttrain's SFC_loss: 0.487549\tvalid's SFC_loss: 0.828371\n",
      "[255]\ttrain's SFC_loss: 0.48648\tvalid's SFC_loss: 0.829367\n",
      "[256]\ttrain's SFC_loss: 0.485882\tvalid's SFC_loss: 0.831147\n",
      "[257]\ttrain's SFC_loss: 0.485075\tvalid's SFC_loss: 0.831166\n",
      "[258]\ttrain's SFC_loss: 0.484381\tvalid's SFC_loss: 0.829643\n",
      "[259]\ttrain's SFC_loss: 0.483785\tvalid's SFC_loss: 0.831454\n",
      "[260]\ttrain's SFC_loss: 0.482984\tvalid's SFC_loss: 0.831141\n",
      "[261]\ttrain's SFC_loss: 0.481808\tvalid's SFC_loss: 0.830127\n",
      "[262]\ttrain's SFC_loss: 0.48048\tvalid's SFC_loss: 0.830521\n",
      "[263]\ttrain's SFC_loss: 0.479548\tvalid's SFC_loss: 0.830829\n",
      "[264]\ttrain's SFC_loss: 0.478682\tvalid's SFC_loss: 0.830065\n",
      "[265]\ttrain's SFC_loss: 0.477202\tvalid's SFC_loss: 0.829512\n",
      "[266]\ttrain's SFC_loss: 0.475783\tvalid's SFC_loss: 0.827156\n",
      "[267]\ttrain's SFC_loss: 0.474421\tvalid's SFC_loss: 0.826513\n",
      "[268]\ttrain's SFC_loss: 0.473005\tvalid's SFC_loss: 0.828122\n",
      "[269]\ttrain's SFC_loss: 0.471953\tvalid's SFC_loss: 0.828735\n",
      "[270]\ttrain's SFC_loss: 0.470941\tvalid's SFC_loss: 0.829368\n",
      "[271]\ttrain's SFC_loss: 0.470156\tvalid's SFC_loss: 0.828528\n",
      "[272]\ttrain's SFC_loss: 0.469114\tvalid's SFC_loss: 0.824309\n",
      "[273]\ttrain's SFC_loss: 0.468339\tvalid's SFC_loss: 0.823505\n",
      "[274]\ttrain's SFC_loss: 0.467692\tvalid's SFC_loss: 0.82085\n",
      "[275]\ttrain's SFC_loss: 0.466751\tvalid's SFC_loss: 0.818272\n",
      "[276]\ttrain's SFC_loss: 0.465576\tvalid's SFC_loss: 0.818171\n",
      "[277]\ttrain's SFC_loss: 0.464439\tvalid's SFC_loss: 0.817439\n",
      "[278]\ttrain's SFC_loss: 0.463646\tvalid's SFC_loss: 0.816877\n",
      "[279]\ttrain's SFC_loss: 0.462598\tvalid's SFC_loss: 0.816071\n",
      "[280]\ttrain's SFC_loss: 0.461621\tvalid's SFC_loss: 0.818854\n",
      "[281]\ttrain's SFC_loss: 0.46069\tvalid's SFC_loss: 0.818459\n",
      "[282]\ttrain's SFC_loss: 0.459489\tvalid's SFC_loss: 0.816326\n",
      "[283]\ttrain's SFC_loss: 0.458333\tvalid's SFC_loss: 0.815633\n",
      "[284]\ttrain's SFC_loss: 0.457294\tvalid's SFC_loss: 0.818323\n",
      "[285]\ttrain's SFC_loss: 0.456276\tvalid's SFC_loss: 0.820169\n",
      "[286]\ttrain's SFC_loss: 0.455744\tvalid's SFC_loss: 0.820578\n",
      "[287]\ttrain's SFC_loss: 0.455174\tvalid's SFC_loss: 0.820478\n",
      "[288]\ttrain's SFC_loss: 0.454324\tvalid's SFC_loss: 0.820112\n",
      "[289]\ttrain's SFC_loss: 0.453728\tvalid's SFC_loss: 0.819978\n",
      "[290]\ttrain's SFC_loss: 0.452924\tvalid's SFC_loss: 0.819067\n",
      "[291]\ttrain's SFC_loss: 0.451978\tvalid's SFC_loss: 0.820057\n",
      "[292]\ttrain's SFC_loss: 0.451148\tvalid's SFC_loss: 0.819919\n",
      "[293]\ttrain's SFC_loss: 0.450474\tvalid's SFC_loss: 0.820687\n",
      "[294]\ttrain's SFC_loss: 0.450103\tvalid's SFC_loss: 0.820488\n",
      "[295]\ttrain's SFC_loss: 0.449596\tvalid's SFC_loss: 0.821485\n",
      "[296]\ttrain's SFC_loss: 0.448885\tvalid's SFC_loss: 0.822534\n",
      "[297]\ttrain's SFC_loss: 0.448215\tvalid's SFC_loss: 0.82362\n",
      "[298]\ttrain's SFC_loss: 0.447748\tvalid's SFC_loss: 0.825183\n",
      "[299]\ttrain's SFC_loss: 0.447323\tvalid's SFC_loss: 0.826071\n",
      "[300]\ttrain's SFC_loss: 0.446863\tvalid's SFC_loss: 0.827638\n",
      "[301]\ttrain's SFC_loss: 0.445946\tvalid's SFC_loss: 0.828252\n",
      "[302]\ttrain's SFC_loss: 0.445329\tvalid's SFC_loss: 0.827525\n",
      "[303]\ttrain's SFC_loss: 0.444334\tvalid's SFC_loss: 0.828396\n",
      "[304]\ttrain's SFC_loss: 0.443378\tvalid's SFC_loss: 0.829306\n",
      "[305]\ttrain's SFC_loss: 0.442826\tvalid's SFC_loss: 0.829376\n",
      "[306]\ttrain's SFC_loss: 0.441473\tvalid's SFC_loss: 0.830761\n",
      "[307]\ttrain's SFC_loss: 0.44023\tvalid's SFC_loss: 0.828535\n",
      "[308]\ttrain's SFC_loss: 0.439177\tvalid's SFC_loss: 0.826096\n",
      "[309]\ttrain's SFC_loss: 0.438144\tvalid's SFC_loss: 0.825545\n",
      "[310]\ttrain's SFC_loss: 0.436984\tvalid's SFC_loss: 0.825804\n",
      "[311]\ttrain's SFC_loss: 0.435887\tvalid's SFC_loss: 0.824507\n",
      "[312]\ttrain's SFC_loss: 0.434728\tvalid's SFC_loss: 0.825158\n",
      "[313]\ttrain's SFC_loss: 0.433689\tvalid's SFC_loss: 0.824979\n",
      "[314]\ttrain's SFC_loss: 0.432636\tvalid's SFC_loss: 0.82488\n",
      "[315]\ttrain's SFC_loss: 0.431496\tvalid's SFC_loss: 0.824477\n",
      "[316]\ttrain's SFC_loss: 0.430947\tvalid's SFC_loss: 0.824405\n",
      "[317]\ttrain's SFC_loss: 0.430365\tvalid's SFC_loss: 0.82403\n",
      "[318]\ttrain's SFC_loss: 0.429881\tvalid's SFC_loss: 0.823781\n",
      "[319]\ttrain's SFC_loss: 0.429392\tvalid's SFC_loss: 0.823601\n",
      "[320]\ttrain's SFC_loss: 0.428592\tvalid's SFC_loss: 0.82336\n",
      "[321]\ttrain's SFC_loss: 0.4277\tvalid's SFC_loss: 0.822515\n",
      "[322]\ttrain's SFC_loss: 0.426968\tvalid's SFC_loss: 0.823204\n",
      "[323]\ttrain's SFC_loss: 0.426127\tvalid's SFC_loss: 0.824442\n",
      "[324]\ttrain's SFC_loss: 0.425297\tvalid's SFC_loss: 0.824054\n",
      "[325]\ttrain's SFC_loss: 0.424493\tvalid's SFC_loss: 0.822732\n",
      "[326]\ttrain's SFC_loss: 0.423661\tvalid's SFC_loss: 0.82318\n",
      "[327]\ttrain's SFC_loss: 0.422879\tvalid's SFC_loss: 0.824064\n",
      "[328]\ttrain's SFC_loss: 0.422072\tvalid's SFC_loss: 0.825715\n",
      "[329]\ttrain's SFC_loss: 0.420742\tvalid's SFC_loss: 0.826404\n",
      "[330]\ttrain's SFC_loss: 0.419705\tvalid's SFC_loss: 0.827714\n",
      "[331]\ttrain's SFC_loss: 0.41899\tvalid's SFC_loss: 0.827771\n",
      "[332]\ttrain's SFC_loss: 0.417985\tvalid's SFC_loss: 0.827913\n",
      "[333]\ttrain's SFC_loss: 0.417137\tvalid's SFC_loss: 0.828193\n",
      "[334]\ttrain's SFC_loss: 0.416105\tvalid's SFC_loss: 0.826832\n",
      "[335]\ttrain's SFC_loss: 0.415111\tvalid's SFC_loss: 0.825532\n",
      "[336]\ttrain's SFC_loss: 0.414464\tvalid's SFC_loss: 0.824919\n",
      "[337]\ttrain's SFC_loss: 0.413658\tvalid's SFC_loss: 0.82526\n",
      "[338]\ttrain's SFC_loss: 0.412845\tvalid's SFC_loss: 0.824698\n",
      "[339]\ttrain's SFC_loss: 0.412266\tvalid's SFC_loss: 0.824166\n",
      "[340]\ttrain's SFC_loss: 0.411742\tvalid's SFC_loss: 0.824379\n",
      "[341]\ttrain's SFC_loss: 0.411119\tvalid's SFC_loss: 0.825174\n",
      "[342]\ttrain's SFC_loss: 0.410489\tvalid's SFC_loss: 0.824506\n",
      "[343]\ttrain's SFC_loss: 0.409889\tvalid's SFC_loss: 0.824868\n",
      "[344]\ttrain's SFC_loss: 0.409145\tvalid's SFC_loss: 0.824567\n",
      "[345]\ttrain's SFC_loss: 0.408466\tvalid's SFC_loss: 0.826105\n",
      "[346]\ttrain's SFC_loss: 0.407892\tvalid's SFC_loss: 0.825048\n",
      "[347]\ttrain's SFC_loss: 0.40727\tvalid's SFC_loss: 0.82382\n",
      "[348]\ttrain's SFC_loss: 0.406555\tvalid's SFC_loss: 0.823059\n",
      "[349]\ttrain's SFC_loss: 0.406054\tvalid's SFC_loss: 0.822181\n",
      "[350]\ttrain's SFC_loss: 0.405374\tvalid's SFC_loss: 0.821083\n",
      "[351]\ttrain's SFC_loss: 0.404751\tvalid's SFC_loss: 0.822187\n",
      "[352]\ttrain's SFC_loss: 0.403918\tvalid's SFC_loss: 0.822065\n",
      "[353]\ttrain's SFC_loss: 0.403067\tvalid's SFC_loss: 0.821541\n",
      "[354]\ttrain's SFC_loss: 0.40226\tvalid's SFC_loss: 0.820126\n",
      "[355]\ttrain's SFC_loss: 0.401523\tvalid's SFC_loss: 0.820273\n",
      "[356]\ttrain's SFC_loss: 0.400842\tvalid's SFC_loss: 0.820425\n",
      "[357]\ttrain's SFC_loss: 0.400203\tvalid's SFC_loss: 0.818644\n",
      "[358]\ttrain's SFC_loss: 0.399641\tvalid's SFC_loss: 0.818734\n",
      "[359]\ttrain's SFC_loss: 0.398845\tvalid's SFC_loss: 0.818879\n",
      "[360]\ttrain's SFC_loss: 0.39812\tvalid's SFC_loss: 0.816809\n",
      "[361]\ttrain's SFC_loss: 0.397076\tvalid's SFC_loss: 0.815274\n",
      "[362]\ttrain's SFC_loss: 0.395977\tvalid's SFC_loss: 0.814649\n",
      "[363]\ttrain's SFC_loss: 0.394625\tvalid's SFC_loss: 0.812999\n",
      "[364]\ttrain's SFC_loss: 0.393466\tvalid's SFC_loss: 0.811512\n",
      "[365]\ttrain's SFC_loss: 0.39239\tvalid's SFC_loss: 0.811772\n",
      "[366]\ttrain's SFC_loss: 0.391476\tvalid's SFC_loss: 0.811535\n",
      "[367]\ttrain's SFC_loss: 0.390878\tvalid's SFC_loss: 0.811923\n",
      "[368]\ttrain's SFC_loss: 0.390055\tvalid's SFC_loss: 0.813195\n",
      "[369]\ttrain's SFC_loss: 0.389549\tvalid's SFC_loss: 0.814135\n",
      "[370]\ttrain's SFC_loss: 0.388794\tvalid's SFC_loss: 0.814312\n",
      "[371]\ttrain's SFC_loss: 0.387905\tvalid's SFC_loss: 0.813642\n",
      "[372]\ttrain's SFC_loss: 0.38708\tvalid's SFC_loss: 0.812846\n",
      "[373]\ttrain's SFC_loss: 0.386164\tvalid's SFC_loss: 0.814467\n",
      "[374]\ttrain's SFC_loss: 0.385493\tvalid's SFC_loss: 0.816064\n",
      "[375]\ttrain's SFC_loss: 0.384501\tvalid's SFC_loss: 0.814998\n",
      "[376]\ttrain's SFC_loss: 0.383931\tvalid's SFC_loss: 0.812945\n",
      "[377]\ttrain's SFC_loss: 0.383327\tvalid's SFC_loss: 0.811985\n",
      "[378]\ttrain's SFC_loss: 0.382664\tvalid's SFC_loss: 0.811271\n",
      "[379]\ttrain's SFC_loss: 0.382163\tvalid's SFC_loss: 0.810455\n",
      "[380]\ttrain's SFC_loss: 0.381407\tvalid's SFC_loss: 0.808163\n",
      "[381]\ttrain's SFC_loss: 0.380809\tvalid's SFC_loss: 0.807169\n",
      "[382]\ttrain's SFC_loss: 0.380234\tvalid's SFC_loss: 0.806202\n",
      "[383]\ttrain's SFC_loss: 0.37978\tvalid's SFC_loss: 0.804087\n",
      "[384]\ttrain's SFC_loss: 0.379288\tvalid's SFC_loss: 0.8036\n",
      "[385]\ttrain's SFC_loss: 0.378711\tvalid's SFC_loss: 0.803926\n",
      "[386]\ttrain's SFC_loss: 0.378062\tvalid's SFC_loss: 0.804652\n",
      "[387]\ttrain's SFC_loss: 0.377306\tvalid's SFC_loss: 0.804791\n",
      "[388]\ttrain's SFC_loss: 0.376361\tvalid's SFC_loss: 0.805019\n",
      "[389]\ttrain's SFC_loss: 0.375509\tvalid's SFC_loss: 0.804225\n",
      "[390]\ttrain's SFC_loss: 0.374717\tvalid's SFC_loss: 0.805016\n",
      "[391]\ttrain's SFC_loss: 0.373969\tvalid's SFC_loss: 0.804341\n",
      "[392]\ttrain's SFC_loss: 0.372901\tvalid's SFC_loss: 0.803861\n",
      "[393]\ttrain's SFC_loss: 0.372277\tvalid's SFC_loss: 0.804947\n",
      "[394]\ttrain's SFC_loss: 0.371556\tvalid's SFC_loss: 0.805869\n",
      "[395]\ttrain's SFC_loss: 0.370821\tvalid's SFC_loss: 0.805333\n",
      "[396]\ttrain's SFC_loss: 0.370246\tvalid's SFC_loss: 0.80507\n",
      "[397]\ttrain's SFC_loss: 0.370044\tvalid's SFC_loss: 0.805511\n",
      "[398]\ttrain's SFC_loss: 0.369673\tvalid's SFC_loss: 0.805105\n",
      "[399]\ttrain's SFC_loss: 0.369496\tvalid's SFC_loss: 0.805581\n",
      "[400]\ttrain's SFC_loss: 0.369382\tvalid's SFC_loss: 0.80566\n",
      "[401]\ttrain's SFC_loss: 0.36864\tvalid's SFC_loss: 0.803391\n",
      "[402]\ttrain's SFC_loss: 0.368146\tvalid's SFC_loss: 0.801846\n",
      "[403]\ttrain's SFC_loss: 0.36741\tvalid's SFC_loss: 0.799845\n",
      "[404]\ttrain's SFC_loss: 0.366873\tvalid's SFC_loss: 0.798972\n",
      "[405]\ttrain's SFC_loss: 0.366294\tvalid's SFC_loss: 0.799069\n",
      "[406]\ttrain's SFC_loss: 0.365699\tvalid's SFC_loss: 0.79706\n",
      "[407]\ttrain's SFC_loss: 0.365267\tvalid's SFC_loss: 0.795162\n",
      "[408]\ttrain's SFC_loss: 0.364737\tvalid's SFC_loss: 0.79454\n",
      "[409]\ttrain's SFC_loss: 0.364248\tvalid's SFC_loss: 0.793507\n",
      "[410]\ttrain's SFC_loss: 0.363446\tvalid's SFC_loss: 0.792426\n",
      "[411]\ttrain's SFC_loss: 0.36279\tvalid's SFC_loss: 0.792262\n",
      "[412]\ttrain's SFC_loss: 0.36225\tvalid's SFC_loss: 0.789968\n",
      "[413]\ttrain's SFC_loss: 0.361633\tvalid's SFC_loss: 0.790588\n",
      "[414]\ttrain's SFC_loss: 0.36084\tvalid's SFC_loss: 0.790287\n",
      "[415]\ttrain's SFC_loss: 0.360412\tvalid's SFC_loss: 0.789451\n",
      "[416]\ttrain's SFC_loss: 0.359855\tvalid's SFC_loss: 0.788649\n",
      "[417]\ttrain's SFC_loss: 0.359321\tvalid's SFC_loss: 0.787878\n",
      "[418]\ttrain's SFC_loss: 0.358766\tvalid's SFC_loss: 0.786611\n",
      "[419]\ttrain's SFC_loss: 0.35824\tvalid's SFC_loss: 0.78528\n",
      "[420]\ttrain's SFC_loss: 0.357741\tvalid's SFC_loss: 0.785558\n",
      "[421]\ttrain's SFC_loss: 0.357058\tvalid's SFC_loss: 0.785293\n",
      "[422]\ttrain's SFC_loss: 0.356389\tvalid's SFC_loss: 0.785086\n",
      "[423]\ttrain's SFC_loss: 0.35576\tvalid's SFC_loss: 0.784528\n",
      "[424]\ttrain's SFC_loss: 0.355145\tvalid's SFC_loss: 0.783406\n",
      "[425]\ttrain's SFC_loss: 0.354635\tvalid's SFC_loss: 0.782872\n",
      "[426]\ttrain's SFC_loss: 0.353925\tvalid's SFC_loss: 0.782936\n",
      "[427]\ttrain's SFC_loss: 0.353027\tvalid's SFC_loss: 0.783628\n",
      "[428]\ttrain's SFC_loss: 0.352263\tvalid's SFC_loss: 0.784582\n",
      "[429]\ttrain's SFC_loss: 0.351402\tvalid's SFC_loss: 0.785301\n",
      "[430]\ttrain's SFC_loss: 0.350724\tvalid's SFC_loss: 0.785416\n",
      "[431]\ttrain's SFC_loss: 0.3498\tvalid's SFC_loss: 0.786667\n",
      "[432]\ttrain's SFC_loss: 0.349037\tvalid's SFC_loss: 0.785992\n",
      "[433]\ttrain's SFC_loss: 0.348212\tvalid's SFC_loss: 0.786407\n",
      "[434]\ttrain's SFC_loss: 0.347467\tvalid's SFC_loss: 0.784875\n",
      "[435]\ttrain's SFC_loss: 0.346798\tvalid's SFC_loss: 0.781942\n",
      "[436]\ttrain's SFC_loss: 0.346339\tvalid's SFC_loss: 0.781104\n",
      "[437]\ttrain's SFC_loss: 0.345991\tvalid's SFC_loss: 0.7806\n",
      "[438]\ttrain's SFC_loss: 0.345523\tvalid's SFC_loss: 0.779384\n",
      "[439]\ttrain's SFC_loss: 0.344776\tvalid's SFC_loss: 0.778057\n",
      "[440]\ttrain's SFC_loss: 0.344221\tvalid's SFC_loss: 0.778685\n",
      "[441]\ttrain's SFC_loss: 0.343745\tvalid's SFC_loss: 0.780197\n",
      "[442]\ttrain's SFC_loss: 0.343083\tvalid's SFC_loss: 0.780109\n",
      "[443]\ttrain's SFC_loss: 0.342628\tvalid's SFC_loss: 0.781018\n",
      "[444]\ttrain's SFC_loss: 0.342137\tvalid's SFC_loss: 0.78077\n",
      "[445]\ttrain's SFC_loss: 0.34164\tvalid's SFC_loss: 0.781476\n",
      "[446]\ttrain's SFC_loss: 0.34121\tvalid's SFC_loss: 0.781118\n",
      "[447]\ttrain's SFC_loss: 0.34041\tvalid's SFC_loss: 0.781212\n",
      "[448]\ttrain's SFC_loss: 0.339726\tvalid's SFC_loss: 0.781613\n",
      "[449]\ttrain's SFC_loss: 0.33918\tvalid's SFC_loss: 0.782907\n",
      "[450]\ttrain's SFC_loss: 0.338648\tvalid's SFC_loss: 0.783499\n",
      "[451]\ttrain's SFC_loss: 0.338033\tvalid's SFC_loss: 0.78218\n",
      "[452]\ttrain's SFC_loss: 0.337265\tvalid's SFC_loss: 0.78173\n",
      "[453]\ttrain's SFC_loss: 0.336713\tvalid's SFC_loss: 0.782042\n",
      "[454]\ttrain's SFC_loss: 0.336091\tvalid's SFC_loss: 0.780708\n",
      "[455]\ttrain's SFC_loss: 0.335401\tvalid's SFC_loss: 0.781937\n",
      "[456]\ttrain's SFC_loss: 0.334651\tvalid's SFC_loss: 0.782292\n",
      "[457]\ttrain's SFC_loss: 0.333928\tvalid's SFC_loss: 0.782476\n",
      "[458]\ttrain's SFC_loss: 0.333222\tvalid's SFC_loss: 0.782969\n",
      "[459]\ttrain's SFC_loss: 0.33252\tvalid's SFC_loss: 0.782966\n",
      "[460]\ttrain's SFC_loss: 0.33178\tvalid's SFC_loss: 0.783636\n",
      "[461]\ttrain's SFC_loss: 0.331164\tvalid's SFC_loss: 0.781368\n",
      "[462]\ttrain's SFC_loss: 0.330653\tvalid's SFC_loss: 0.780371\n",
      "[463]\ttrain's SFC_loss: 0.32982\tvalid's SFC_loss: 0.779718\n",
      "[464]\ttrain's SFC_loss: 0.329134\tvalid's SFC_loss: 0.779649\n",
      "[465]\ttrain's SFC_loss: 0.328562\tvalid's SFC_loss: 0.778312\n",
      "[466]\ttrain's SFC_loss: 0.328304\tvalid's SFC_loss: 0.777854\n",
      "[467]\ttrain's SFC_loss: 0.327732\tvalid's SFC_loss: 0.775375\n",
      "[468]\ttrain's SFC_loss: 0.32745\tvalid's SFC_loss: 0.773469\n",
      "[469]\ttrain's SFC_loss: 0.326867\tvalid's SFC_loss: 0.771529\n",
      "[470]\ttrain's SFC_loss: 0.326574\tvalid's SFC_loss: 0.769093\n",
      "[471]\ttrain's SFC_loss: 0.326095\tvalid's SFC_loss: 0.768973\n",
      "[472]\ttrain's SFC_loss: 0.325625\tvalid's SFC_loss: 0.767739\n",
      "[473]\ttrain's SFC_loss: 0.324974\tvalid's SFC_loss: 0.767471\n",
      "[474]\ttrain's SFC_loss: 0.324513\tvalid's SFC_loss: 0.76648\n",
      "[475]\ttrain's SFC_loss: 0.324089\tvalid's SFC_loss: 0.765833\n",
      "[476]\ttrain's SFC_loss: 0.32342\tvalid's SFC_loss: 0.766355\n",
      "[477]\ttrain's SFC_loss: 0.322945\tvalid's SFC_loss: 0.766745\n",
      "[478]\ttrain's SFC_loss: 0.322634\tvalid's SFC_loss: 0.766383\n",
      "[479]\ttrain's SFC_loss: 0.322034\tvalid's SFC_loss: 0.766983\n",
      "[480]\ttrain's SFC_loss: 0.321828\tvalid's SFC_loss: 0.766446\n",
      "[481]\ttrain's SFC_loss: 0.321059\tvalid's SFC_loss: 0.76852\n",
      "[482]\ttrain's SFC_loss: 0.320261\tvalid's SFC_loss: 0.769526\n",
      "[483]\ttrain's SFC_loss: 0.319276\tvalid's SFC_loss: 0.769755\n",
      "[484]\ttrain's SFC_loss: 0.318515\tvalid's SFC_loss: 0.770363\n",
      "[485]\ttrain's SFC_loss: 0.317977\tvalid's SFC_loss: 0.770612\n",
      "[486]\ttrain's SFC_loss: 0.317477\tvalid's SFC_loss: 0.770268\n",
      "[487]\ttrain's SFC_loss: 0.317131\tvalid's SFC_loss: 0.770156\n",
      "[488]\ttrain's SFC_loss: 0.316691\tvalid's SFC_loss: 0.770308\n",
      "[489]\ttrain's SFC_loss: 0.316192\tvalid's SFC_loss: 0.770161\n",
      "[490]\ttrain's SFC_loss: 0.315691\tvalid's SFC_loss: 0.770105\n",
      "[491]\ttrain's SFC_loss: 0.315003\tvalid's SFC_loss: 0.768803\n",
      "[492]\ttrain's SFC_loss: 0.314494\tvalid's SFC_loss: 0.76722\n",
      "[493]\ttrain's SFC_loss: 0.314073\tvalid's SFC_loss: 0.765085\n",
      "[494]\ttrain's SFC_loss: 0.31374\tvalid's SFC_loss: 0.763901\n",
      "[495]\ttrain's SFC_loss: 0.313113\tvalid's SFC_loss: 0.761503\n",
      "[496]\ttrain's SFC_loss: 0.312636\tvalid's SFC_loss: 0.762182\n",
      "[497]\ttrain's SFC_loss: 0.312116\tvalid's SFC_loss: 0.762977\n",
      "[498]\ttrain's SFC_loss: 0.311505\tvalid's SFC_loss: 0.763365\n",
      "[499]\ttrain's SFC_loss: 0.311142\tvalid's SFC_loss: 0.764507\n",
      "[500]\ttrain's SFC_loss: 0.310208\tvalid's SFC_loss: 0.763101\n",
      "[501]\ttrain's SFC_loss: 0.309929\tvalid's SFC_loss: 0.763546\n",
      "[502]\ttrain's SFC_loss: 0.309265\tvalid's SFC_loss: 0.763153\n",
      "[503]\ttrain's SFC_loss: 0.308745\tvalid's SFC_loss: 0.762372\n",
      "[504]\ttrain's SFC_loss: 0.308395\tvalid's SFC_loss: 0.762706\n",
      "[505]\ttrain's SFC_loss: 0.307841\tvalid's SFC_loss: 0.762808\n",
      "[506]\ttrain's SFC_loss: 0.307278\tvalid's SFC_loss: 0.762195\n",
      "[507]\ttrain's SFC_loss: 0.306582\tvalid's SFC_loss: 0.762549\n",
      "[508]\ttrain's SFC_loss: 0.305817\tvalid's SFC_loss: 0.76251\n",
      "[509]\ttrain's SFC_loss: 0.305116\tvalid's SFC_loss: 0.761842\n",
      "[510]\ttrain's SFC_loss: 0.304395\tvalid's SFC_loss: 0.761068\n",
      "[511]\ttrain's SFC_loss: 0.303927\tvalid's SFC_loss: 0.758966\n",
      "[512]\ttrain's SFC_loss: 0.303546\tvalid's SFC_loss: 0.757968\n",
      "[513]\ttrain's SFC_loss: 0.303146\tvalid's SFC_loss: 0.756937\n",
      "[514]\ttrain's SFC_loss: 0.302865\tvalid's SFC_loss: 0.756502\n",
      "[515]\ttrain's SFC_loss: 0.302595\tvalid's SFC_loss: 0.756081\n",
      "[516]\ttrain's SFC_loss: 0.301879\tvalid's SFC_loss: 0.756346\n",
      "[517]\ttrain's SFC_loss: 0.301384\tvalid's SFC_loss: 0.758963\n",
      "[518]\ttrain's SFC_loss: 0.300778\tvalid's SFC_loss: 0.760497\n",
      "[519]\ttrain's SFC_loss: 0.300027\tvalid's SFC_loss: 0.760313\n",
      "[520]\ttrain's SFC_loss: 0.299462\tvalid's SFC_loss: 0.761285\n",
      "[521]\ttrain's SFC_loss: 0.298858\tvalid's SFC_loss: 0.760761\n",
      "[522]\ttrain's SFC_loss: 0.298382\tvalid's SFC_loss: 0.760033\n",
      "[523]\ttrain's SFC_loss: 0.298047\tvalid's SFC_loss: 0.758898\n",
      "[524]\ttrain's SFC_loss: 0.297739\tvalid's SFC_loss: 0.757451\n",
      "[525]\ttrain's SFC_loss: 0.297285\tvalid's SFC_loss: 0.756784\n",
      "[526]\ttrain's SFC_loss: 0.296621\tvalid's SFC_loss: 0.755663\n",
      "[527]\ttrain's SFC_loss: 0.296183\tvalid's SFC_loss: 0.756271\n",
      "[528]\ttrain's SFC_loss: 0.295708\tvalid's SFC_loss: 0.756911\n",
      "[529]\ttrain's SFC_loss: 0.295395\tvalid's SFC_loss: 0.755813\n",
      "[530]\ttrain's SFC_loss: 0.294896\tvalid's SFC_loss: 0.756019\n",
      "[531]\ttrain's SFC_loss: 0.294363\tvalid's SFC_loss: 0.755759\n",
      "[532]\ttrain's SFC_loss: 0.29401\tvalid's SFC_loss: 0.756347\n",
      "[533]\ttrain's SFC_loss: 0.293404\tvalid's SFC_loss: 0.757849\n",
      "[534]\ttrain's SFC_loss: 0.292956\tvalid's SFC_loss: 0.75758\n",
      "[535]\ttrain's SFC_loss: 0.292376\tvalid's SFC_loss: 0.758323\n",
      "[536]\ttrain's SFC_loss: 0.291975\tvalid's SFC_loss: 0.759405\n",
      "[537]\ttrain's SFC_loss: 0.291433\tvalid's SFC_loss: 0.758707\n",
      "[538]\ttrain's SFC_loss: 0.290798\tvalid's SFC_loss: 0.757488\n",
      "[539]\ttrain's SFC_loss: 0.290312\tvalid's SFC_loss: 0.758893\n",
      "[540]\ttrain's SFC_loss: 0.289843\tvalid's SFC_loss: 0.760313\n",
      "[541]\ttrain's SFC_loss: 0.289316\tvalid's SFC_loss: 0.758995\n",
      "[542]\ttrain's SFC_loss: 0.288904\tvalid's SFC_loss: 0.758654\n",
      "[543]\ttrain's SFC_loss: 0.288621\tvalid's SFC_loss: 0.758246\n",
      "[544]\ttrain's SFC_loss: 0.288312\tvalid's SFC_loss: 0.756907\n",
      "[545]\ttrain's SFC_loss: 0.287954\tvalid's SFC_loss: 0.757129\n",
      "[546]\ttrain's SFC_loss: 0.287248\tvalid's SFC_loss: 0.757713\n",
      "[547]\ttrain's SFC_loss: 0.28673\tvalid's SFC_loss: 0.759402\n",
      "[548]\ttrain's SFC_loss: 0.286326\tvalid's SFC_loss: 0.759952\n",
      "[549]\ttrain's SFC_loss: 0.285677\tvalid's SFC_loss: 0.759983\n",
      "[550]\ttrain's SFC_loss: 0.285234\tvalid's SFC_loss: 0.760413\n",
      "[551]\ttrain's SFC_loss: 0.284953\tvalid's SFC_loss: 0.761041\n",
      "[552]\ttrain's SFC_loss: 0.284502\tvalid's SFC_loss: 0.760808\n",
      "[553]\ttrain's SFC_loss: 0.284335\tvalid's SFC_loss: 0.758846\n",
      "[554]\ttrain's SFC_loss: 0.284019\tvalid's SFC_loss: 0.757996\n",
      "[555]\ttrain's SFC_loss: 0.283527\tvalid's SFC_loss: 0.758167\n",
      "[556]\ttrain's SFC_loss: 0.282988\tvalid's SFC_loss: 0.756675\n",
      "[557]\ttrain's SFC_loss: 0.282624\tvalid's SFC_loss: 0.755721\n",
      "[558]\ttrain's SFC_loss: 0.282118\tvalid's SFC_loss: 0.755223\n",
      "[559]\ttrain's SFC_loss: 0.281716\tvalid's SFC_loss: 0.754553\n",
      "[560]\ttrain's SFC_loss: 0.281317\tvalid's SFC_loss: 0.753975\n",
      "[561]\ttrain's SFC_loss: 0.280874\tvalid's SFC_loss: 0.753049\n",
      "[562]\ttrain's SFC_loss: 0.280356\tvalid's SFC_loss: 0.751683\n",
      "[563]\ttrain's SFC_loss: 0.279889\tvalid's SFC_loss: 0.751065\n",
      "[564]\ttrain's SFC_loss: 0.279369\tvalid's SFC_loss: 0.749324\n",
      "[565]\ttrain's SFC_loss: 0.278903\tvalid's SFC_loss: 0.749346\n",
      "[566]\ttrain's SFC_loss: 0.278636\tvalid's SFC_loss: 0.748906\n",
      "[567]\ttrain's SFC_loss: 0.278345\tvalid's SFC_loss: 0.749089\n",
      "[568]\ttrain's SFC_loss: 0.277897\tvalid's SFC_loss: 0.748219\n",
      "[569]\ttrain's SFC_loss: 0.277469\tvalid's SFC_loss: 0.747573\n",
      "[570]\ttrain's SFC_loss: 0.277229\tvalid's SFC_loss: 0.748418\n",
      "[571]\ttrain's SFC_loss: 0.276782\tvalid's SFC_loss: 0.748522\n",
      "[572]\ttrain's SFC_loss: 0.276258\tvalid's SFC_loss: 0.750387\n",
      "[573]\ttrain's SFC_loss: 0.27574\tvalid's SFC_loss: 0.753052\n",
      "[574]\ttrain's SFC_loss: 0.275319\tvalid's SFC_loss: 0.753151\n",
      "[575]\ttrain's SFC_loss: 0.27473\tvalid's SFC_loss: 0.753752\n",
      "[576]\ttrain's SFC_loss: 0.274267\tvalid's SFC_loss: 0.754107\n",
      "[577]\ttrain's SFC_loss: 0.273823\tvalid's SFC_loss: 0.754699\n",
      "[578]\ttrain's SFC_loss: 0.27335\tvalid's SFC_loss: 0.753811\n",
      "[579]\ttrain's SFC_loss: 0.272946\tvalid's SFC_loss: 0.75335\n",
      "[580]\ttrain's SFC_loss: 0.27247\tvalid's SFC_loss: 0.752657\n",
      "[581]\ttrain's SFC_loss: 0.272083\tvalid's SFC_loss: 0.752568\n",
      "[582]\ttrain's SFC_loss: 0.271779\tvalid's SFC_loss: 0.751929\n",
      "[583]\ttrain's SFC_loss: 0.271446\tvalid's SFC_loss: 0.752088\n",
      "[584]\ttrain's SFC_loss: 0.271155\tvalid's SFC_loss: 0.751306\n",
      "[585]\ttrain's SFC_loss: 0.270876\tvalid's SFC_loss: 0.750959\n",
      "[586]\ttrain's SFC_loss: 0.270245\tvalid's SFC_loss: 0.751001\n",
      "[587]\ttrain's SFC_loss: 0.269738\tvalid's SFC_loss: 0.750354\n",
      "[588]\ttrain's SFC_loss: 0.269357\tvalid's SFC_loss: 0.752139\n",
      "[589]\ttrain's SFC_loss: 0.269007\tvalid's SFC_loss: 0.754219\n",
      "[590]\ttrain's SFC_loss: 0.268656\tvalid's SFC_loss: 0.756001\n",
      "[591]\ttrain's SFC_loss: 0.268185\tvalid's SFC_loss: 0.755662\n",
      "[592]\ttrain's SFC_loss: 0.267645\tvalid's SFC_loss: 0.755793\n",
      "[593]\ttrain's SFC_loss: 0.26726\tvalid's SFC_loss: 0.756725\n",
      "[594]\ttrain's SFC_loss: 0.266749\tvalid's SFC_loss: 0.757511\n",
      "[595]\ttrain's SFC_loss: 0.266412\tvalid's SFC_loss: 0.757701\n",
      "[596]\ttrain's SFC_loss: 0.265727\tvalid's SFC_loss: 0.756719\n",
      "[597]\ttrain's SFC_loss: 0.265339\tvalid's SFC_loss: 0.756192\n",
      "[598]\ttrain's SFC_loss: 0.264922\tvalid's SFC_loss: 0.755234\n",
      "[599]\ttrain's SFC_loss: 0.264473\tvalid's SFC_loss: 0.755488\n",
      "[600]\ttrain's SFC_loss: 0.264071\tvalid's SFC_loss: 0.755003\n",
      "[601]\ttrain's SFC_loss: 0.263571\tvalid's SFC_loss: 0.755194\n",
      "[602]\ttrain's SFC_loss: 0.263135\tvalid's SFC_loss: 0.754297\n",
      "[603]\ttrain's SFC_loss: 0.262628\tvalid's SFC_loss: 0.755126\n",
      "[604]\ttrain's SFC_loss: 0.262116\tvalid's SFC_loss: 0.756602\n",
      "[605]\ttrain's SFC_loss: 0.261647\tvalid's SFC_loss: 0.757449\n",
      "[606]\ttrain's SFC_loss: 0.261202\tvalid's SFC_loss: 0.757092\n",
      "[607]\ttrain's SFC_loss: 0.260864\tvalid's SFC_loss: 0.756622\n",
      "[608]\ttrain's SFC_loss: 0.260478\tvalid's SFC_loss: 0.757023\n",
      "[609]\ttrain's SFC_loss: 0.260069\tvalid's SFC_loss: 0.756887\n",
      "[610]\ttrain's SFC_loss: 0.259708\tvalid's SFC_loss: 0.756188\n",
      "[611]\ttrain's SFC_loss: 0.259441\tvalid's SFC_loss: 0.756053\n",
      "[612]\ttrain's SFC_loss: 0.259186\tvalid's SFC_loss: 0.755937\n",
      "[613]\ttrain's SFC_loss: 0.258958\tvalid's SFC_loss: 0.757506\n",
      "[614]\ttrain's SFC_loss: 0.25876\tvalid's SFC_loss: 0.757856\n",
      "[615]\ttrain's SFC_loss: 0.258573\tvalid's SFC_loss: 0.758226\n",
      "[616]\ttrain's SFC_loss: 0.258161\tvalid's SFC_loss: 0.758728\n",
      "[617]\ttrain's SFC_loss: 0.257825\tvalid's SFC_loss: 0.758893\n",
      "[618]\ttrain's SFC_loss: 0.257374\tvalid's SFC_loss: 0.7576\n",
      "[619]\ttrain's SFC_loss: 0.257081\tvalid's SFC_loss: 0.758043\n",
      "[620]\ttrain's SFC_loss: 0.256751\tvalid's SFC_loss: 0.7574\n",
      "[621]\ttrain's SFC_loss: 0.256185\tvalid's SFC_loss: 0.756333\n",
      "[622]\ttrain's SFC_loss: 0.255921\tvalid's SFC_loss: 0.755564\n",
      "[623]\ttrain's SFC_loss: 0.255592\tvalid's SFC_loss: 0.757007\n",
      "[624]\ttrain's SFC_loss: 0.255113\tvalid's SFC_loss: 0.757016\n",
      "[625]\ttrain's SFC_loss: 0.254728\tvalid's SFC_loss: 0.756846\n",
      "[626]\ttrain's SFC_loss: 0.254282\tvalid's SFC_loss: 0.755906\n",
      "[627]\ttrain's SFC_loss: 0.253839\tvalid's SFC_loss: 0.756161\n",
      "[628]\ttrain's SFC_loss: 0.253417\tvalid's SFC_loss: 0.757849\n",
      "[629]\ttrain's SFC_loss: 0.252922\tvalid's SFC_loss: 0.75782\n",
      "[630]\ttrain's SFC_loss: 0.252591\tvalid's SFC_loss: 0.758458\n",
      "[631]\ttrain's SFC_loss: 0.252254\tvalid's SFC_loss: 0.759563\n",
      "[632]\ttrain's SFC_loss: 0.251925\tvalid's SFC_loss: 0.76046\n",
      "[633]\ttrain's SFC_loss: 0.251579\tvalid's SFC_loss: 0.761398\n",
      "[634]\ttrain's SFC_loss: 0.251273\tvalid's SFC_loss: 0.762591\n",
      "[635]\ttrain's SFC_loss: 0.250978\tvalid's SFC_loss: 0.76352\n",
      "[636]\ttrain's SFC_loss: 0.250483\tvalid's SFC_loss: 0.761589\n",
      "[637]\ttrain's SFC_loss: 0.250089\tvalid's SFC_loss: 0.760074\n",
      "[638]\ttrain's SFC_loss: 0.249767\tvalid's SFC_loss: 0.759896\n",
      "[639]\ttrain's SFC_loss: 0.249444\tvalid's SFC_loss: 0.759737\n",
      "[640]\ttrain's SFC_loss: 0.249135\tvalid's SFC_loss: 0.759812\n",
      "[641]\ttrain's SFC_loss: 0.248827\tvalid's SFC_loss: 0.760629\n",
      "[642]\ttrain's SFC_loss: 0.248434\tvalid's SFC_loss: 0.761231\n",
      "[643]\ttrain's SFC_loss: 0.24808\tvalid's SFC_loss: 0.762381\n",
      "[644]\ttrain's SFC_loss: 0.247695\tvalid's SFC_loss: 0.763309\n",
      "[645]\ttrain's SFC_loss: 0.24733\tvalid's SFC_loss: 0.765086\n",
      "[646]\ttrain's SFC_loss: 0.247058\tvalid's SFC_loss: 0.763361\n",
      "[647]\ttrain's SFC_loss: 0.246747\tvalid's SFC_loss: 0.762421\n",
      "[648]\ttrain's SFC_loss: 0.2464\tvalid's SFC_loss: 0.762039\n",
      "[649]\ttrain's SFC_loss: 0.246008\tvalid's SFC_loss: 0.763306\n",
      "[650]\ttrain's SFC_loss: 0.245649\tvalid's SFC_loss: 0.763816\n",
      "[651]\ttrain's SFC_loss: 0.245457\tvalid's SFC_loss: 0.766169\n",
      "[652]\ttrain's SFC_loss: 0.245056\tvalid's SFC_loss: 0.766231\n",
      "[653]\ttrain's SFC_loss: 0.244659\tvalid's SFC_loss: 0.766909\n",
      "[654]\ttrain's SFC_loss: 0.244523\tvalid's SFC_loss: 0.767636\n",
      "[655]\ttrain's SFC_loss: 0.244228\tvalid's SFC_loss: 0.767952\n",
      "[656]\ttrain's SFC_loss: 0.24397\tvalid's SFC_loss: 0.766675\n",
      "[657]\ttrain's SFC_loss: 0.243759\tvalid's SFC_loss: 0.765353\n",
      "[658]\ttrain's SFC_loss: 0.243461\tvalid's SFC_loss: 0.76358\n",
      "[659]\ttrain's SFC_loss: 0.243212\tvalid's SFC_loss: 0.763156\n",
      "[660]\ttrain's SFC_loss: 0.242878\tvalid's SFC_loss: 0.762183\n",
      "[661]\ttrain's SFC_loss: 0.242247\tvalid's SFC_loss: 0.762174\n",
      "[662]\ttrain's SFC_loss: 0.241838\tvalid's SFC_loss: 0.764611\n",
      "[663]\ttrain's SFC_loss: 0.241417\tvalid's SFC_loss: 0.767399\n",
      "[664]\ttrain's SFC_loss: 0.240906\tvalid's SFC_loss: 0.769785\n",
      "[665]\ttrain's SFC_loss: 0.240606\tvalid's SFC_loss: 0.771624\n",
      "[666]\ttrain's SFC_loss: 0.240208\tvalid's SFC_loss: 0.770568\n",
      "[667]\ttrain's SFC_loss: 0.239759\tvalid's SFC_loss: 0.77064\n",
      "[668]\ttrain's SFC_loss: 0.239343\tvalid's SFC_loss: 0.770165\n",
      "[669]\ttrain's SFC_loss: 0.238905\tvalid's SFC_loss: 0.770355\n",
      "Early stopping, best iteration is:\n",
      "[569]\ttrain's SFC_loss: 0.277469\tvalid's SFC_loss: 0.747573\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1    0.062383\n",
      "1    feature2    0.001604\n",
      "2    feature3   -0.008306\n",
      "3    feature4    0.003352\n",
      "4    feature5    0.131888\n",
      "5    feature6   -0.096404\n",
      "6    feature7    0.018617\n",
      "7    feature8    0.057767\n",
      "8    feature9   -0.019123\n",
      "9   feature10    0.019091\n",
      "10  feature11    0.024252\n",
      "11  feature12   -0.051615\n",
      "12  feature13    0.047826\n",
      "13  feature14   -0.028835\n",
      "14  feature15   -0.187129\n",
      "15  feature16   -0.005571\n",
      "16  feature17    0.014208\n",
      "17  feature18    0.049004\n",
      "18  feature19    0.070009\n",
      "19  feature20   -0.103018\n",
      "-------------------- 3 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05855\tvalid's multi_logloss: 1.06411\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05499\tvalid's multi_logloss: 1.06104\n",
      "[3]\ttrain's multi_logloss: 1.05184\tvalid's multi_logloss: 1.05892\n",
      "[4]\ttrain's multi_logloss: 1.04922\tvalid's multi_logloss: 1.05913\n",
      "[5]\ttrain's multi_logloss: 1.04572\tvalid's multi_logloss: 1.05927\n",
      "[6]\ttrain's multi_logloss: 1.04112\tvalid's multi_logloss: 1.05542\n",
      "[7]\ttrain's multi_logloss: 1.03765\tvalid's multi_logloss: 1.05385\n",
      "[8]\ttrain's multi_logloss: 1.03425\tvalid's multi_logloss: 1.05232\n",
      "[9]\ttrain's multi_logloss: 1.03183\tvalid's multi_logloss: 1.04994\n",
      "[10]\ttrain's multi_logloss: 1.02874\tvalid's multi_logloss: 1.04702\n",
      "[11]\ttrain's multi_logloss: 1.02555\tvalid's multi_logloss: 1.04235\n",
      "[12]\ttrain's multi_logloss: 1.02203\tvalid's multi_logloss: 1.03908\n",
      "[13]\ttrain's multi_logloss: 1.01835\tvalid's multi_logloss: 1.03909\n",
      "[14]\ttrain's multi_logloss: 1.01569\tvalid's multi_logloss: 1.0372\n",
      "[15]\ttrain's multi_logloss: 1.01214\tvalid's multi_logloss: 1.03332\n",
      "[16]\ttrain's multi_logloss: 1.00817\tvalid's multi_logloss: 1.0277\n",
      "[17]\ttrain's multi_logloss: 1.00489\tvalid's multi_logloss: 1.02389\n",
      "[18]\ttrain's multi_logloss: 1.00193\tvalid's multi_logloss: 1.02232\n",
      "[19]\ttrain's multi_logloss: 0.998332\tvalid's multi_logloss: 1.01814\n",
      "[20]\ttrain's multi_logloss: 0.995622\tvalid's multi_logloss: 1.01692\n",
      "[21]\ttrain's multi_logloss: 0.991509\tvalid's multi_logloss: 1.01349\n",
      "[22]\ttrain's multi_logloss: 0.987813\tvalid's multi_logloss: 1.01089\n",
      "[23]\ttrain's multi_logloss: 0.983702\tvalid's multi_logloss: 1.0084\n",
      "[24]\ttrain's multi_logloss: 0.980254\tvalid's multi_logloss: 1.0058\n",
      "[25]\ttrain's multi_logloss: 0.976106\tvalid's multi_logloss: 1.00268\n",
      "[26]\ttrain's multi_logloss: 0.972903\tvalid's multi_logloss: 0.999443\n",
      "[27]\ttrain's multi_logloss: 0.97008\tvalid's multi_logloss: 0.996043\n",
      "[28]\ttrain's multi_logloss: 0.966813\tvalid's multi_logloss: 0.9934\n",
      "[29]\ttrain's multi_logloss: 0.963729\tvalid's multi_logloss: 0.992677\n",
      "[30]\ttrain's multi_logloss: 0.960685\tvalid's multi_logloss: 0.991454\n",
      "[31]\ttrain's multi_logloss: 0.957793\tvalid's multi_logloss: 0.991014\n",
      "[32]\ttrain's multi_logloss: 0.954916\tvalid's multi_logloss: 0.987059\n",
      "[33]\ttrain's multi_logloss: 0.952215\tvalid's multi_logloss: 0.983191\n",
      "[34]\ttrain's multi_logloss: 0.949337\tvalid's multi_logloss: 0.981073\n",
      "[35]\ttrain's multi_logloss: 0.946328\tvalid's multi_logloss: 0.979771\n",
      "[36]\ttrain's multi_logloss: 0.94286\tvalid's multi_logloss: 0.979716\n",
      "[37]\ttrain's multi_logloss: 0.940294\tvalid's multi_logloss: 0.978848\n",
      "[38]\ttrain's multi_logloss: 0.93733\tvalid's multi_logloss: 0.977364\n",
      "[39]\ttrain's multi_logloss: 0.934323\tvalid's multi_logloss: 0.975262\n",
      "[40]\ttrain's multi_logloss: 0.931445\tvalid's multi_logloss: 0.974286\n",
      "[41]\ttrain's multi_logloss: 0.92837\tvalid's multi_logloss: 0.970792\n",
      "[42]\ttrain's multi_logloss: 0.92487\tvalid's multi_logloss: 0.969582\n",
      "[43]\ttrain's multi_logloss: 0.921761\tvalid's multi_logloss: 0.969416\n",
      "[44]\ttrain's multi_logloss: 0.918613\tvalid's multi_logloss: 0.967052\n",
      "[45]\ttrain's multi_logloss: 0.915626\tvalid's multi_logloss: 0.96469\n",
      "[46]\ttrain's multi_logloss: 0.912681\tvalid's multi_logloss: 0.964013\n",
      "[47]\ttrain's multi_logloss: 0.909789\tvalid's multi_logloss: 0.963289\n",
      "[48]\ttrain's multi_logloss: 0.906959\tvalid's multi_logloss: 0.962692\n",
      "[49]\ttrain's multi_logloss: 0.904796\tvalid's multi_logloss: 0.962884\n",
      "[50]\ttrain's multi_logloss: 0.902228\tvalid's multi_logloss: 0.962964\n",
      "[51]\ttrain's multi_logloss: 0.900029\tvalid's multi_logloss: 0.95982\n",
      "[52]\ttrain's multi_logloss: 0.898349\tvalid's multi_logloss: 0.957553\n",
      "[53]\ttrain's multi_logloss: 0.896264\tvalid's multi_logloss: 0.954597\n",
      "[54]\ttrain's multi_logloss: 0.894256\tvalid's multi_logloss: 0.952723\n",
      "[55]\ttrain's multi_logloss: 0.891906\tvalid's multi_logloss: 0.950928\n",
      "[56]\ttrain's multi_logloss: 0.890265\tvalid's multi_logloss: 0.951034\n",
      "[57]\ttrain's multi_logloss: 0.888311\tvalid's multi_logloss: 0.950512\n",
      "[58]\ttrain's multi_logloss: 0.886308\tvalid's multi_logloss: 0.949221\n",
      "[59]\ttrain's multi_logloss: 0.883651\tvalid's multi_logloss: 0.945964\n",
      "[60]\ttrain's multi_logloss: 0.88181\tvalid's multi_logloss: 0.944763\n",
      "[61]\ttrain's multi_logloss: 0.878629\tvalid's multi_logloss: 0.941226\n",
      "[62]\ttrain's multi_logloss: 0.876146\tvalid's multi_logloss: 0.938582\n",
      "[63]\ttrain's multi_logloss: 0.873549\tvalid's multi_logloss: 0.935728\n",
      "[64]\ttrain's multi_logloss: 0.871132\tvalid's multi_logloss: 0.934777\n",
      "[65]\ttrain's multi_logloss: 0.868553\tvalid's multi_logloss: 0.93288\n",
      "[66]\ttrain's multi_logloss: 0.865667\tvalid's multi_logloss: 0.929837\n",
      "[67]\ttrain's multi_logloss: 0.86282\tvalid's multi_logloss: 0.926317\n",
      "[68]\ttrain's multi_logloss: 0.860469\tvalid's multi_logloss: 0.923834\n",
      "[69]\ttrain's multi_logloss: 0.857701\tvalid's multi_logloss: 0.921403\n",
      "[70]\ttrain's multi_logloss: 0.854769\tvalid's multi_logloss: 0.91835\n",
      "[71]\ttrain's multi_logloss: 0.852276\tvalid's multi_logloss: 0.915813\n",
      "[72]\ttrain's multi_logloss: 0.849561\tvalid's multi_logloss: 0.91417\n",
      "[73]\ttrain's multi_logloss: 0.847117\tvalid's multi_logloss: 0.912605\n",
      "[74]\ttrain's multi_logloss: 0.844831\tvalid's multi_logloss: 0.910876\n",
      "[75]\ttrain's multi_logloss: 0.842344\tvalid's multi_logloss: 0.909924\n",
      "[76]\ttrain's multi_logloss: 0.840144\tvalid's multi_logloss: 0.907636\n",
      "[77]\ttrain's multi_logloss: 0.837652\tvalid's multi_logloss: 0.904876\n",
      "[78]\ttrain's multi_logloss: 0.83496\tvalid's multi_logloss: 0.903574\n",
      "[79]\ttrain's multi_logloss: 0.83281\tvalid's multi_logloss: 0.901307\n",
      "[80]\ttrain's multi_logloss: 0.830086\tvalid's multi_logloss: 0.900689\n",
      "[81]\ttrain's multi_logloss: 0.827131\tvalid's multi_logloss: 0.89933\n",
      "[82]\ttrain's multi_logloss: 0.825135\tvalid's multi_logloss: 0.89982\n",
      "[83]\ttrain's multi_logloss: 0.82309\tvalid's multi_logloss: 0.897465\n",
      "[84]\ttrain's multi_logloss: 0.821013\tvalid's multi_logloss: 0.894849\n",
      "[85]\ttrain's multi_logloss: 0.818406\tvalid's multi_logloss: 0.89109\n",
      "[86]\ttrain's multi_logloss: 0.816981\tvalid's multi_logloss: 0.890152\n",
      "[87]\ttrain's multi_logloss: 0.815489\tvalid's multi_logloss: 0.889544\n",
      "[88]\ttrain's multi_logloss: 0.813712\tvalid's multi_logloss: 0.887632\n",
      "[89]\ttrain's multi_logloss: 0.812394\tvalid's multi_logloss: 0.885795\n",
      "[90]\ttrain's multi_logloss: 0.811278\tvalid's multi_logloss: 0.88425\n",
      "[91]\ttrain's multi_logloss: 0.80875\tvalid's multi_logloss: 0.883836\n",
      "[92]\ttrain's multi_logloss: 0.806284\tvalid's multi_logloss: 0.882673\n",
      "[93]\ttrain's multi_logloss: 0.804041\tvalid's multi_logloss: 0.883149\n",
      "[94]\ttrain's multi_logloss: 0.802308\tvalid's multi_logloss: 0.88268\n",
      "[95]\ttrain's multi_logloss: 0.799731\tvalid's multi_logloss: 0.88213\n",
      "[96]\ttrain's multi_logloss: 0.797943\tvalid's multi_logloss: 0.880241\n",
      "[97]\ttrain's multi_logloss: 0.796016\tvalid's multi_logloss: 0.876979\n",
      "[98]\ttrain's multi_logloss: 0.794074\tvalid's multi_logloss: 0.876653\n",
      "[99]\ttrain's multi_logloss: 0.79171\tvalid's multi_logloss: 0.876839\n",
      "[100]\ttrain's multi_logloss: 0.789762\tvalid's multi_logloss: 0.878668\n",
      "[101]\ttrain's multi_logloss: 0.787083\tvalid's multi_logloss: 0.878418\n",
      "[102]\ttrain's multi_logloss: 0.784738\tvalid's multi_logloss: 0.878704\n",
      "[103]\ttrain's multi_logloss: 0.78272\tvalid's multi_logloss: 0.878479\n",
      "[104]\ttrain's multi_logloss: 0.780773\tvalid's multi_logloss: 0.877946\n",
      "[105]\ttrain's multi_logloss: 0.778073\tvalid's multi_logloss: 0.877959\n",
      "[106]\ttrain's multi_logloss: 0.776111\tvalid's multi_logloss: 0.877499\n",
      "[107]\ttrain's multi_logloss: 0.773974\tvalid's multi_logloss: 0.877376\n",
      "[108]\ttrain's multi_logloss: 0.772369\tvalid's multi_logloss: 0.876988\n",
      "[109]\ttrain's multi_logloss: 0.770766\tvalid's multi_logloss: 0.876803\n",
      "[110]\ttrain's multi_logloss: 0.76904\tvalid's multi_logloss: 0.876091\n",
      "[111]\ttrain's multi_logloss: 0.767041\tvalid's multi_logloss: 0.877679\n",
      "[112]\ttrain's multi_logloss: 0.765696\tvalid's multi_logloss: 0.877851\n",
      "[113]\ttrain's multi_logloss: 0.763637\tvalid's multi_logloss: 0.879596\n",
      "[114]\ttrain's multi_logloss: 0.761675\tvalid's multi_logloss: 0.881346\n",
      "[115]\ttrain's multi_logloss: 0.760156\tvalid's multi_logloss: 0.881428\n",
      "[116]\ttrain's multi_logloss: 0.758297\tvalid's multi_logloss: 0.881736\n",
      "[117]\ttrain's multi_logloss: 0.756705\tvalid's multi_logloss: 0.88065\n",
      "[118]\ttrain's multi_logloss: 0.755531\tvalid's multi_logloss: 0.878048\n",
      "[119]\ttrain's multi_logloss: 0.753914\tvalid's multi_logloss: 0.877091\n",
      "[120]\ttrain's multi_logloss: 0.752715\tvalid's multi_logloss: 0.875331\n",
      "[121]\ttrain's multi_logloss: 0.750533\tvalid's multi_logloss: 0.87406\n",
      "[122]\ttrain's multi_logloss: 0.748434\tvalid's multi_logloss: 0.87196\n",
      "[123]\ttrain's multi_logloss: 0.746527\tvalid's multi_logloss: 0.870241\n",
      "[124]\ttrain's multi_logloss: 0.744319\tvalid's multi_logloss: 0.868178\n",
      "[125]\ttrain's multi_logloss: 0.742197\tvalid's multi_logloss: 0.866328\n",
      "[126]\ttrain's multi_logloss: 0.741035\tvalid's multi_logloss: 0.864845\n",
      "[127]\ttrain's multi_logloss: 0.739477\tvalid's multi_logloss: 0.863441\n",
      "[128]\ttrain's multi_logloss: 0.738324\tvalid's multi_logloss: 0.863693\n",
      "[129]\ttrain's multi_logloss: 0.737102\tvalid's multi_logloss: 0.8639\n",
      "[130]\ttrain's multi_logloss: 0.735712\tvalid's multi_logloss: 0.863857\n",
      "[131]\ttrain's multi_logloss: 0.734207\tvalid's multi_logloss: 0.865454\n",
      "[132]\ttrain's multi_logloss: 0.731891\tvalid's multi_logloss: 0.863883\n",
      "[133]\ttrain's multi_logloss: 0.729584\tvalid's multi_logloss: 0.864596\n",
      "[134]\ttrain's multi_logloss: 0.727654\tvalid's multi_logloss: 0.864803\n",
      "[135]\ttrain's multi_logloss: 0.725778\tvalid's multi_logloss: 0.865625\n",
      "[136]\ttrain's multi_logloss: 0.723772\tvalid's multi_logloss: 0.866365\n",
      "[137]\ttrain's multi_logloss: 0.721814\tvalid's multi_logloss: 0.865564\n",
      "[138]\ttrain's multi_logloss: 0.720375\tvalid's multi_logloss: 0.864366\n",
      "[139]\ttrain's multi_logloss: 0.71887\tvalid's multi_logloss: 0.863636\n",
      "[140]\ttrain's multi_logloss: 0.717352\tvalid's multi_logloss: 0.863119\n",
      "[141]\ttrain's multi_logloss: 0.715541\tvalid's multi_logloss: 0.862858\n",
      "[142]\ttrain's multi_logloss: 0.714041\tvalid's multi_logloss: 0.864125\n",
      "[143]\ttrain's multi_logloss: 0.711724\tvalid's multi_logloss: 0.864479\n",
      "[144]\ttrain's multi_logloss: 0.710095\tvalid's multi_logloss: 0.863742\n",
      "[145]\ttrain's multi_logloss: 0.708519\tvalid's multi_logloss: 0.865113\n",
      "[146]\ttrain's multi_logloss: 0.706485\tvalid's multi_logloss: 0.864071\n",
      "[147]\ttrain's multi_logloss: 0.704502\tvalid's multi_logloss: 0.864385\n",
      "[148]\ttrain's multi_logloss: 0.702621\tvalid's multi_logloss: 0.861758\n",
      "[149]\ttrain's multi_logloss: 0.700446\tvalid's multi_logloss: 0.859509\n",
      "[150]\ttrain's multi_logloss: 0.698461\tvalid's multi_logloss: 0.858587\n",
      "[151]\ttrain's multi_logloss: 0.696812\tvalid's multi_logloss: 0.85649\n",
      "[152]\ttrain's multi_logloss: 0.695056\tvalid's multi_logloss: 0.85546\n",
      "[153]\ttrain's multi_logloss: 0.693629\tvalid's multi_logloss: 0.853309\n",
      "[154]\ttrain's multi_logloss: 0.692224\tvalid's multi_logloss: 0.851147\n",
      "[155]\ttrain's multi_logloss: 0.690483\tvalid's multi_logloss: 0.849195\n",
      "[156]\ttrain's multi_logloss: 0.688789\tvalid's multi_logloss: 0.848673\n",
      "[157]\ttrain's multi_logloss: 0.686879\tvalid's multi_logloss: 0.848182\n",
      "[158]\ttrain's multi_logloss: 0.684938\tvalid's multi_logloss: 0.848629\n",
      "[159]\ttrain's multi_logloss: 0.682983\tvalid's multi_logloss: 0.846429\n",
      "[160]\ttrain's multi_logloss: 0.681138\tvalid's multi_logloss: 0.846523\n",
      "[161]\ttrain's multi_logloss: 0.679733\tvalid's multi_logloss: 0.846557\n",
      "[162]\ttrain's multi_logloss: 0.678363\tvalid's multi_logloss: 0.846217\n",
      "[163]\ttrain's multi_logloss: 0.676893\tvalid's multi_logloss: 0.846688\n",
      "[164]\ttrain's multi_logloss: 0.675572\tvalid's multi_logloss: 0.846382\n",
      "[165]\ttrain's multi_logloss: 0.674236\tvalid's multi_logloss: 0.846158\n",
      "[166]\ttrain's multi_logloss: 0.672561\tvalid's multi_logloss: 0.844878\n",
      "[167]\ttrain's multi_logloss: 0.671025\tvalid's multi_logloss: 0.843674\n",
      "[168]\ttrain's multi_logloss: 0.669329\tvalid's multi_logloss: 0.842324\n",
      "[169]\ttrain's multi_logloss: 0.667636\tvalid's multi_logloss: 0.840841\n",
      "[170]\ttrain's multi_logloss: 0.666374\tvalid's multi_logloss: 0.838307\n",
      "[171]\ttrain's multi_logloss: 0.665118\tvalid's multi_logloss: 0.837193\n",
      "[172]\ttrain's multi_logloss: 0.663509\tvalid's multi_logloss: 0.836097\n",
      "[173]\ttrain's multi_logloss: 0.662059\tvalid's multi_logloss: 0.834665\n",
      "[174]\ttrain's multi_logloss: 0.660587\tvalid's multi_logloss: 0.832668\n",
      "[175]\ttrain's multi_logloss: 0.659193\tvalid's multi_logloss: 0.831279\n",
      "[176]\ttrain's multi_logloss: 0.657418\tvalid's multi_logloss: 0.829234\n",
      "[177]\ttrain's multi_logloss: 0.655584\tvalid's multi_logloss: 0.828252\n",
      "[178]\ttrain's multi_logloss: 0.653933\tvalid's multi_logloss: 0.828892\n",
      "[179]\ttrain's multi_logloss: 0.652401\tvalid's multi_logloss: 0.829088\n",
      "[180]\ttrain's multi_logloss: 0.650662\tvalid's multi_logloss: 0.828258\n",
      "[181]\ttrain's multi_logloss: 0.649073\tvalid's multi_logloss: 0.828503\n",
      "[182]\ttrain's multi_logloss: 0.647561\tvalid's multi_logloss: 0.828505\n",
      "[183]\ttrain's multi_logloss: 0.645991\tvalid's multi_logloss: 0.827938\n",
      "[184]\ttrain's multi_logloss: 0.644462\tvalid's multi_logloss: 0.82858\n",
      "[185]\ttrain's multi_logloss: 0.642853\tvalid's multi_logloss: 0.828775\n",
      "[186]\ttrain's multi_logloss: 0.641022\tvalid's multi_logloss: 0.82808\n",
      "[187]\ttrain's multi_logloss: 0.639573\tvalid's multi_logloss: 0.827493\n",
      "[188]\ttrain's multi_logloss: 0.63798\tvalid's multi_logloss: 0.827962\n",
      "[189]\ttrain's multi_logloss: 0.636152\tvalid's multi_logloss: 0.826172\n",
      "[190]\ttrain's multi_logloss: 0.6345\tvalid's multi_logloss: 0.824587\n",
      "[191]\ttrain's multi_logloss: 0.63303\tvalid's multi_logloss: 0.824288\n",
      "[192]\ttrain's multi_logloss: 0.631778\tvalid's multi_logloss: 0.823217\n",
      "[193]\ttrain's multi_logloss: 0.630442\tvalid's multi_logloss: 0.822972\n",
      "[194]\ttrain's multi_logloss: 0.628666\tvalid's multi_logloss: 0.824408\n",
      "[195]\ttrain's multi_logloss: 0.62707\tvalid's multi_logloss: 0.826234\n",
      "[196]\ttrain's multi_logloss: 0.62555\tvalid's multi_logloss: 0.826765\n",
      "[197]\ttrain's multi_logloss: 0.623768\tvalid's multi_logloss: 0.827418\n",
      "[198]\ttrain's multi_logloss: 0.622299\tvalid's multi_logloss: 0.829471\n",
      "[199]\ttrain's multi_logloss: 0.620895\tvalid's multi_logloss: 0.832257\n",
      "[200]\ttrain's multi_logloss: 0.619368\tvalid's multi_logloss: 0.831553\n",
      "[201]\ttrain's multi_logloss: 0.617622\tvalid's multi_logloss: 0.831232\n",
      "[202]\ttrain's multi_logloss: 0.616255\tvalid's multi_logloss: 0.831602\n",
      "[203]\ttrain's multi_logloss: 0.614415\tvalid's multi_logloss: 0.831411\n",
      "[204]\ttrain's multi_logloss: 0.612568\tvalid's multi_logloss: 0.830563\n",
      "[205]\ttrain's multi_logloss: 0.610823\tvalid's multi_logloss: 0.830085\n",
      "[206]\ttrain's multi_logloss: 0.609869\tvalid's multi_logloss: 0.831677\n",
      "[207]\ttrain's multi_logloss: 0.609112\tvalid's multi_logloss: 0.833687\n",
      "[208]\ttrain's multi_logloss: 0.608183\tvalid's multi_logloss: 0.835144\n",
      "[209]\ttrain's multi_logloss: 0.607179\tvalid's multi_logloss: 0.836965\n",
      "[210]\ttrain's multi_logloss: 0.60632\tvalid's multi_logloss: 0.838302\n",
      "[211]\ttrain's multi_logloss: 0.605047\tvalid's multi_logloss: 0.837527\n",
      "[212]\ttrain's multi_logloss: 0.603919\tvalid's multi_logloss: 0.837031\n",
      "[213]\ttrain's multi_logloss: 0.602859\tvalid's multi_logloss: 0.835497\n",
      "[214]\ttrain's multi_logloss: 0.601629\tvalid's multi_logloss: 0.834027\n",
      "[215]\ttrain's multi_logloss: 0.600864\tvalid's multi_logloss: 0.83454\n",
      "[216]\ttrain's multi_logloss: 0.599814\tvalid's multi_logloss: 0.837306\n",
      "[217]\ttrain's multi_logloss: 0.598422\tvalid's multi_logloss: 0.839375\n",
      "[218]\ttrain's multi_logloss: 0.597032\tvalid's multi_logloss: 0.840638\n",
      "[219]\ttrain's multi_logloss: 0.595397\tvalid's multi_logloss: 0.841438\n",
      "[220]\ttrain's multi_logloss: 0.594106\tvalid's multi_logloss: 0.843416\n",
      "[221]\ttrain's multi_logloss: 0.592593\tvalid's multi_logloss: 0.842362\n",
      "[222]\ttrain's multi_logloss: 0.591219\tvalid's multi_logloss: 0.843858\n",
      "[223]\ttrain's multi_logloss: 0.589911\tvalid's multi_logloss: 0.842641\n",
      "[224]\ttrain's multi_logloss: 0.588381\tvalid's multi_logloss: 0.841836\n",
      "[225]\ttrain's multi_logloss: 0.587115\tvalid's multi_logloss: 0.841782\n",
      "[226]\ttrain's multi_logloss: 0.585613\tvalid's multi_logloss: 0.843142\n",
      "[227]\ttrain's multi_logloss: 0.584194\tvalid's multi_logloss: 0.843599\n",
      "[228]\ttrain's multi_logloss: 0.582829\tvalid's multi_logloss: 0.844805\n",
      "[229]\ttrain's multi_logloss: 0.581258\tvalid's multi_logloss: 0.845777\n",
      "[230]\ttrain's multi_logloss: 0.579798\tvalid's multi_logloss: 0.846361\n",
      "[231]\ttrain's multi_logloss: 0.578615\tvalid's multi_logloss: 0.845641\n",
      "[232]\ttrain's multi_logloss: 0.577211\tvalid's multi_logloss: 0.844166\n",
      "[233]\ttrain's multi_logloss: 0.576198\tvalid's multi_logloss: 0.843739\n",
      "[234]\ttrain's multi_logloss: 0.575048\tvalid's multi_logloss: 0.842953\n",
      "[235]\ttrain's multi_logloss: 0.573647\tvalid's multi_logloss: 0.842506\n",
      "[236]\ttrain's multi_logloss: 0.572348\tvalid's multi_logloss: 0.842034\n",
      "[237]\ttrain's multi_logloss: 0.571242\tvalid's multi_logloss: 0.842507\n",
      "[238]\ttrain's multi_logloss: 0.569726\tvalid's multi_logloss: 0.840937\n",
      "[239]\ttrain's multi_logloss: 0.568287\tvalid's multi_logloss: 0.841789\n",
      "[240]\ttrain's multi_logloss: 0.566993\tvalid's multi_logloss: 0.840672\n",
      "[241]\ttrain's multi_logloss: 0.565548\tvalid's multi_logloss: 0.841643\n",
      "[242]\ttrain's multi_logloss: 0.564275\tvalid's multi_logloss: 0.842388\n",
      "[243]\ttrain's multi_logloss: 0.563414\tvalid's multi_logloss: 0.841707\n",
      "[244]\ttrain's multi_logloss: 0.562419\tvalid's multi_logloss: 0.84145\n",
      "[245]\ttrain's multi_logloss: 0.561528\tvalid's multi_logloss: 0.841233\n",
      "[246]\ttrain's multi_logloss: 0.560448\tvalid's multi_logloss: 0.842237\n",
      "[247]\ttrain's multi_logloss: 0.559299\tvalid's multi_logloss: 0.842978\n",
      "[248]\ttrain's multi_logloss: 0.5581\tvalid's multi_logloss: 0.843222\n",
      "[249]\ttrain's multi_logloss: 0.556678\tvalid's multi_logloss: 0.842396\n",
      "[250]\ttrain's multi_logloss: 0.555434\tvalid's multi_logloss: 0.841988\n",
      "[251]\ttrain's multi_logloss: 0.554456\tvalid's multi_logloss: 0.841311\n",
      "[252]\ttrain's multi_logloss: 0.553573\tvalid's multi_logloss: 0.839733\n",
      "[253]\ttrain's multi_logloss: 0.552524\tvalid's multi_logloss: 0.838227\n",
      "[254]\ttrain's multi_logloss: 0.551623\tvalid's multi_logloss: 0.836908\n",
      "[255]\ttrain's multi_logloss: 0.550687\tvalid's multi_logloss: 0.836209\n",
      "[256]\ttrain's multi_logloss: 0.549557\tvalid's multi_logloss: 0.836594\n",
      "[257]\ttrain's multi_logloss: 0.548573\tvalid's multi_logloss: 0.834364\n",
      "[258]\ttrain's multi_logloss: 0.547586\tvalid's multi_logloss: 0.834015\n",
      "[259]\ttrain's multi_logloss: 0.546695\tvalid's multi_logloss: 0.834023\n",
      "[260]\ttrain's multi_logloss: 0.545643\tvalid's multi_logloss: 0.832713\n",
      "[261]\ttrain's multi_logloss: 0.544534\tvalid's multi_logloss: 0.8325\n",
      "[262]\ttrain's multi_logloss: 0.543284\tvalid's multi_logloss: 0.835136\n",
      "[263]\ttrain's multi_logloss: 0.542222\tvalid's multi_logloss: 0.83491\n",
      "[264]\ttrain's multi_logloss: 0.541151\tvalid's multi_logloss: 0.834603\n",
      "[265]\ttrain's multi_logloss: 0.540041\tvalid's multi_logloss: 0.834375\n",
      "[266]\ttrain's multi_logloss: 0.538725\tvalid's multi_logloss: 0.834747\n",
      "[267]\ttrain's multi_logloss: 0.537713\tvalid's multi_logloss: 0.834219\n",
      "[268]\ttrain's multi_logloss: 0.536467\tvalid's multi_logloss: 0.833593\n",
      "[269]\ttrain's multi_logloss: 0.535397\tvalid's multi_logloss: 0.83429\n",
      "[270]\ttrain's multi_logloss: 0.534391\tvalid's multi_logloss: 0.834373\n",
      "[271]\ttrain's multi_logloss: 0.533551\tvalid's multi_logloss: 0.834568\n",
      "[272]\ttrain's multi_logloss: 0.532502\tvalid's multi_logloss: 0.83616\n",
      "[273]\ttrain's multi_logloss: 0.531604\tvalid's multi_logloss: 0.83715\n",
      "[274]\ttrain's multi_logloss: 0.530569\tvalid's multi_logloss: 0.8386\n",
      "[275]\ttrain's multi_logloss: 0.529397\tvalid's multi_logloss: 0.839812\n",
      "[276]\ttrain's multi_logloss: 0.528247\tvalid's multi_logloss: 0.840574\n",
      "[277]\ttrain's multi_logloss: 0.526983\tvalid's multi_logloss: 0.841188\n",
      "[278]\ttrain's multi_logloss: 0.525805\tvalid's multi_logloss: 0.842935\n",
      "[279]\ttrain's multi_logloss: 0.524568\tvalid's multi_logloss: 0.84374\n",
      "[280]\ttrain's multi_logloss: 0.52347\tvalid's multi_logloss: 0.844055\n",
      "[281]\ttrain's multi_logloss: 0.522217\tvalid's multi_logloss: 0.844235\n",
      "[282]\ttrain's multi_logloss: 0.521158\tvalid's multi_logloss: 0.843881\n",
      "[283]\ttrain's multi_logloss: 0.520126\tvalid's multi_logloss: 0.843517\n",
      "[284]\ttrain's multi_logloss: 0.519004\tvalid's multi_logloss: 0.844741\n",
      "[285]\ttrain's multi_logloss: 0.517839\tvalid's multi_logloss: 0.843977\n",
      "[286]\ttrain's multi_logloss: 0.516768\tvalid's multi_logloss: 0.844317\n",
      "[287]\ttrain's multi_logloss: 0.515335\tvalid's multi_logloss: 0.846866\n",
      "[288]\ttrain's multi_logloss: 0.514447\tvalid's multi_logloss: 0.846957\n",
      "[289]\ttrain's multi_logloss: 0.513134\tvalid's multi_logloss: 0.849737\n",
      "[290]\ttrain's multi_logloss: 0.512301\tvalid's multi_logloss: 0.849817\n",
      "[291]\ttrain's multi_logloss: 0.511244\tvalid's multi_logloss: 0.851064\n",
      "[292]\ttrain's multi_logloss: 0.510134\tvalid's multi_logloss: 0.853085\n",
      "[293]\ttrain's multi_logloss: 0.508996\tvalid's multi_logloss: 0.8543\n",
      "Early stopping, best iteration is:\n",
      "[193]\ttrain's multi_logloss: 0.630442\tvalid's multi_logloss: 0.822972\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09271\tvalid's SFC_loss: 1.09305\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08702\tvalid's SFC_loss: 1.08746\n",
      "[3]\ttrain's SFC_loss: 1.0806\tvalid's SFC_loss: 1.08189\n",
      "[4]\ttrain's SFC_loss: 1.07419\tvalid's SFC_loss: 1.07927\n",
      "[5]\ttrain's SFC_loss: 1.06916\tvalid's SFC_loss: 1.07543\n",
      "[6]\ttrain's SFC_loss: 1.06402\tvalid's SFC_loss: 1.07082\n",
      "[7]\ttrain's SFC_loss: 1.05854\tvalid's SFC_loss: 1.06715\n",
      "[8]\ttrain's SFC_loss: 1.05235\tvalid's SFC_loss: 1.06363\n",
      "[9]\ttrain's SFC_loss: 1.04679\tvalid's SFC_loss: 1.06005\n",
      "[10]\ttrain's SFC_loss: 1.03992\tvalid's SFC_loss: 1.05415\n",
      "[11]\ttrain's SFC_loss: 1.03458\tvalid's SFC_loss: 1.04969\n",
      "[12]\ttrain's SFC_loss: 1.02983\tvalid's SFC_loss: 1.04732\n",
      "[13]\ttrain's SFC_loss: 1.02413\tvalid's SFC_loss: 1.0409\n",
      "[14]\ttrain's SFC_loss: 1.01961\tvalid's SFC_loss: 1.0381\n",
      "[15]\ttrain's SFC_loss: 1.01474\tvalid's SFC_loss: 1.03471\n",
      "[16]\ttrain's SFC_loss: 1.00956\tvalid's SFC_loss: 1.02692\n",
      "[17]\ttrain's SFC_loss: 1.00369\tvalid's SFC_loss: 1.0228\n",
      "[18]\ttrain's SFC_loss: 0.999799\tvalid's SFC_loss: 1.01916\n",
      "[19]\ttrain's SFC_loss: 0.995546\tvalid's SFC_loss: 1.01658\n",
      "[20]\ttrain's SFC_loss: 0.991747\tvalid's SFC_loss: 1.01724\n",
      "[21]\ttrain's SFC_loss: 0.985277\tvalid's SFC_loss: 1.01351\n",
      "[22]\ttrain's SFC_loss: 0.979698\tvalid's SFC_loss: 1.00889\n",
      "[23]\ttrain's SFC_loss: 0.973102\tvalid's SFC_loss: 1.00282\n",
      "[24]\ttrain's SFC_loss: 0.968056\tvalid's SFC_loss: 0.998302\n",
      "[25]\ttrain's SFC_loss: 0.96129\tvalid's SFC_loss: 0.993748\n",
      "[26]\ttrain's SFC_loss: 0.956804\tvalid's SFC_loss: 0.990186\n",
      "[27]\ttrain's SFC_loss: 0.952313\tvalid's SFC_loss: 0.988992\n",
      "[28]\ttrain's SFC_loss: 0.947475\tvalid's SFC_loss: 0.988213\n",
      "[29]\ttrain's SFC_loss: 0.943615\tvalid's SFC_loss: 0.987764\n",
      "[30]\ttrain's SFC_loss: 0.939611\tvalid's SFC_loss: 0.983082\n",
      "[31]\ttrain's SFC_loss: 0.935162\tvalid's SFC_loss: 0.983966\n",
      "[32]\ttrain's SFC_loss: 0.930976\tvalid's SFC_loss: 0.97929\n",
      "[33]\ttrain's SFC_loss: 0.926682\tvalid's SFC_loss: 0.976238\n",
      "[34]\ttrain's SFC_loss: 0.922219\tvalid's SFC_loss: 0.973802\n",
      "[35]\ttrain's SFC_loss: 0.917671\tvalid's SFC_loss: 0.971294\n",
      "[36]\ttrain's SFC_loss: 0.912861\tvalid's SFC_loss: 0.969994\n",
      "[37]\ttrain's SFC_loss: 0.908552\tvalid's SFC_loss: 0.968506\n",
      "[38]\ttrain's SFC_loss: 0.904638\tvalid's SFC_loss: 0.968198\n",
      "[39]\ttrain's SFC_loss: 0.901798\tvalid's SFC_loss: 0.967222\n",
      "[40]\ttrain's SFC_loss: 0.898332\tvalid's SFC_loss: 0.963924\n",
      "[41]\ttrain's SFC_loss: 0.894266\tvalid's SFC_loss: 0.959257\n",
      "[42]\ttrain's SFC_loss: 0.891203\tvalid's SFC_loss: 0.957466\n",
      "[43]\ttrain's SFC_loss: 0.887752\tvalid's SFC_loss: 0.953847\n",
      "[44]\ttrain's SFC_loss: 0.884814\tvalid's SFC_loss: 0.951514\n",
      "[45]\ttrain's SFC_loss: 0.881028\tvalid's SFC_loss: 0.948899\n",
      "[46]\ttrain's SFC_loss: 0.8773\tvalid's SFC_loss: 0.946499\n",
      "[47]\ttrain's SFC_loss: 0.873525\tvalid's SFC_loss: 0.945428\n",
      "[48]\ttrain's SFC_loss: 0.870222\tvalid's SFC_loss: 0.941807\n",
      "[49]\ttrain's SFC_loss: 0.86691\tvalid's SFC_loss: 0.941559\n",
      "[50]\ttrain's SFC_loss: 0.863822\tvalid's SFC_loss: 0.941291\n",
      "[51]\ttrain's SFC_loss: 0.861365\tvalid's SFC_loss: 0.938217\n",
      "[52]\ttrain's SFC_loss: 0.858685\tvalid's SFC_loss: 0.935212\n",
      "[53]\ttrain's SFC_loss: 0.856106\tvalid's SFC_loss: 0.931983\n",
      "[54]\ttrain's SFC_loss: 0.853213\tvalid's SFC_loss: 0.929706\n",
      "[55]\ttrain's SFC_loss: 0.850382\tvalid's SFC_loss: 0.926179\n",
      "[56]\ttrain's SFC_loss: 0.847989\tvalid's SFC_loss: 0.922862\n",
      "[57]\ttrain's SFC_loss: 0.84575\tvalid's SFC_loss: 0.920007\n",
      "[58]\ttrain's SFC_loss: 0.842766\tvalid's SFC_loss: 0.917177\n",
      "[59]\ttrain's SFC_loss: 0.840045\tvalid's SFC_loss: 0.913862\n",
      "[60]\ttrain's SFC_loss: 0.837038\tvalid's SFC_loss: 0.909577\n",
      "[61]\ttrain's SFC_loss: 0.833485\tvalid's SFC_loss: 0.907234\n",
      "[62]\ttrain's SFC_loss: 0.830219\tvalid's SFC_loss: 0.904879\n",
      "[63]\ttrain's SFC_loss: 0.826924\tvalid's SFC_loss: 0.902483\n",
      "[64]\ttrain's SFC_loss: 0.823971\tvalid's SFC_loss: 0.900475\n",
      "[65]\ttrain's SFC_loss: 0.820801\tvalid's SFC_loss: 0.897466\n",
      "[66]\ttrain's SFC_loss: 0.817928\tvalid's SFC_loss: 0.896538\n",
      "[67]\ttrain's SFC_loss: 0.814229\tvalid's SFC_loss: 0.892496\n",
      "[68]\ttrain's SFC_loss: 0.810942\tvalid's SFC_loss: 0.887869\n",
      "[69]\ttrain's SFC_loss: 0.8073\tvalid's SFC_loss: 0.884506\n",
      "[70]\ttrain's SFC_loss: 0.803876\tvalid's SFC_loss: 0.880798\n",
      "[71]\ttrain's SFC_loss: 0.800548\tvalid's SFC_loss: 0.880018\n",
      "[72]\ttrain's SFC_loss: 0.79726\tvalid's SFC_loss: 0.878248\n",
      "[73]\ttrain's SFC_loss: 0.794256\tvalid's SFC_loss: 0.877698\n",
      "[74]\ttrain's SFC_loss: 0.791085\tvalid's SFC_loss: 0.87607\n",
      "[75]\ttrain's SFC_loss: 0.787923\tvalid's SFC_loss: 0.874531\n",
      "[76]\ttrain's SFC_loss: 0.78559\tvalid's SFC_loss: 0.872261\n",
      "[77]\ttrain's SFC_loss: 0.782304\tvalid's SFC_loss: 0.87301\n",
      "[78]\ttrain's SFC_loss: 0.780074\tvalid's SFC_loss: 0.870903\n",
      "[79]\ttrain's SFC_loss: 0.776903\tvalid's SFC_loss: 0.86924\n",
      "[80]\ttrain's SFC_loss: 0.774268\tvalid's SFC_loss: 0.866155\n",
      "[81]\ttrain's SFC_loss: 0.771305\tvalid's SFC_loss: 0.864493\n",
      "[82]\ttrain's SFC_loss: 0.768721\tvalid's SFC_loss: 0.864275\n",
      "[83]\ttrain's SFC_loss: 0.766634\tvalid's SFC_loss: 0.862496\n",
      "[84]\ttrain's SFC_loss: 0.764127\tvalid's SFC_loss: 0.859525\n",
      "[85]\ttrain's SFC_loss: 0.760909\tvalid's SFC_loss: 0.858279\n",
      "[86]\ttrain's SFC_loss: 0.758892\tvalid's SFC_loss: 0.856432\n",
      "[87]\ttrain's SFC_loss: 0.756697\tvalid's SFC_loss: 0.854677\n",
      "[88]\ttrain's SFC_loss: 0.754736\tvalid's SFC_loss: 0.853862\n",
      "[89]\ttrain's SFC_loss: 0.753189\tvalid's SFC_loss: 0.853574\n",
      "[90]\ttrain's SFC_loss: 0.751587\tvalid's SFC_loss: 0.853249\n",
      "[91]\ttrain's SFC_loss: 0.748698\tvalid's SFC_loss: 0.85187\n",
      "[92]\ttrain's SFC_loss: 0.745682\tvalid's SFC_loss: 0.852184\n",
      "[93]\ttrain's SFC_loss: 0.743003\tvalid's SFC_loss: 0.851437\n",
      "[94]\ttrain's SFC_loss: 0.740515\tvalid's SFC_loss: 0.852339\n",
      "[95]\ttrain's SFC_loss: 0.738329\tvalid's SFC_loss: 0.853373\n",
      "[96]\ttrain's SFC_loss: 0.736171\tvalid's SFC_loss: 0.851026\n",
      "[97]\ttrain's SFC_loss: 0.733957\tvalid's SFC_loss: 0.848487\n",
      "[98]\ttrain's SFC_loss: 0.731182\tvalid's SFC_loss: 0.84481\n",
      "[99]\ttrain's SFC_loss: 0.728311\tvalid's SFC_loss: 0.841086\n",
      "[100]\ttrain's SFC_loss: 0.726083\tvalid's SFC_loss: 0.838052\n",
      "[101]\ttrain's SFC_loss: 0.724054\tvalid's SFC_loss: 0.836107\n",
      "[102]\ttrain's SFC_loss: 0.720991\tvalid's SFC_loss: 0.835835\n",
      "[103]\ttrain's SFC_loss: 0.718186\tvalid's SFC_loss: 0.835349\n",
      "[104]\ttrain's SFC_loss: 0.715949\tvalid's SFC_loss: 0.833277\n",
      "[105]\ttrain's SFC_loss: 0.713261\tvalid's SFC_loss: 0.831188\n",
      "[106]\ttrain's SFC_loss: 0.710869\tvalid's SFC_loss: 0.830128\n",
      "[107]\ttrain's SFC_loss: 0.70885\tvalid's SFC_loss: 0.830637\n",
      "[108]\ttrain's SFC_loss: 0.707031\tvalid's SFC_loss: 0.830578\n",
      "[109]\ttrain's SFC_loss: 0.705518\tvalid's SFC_loss: 0.829466\n",
      "[110]\ttrain's SFC_loss: 0.703581\tvalid's SFC_loss: 0.828787\n",
      "[111]\ttrain's SFC_loss: 0.700691\tvalid's SFC_loss: 0.832564\n",
      "[112]\ttrain's SFC_loss: 0.698323\tvalid's SFC_loss: 0.834809\n",
      "[113]\ttrain's SFC_loss: 0.695933\tvalid's SFC_loss: 0.833507\n",
      "[114]\ttrain's SFC_loss: 0.693392\tvalid's SFC_loss: 0.836636\n",
      "[115]\ttrain's SFC_loss: 0.69091\tvalid's SFC_loss: 0.840252\n",
      "[116]\ttrain's SFC_loss: 0.689117\tvalid's SFC_loss: 0.837924\n",
      "[117]\ttrain's SFC_loss: 0.687146\tvalid's SFC_loss: 0.836469\n",
      "[118]\ttrain's SFC_loss: 0.68533\tvalid's SFC_loss: 0.832788\n",
      "[119]\ttrain's SFC_loss: 0.683443\tvalid's SFC_loss: 0.831177\n",
      "[120]\ttrain's SFC_loss: 0.681958\tvalid's SFC_loss: 0.827579\n",
      "[121]\ttrain's SFC_loss: 0.679167\tvalid's SFC_loss: 0.828055\n",
      "[122]\ttrain's SFC_loss: 0.677124\tvalid's SFC_loss: 0.824369\n",
      "[123]\ttrain's SFC_loss: 0.674992\tvalid's SFC_loss: 0.822699\n",
      "[124]\ttrain's SFC_loss: 0.672675\tvalid's SFC_loss: 0.824141\n",
      "[125]\ttrain's SFC_loss: 0.670216\tvalid's SFC_loss: 0.823489\n",
      "[126]\ttrain's SFC_loss: 0.668219\tvalid's SFC_loss: 0.821805\n",
      "[127]\ttrain's SFC_loss: 0.666462\tvalid's SFC_loss: 0.820353\n",
      "[128]\ttrain's SFC_loss: 0.665067\tvalid's SFC_loss: 0.819715\n",
      "[129]\ttrain's SFC_loss: 0.66385\tvalid's SFC_loss: 0.819978\n",
      "[130]\ttrain's SFC_loss: 0.662459\tvalid's SFC_loss: 0.820709\n",
      "[131]\ttrain's SFC_loss: 0.660338\tvalid's SFC_loss: 0.820352\n",
      "[132]\ttrain's SFC_loss: 0.65783\tvalid's SFC_loss: 0.821138\n",
      "[133]\ttrain's SFC_loss: 0.655532\tvalid's SFC_loss: 0.819212\n",
      "[134]\ttrain's SFC_loss: 0.653448\tvalid's SFC_loss: 0.820762\n",
      "[135]\ttrain's SFC_loss: 0.651256\tvalid's SFC_loss: 0.821978\n",
      "[136]\ttrain's SFC_loss: 0.649224\tvalid's SFC_loss: 0.824677\n",
      "[137]\ttrain's SFC_loss: 0.647453\tvalid's SFC_loss: 0.824519\n",
      "[138]\ttrain's SFC_loss: 0.645656\tvalid's SFC_loss: 0.824585\n",
      "[139]\ttrain's SFC_loss: 0.643909\tvalid's SFC_loss: 0.824672\n",
      "[140]\ttrain's SFC_loss: 0.642208\tvalid's SFC_loss: 0.824106\n",
      "[141]\ttrain's SFC_loss: 0.640982\tvalid's SFC_loss: 0.825146\n",
      "[142]\ttrain's SFC_loss: 0.639581\tvalid's SFC_loss: 0.826267\n",
      "[143]\ttrain's SFC_loss: 0.637826\tvalid's SFC_loss: 0.8272\n",
      "[144]\ttrain's SFC_loss: 0.635971\tvalid's SFC_loss: 0.828194\n",
      "[145]\ttrain's SFC_loss: 0.634334\tvalid's SFC_loss: 0.830071\n",
      "[146]\ttrain's SFC_loss: 0.632039\tvalid's SFC_loss: 0.827028\n",
      "[147]\ttrain's SFC_loss: 0.630255\tvalid's SFC_loss: 0.826012\n",
      "[148]\ttrain's SFC_loss: 0.628429\tvalid's SFC_loss: 0.822447\n",
      "[149]\ttrain's SFC_loss: 0.626182\tvalid's SFC_loss: 0.820257\n",
      "[150]\ttrain's SFC_loss: 0.624067\tvalid's SFC_loss: 0.817388\n",
      "[151]\ttrain's SFC_loss: 0.62225\tvalid's SFC_loss: 0.816607\n",
      "[152]\ttrain's SFC_loss: 0.620495\tvalid's SFC_loss: 0.814572\n",
      "[153]\ttrain's SFC_loss: 0.618377\tvalid's SFC_loss: 0.814458\n",
      "[154]\ttrain's SFC_loss: 0.616629\tvalid's SFC_loss: 0.813199\n",
      "[155]\ttrain's SFC_loss: 0.615116\tvalid's SFC_loss: 0.811289\n",
      "[156]\ttrain's SFC_loss: 0.613454\tvalid's SFC_loss: 0.810795\n",
      "[157]\ttrain's SFC_loss: 0.611226\tvalid's SFC_loss: 0.812296\n",
      "[158]\ttrain's SFC_loss: 0.609294\tvalid's SFC_loss: 0.814015\n",
      "[159]\ttrain's SFC_loss: 0.607339\tvalid's SFC_loss: 0.812889\n",
      "[160]\ttrain's SFC_loss: 0.605509\tvalid's SFC_loss: 0.814638\n",
      "[161]\ttrain's SFC_loss: 0.60421\tvalid's SFC_loss: 0.813595\n",
      "[162]\ttrain's SFC_loss: 0.603133\tvalid's SFC_loss: 0.814584\n",
      "[163]\ttrain's SFC_loss: 0.601791\tvalid's SFC_loss: 0.816475\n",
      "[164]\ttrain's SFC_loss: 0.600429\tvalid's SFC_loss: 0.815099\n",
      "[165]\ttrain's SFC_loss: 0.599218\tvalid's SFC_loss: 0.816377\n",
      "[166]\ttrain's SFC_loss: 0.59725\tvalid's SFC_loss: 0.814468\n",
      "[167]\ttrain's SFC_loss: 0.595242\tvalid's SFC_loss: 0.813017\n",
      "[168]\ttrain's SFC_loss: 0.59368\tvalid's SFC_loss: 0.812482\n",
      "[169]\ttrain's SFC_loss: 0.59178\tvalid's SFC_loss: 0.814129\n",
      "[170]\ttrain's SFC_loss: 0.589618\tvalid's SFC_loss: 0.813812\n",
      "[171]\ttrain's SFC_loss: 0.588309\tvalid's SFC_loss: 0.812487\n",
      "[172]\ttrain's SFC_loss: 0.586593\tvalid's SFC_loss: 0.810815\n",
      "[173]\ttrain's SFC_loss: 0.585395\tvalid's SFC_loss: 0.809745\n",
      "[174]\ttrain's SFC_loss: 0.58411\tvalid's SFC_loss: 0.809131\n",
      "[175]\ttrain's SFC_loss: 0.582592\tvalid's SFC_loss: 0.809778\n",
      "[176]\ttrain's SFC_loss: 0.580803\tvalid's SFC_loss: 0.807089\n",
      "[177]\ttrain's SFC_loss: 0.579073\tvalid's SFC_loss: 0.807868\n",
      "[178]\ttrain's SFC_loss: 0.577271\tvalid's SFC_loss: 0.80657\n",
      "[179]\ttrain's SFC_loss: 0.575109\tvalid's SFC_loss: 0.804479\n",
      "[180]\ttrain's SFC_loss: 0.573512\tvalid's SFC_loss: 0.803779\n",
      "[181]\ttrain's SFC_loss: 0.571784\tvalid's SFC_loss: 0.80442\n",
      "[182]\ttrain's SFC_loss: 0.570088\tvalid's SFC_loss: 0.805971\n",
      "[183]\ttrain's SFC_loss: 0.568593\tvalid's SFC_loss: 0.806197\n",
      "[184]\ttrain's SFC_loss: 0.566672\tvalid's SFC_loss: 0.809455\n",
      "[185]\ttrain's SFC_loss: 0.56531\tvalid's SFC_loss: 0.811196\n",
      "[186]\ttrain's SFC_loss: 0.563659\tvalid's SFC_loss: 0.811358\n",
      "[187]\ttrain's SFC_loss: 0.561682\tvalid's SFC_loss: 0.810651\n",
      "[188]\ttrain's SFC_loss: 0.560127\tvalid's SFC_loss: 0.808528\n",
      "[189]\ttrain's SFC_loss: 0.558774\tvalid's SFC_loss: 0.806596\n",
      "[190]\ttrain's SFC_loss: 0.557168\tvalid's SFC_loss: 0.809294\n",
      "[191]\ttrain's SFC_loss: 0.555881\tvalid's SFC_loss: 0.810567\n",
      "[192]\ttrain's SFC_loss: 0.554041\tvalid's SFC_loss: 0.813138\n",
      "[193]\ttrain's SFC_loss: 0.552407\tvalid's SFC_loss: 0.815856\n",
      "[194]\ttrain's SFC_loss: 0.551457\tvalid's SFC_loss: 0.816094\n",
      "[195]\ttrain's SFC_loss: 0.549794\tvalid's SFC_loss: 0.81876\n",
      "[196]\ttrain's SFC_loss: 0.548418\tvalid's SFC_loss: 0.818216\n",
      "[197]\ttrain's SFC_loss: 0.546239\tvalid's SFC_loss: 0.820373\n",
      "[198]\ttrain's SFC_loss: 0.544375\tvalid's SFC_loss: 0.821177\n",
      "[199]\ttrain's SFC_loss: 0.542846\tvalid's SFC_loss: 0.822299\n",
      "[200]\ttrain's SFC_loss: 0.540935\tvalid's SFC_loss: 0.822636\n",
      "[201]\ttrain's SFC_loss: 0.539149\tvalid's SFC_loss: 0.822243\n",
      "[202]\ttrain's SFC_loss: 0.537277\tvalid's SFC_loss: 0.822789\n",
      "[203]\ttrain's SFC_loss: 0.535105\tvalid's SFC_loss: 0.823584\n",
      "[204]\ttrain's SFC_loss: 0.533252\tvalid's SFC_loss: 0.824604\n",
      "[205]\ttrain's SFC_loss: 0.531385\tvalid's SFC_loss: 0.821468\n",
      "[206]\ttrain's SFC_loss: 0.53036\tvalid's SFC_loss: 0.821646\n",
      "[207]\ttrain's SFC_loss: 0.528989\tvalid's SFC_loss: 0.821486\n",
      "[208]\ttrain's SFC_loss: 0.528072\tvalid's SFC_loss: 0.823675\n",
      "[209]\ttrain's SFC_loss: 0.527128\tvalid's SFC_loss: 0.823614\n",
      "[210]\ttrain's SFC_loss: 0.526081\tvalid's SFC_loss: 0.825026\n",
      "[211]\ttrain's SFC_loss: 0.525069\tvalid's SFC_loss: 0.824995\n",
      "[212]\ttrain's SFC_loss: 0.523917\tvalid's SFC_loss: 0.823103\n",
      "[213]\ttrain's SFC_loss: 0.522659\tvalid's SFC_loss: 0.823479\n",
      "[214]\ttrain's SFC_loss: 0.521952\tvalid's SFC_loss: 0.824099\n",
      "[215]\ttrain's SFC_loss: 0.52075\tvalid's SFC_loss: 0.823984\n",
      "[216]\ttrain's SFC_loss: 0.519104\tvalid's SFC_loss: 0.827408\n",
      "[217]\ttrain's SFC_loss: 0.517738\tvalid's SFC_loss: 0.828384\n",
      "[218]\ttrain's SFC_loss: 0.516312\tvalid's SFC_loss: 0.832972\n",
      "[219]\ttrain's SFC_loss: 0.514964\tvalid's SFC_loss: 0.832121\n",
      "[220]\ttrain's SFC_loss: 0.513415\tvalid's SFC_loss: 0.833051\n",
      "[221]\ttrain's SFC_loss: 0.511939\tvalid's SFC_loss: 0.835726\n",
      "[222]\ttrain's SFC_loss: 0.510542\tvalid's SFC_loss: 0.836\n",
      "[223]\ttrain's SFC_loss: 0.509289\tvalid's SFC_loss: 0.837313\n",
      "[224]\ttrain's SFC_loss: 0.507837\tvalid's SFC_loss: 0.839958\n",
      "[225]\ttrain's SFC_loss: 0.506507\tvalid's SFC_loss: 0.841458\n",
      "[226]\ttrain's SFC_loss: 0.505412\tvalid's SFC_loss: 0.841591\n",
      "[227]\ttrain's SFC_loss: 0.503757\tvalid's SFC_loss: 0.842474\n",
      "[228]\ttrain's SFC_loss: 0.50214\tvalid's SFC_loss: 0.844942\n",
      "[229]\ttrain's SFC_loss: 0.500619\tvalid's SFC_loss: 0.847498\n",
      "[230]\ttrain's SFC_loss: 0.499055\tvalid's SFC_loss: 0.848136\n",
      "[231]\ttrain's SFC_loss: 0.498051\tvalid's SFC_loss: 0.846747\n",
      "[232]\ttrain's SFC_loss: 0.496854\tvalid's SFC_loss: 0.845677\n",
      "[233]\ttrain's SFC_loss: 0.49577\tvalid's SFC_loss: 0.845763\n",
      "[234]\ttrain's SFC_loss: 0.494353\tvalid's SFC_loss: 0.845604\n",
      "[235]\ttrain's SFC_loss: 0.493142\tvalid's SFC_loss: 0.845078\n",
      "[236]\ttrain's SFC_loss: 0.491855\tvalid's SFC_loss: 0.843315\n",
      "[237]\ttrain's SFC_loss: 0.49068\tvalid's SFC_loss: 0.845353\n",
      "[238]\ttrain's SFC_loss: 0.489553\tvalid's SFC_loss: 0.844743\n",
      "[239]\ttrain's SFC_loss: 0.488093\tvalid's SFC_loss: 0.846842\n",
      "[240]\ttrain's SFC_loss: 0.486387\tvalid's SFC_loss: 0.851845\n",
      "[241]\ttrain's SFC_loss: 0.485268\tvalid's SFC_loss: 0.852439\n",
      "[242]\ttrain's SFC_loss: 0.484287\tvalid's SFC_loss: 0.851859\n",
      "[243]\ttrain's SFC_loss: 0.4832\tvalid's SFC_loss: 0.852557\n",
      "[244]\ttrain's SFC_loss: 0.482312\tvalid's SFC_loss: 0.852773\n",
      "[245]\ttrain's SFC_loss: 0.481289\tvalid's SFC_loss: 0.853535\n",
      "[246]\ttrain's SFC_loss: 0.479999\tvalid's SFC_loss: 0.854722\n",
      "[247]\ttrain's SFC_loss: 0.478943\tvalid's SFC_loss: 0.854015\n",
      "[248]\ttrain's SFC_loss: 0.477915\tvalid's SFC_loss: 0.853991\n",
      "[249]\ttrain's SFC_loss: 0.47669\tvalid's SFC_loss: 0.854829\n",
      "[250]\ttrain's SFC_loss: 0.475652\tvalid's SFC_loss: 0.856895\n",
      "[251]\ttrain's SFC_loss: 0.474809\tvalid's SFC_loss: 0.857819\n",
      "[252]\ttrain's SFC_loss: 0.473588\tvalid's SFC_loss: 0.857566\n",
      "[253]\ttrain's SFC_loss: 0.472522\tvalid's SFC_loss: 0.857385\n",
      "[254]\ttrain's SFC_loss: 0.471862\tvalid's SFC_loss: 0.858244\n",
      "[255]\ttrain's SFC_loss: 0.470615\tvalid's SFC_loss: 0.857484\n",
      "[256]\ttrain's SFC_loss: 0.469946\tvalid's SFC_loss: 0.856959\n",
      "[257]\ttrain's SFC_loss: 0.468668\tvalid's SFC_loss: 0.8554\n",
      "[258]\ttrain's SFC_loss: 0.467429\tvalid's SFC_loss: 0.853874\n",
      "[259]\ttrain's SFC_loss: 0.46646\tvalid's SFC_loss: 0.852528\n",
      "[260]\ttrain's SFC_loss: 0.46554\tvalid's SFC_loss: 0.850139\n",
      "[261]\ttrain's SFC_loss: 0.464266\tvalid's SFC_loss: 0.851933\n",
      "[262]\ttrain's SFC_loss: 0.463469\tvalid's SFC_loss: 0.851615\n",
      "[263]\ttrain's SFC_loss: 0.46241\tvalid's SFC_loss: 0.851978\n",
      "[264]\ttrain's SFC_loss: 0.461375\tvalid's SFC_loss: 0.851909\n",
      "[265]\ttrain's SFC_loss: 0.460511\tvalid's SFC_loss: 0.850932\n",
      "[266]\ttrain's SFC_loss: 0.459234\tvalid's SFC_loss: 0.851517\n",
      "[267]\ttrain's SFC_loss: 0.458097\tvalid's SFC_loss: 0.851404\n",
      "[268]\ttrain's SFC_loss: 0.456636\tvalid's SFC_loss: 0.851697\n",
      "[269]\ttrain's SFC_loss: 0.45581\tvalid's SFC_loss: 0.851805\n",
      "[270]\ttrain's SFC_loss: 0.4548\tvalid's SFC_loss: 0.851711\n",
      "[271]\ttrain's SFC_loss: 0.453613\tvalid's SFC_loss: 0.853924\n",
      "[272]\ttrain's SFC_loss: 0.452611\tvalid's SFC_loss: 0.854843\n",
      "[273]\ttrain's SFC_loss: 0.451628\tvalid's SFC_loss: 0.857647\n",
      "[274]\ttrain's SFC_loss: 0.450821\tvalid's SFC_loss: 0.856905\n",
      "[275]\ttrain's SFC_loss: 0.449813\tvalid's SFC_loss: 0.858419\n",
      "[276]\ttrain's SFC_loss: 0.448733\tvalid's SFC_loss: 0.861864\n",
      "[277]\ttrain's SFC_loss: 0.447727\tvalid's SFC_loss: 0.863982\n",
      "[278]\ttrain's SFC_loss: 0.446668\tvalid's SFC_loss: 0.863702\n",
      "[279]\ttrain's SFC_loss: 0.445836\tvalid's SFC_loss: 0.863074\n",
      "[280]\ttrain's SFC_loss: 0.44486\tvalid's SFC_loss: 0.865332\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttrain's SFC_loss: 0.573512\tvalid's SFC_loss: 0.803779\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1    0.037444\n",
      "1    feature2   -0.005705\n",
      "2    feature3   -0.129659\n",
      "3    feature4    0.047451\n",
      "4    feature5    0.042686\n",
      "5    feature6    0.040287\n",
      "6    feature7    0.117102\n",
      "7    feature8   -0.025477\n",
      "8    feature9   -0.052154\n",
      "9   feature10    0.076186\n",
      "10  feature11   -0.010138\n",
      "11  feature12   -0.066027\n",
      "12  feature13    0.010415\n",
      "13  feature14    0.026059\n",
      "14  feature15   -0.070552\n",
      "15  feature16   -0.000718\n",
      "16  feature17    0.015980\n",
      "17  feature18   -0.139571\n",
      "18  feature19    0.051236\n",
      "19  feature20    0.035154\n",
      "-------------------- 4 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.0585\tvalid's multi_logloss: 1.06405\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05474\tvalid's multi_logloss: 1.06049\n",
      "[3]\ttrain's multi_logloss: 1.05089\tvalid's multi_logloss: 1.05664\n",
      "[4]\ttrain's multi_logloss: 1.04776\tvalid's multi_logloss: 1.0578\n",
      "[5]\ttrain's multi_logloss: 1.04456\tvalid's multi_logloss: 1.0546\n",
      "[6]\ttrain's multi_logloss: 1.04106\tvalid's multi_logloss: 1.05112\n",
      "[7]\ttrain's multi_logloss: 1.03843\tvalid's multi_logloss: 1.04806\n",
      "[8]\ttrain's multi_logloss: 1.03547\tvalid's multi_logloss: 1.04611\n",
      "[9]\ttrain's multi_logloss: 1.03372\tvalid's multi_logloss: 1.04317\n",
      "[10]\ttrain's multi_logloss: 1.03086\tvalid's multi_logloss: 1.04131\n",
      "[11]\ttrain's multi_logloss: 1.02758\tvalid's multi_logloss: 1.03964\n",
      "[12]\ttrain's multi_logloss: 1.02432\tvalid's multi_logloss: 1.03806\n",
      "[13]\ttrain's multi_logloss: 1.02067\tvalid's multi_logloss: 1.03487\n",
      "[14]\ttrain's multi_logloss: 1.0178\tvalid's multi_logloss: 1.03294\n",
      "[15]\ttrain's multi_logloss: 1.01482\tvalid's multi_logloss: 1.03062\n",
      "[16]\ttrain's multi_logloss: 1.01099\tvalid's multi_logloss: 1.02864\n",
      "[17]\ttrain's multi_logloss: 1.0072\tvalid's multi_logloss: 1.02579\n",
      "[18]\ttrain's multi_logloss: 1.00349\tvalid's multi_logloss: 1.02301\n",
      "[19]\ttrain's multi_logloss: 1.00043\tvalid's multi_logloss: 1.02185\n",
      "[20]\ttrain's multi_logloss: 0.997319\tvalid's multi_logloss: 1.01898\n",
      "[21]\ttrain's multi_logloss: 0.99358\tvalid's multi_logloss: 1.01689\n",
      "[22]\ttrain's multi_logloss: 0.989896\tvalid's multi_logloss: 1.01579\n",
      "[23]\ttrain's multi_logloss: 0.986482\tvalid's multi_logloss: 1.01668\n",
      "[24]\ttrain's multi_logloss: 0.983208\tvalid's multi_logloss: 1.01597\n",
      "[25]\ttrain's multi_logloss: 0.980314\tvalid's multi_logloss: 1.01683\n",
      "[26]\ttrain's multi_logloss: 0.976736\tvalid's multi_logloss: 1.01749\n",
      "[27]\ttrain's multi_logloss: 0.973003\tvalid's multi_logloss: 1.01286\n",
      "[28]\ttrain's multi_logloss: 0.969961\tvalid's multi_logloss: 1.0124\n",
      "[29]\ttrain's multi_logloss: 0.966217\tvalid's multi_logloss: 1.01391\n",
      "[30]\ttrain's multi_logloss: 0.962933\tvalid's multi_logloss: 1.01535\n",
      "[31]\ttrain's multi_logloss: 0.959957\tvalid's multi_logloss: 1.01213\n",
      "[32]\ttrain's multi_logloss: 0.957175\tvalid's multi_logloss: 1.00794\n",
      "[33]\ttrain's multi_logloss: 0.954465\tvalid's multi_logloss: 1.00354\n",
      "[34]\ttrain's multi_logloss: 0.951666\tvalid's multi_logloss: 1.00037\n",
      "[35]\ttrain's multi_logloss: 0.948889\tvalid's multi_logloss: 1.00022\n",
      "[36]\ttrain's multi_logloss: 0.946328\tvalid's multi_logloss: 1.00139\n",
      "[37]\ttrain's multi_logloss: 0.943279\tvalid's multi_logloss: 0.998438\n",
      "[38]\ttrain's multi_logloss: 0.939971\tvalid's multi_logloss: 0.993203\n",
      "[39]\ttrain's multi_logloss: 0.936873\tvalid's multi_logloss: 0.991061\n",
      "[40]\ttrain's multi_logloss: 0.934131\tvalid's multi_logloss: 0.98871\n",
      "[41]\ttrain's multi_logloss: 0.931253\tvalid's multi_logloss: 0.985868\n",
      "[42]\ttrain's multi_logloss: 0.928453\tvalid's multi_logloss: 0.983949\n",
      "[43]\ttrain's multi_logloss: 0.925999\tvalid's multi_logloss: 0.983588\n",
      "[44]\ttrain's multi_logloss: 0.923239\tvalid's multi_logloss: 0.981917\n",
      "[45]\ttrain's multi_logloss: 0.920554\tvalid's multi_logloss: 0.981542\n",
      "[46]\ttrain's multi_logloss: 0.91771\tvalid's multi_logloss: 0.978908\n",
      "[47]\ttrain's multi_logloss: 0.914993\tvalid's multi_logloss: 0.975861\n",
      "[48]\ttrain's multi_logloss: 0.912318\tvalid's multi_logloss: 0.973725\n",
      "[49]\ttrain's multi_logloss: 0.909978\tvalid's multi_logloss: 0.971145\n",
      "[50]\ttrain's multi_logloss: 0.907351\tvalid's multi_logloss: 0.967315\n",
      "[51]\ttrain's multi_logloss: 0.904619\tvalid's multi_logloss: 0.96793\n",
      "[52]\ttrain's multi_logloss: 0.902679\tvalid's multi_logloss: 0.966931\n",
      "[53]\ttrain's multi_logloss: 0.900022\tvalid's multi_logloss: 0.967188\n",
      "[54]\ttrain's multi_logloss: 0.89771\tvalid's multi_logloss: 0.966523\n",
      "[55]\ttrain's multi_logloss: 0.895107\tvalid's multi_logloss: 0.967035\n",
      "[56]\ttrain's multi_logloss: 0.893199\tvalid's multi_logloss: 0.968053\n",
      "[57]\ttrain's multi_logloss: 0.891316\tvalid's multi_logloss: 0.969084\n",
      "[58]\ttrain's multi_logloss: 0.889471\tvalid's multi_logloss: 0.969924\n",
      "[59]\ttrain's multi_logloss: 0.887577\tvalid's multi_logloss: 0.970385\n",
      "[60]\ttrain's multi_logloss: 0.885773\tvalid's multi_logloss: 0.970596\n",
      "[61]\ttrain's multi_logloss: 0.882897\tvalid's multi_logloss: 0.968777\n",
      "[62]\ttrain's multi_logloss: 0.880011\tvalid's multi_logloss: 0.967356\n",
      "[63]\ttrain's multi_logloss: 0.877218\tvalid's multi_logloss: 0.966056\n",
      "[64]\ttrain's multi_logloss: 0.87426\tvalid's multi_logloss: 0.965287\n",
      "[65]\ttrain's multi_logloss: 0.87213\tvalid's multi_logloss: 0.963795\n",
      "[66]\ttrain's multi_logloss: 0.869569\tvalid's multi_logloss: 0.961548\n",
      "[67]\ttrain's multi_logloss: 0.867235\tvalid's multi_logloss: 0.961388\n",
      "[68]\ttrain's multi_logloss: 0.864605\tvalid's multi_logloss: 0.959968\n",
      "[69]\ttrain's multi_logloss: 0.862473\tvalid's multi_logloss: 0.957303\n",
      "[70]\ttrain's multi_logloss: 0.860344\tvalid's multi_logloss: 0.954416\n",
      "[71]\ttrain's multi_logloss: 0.857928\tvalid's multi_logloss: 0.95092\n",
      "[72]\ttrain's multi_logloss: 0.855045\tvalid's multi_logloss: 0.948844\n",
      "[73]\ttrain's multi_logloss: 0.852692\tvalid's multi_logloss: 0.946153\n",
      "[74]\ttrain's multi_logloss: 0.850467\tvalid's multi_logloss: 0.944551\n",
      "[75]\ttrain's multi_logloss: 0.848156\tvalid's multi_logloss: 0.942652\n",
      "[76]\ttrain's multi_logloss: 0.845528\tvalid's multi_logloss: 0.940533\n",
      "[77]\ttrain's multi_logloss: 0.84307\tvalid's multi_logloss: 0.93826\n",
      "[78]\ttrain's multi_logloss: 0.840659\tvalid's multi_logloss: 0.936034\n",
      "[79]\ttrain's multi_logloss: 0.8385\tvalid's multi_logloss: 0.93616\n",
      "[80]\ttrain's multi_logloss: 0.83635\tvalid's multi_logloss: 0.937241\n",
      "[81]\ttrain's multi_logloss: 0.834037\tvalid's multi_logloss: 0.937955\n",
      "[82]\ttrain's multi_logloss: 0.831736\tvalid's multi_logloss: 0.936462\n",
      "[83]\ttrain's multi_logloss: 0.829247\tvalid's multi_logloss: 0.93698\n",
      "[84]\ttrain's multi_logloss: 0.826897\tvalid's multi_logloss: 0.937745\n",
      "[85]\ttrain's multi_logloss: 0.824695\tvalid's multi_logloss: 0.938509\n",
      "[86]\ttrain's multi_logloss: 0.822413\tvalid's multi_logloss: 0.937034\n",
      "[87]\ttrain's multi_logloss: 0.820284\tvalid's multi_logloss: 0.934931\n",
      "[88]\ttrain's multi_logloss: 0.818181\tvalid's multi_logloss: 0.933696\n",
      "[89]\ttrain's multi_logloss: 0.815992\tvalid's multi_logloss: 0.930748\n",
      "[90]\ttrain's multi_logloss: 0.813821\tvalid's multi_logloss: 0.929026\n",
      "[91]\ttrain's multi_logloss: 0.81143\tvalid's multi_logloss: 0.931088\n",
      "[92]\ttrain's multi_logloss: 0.809153\tvalid's multi_logloss: 0.930987\n",
      "[93]\ttrain's multi_logloss: 0.806602\tvalid's multi_logloss: 0.929572\n",
      "[94]\ttrain's multi_logloss: 0.803971\tvalid's multi_logloss: 0.92979\n",
      "[95]\ttrain's multi_logloss: 0.801444\tvalid's multi_logloss: 0.929382\n",
      "[96]\ttrain's multi_logloss: 0.799356\tvalid's multi_logloss: 0.92613\n",
      "[97]\ttrain's multi_logloss: 0.797306\tvalid's multi_logloss: 0.923664\n",
      "[98]\ttrain's multi_logloss: 0.795032\tvalid's multi_logloss: 0.921486\n",
      "[99]\ttrain's multi_logloss: 0.792918\tvalid's multi_logloss: 0.918429\n",
      "[100]\ttrain's multi_logloss: 0.790827\tvalid's multi_logloss: 0.914947\n",
      "[101]\ttrain's multi_logloss: 0.788659\tvalid's multi_logloss: 0.913942\n",
      "[102]\ttrain's multi_logloss: 0.786464\tvalid's multi_logloss: 0.914599\n",
      "[103]\ttrain's multi_logloss: 0.784083\tvalid's multi_logloss: 0.91371\n",
      "[104]\ttrain's multi_logloss: 0.781911\tvalid's multi_logloss: 0.911415\n",
      "[105]\ttrain's multi_logloss: 0.779581\tvalid's multi_logloss: 0.912378\n",
      "[106]\ttrain's multi_logloss: 0.778018\tvalid's multi_logloss: 0.909314\n",
      "[107]\ttrain's multi_logloss: 0.776643\tvalid's multi_logloss: 0.907431\n",
      "[108]\ttrain's multi_logloss: 0.775278\tvalid's multi_logloss: 0.906365\n",
      "[109]\ttrain's multi_logloss: 0.773631\tvalid's multi_logloss: 0.905776\n",
      "[110]\ttrain's multi_logloss: 0.772126\tvalid's multi_logloss: 0.90364\n",
      "[111]\ttrain's multi_logloss: 0.77049\tvalid's multi_logloss: 0.902747\n",
      "[112]\ttrain's multi_logloss: 0.768759\tvalid's multi_logloss: 0.903698\n",
      "[113]\ttrain's multi_logloss: 0.767073\tvalid's multi_logloss: 0.90046\n",
      "[114]\ttrain's multi_logloss: 0.765987\tvalid's multi_logloss: 0.899818\n",
      "[115]\ttrain's multi_logloss: 0.76421\tvalid's multi_logloss: 0.899497\n",
      "[116]\ttrain's multi_logloss: 0.76279\tvalid's multi_logloss: 0.898644\n",
      "[117]\ttrain's multi_logloss: 0.760985\tvalid's multi_logloss: 0.899455\n",
      "[118]\ttrain's multi_logloss: 0.759361\tvalid's multi_logloss: 0.898952\n",
      "[119]\ttrain's multi_logloss: 0.757747\tvalid's multi_logloss: 0.898071\n",
      "[120]\ttrain's multi_logloss: 0.756716\tvalid's multi_logloss: 0.897354\n",
      "[121]\ttrain's multi_logloss: 0.754851\tvalid's multi_logloss: 0.89529\n",
      "[122]\ttrain's multi_logloss: 0.753319\tvalid's multi_logloss: 0.893421\n",
      "[123]\ttrain's multi_logloss: 0.751872\tvalid's multi_logloss: 0.893065\n",
      "[124]\ttrain's multi_logloss: 0.750432\tvalid's multi_logloss: 0.891291\n",
      "[125]\ttrain's multi_logloss: 0.749069\tvalid's multi_logloss: 0.890283\n",
      "[126]\ttrain's multi_logloss: 0.748111\tvalid's multi_logloss: 0.888393\n",
      "[127]\ttrain's multi_logloss: 0.746845\tvalid's multi_logloss: 0.889716\n",
      "[128]\ttrain's multi_logloss: 0.74531\tvalid's multi_logloss: 0.888207\n",
      "[129]\ttrain's multi_logloss: 0.743676\tvalid's multi_logloss: 0.887121\n",
      "[130]\ttrain's multi_logloss: 0.742072\tvalid's multi_logloss: 0.886061\n",
      "[131]\ttrain's multi_logloss: 0.740593\tvalid's multi_logloss: 0.886804\n",
      "[132]\ttrain's multi_logloss: 0.738012\tvalid's multi_logloss: 0.888204\n",
      "[133]\ttrain's multi_logloss: 0.735195\tvalid's multi_logloss: 0.888232\n",
      "[134]\ttrain's multi_logloss: 0.733031\tvalid's multi_logloss: 0.89021\n",
      "[135]\ttrain's multi_logloss: 0.730595\tvalid's multi_logloss: 0.889195\n",
      "[136]\ttrain's multi_logloss: 0.728641\tvalid's multi_logloss: 0.887166\n",
      "[137]\ttrain's multi_logloss: 0.726855\tvalid's multi_logloss: 0.884653\n",
      "[138]\ttrain's multi_logloss: 0.725214\tvalid's multi_logloss: 0.883705\n",
      "[139]\ttrain's multi_logloss: 0.723322\tvalid's multi_logloss: 0.884011\n",
      "[140]\ttrain's multi_logloss: 0.721528\tvalid's multi_logloss: 0.883823\n",
      "[141]\ttrain's multi_logloss: 0.719992\tvalid's multi_logloss: 0.881427\n",
      "[142]\ttrain's multi_logloss: 0.717903\tvalid's multi_logloss: 0.880967\n",
      "[143]\ttrain's multi_logloss: 0.71629\tvalid's multi_logloss: 0.878648\n",
      "[144]\ttrain's multi_logloss: 0.714901\tvalid's multi_logloss: 0.879131\n",
      "[145]\ttrain's multi_logloss: 0.713559\tvalid's multi_logloss: 0.877208\n",
      "[146]\ttrain's multi_logloss: 0.712215\tvalid's multi_logloss: 0.875299\n",
      "[147]\ttrain's multi_logloss: 0.710401\tvalid's multi_logloss: 0.871586\n",
      "[148]\ttrain's multi_logloss: 0.708621\tvalid's multi_logloss: 0.867919\n",
      "[149]\ttrain's multi_logloss: 0.706744\tvalid's multi_logloss: 0.865238\n",
      "[150]\ttrain's multi_logloss: 0.704943\tvalid's multi_logloss: 0.862795\n",
      "[151]\ttrain's multi_logloss: 0.702967\tvalid's multi_logloss: 0.862578\n",
      "[152]\ttrain's multi_logloss: 0.70165\tvalid's multi_logloss: 0.860637\n",
      "[153]\ttrain's multi_logloss: 0.699589\tvalid's multi_logloss: 0.862259\n",
      "[154]\ttrain's multi_logloss: 0.697838\tvalid's multi_logloss: 0.861264\n",
      "[155]\ttrain's multi_logloss: 0.696038\tvalid's multi_logloss: 0.860794\n",
      "[156]\ttrain's multi_logloss: 0.694836\tvalid's multi_logloss: 0.861479\n",
      "[157]\ttrain's multi_logloss: 0.693295\tvalid's multi_logloss: 0.862986\n",
      "[158]\ttrain's multi_logloss: 0.691544\tvalid's multi_logloss: 0.862077\n",
      "[159]\ttrain's multi_logloss: 0.68973\tvalid's multi_logloss: 0.861831\n",
      "[160]\ttrain's multi_logloss: 0.687595\tvalid's multi_logloss: 0.862123\n",
      "[161]\ttrain's multi_logloss: 0.686345\tvalid's multi_logloss: 0.861306\n",
      "[162]\ttrain's multi_logloss: 0.685342\tvalid's multi_logloss: 0.862009\n",
      "[163]\ttrain's multi_logloss: 0.684395\tvalid's multi_logloss: 0.861574\n",
      "[164]\ttrain's multi_logloss: 0.683176\tvalid's multi_logloss: 0.86021\n",
      "[165]\ttrain's multi_logloss: 0.681875\tvalid's multi_logloss: 0.859502\n",
      "[166]\ttrain's multi_logloss: 0.680314\tvalid's multi_logloss: 0.858341\n",
      "[167]\ttrain's multi_logloss: 0.679091\tvalid's multi_logloss: 0.857389\n",
      "[168]\ttrain's multi_logloss: 0.677707\tvalid's multi_logloss: 0.856485\n",
      "[169]\ttrain's multi_logloss: 0.676443\tvalid's multi_logloss: 0.85493\n",
      "[170]\ttrain's multi_logloss: 0.67523\tvalid's multi_logloss: 0.853488\n",
      "[171]\ttrain's multi_logloss: 0.674024\tvalid's multi_logloss: 0.852478\n",
      "[172]\ttrain's multi_logloss: 0.672743\tvalid's multi_logloss: 0.85237\n",
      "[173]\ttrain's multi_logloss: 0.671435\tvalid's multi_logloss: 0.851837\n",
      "[174]\ttrain's multi_logloss: 0.670202\tvalid's multi_logloss: 0.851009\n",
      "[175]\ttrain's multi_logloss: 0.669146\tvalid's multi_logloss: 0.849804\n",
      "[176]\ttrain's multi_logloss: 0.667442\tvalid's multi_logloss: 0.852015\n",
      "[177]\ttrain's multi_logloss: 0.665755\tvalid's multi_logloss: 0.853529\n",
      "[178]\ttrain's multi_logloss: 0.664164\tvalid's multi_logloss: 0.854646\n",
      "[179]\ttrain's multi_logloss: 0.66272\tvalid's multi_logloss: 0.854494\n",
      "[180]\ttrain's multi_logloss: 0.661037\tvalid's multi_logloss: 0.854051\n",
      "[181]\ttrain's multi_logloss: 0.659362\tvalid's multi_logloss: 0.8547\n",
      "[182]\ttrain's multi_logloss: 0.657506\tvalid's multi_logloss: 0.854764\n",
      "[183]\ttrain's multi_logloss: 0.656113\tvalid's multi_logloss: 0.853871\n",
      "[184]\ttrain's multi_logloss: 0.654604\tvalid's multi_logloss: 0.853881\n",
      "[185]\ttrain's multi_logloss: 0.653052\tvalid's multi_logloss: 0.853414\n",
      "[186]\ttrain's multi_logloss: 0.651139\tvalid's multi_logloss: 0.855136\n",
      "[187]\ttrain's multi_logloss: 0.649211\tvalid's multi_logloss: 0.854679\n",
      "[188]\ttrain's multi_logloss: 0.647787\tvalid's multi_logloss: 0.855057\n",
      "[189]\ttrain's multi_logloss: 0.645744\tvalid's multi_logloss: 0.857553\n",
      "[190]\ttrain's multi_logloss: 0.643941\tvalid's multi_logloss: 0.857796\n",
      "[191]\ttrain's multi_logloss: 0.642412\tvalid's multi_logloss: 0.857605\n",
      "[192]\ttrain's multi_logloss: 0.640954\tvalid's multi_logloss: 0.856525\n",
      "[193]\ttrain's multi_logloss: 0.639605\tvalid's multi_logloss: 0.856186\n",
      "[194]\ttrain's multi_logloss: 0.63828\tvalid's multi_logloss: 0.857473\n",
      "[195]\ttrain's multi_logloss: 0.637132\tvalid's multi_logloss: 0.859158\n",
      "[196]\ttrain's multi_logloss: 0.63571\tvalid's multi_logloss: 0.860061\n",
      "[197]\ttrain's multi_logloss: 0.634264\tvalid's multi_logloss: 0.858483\n",
      "[198]\ttrain's multi_logloss: 0.632848\tvalid's multi_logloss: 0.856951\n",
      "[199]\ttrain's multi_logloss: 0.631533\tvalid's multi_logloss: 0.858085\n",
      "[200]\ttrain's multi_logloss: 0.630113\tvalid's multi_logloss: 0.859135\n",
      "[201]\ttrain's multi_logloss: 0.62812\tvalid's multi_logloss: 0.856396\n",
      "[202]\ttrain's multi_logloss: 0.626085\tvalid's multi_logloss: 0.856171\n",
      "[203]\ttrain's multi_logloss: 0.624272\tvalid's multi_logloss: 0.85586\n",
      "[204]\ttrain's multi_logloss: 0.622288\tvalid's multi_logloss: 0.855216\n",
      "[205]\ttrain's multi_logloss: 0.620049\tvalid's multi_logloss: 0.855155\n",
      "[206]\ttrain's multi_logloss: 0.619205\tvalid's multi_logloss: 0.852736\n",
      "[207]\ttrain's multi_logloss: 0.618093\tvalid's multi_logloss: 0.851086\n",
      "[208]\ttrain's multi_logloss: 0.617009\tvalid's multi_logloss: 0.849229\n",
      "[209]\ttrain's multi_logloss: 0.616191\tvalid's multi_logloss: 0.846573\n",
      "[210]\ttrain's multi_logloss: 0.615149\tvalid's multi_logloss: 0.844452\n",
      "[211]\ttrain's multi_logloss: 0.613961\tvalid's multi_logloss: 0.846238\n",
      "[212]\ttrain's multi_logloss: 0.613119\tvalid's multi_logloss: 0.846036\n",
      "[213]\ttrain's multi_logloss: 0.611875\tvalid's multi_logloss: 0.847038\n",
      "[214]\ttrain's multi_logloss: 0.610851\tvalid's multi_logloss: 0.846392\n",
      "[215]\ttrain's multi_logloss: 0.609676\tvalid's multi_logloss: 0.84752\n",
      "[216]\ttrain's multi_logloss: 0.608262\tvalid's multi_logloss: 0.846541\n",
      "[217]\ttrain's multi_logloss: 0.607105\tvalid's multi_logloss: 0.847795\n",
      "[218]\ttrain's multi_logloss: 0.605701\tvalid's multi_logloss: 0.848082\n",
      "[219]\ttrain's multi_logloss: 0.604563\tvalid's multi_logloss: 0.847217\n",
      "[220]\ttrain's multi_logloss: 0.603393\tvalid's multi_logloss: 0.846099\n",
      "[221]\ttrain's multi_logloss: 0.601757\tvalid's multi_logloss: 0.844865\n",
      "[222]\ttrain's multi_logloss: 0.600252\tvalid's multi_logloss: 0.844329\n",
      "[223]\ttrain's multi_logloss: 0.598912\tvalid's multi_logloss: 0.842175\n",
      "[224]\ttrain's multi_logloss: 0.597659\tvalid's multi_logloss: 0.84011\n",
      "[225]\ttrain's multi_logloss: 0.5962\tvalid's multi_logloss: 0.839464\n",
      "[226]\ttrain's multi_logloss: 0.595158\tvalid's multi_logloss: 0.837501\n",
      "[227]\ttrain's multi_logloss: 0.593955\tvalid's multi_logloss: 0.837376\n",
      "[228]\ttrain's multi_logloss: 0.592663\tvalid's multi_logloss: 0.836764\n",
      "[229]\ttrain's multi_logloss: 0.591373\tvalid's multi_logloss: 0.837068\n",
      "[230]\ttrain's multi_logloss: 0.59029\tvalid's multi_logloss: 0.838037\n",
      "[231]\ttrain's multi_logloss: 0.589117\tvalid's multi_logloss: 0.837463\n",
      "[232]\ttrain's multi_logloss: 0.587819\tvalid's multi_logloss: 0.835322\n",
      "[233]\ttrain's multi_logloss: 0.586641\tvalid's multi_logloss: 0.833367\n",
      "[234]\ttrain's multi_logloss: 0.585077\tvalid's multi_logloss: 0.830983\n",
      "[235]\ttrain's multi_logloss: 0.583939\tvalid's multi_logloss: 0.828729\n",
      "[236]\ttrain's multi_logloss: 0.582559\tvalid's multi_logloss: 0.827274\n",
      "[237]\ttrain's multi_logloss: 0.58108\tvalid's multi_logloss: 0.826453\n",
      "[238]\ttrain's multi_logloss: 0.579748\tvalid's multi_logloss: 0.824975\n",
      "[239]\ttrain's multi_logloss: 0.57828\tvalid's multi_logloss: 0.823523\n",
      "[240]\ttrain's multi_logloss: 0.576964\tvalid's multi_logloss: 0.823252\n",
      "[241]\ttrain's multi_logloss: 0.575778\tvalid's multi_logloss: 0.820172\n",
      "[242]\ttrain's multi_logloss: 0.574848\tvalid's multi_logloss: 0.819189\n",
      "[243]\ttrain's multi_logloss: 0.573669\tvalid's multi_logloss: 0.817206\n",
      "[244]\ttrain's multi_logloss: 0.57236\tvalid's multi_logloss: 0.815489\n",
      "[245]\ttrain's multi_logloss: 0.570885\tvalid's multi_logloss: 0.813675\n",
      "[246]\ttrain's multi_logloss: 0.569662\tvalid's multi_logloss: 0.814559\n",
      "[247]\ttrain's multi_logloss: 0.568158\tvalid's multi_logloss: 0.815597\n",
      "[248]\ttrain's multi_logloss: 0.566581\tvalid's multi_logloss: 0.8159\n",
      "[249]\ttrain's multi_logloss: 0.565146\tvalid's multi_logloss: 0.817568\n",
      "[250]\ttrain's multi_logloss: 0.563711\tvalid's multi_logloss: 0.816429\n",
      "[251]\ttrain's multi_logloss: 0.562366\tvalid's multi_logloss: 0.813704\n",
      "[252]\ttrain's multi_logloss: 0.561202\tvalid's multi_logloss: 0.810519\n",
      "[253]\ttrain's multi_logloss: 0.560015\tvalid's multi_logloss: 0.809945\n",
      "[254]\ttrain's multi_logloss: 0.558728\tvalid's multi_logloss: 0.807683\n",
      "[255]\ttrain's multi_logloss: 0.557394\tvalid's multi_logloss: 0.805582\n",
      "[256]\ttrain's multi_logloss: 0.55669\tvalid's multi_logloss: 0.804973\n",
      "[257]\ttrain's multi_logloss: 0.55549\tvalid's multi_logloss: 0.804141\n",
      "[258]\ttrain's multi_logloss: 0.554683\tvalid's multi_logloss: 0.804622\n",
      "[259]\ttrain's multi_logloss: 0.553891\tvalid's multi_logloss: 0.803231\n",
      "[260]\ttrain's multi_logloss: 0.55332\tvalid's multi_logloss: 0.80239\n",
      "[261]\ttrain's multi_logloss: 0.55248\tvalid's multi_logloss: 0.801276\n",
      "[262]\ttrain's multi_logloss: 0.551927\tvalid's multi_logloss: 0.800701\n",
      "[263]\ttrain's multi_logloss: 0.550967\tvalid's multi_logloss: 0.799036\n",
      "[264]\ttrain's multi_logloss: 0.549924\tvalid's multi_logloss: 0.799759\n",
      "[265]\ttrain's multi_logloss: 0.549144\tvalid's multi_logloss: 0.799198\n",
      "[266]\ttrain's multi_logloss: 0.547812\tvalid's multi_logloss: 0.801277\n",
      "[267]\ttrain's multi_logloss: 0.546835\tvalid's multi_logloss: 0.799938\n",
      "[268]\ttrain's multi_logloss: 0.5455\tvalid's multi_logloss: 0.802392\n",
      "[269]\ttrain's multi_logloss: 0.543935\tvalid's multi_logloss: 0.804165\n",
      "[270]\ttrain's multi_logloss: 0.542945\tvalid's multi_logloss: 0.803299\n",
      "[271]\ttrain's multi_logloss: 0.541617\tvalid's multi_logloss: 0.806094\n",
      "[272]\ttrain's multi_logloss: 0.54048\tvalid's multi_logloss: 0.806117\n",
      "[273]\ttrain's multi_logloss: 0.539262\tvalid's multi_logloss: 0.806612\n",
      "[274]\ttrain's multi_logloss: 0.538115\tvalid's multi_logloss: 0.808028\n",
      "[275]\ttrain's multi_logloss: 0.536839\tvalid's multi_logloss: 0.809444\n",
      "[276]\ttrain's multi_logloss: 0.535568\tvalid's multi_logloss: 0.809823\n",
      "[277]\ttrain's multi_logloss: 0.534078\tvalid's multi_logloss: 0.809876\n",
      "[278]\ttrain's multi_logloss: 0.532624\tvalid's multi_logloss: 0.810733\n",
      "[279]\ttrain's multi_logloss: 0.531753\tvalid's multi_logloss: 0.811935\n",
      "[280]\ttrain's multi_logloss: 0.530809\tvalid's multi_logloss: 0.812362\n",
      "[281]\ttrain's multi_logloss: 0.529547\tvalid's multi_logloss: 0.812116\n",
      "[282]\ttrain's multi_logloss: 0.528407\tvalid's multi_logloss: 0.811802\n",
      "[283]\ttrain's multi_logloss: 0.527622\tvalid's multi_logloss: 0.811062\n",
      "[284]\ttrain's multi_logloss: 0.526186\tvalid's multi_logloss: 0.812252\n",
      "[285]\ttrain's multi_logloss: 0.525428\tvalid's multi_logloss: 0.811962\n",
      "[286]\ttrain's multi_logloss: 0.524436\tvalid's multi_logloss: 0.811578\n",
      "[287]\ttrain's multi_logloss: 0.523276\tvalid's multi_logloss: 0.809134\n",
      "[288]\ttrain's multi_logloss: 0.522411\tvalid's multi_logloss: 0.807954\n",
      "[289]\ttrain's multi_logloss: 0.521172\tvalid's multi_logloss: 0.807684\n",
      "[290]\ttrain's multi_logloss: 0.519775\tvalid's multi_logloss: 0.807967\n",
      "[291]\ttrain's multi_logloss: 0.518619\tvalid's multi_logloss: 0.807025\n",
      "[292]\ttrain's multi_logloss: 0.517507\tvalid's multi_logloss: 0.807601\n",
      "[293]\ttrain's multi_logloss: 0.516425\tvalid's multi_logloss: 0.806777\n",
      "[294]\ttrain's multi_logloss: 0.515539\tvalid's multi_logloss: 0.809228\n",
      "[295]\ttrain's multi_logloss: 0.514421\tvalid's multi_logloss: 0.809514\n",
      "[296]\ttrain's multi_logloss: 0.513885\tvalid's multi_logloss: 0.809029\n",
      "[297]\ttrain's multi_logloss: 0.513398\tvalid's multi_logloss: 0.809403\n",
      "[298]\ttrain's multi_logloss: 0.512667\tvalid's multi_logloss: 0.810148\n",
      "[299]\ttrain's multi_logloss: 0.511912\tvalid's multi_logloss: 0.809922\n",
      "[300]\ttrain's multi_logloss: 0.511336\tvalid's multi_logloss: 0.810007\n",
      "[301]\ttrain's multi_logloss: 0.510671\tvalid's multi_logloss: 0.809165\n",
      "[302]\ttrain's multi_logloss: 0.509923\tvalid's multi_logloss: 0.807855\n",
      "[303]\ttrain's multi_logloss: 0.509129\tvalid's multi_logloss: 0.807443\n",
      "[304]\ttrain's multi_logloss: 0.508439\tvalid's multi_logloss: 0.807719\n",
      "[305]\ttrain's multi_logloss: 0.507888\tvalid's multi_logloss: 0.80671\n",
      "[306]\ttrain's multi_logloss: 0.50695\tvalid's multi_logloss: 0.807493\n",
      "[307]\ttrain's multi_logloss: 0.505853\tvalid's multi_logloss: 0.806761\n",
      "[308]\ttrain's multi_logloss: 0.504364\tvalid's multi_logloss: 0.80613\n",
      "[309]\ttrain's multi_logloss: 0.503548\tvalid's multi_logloss: 0.805652\n",
      "[310]\ttrain's multi_logloss: 0.50234\tvalid's multi_logloss: 0.805251\n",
      "[311]\ttrain's multi_logloss: 0.501371\tvalid's multi_logloss: 0.803675\n",
      "[312]\ttrain's multi_logloss: 0.500695\tvalid's multi_logloss: 0.803008\n",
      "[313]\ttrain's multi_logloss: 0.499579\tvalid's multi_logloss: 0.804156\n",
      "[314]\ttrain's multi_logloss: 0.499109\tvalid's multi_logloss: 0.803671\n",
      "[315]\ttrain's multi_logloss: 0.498257\tvalid's multi_logloss: 0.802699\n",
      "[316]\ttrain's multi_logloss: 0.497417\tvalid's multi_logloss: 0.803097\n",
      "[317]\ttrain's multi_logloss: 0.496685\tvalid's multi_logloss: 0.802862\n",
      "[318]\ttrain's multi_logloss: 0.495926\tvalid's multi_logloss: 0.801655\n",
      "[319]\ttrain's multi_logloss: 0.495079\tvalid's multi_logloss: 0.802604\n",
      "[320]\ttrain's multi_logloss: 0.494356\tvalid's multi_logloss: 0.8025\n",
      "[321]\ttrain's multi_logloss: 0.493358\tvalid's multi_logloss: 0.802692\n",
      "[322]\ttrain's multi_logloss: 0.49228\tvalid's multi_logloss: 0.803156\n",
      "[323]\ttrain's multi_logloss: 0.491257\tvalid's multi_logloss: 0.803018\n",
      "[324]\ttrain's multi_logloss: 0.490103\tvalid's multi_logloss: 0.80324\n",
      "[325]\ttrain's multi_logloss: 0.488886\tvalid's multi_logloss: 0.804312\n",
      "[326]\ttrain's multi_logloss: 0.487976\tvalid's multi_logloss: 0.805299\n",
      "[327]\ttrain's multi_logloss: 0.487222\tvalid's multi_logloss: 0.804923\n",
      "[328]\ttrain's multi_logloss: 0.486487\tvalid's multi_logloss: 0.804814\n",
      "[329]\ttrain's multi_logloss: 0.485846\tvalid's multi_logloss: 0.806267\n",
      "[330]\ttrain's multi_logloss: 0.485077\tvalid's multi_logloss: 0.80469\n",
      "[331]\ttrain's multi_logloss: 0.484156\tvalid's multi_logloss: 0.804989\n",
      "[332]\ttrain's multi_logloss: 0.482871\tvalid's multi_logloss: 0.806639\n",
      "[333]\ttrain's multi_logloss: 0.482152\tvalid's multi_logloss: 0.807463\n",
      "[334]\ttrain's multi_logloss: 0.480995\tvalid's multi_logloss: 0.807798\n",
      "[335]\ttrain's multi_logloss: 0.479822\tvalid's multi_logloss: 0.809886\n",
      "[336]\ttrain's multi_logloss: 0.479286\tvalid's multi_logloss: 0.809077\n",
      "[337]\ttrain's multi_logloss: 0.478577\tvalid's multi_logloss: 0.808756\n",
      "[338]\ttrain's multi_logloss: 0.477967\tvalid's multi_logloss: 0.808254\n",
      "[339]\ttrain's multi_logloss: 0.47747\tvalid's multi_logloss: 0.807918\n",
      "[340]\ttrain's multi_logloss: 0.4769\tvalid's multi_logloss: 0.807101\n",
      "[341]\ttrain's multi_logloss: 0.476022\tvalid's multi_logloss: 0.806145\n",
      "[342]\ttrain's multi_logloss: 0.475252\tvalid's multi_logloss: 0.804915\n",
      "[343]\ttrain's multi_logloss: 0.474438\tvalid's multi_logloss: 0.804288\n",
      "[344]\ttrain's multi_logloss: 0.47366\tvalid's multi_logloss: 0.804618\n",
      "[345]\ttrain's multi_logloss: 0.473002\tvalid's multi_logloss: 0.804798\n",
      "[346]\ttrain's multi_logloss: 0.472391\tvalid's multi_logloss: 0.805505\n",
      "[347]\ttrain's multi_logloss: 0.471636\tvalid's multi_logloss: 0.806465\n",
      "[348]\ttrain's multi_logloss: 0.470962\tvalid's multi_logloss: 0.806393\n",
      "[349]\ttrain's multi_logloss: 0.47021\tvalid's multi_logloss: 0.808174\n",
      "[350]\ttrain's multi_logloss: 0.469459\tvalid's multi_logloss: 0.807734\n",
      "[351]\ttrain's multi_logloss: 0.46838\tvalid's multi_logloss: 0.807579\n",
      "[352]\ttrain's multi_logloss: 0.467528\tvalid's multi_logloss: 0.807176\n",
      "[353]\ttrain's multi_logloss: 0.466627\tvalid's multi_logloss: 0.806784\n",
      "[354]\ttrain's multi_logloss: 0.465981\tvalid's multi_logloss: 0.807466\n",
      "[355]\ttrain's multi_logloss: 0.465154\tvalid's multi_logloss: 0.807023\n",
      "[356]\ttrain's multi_logloss: 0.464435\tvalid's multi_logloss: 0.806302\n",
      "[357]\ttrain's multi_logloss: 0.463482\tvalid's multi_logloss: 0.807528\n",
      "[358]\ttrain's multi_logloss: 0.462653\tvalid's multi_logloss: 0.806887\n",
      "[359]\ttrain's multi_logloss: 0.462026\tvalid's multi_logloss: 0.806252\n",
      "[360]\ttrain's multi_logloss: 0.461167\tvalid's multi_logloss: 0.807909\n",
      "[361]\ttrain's multi_logloss: 0.460052\tvalid's multi_logloss: 0.80745\n",
      "[362]\ttrain's multi_logloss: 0.458586\tvalid's multi_logloss: 0.809048\n",
      "[363]\ttrain's multi_logloss: 0.457668\tvalid's multi_logloss: 0.809128\n",
      "Early stopping, best iteration is:\n",
      "[263]\ttrain's multi_logloss: 0.550967\tvalid's multi_logloss: 0.799036\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09284\tvalid's SFC_loss: 1.09343\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08727\tvalid's SFC_loss: 1.09349\n",
      "[3]\ttrain's SFC_loss: 1.08063\tvalid's SFC_loss: 1.08804\n",
      "[4]\ttrain's SFC_loss: 1.07517\tvalid's SFC_loss: 1.08599\n",
      "[5]\ttrain's SFC_loss: 1.07124\tvalid's SFC_loss: 1.08324\n",
      "[6]\ttrain's SFC_loss: 1.06664\tvalid's SFC_loss: 1.07971\n",
      "[7]\ttrain's SFC_loss: 1.06126\tvalid's SFC_loss: 1.07793\n",
      "[8]\ttrain's SFC_loss: 1.05574\tvalid's SFC_loss: 1.0746\n",
      "[9]\ttrain's SFC_loss: 1.05103\tvalid's SFC_loss: 1.07258\n",
      "[10]\ttrain's SFC_loss: 1.04532\tvalid's SFC_loss: 1.06988\n",
      "[11]\ttrain's SFC_loss: 1.04055\tvalid's SFC_loss: 1.06494\n",
      "[12]\ttrain's SFC_loss: 1.03523\tvalid's SFC_loss: 1.05824\n",
      "[13]\ttrain's SFC_loss: 1.02976\tvalid's SFC_loss: 1.05382\n",
      "[14]\ttrain's SFC_loss: 1.02533\tvalid's SFC_loss: 1.05159\n",
      "[15]\ttrain's SFC_loss: 1.02072\tvalid's SFC_loss: 1.04743\n",
      "[16]\ttrain's SFC_loss: 1.01585\tvalid's SFC_loss: 1.04376\n",
      "[17]\ttrain's SFC_loss: 1.00987\tvalid's SFC_loss: 1.03966\n",
      "[18]\ttrain's SFC_loss: 1.00495\tvalid's SFC_loss: 1.03816\n",
      "[19]\ttrain's SFC_loss: 0.999977\tvalid's SFC_loss: 1.03363\n",
      "[20]\ttrain's SFC_loss: 0.995604\tvalid's SFC_loss: 1.03073\n",
      "[21]\ttrain's SFC_loss: 0.99082\tvalid's SFC_loss: 1.02704\n",
      "[22]\ttrain's SFC_loss: 0.985268\tvalid's SFC_loss: 1.02573\n",
      "[23]\ttrain's SFC_loss: 0.980309\tvalid's SFC_loss: 1.02454\n",
      "[24]\ttrain's SFC_loss: 0.975079\tvalid's SFC_loss: 1.02274\n",
      "[25]\ttrain's SFC_loss: 0.970345\tvalid's SFC_loss: 1.02138\n",
      "[26]\ttrain's SFC_loss: 0.964945\tvalid's SFC_loss: 1.01774\n",
      "[27]\ttrain's SFC_loss: 0.960518\tvalid's SFC_loss: 1.01663\n",
      "[28]\ttrain's SFC_loss: 0.955319\tvalid's SFC_loss: 1.01826\n",
      "[29]\ttrain's SFC_loss: 0.950857\tvalid's SFC_loss: 1.02062\n",
      "[30]\ttrain's SFC_loss: 0.946807\tvalid's SFC_loss: 1.01768\n",
      "[31]\ttrain's SFC_loss: 0.942623\tvalid's SFC_loss: 1.01472\n",
      "[32]\ttrain's SFC_loss: 0.938181\tvalid's SFC_loss: 1.00939\n",
      "[33]\ttrain's SFC_loss: 0.93382\tvalid's SFC_loss: 1.00307\n",
      "[34]\ttrain's SFC_loss: 0.92955\tvalid's SFC_loss: 0.996568\n",
      "[35]\ttrain's SFC_loss: 0.925283\tvalid's SFC_loss: 0.995696\n",
      "[36]\ttrain's SFC_loss: 0.921138\tvalid's SFC_loss: 0.995394\n",
      "[37]\ttrain's SFC_loss: 0.917349\tvalid's SFC_loss: 0.993998\n",
      "[38]\ttrain's SFC_loss: 0.913588\tvalid's SFC_loss: 0.992466\n",
      "[39]\ttrain's SFC_loss: 0.909885\tvalid's SFC_loss: 0.98585\n",
      "[40]\ttrain's SFC_loss: 0.906029\tvalid's SFC_loss: 0.983385\n",
      "[41]\ttrain's SFC_loss: 0.901474\tvalid's SFC_loss: 0.981191\n",
      "[42]\ttrain's SFC_loss: 0.898273\tvalid's SFC_loss: 0.979858\n",
      "[43]\ttrain's SFC_loss: 0.894978\tvalid's SFC_loss: 0.979174\n",
      "[44]\ttrain's SFC_loss: 0.891932\tvalid's SFC_loss: 0.978798\n",
      "[45]\ttrain's SFC_loss: 0.888686\tvalid's SFC_loss: 0.978029\n",
      "[46]\ttrain's SFC_loss: 0.885161\tvalid's SFC_loss: 0.975202\n",
      "[47]\ttrain's SFC_loss: 0.881572\tvalid's SFC_loss: 0.972102\n",
      "[48]\ttrain's SFC_loss: 0.878618\tvalid's SFC_loss: 0.96995\n",
      "[49]\ttrain's SFC_loss: 0.874998\tvalid's SFC_loss: 0.966264\n",
      "[50]\ttrain's SFC_loss: 0.871271\tvalid's SFC_loss: 0.964401\n",
      "[51]\ttrain's SFC_loss: 0.867683\tvalid's SFC_loss: 0.963305\n",
      "[52]\ttrain's SFC_loss: 0.864793\tvalid's SFC_loss: 0.960957\n",
      "[53]\ttrain's SFC_loss: 0.862186\tvalid's SFC_loss: 0.960334\n",
      "[54]\ttrain's SFC_loss: 0.859215\tvalid's SFC_loss: 0.959932\n",
      "[55]\ttrain's SFC_loss: 0.855721\tvalid's SFC_loss: 0.961052\n",
      "[56]\ttrain's SFC_loss: 0.85322\tvalid's SFC_loss: 0.962633\n",
      "[57]\ttrain's SFC_loss: 0.850758\tvalid's SFC_loss: 0.962671\n",
      "[58]\ttrain's SFC_loss: 0.848396\tvalid's SFC_loss: 0.964281\n",
      "[59]\ttrain's SFC_loss: 0.846056\tvalid's SFC_loss: 0.964361\n",
      "[60]\ttrain's SFC_loss: 0.84356\tvalid's SFC_loss: 0.963556\n",
      "[61]\ttrain's SFC_loss: 0.841216\tvalid's SFC_loss: 0.963415\n",
      "[62]\ttrain's SFC_loss: 0.838511\tvalid's SFC_loss: 0.961147\n",
      "[63]\ttrain's SFC_loss: 0.835062\tvalid's SFC_loss: 0.958679\n",
      "[64]\ttrain's SFC_loss: 0.83168\tvalid's SFC_loss: 0.957255\n",
      "[65]\ttrain's SFC_loss: 0.82854\tvalid's SFC_loss: 0.958377\n",
      "[66]\ttrain's SFC_loss: 0.824932\tvalid's SFC_loss: 0.959666\n",
      "[67]\ttrain's SFC_loss: 0.822135\tvalid's SFC_loss: 0.961339\n",
      "[68]\ttrain's SFC_loss: 0.818675\tvalid's SFC_loss: 0.960914\n",
      "[69]\ttrain's SFC_loss: 0.815294\tvalid's SFC_loss: 0.959228\n",
      "[70]\ttrain's SFC_loss: 0.812117\tvalid's SFC_loss: 0.955455\n",
      "[71]\ttrain's SFC_loss: 0.809094\tvalid's SFC_loss: 0.953932\n",
      "[72]\ttrain's SFC_loss: 0.806148\tvalid's SFC_loss: 0.949027\n",
      "[73]\ttrain's SFC_loss: 0.803046\tvalid's SFC_loss: 0.945424\n",
      "[74]\ttrain's SFC_loss: 0.799688\tvalid's SFC_loss: 0.943395\n",
      "[75]\ttrain's SFC_loss: 0.796842\tvalid's SFC_loss: 0.938807\n",
      "[76]\ttrain's SFC_loss: 0.794426\tvalid's SFC_loss: 0.934762\n",
      "[77]\ttrain's SFC_loss: 0.791852\tvalid's SFC_loss: 0.935638\n",
      "[78]\ttrain's SFC_loss: 0.788784\tvalid's SFC_loss: 0.933796\n",
      "[79]\ttrain's SFC_loss: 0.785975\tvalid's SFC_loss: 0.932787\n",
      "[80]\ttrain's SFC_loss: 0.782875\tvalid's SFC_loss: 0.930473\n",
      "[81]\ttrain's SFC_loss: 0.779418\tvalid's SFC_loss: 0.931128\n",
      "[82]\ttrain's SFC_loss: 0.776594\tvalid's SFC_loss: 0.931463\n",
      "[83]\ttrain's SFC_loss: 0.773892\tvalid's SFC_loss: 0.92857\n",
      "[84]\ttrain's SFC_loss: 0.771571\tvalid's SFC_loss: 0.926002\n",
      "[85]\ttrain's SFC_loss: 0.768948\tvalid's SFC_loss: 0.923185\n",
      "[86]\ttrain's SFC_loss: 0.766551\tvalid's SFC_loss: 0.922159\n",
      "[87]\ttrain's SFC_loss: 0.763966\tvalid's SFC_loss: 0.919252\n",
      "[88]\ttrain's SFC_loss: 0.761847\tvalid's SFC_loss: 0.914965\n",
      "[89]\ttrain's SFC_loss: 0.759326\tvalid's SFC_loss: 0.911512\n",
      "[90]\ttrain's SFC_loss: 0.757279\tvalid's SFC_loss: 0.908005\n",
      "[91]\ttrain's SFC_loss: 0.753792\tvalid's SFC_loss: 0.908295\n",
      "[92]\ttrain's SFC_loss: 0.750392\tvalid's SFC_loss: 0.908638\n",
      "[93]\ttrain's SFC_loss: 0.747292\tvalid's SFC_loss: 0.907502\n",
      "[94]\ttrain's SFC_loss: 0.744371\tvalid's SFC_loss: 0.904832\n",
      "[95]\ttrain's SFC_loss: 0.741151\tvalid's SFC_loss: 0.907116\n",
      "[96]\ttrain's SFC_loss: 0.738943\tvalid's SFC_loss: 0.904428\n",
      "[97]\ttrain's SFC_loss: 0.736805\tvalid's SFC_loss: 0.903791\n",
      "[98]\ttrain's SFC_loss: 0.734815\tvalid's SFC_loss: 0.902275\n",
      "[99]\ttrain's SFC_loss: 0.732439\tvalid's SFC_loss: 0.900372\n",
      "[100]\ttrain's SFC_loss: 0.730233\tvalid's SFC_loss: 0.898906\n",
      "[101]\ttrain's SFC_loss: 0.727779\tvalid's SFC_loss: 0.896593\n",
      "[102]\ttrain's SFC_loss: 0.725188\tvalid's SFC_loss: 0.897343\n",
      "[103]\ttrain's SFC_loss: 0.722712\tvalid's SFC_loss: 0.89531\n",
      "[104]\ttrain's SFC_loss: 0.720684\tvalid's SFC_loss: 0.894558\n",
      "[105]\ttrain's SFC_loss: 0.718699\tvalid's SFC_loss: 0.895261\n",
      "[106]\ttrain's SFC_loss: 0.716369\tvalid's SFC_loss: 0.895686\n",
      "[107]\ttrain's SFC_loss: 0.714529\tvalid's SFC_loss: 0.891647\n",
      "[108]\ttrain's SFC_loss: 0.713036\tvalid's SFC_loss: 0.890935\n",
      "[109]\ttrain's SFC_loss: 0.71127\tvalid's SFC_loss: 0.892797\n",
      "[110]\ttrain's SFC_loss: 0.70949\tvalid's SFC_loss: 0.890983\n",
      "[111]\ttrain's SFC_loss: 0.707638\tvalid's SFC_loss: 0.891066\n",
      "[112]\ttrain's SFC_loss: 0.705699\tvalid's SFC_loss: 0.893856\n",
      "[113]\ttrain's SFC_loss: 0.70382\tvalid's SFC_loss: 0.896682\n",
      "[114]\ttrain's SFC_loss: 0.702086\tvalid's SFC_loss: 0.893972\n",
      "[115]\ttrain's SFC_loss: 0.699986\tvalid's SFC_loss: 0.891743\n",
      "[116]\ttrain's SFC_loss: 0.698242\tvalid's SFC_loss: 0.89024\n",
      "[117]\ttrain's SFC_loss: 0.696489\tvalid's SFC_loss: 0.889019\n",
      "[118]\ttrain's SFC_loss: 0.694578\tvalid's SFC_loss: 0.886804\n",
      "[119]\ttrain's SFC_loss: 0.692773\tvalid's SFC_loss: 0.886401\n",
      "[120]\ttrain's SFC_loss: 0.691042\tvalid's SFC_loss: 0.885098\n",
      "[121]\ttrain's SFC_loss: 0.689093\tvalid's SFC_loss: 0.886555\n",
      "[122]\ttrain's SFC_loss: 0.687241\tvalid's SFC_loss: 0.883348\n",
      "[123]\ttrain's SFC_loss: 0.684803\tvalid's SFC_loss: 0.881447\n",
      "[124]\ttrain's SFC_loss: 0.682557\tvalid's SFC_loss: 0.879345\n",
      "[125]\ttrain's SFC_loss: 0.680655\tvalid's SFC_loss: 0.876097\n",
      "[126]\ttrain's SFC_loss: 0.679132\tvalid's SFC_loss: 0.876546\n",
      "[127]\ttrain's SFC_loss: 0.677317\tvalid's SFC_loss: 0.87544\n",
      "[128]\ttrain's SFC_loss: 0.67555\tvalid's SFC_loss: 0.874378\n",
      "[129]\ttrain's SFC_loss: 0.673831\tvalid's SFC_loss: 0.873357\n",
      "[130]\ttrain's SFC_loss: 0.671893\tvalid's SFC_loss: 0.872501\n",
      "[131]\ttrain's SFC_loss: 0.670075\tvalid's SFC_loss: 0.870891\n",
      "[132]\ttrain's SFC_loss: 0.667386\tvalid's SFC_loss: 0.873443\n",
      "[133]\ttrain's SFC_loss: 0.664548\tvalid's SFC_loss: 0.873298\n",
      "[134]\ttrain's SFC_loss: 0.662669\tvalid's SFC_loss: 0.873115\n",
      "[135]\ttrain's SFC_loss: 0.660305\tvalid's SFC_loss: 0.872918\n",
      "[136]\ttrain's SFC_loss: 0.658054\tvalid's SFC_loss: 0.8722\n",
      "[137]\ttrain's SFC_loss: 0.65595\tvalid's SFC_loss: 0.867054\n",
      "[138]\ttrain's SFC_loss: 0.654116\tvalid's SFC_loss: 0.864612\n",
      "[139]\ttrain's SFC_loss: 0.651915\tvalid's SFC_loss: 0.860761\n",
      "[140]\ttrain's SFC_loss: 0.650166\tvalid's SFC_loss: 0.85964\n",
      "[141]\ttrain's SFC_loss: 0.64878\tvalid's SFC_loss: 0.860157\n",
      "[142]\ttrain's SFC_loss: 0.646666\tvalid's SFC_loss: 0.860046\n",
      "[143]\ttrain's SFC_loss: 0.644888\tvalid's SFC_loss: 0.860202\n",
      "[144]\ttrain's SFC_loss: 0.643339\tvalid's SFC_loss: 0.859286\n",
      "[145]\ttrain's SFC_loss: 0.64158\tvalid's SFC_loss: 0.858703\n",
      "[146]\ttrain's SFC_loss: 0.640145\tvalid's SFC_loss: 0.856913\n",
      "[147]\ttrain's SFC_loss: 0.63826\tvalid's SFC_loss: 0.854661\n",
      "[148]\ttrain's SFC_loss: 0.636694\tvalid's SFC_loss: 0.854372\n",
      "[149]\ttrain's SFC_loss: 0.635105\tvalid's SFC_loss: 0.854085\n",
      "[150]\ttrain's SFC_loss: 0.633943\tvalid's SFC_loss: 0.853671\n",
      "[151]\ttrain's SFC_loss: 0.631666\tvalid's SFC_loss: 0.854045\n",
      "[152]\ttrain's SFC_loss: 0.629792\tvalid's SFC_loss: 0.854737\n",
      "[153]\ttrain's SFC_loss: 0.627519\tvalid's SFC_loss: 0.854683\n",
      "[154]\ttrain's SFC_loss: 0.625548\tvalid's SFC_loss: 0.856591\n",
      "[155]\ttrain's SFC_loss: 0.623519\tvalid's SFC_loss: 0.857495\n",
      "[156]\ttrain's SFC_loss: 0.621397\tvalid's SFC_loss: 0.859501\n",
      "[157]\ttrain's SFC_loss: 0.61978\tvalid's SFC_loss: 0.860973\n",
      "[158]\ttrain's SFC_loss: 0.617928\tvalid's SFC_loss: 0.864318\n",
      "[159]\ttrain's SFC_loss: 0.6158\tvalid's SFC_loss: 0.866249\n",
      "[160]\ttrain's SFC_loss: 0.614657\tvalid's SFC_loss: 0.868161\n",
      "[161]\ttrain's SFC_loss: 0.613602\tvalid's SFC_loss: 0.868437\n",
      "[162]\ttrain's SFC_loss: 0.612255\tvalid's SFC_loss: 0.86773\n",
      "[163]\ttrain's SFC_loss: 0.610955\tvalid's SFC_loss: 0.867066\n",
      "[164]\ttrain's SFC_loss: 0.609911\tvalid's SFC_loss: 0.867212\n",
      "[165]\ttrain's SFC_loss: 0.608798\tvalid's SFC_loss: 0.866851\n",
      "[166]\ttrain's SFC_loss: 0.607437\tvalid's SFC_loss: 0.865193\n",
      "[167]\ttrain's SFC_loss: 0.605731\tvalid's SFC_loss: 0.86426\n",
      "[168]\ttrain's SFC_loss: 0.604271\tvalid's SFC_loss: 0.864085\n",
      "[169]\ttrain's SFC_loss: 0.602376\tvalid's SFC_loss: 0.864815\n",
      "[170]\ttrain's SFC_loss: 0.600654\tvalid's SFC_loss: 0.863943\n",
      "[171]\ttrain's SFC_loss: 0.599196\tvalid's SFC_loss: 0.863211\n",
      "[172]\ttrain's SFC_loss: 0.597569\tvalid's SFC_loss: 0.863208\n",
      "[173]\ttrain's SFC_loss: 0.596056\tvalid's SFC_loss: 0.862719\n",
      "[174]\ttrain's SFC_loss: 0.594984\tvalid's SFC_loss: 0.862621\n",
      "[175]\ttrain's SFC_loss: 0.593834\tvalid's SFC_loss: 0.862891\n",
      "[176]\ttrain's SFC_loss: 0.59208\tvalid's SFC_loss: 0.864494\n",
      "[177]\ttrain's SFC_loss: 0.590543\tvalid's SFC_loss: 0.866591\n",
      "[178]\ttrain's SFC_loss: 0.588973\tvalid's SFC_loss: 0.868838\n",
      "[179]\ttrain's SFC_loss: 0.587099\tvalid's SFC_loss: 0.871033\n",
      "[180]\ttrain's SFC_loss: 0.585358\tvalid's SFC_loss: 0.869328\n",
      "[181]\ttrain's SFC_loss: 0.583275\tvalid's SFC_loss: 0.869557\n",
      "[182]\ttrain's SFC_loss: 0.581709\tvalid's SFC_loss: 0.870244\n",
      "[183]\ttrain's SFC_loss: 0.579796\tvalid's SFC_loss: 0.871436\n",
      "[184]\ttrain's SFC_loss: 0.57833\tvalid's SFC_loss: 0.871972\n",
      "[185]\ttrain's SFC_loss: 0.576847\tvalid's SFC_loss: 0.874269\n",
      "[186]\ttrain's SFC_loss: 0.575398\tvalid's SFC_loss: 0.874845\n",
      "[187]\ttrain's SFC_loss: 0.573491\tvalid's SFC_loss: 0.876822\n",
      "[188]\ttrain's SFC_loss: 0.572115\tvalid's SFC_loss: 0.87744\n",
      "[189]\ttrain's SFC_loss: 0.570847\tvalid's SFC_loss: 0.877265\n",
      "[190]\ttrain's SFC_loss: 0.569202\tvalid's SFC_loss: 0.878481\n",
      "[191]\ttrain's SFC_loss: 0.568063\tvalid's SFC_loss: 0.882352\n",
      "[192]\ttrain's SFC_loss: 0.566992\tvalid's SFC_loss: 0.882167\n",
      "[193]\ttrain's SFC_loss: 0.565966\tvalid's SFC_loss: 0.882016\n",
      "[194]\ttrain's SFC_loss: 0.564893\tvalid's SFC_loss: 0.883855\n",
      "[195]\ttrain's SFC_loss: 0.563765\tvalid's SFC_loss: 0.884757\n",
      "[196]\ttrain's SFC_loss: 0.562333\tvalid's SFC_loss: 0.88287\n",
      "[197]\ttrain's SFC_loss: 0.560873\tvalid's SFC_loss: 0.881157\n",
      "[198]\ttrain's SFC_loss: 0.559803\tvalid's SFC_loss: 0.883511\n",
      "[199]\ttrain's SFC_loss: 0.558408\tvalid's SFC_loss: 0.88576\n",
      "[200]\ttrain's SFC_loss: 0.556666\tvalid's SFC_loss: 0.886573\n",
      "[201]\ttrain's SFC_loss: 0.55481\tvalid's SFC_loss: 0.884399\n",
      "[202]\ttrain's SFC_loss: 0.553038\tvalid's SFC_loss: 0.883908\n",
      "[203]\ttrain's SFC_loss: 0.551225\tvalid's SFC_loss: 0.883487\n",
      "[204]\ttrain's SFC_loss: 0.548852\tvalid's SFC_loss: 0.88419\n",
      "[205]\ttrain's SFC_loss: 0.547202\tvalid's SFC_loss: 0.880508\n",
      "[206]\ttrain's SFC_loss: 0.546402\tvalid's SFC_loss: 0.877947\n",
      "[207]\ttrain's SFC_loss: 0.54517\tvalid's SFC_loss: 0.877008\n",
      "[208]\ttrain's SFC_loss: 0.544518\tvalid's SFC_loss: 0.875834\n",
      "[209]\ttrain's SFC_loss: 0.543802\tvalid's SFC_loss: 0.872822\n",
      "[210]\ttrain's SFC_loss: 0.542585\tvalid's SFC_loss: 0.870774\n",
      "[211]\ttrain's SFC_loss: 0.541198\tvalid's SFC_loss: 0.871091\n",
      "[212]\ttrain's SFC_loss: 0.540013\tvalid's SFC_loss: 0.872621\n",
      "[213]\ttrain's SFC_loss: 0.538945\tvalid's SFC_loss: 0.872767\n",
      "[214]\ttrain's SFC_loss: 0.537768\tvalid's SFC_loss: 0.876045\n",
      "[215]\ttrain's SFC_loss: 0.536419\tvalid's SFC_loss: 0.878069\n",
      "[216]\ttrain's SFC_loss: 0.534767\tvalid's SFC_loss: 0.876993\n",
      "[217]\ttrain's SFC_loss: 0.533414\tvalid's SFC_loss: 0.877332\n",
      "[218]\ttrain's SFC_loss: 0.531828\tvalid's SFC_loss: 0.875861\n",
      "[219]\ttrain's SFC_loss: 0.530455\tvalid's SFC_loss: 0.875291\n",
      "[220]\ttrain's SFC_loss: 0.52929\tvalid's SFC_loss: 0.875796\n",
      "[221]\ttrain's SFC_loss: 0.527805\tvalid's SFC_loss: 0.876714\n",
      "[222]\ttrain's SFC_loss: 0.526089\tvalid's SFC_loss: 0.874202\n",
      "[223]\ttrain's SFC_loss: 0.524606\tvalid's SFC_loss: 0.872706\n",
      "[224]\ttrain's SFC_loss: 0.523346\tvalid's SFC_loss: 0.872476\n",
      "[225]\ttrain's SFC_loss: 0.521944\tvalid's SFC_loss: 0.870193\n",
      "[226]\ttrain's SFC_loss: 0.520643\tvalid's SFC_loss: 0.870729\n",
      "[227]\ttrain's SFC_loss: 0.51963\tvalid's SFC_loss: 0.871381\n",
      "[228]\ttrain's SFC_loss: 0.518717\tvalid's SFC_loss: 0.869901\n",
      "[229]\ttrain's SFC_loss: 0.517603\tvalid's SFC_loss: 0.86922\n",
      "[230]\ttrain's SFC_loss: 0.516564\tvalid's SFC_loss: 0.870113\n",
      "[231]\ttrain's SFC_loss: 0.515655\tvalid's SFC_loss: 0.868989\n",
      "[232]\ttrain's SFC_loss: 0.514477\tvalid's SFC_loss: 0.867461\n",
      "[233]\ttrain's SFC_loss: 0.513587\tvalid's SFC_loss: 0.865914\n",
      "[234]\ttrain's SFC_loss: 0.51261\tvalid's SFC_loss: 0.864635\n",
      "[235]\ttrain's SFC_loss: 0.511645\tvalid's SFC_loss: 0.86295\n",
      "[236]\ttrain's SFC_loss: 0.510469\tvalid's SFC_loss: 0.861009\n",
      "[237]\ttrain's SFC_loss: 0.509283\tvalid's SFC_loss: 0.863354\n",
      "[238]\ttrain's SFC_loss: 0.508099\tvalid's SFC_loss: 0.863409\n",
      "[239]\ttrain's SFC_loss: 0.506802\tvalid's SFC_loss: 0.862827\n",
      "[240]\ttrain's SFC_loss: 0.505239\tvalid's SFC_loss: 0.864768\n",
      "[241]\ttrain's SFC_loss: 0.504058\tvalid's SFC_loss: 0.862759\n",
      "[242]\ttrain's SFC_loss: 0.502653\tvalid's SFC_loss: 0.862089\n",
      "[243]\ttrain's SFC_loss: 0.501572\tvalid's SFC_loss: 0.859532\n",
      "[244]\ttrain's SFC_loss: 0.500303\tvalid's SFC_loss: 0.856657\n",
      "[245]\ttrain's SFC_loss: 0.499133\tvalid's SFC_loss: 0.854008\n",
      "[246]\ttrain's SFC_loss: 0.497734\tvalid's SFC_loss: 0.85399\n",
      "[247]\ttrain's SFC_loss: 0.496964\tvalid's SFC_loss: 0.849079\n",
      "[248]\ttrain's SFC_loss: 0.495609\tvalid's SFC_loss: 0.849104\n",
      "[249]\ttrain's SFC_loss: 0.494241\tvalid's SFC_loss: 0.848867\n",
      "[250]\ttrain's SFC_loss: 0.493424\tvalid's SFC_loss: 0.846835\n",
      "[251]\ttrain's SFC_loss: 0.491801\tvalid's SFC_loss: 0.843499\n",
      "[252]\ttrain's SFC_loss: 0.490542\tvalid's SFC_loss: 0.841216\n",
      "[253]\ttrain's SFC_loss: 0.488991\tvalid's SFC_loss: 0.837891\n",
      "[254]\ttrain's SFC_loss: 0.487487\tvalid's SFC_loss: 0.834528\n",
      "[255]\ttrain's SFC_loss: 0.486281\tvalid's SFC_loss: 0.829941\n",
      "[256]\ttrain's SFC_loss: 0.485011\tvalid's SFC_loss: 0.828947\n",
      "[257]\ttrain's SFC_loss: 0.483763\tvalid's SFC_loss: 0.828787\n",
      "[258]\ttrain's SFC_loss: 0.482326\tvalid's SFC_loss: 0.831078\n",
      "[259]\ttrain's SFC_loss: 0.481272\tvalid's SFC_loss: 0.832175\n",
      "[260]\ttrain's SFC_loss: 0.480311\tvalid's SFC_loss: 0.830265\n",
      "[261]\ttrain's SFC_loss: 0.479299\tvalid's SFC_loss: 0.829798\n",
      "[262]\ttrain's SFC_loss: 0.478516\tvalid's SFC_loss: 0.830381\n",
      "[263]\ttrain's SFC_loss: 0.478022\tvalid's SFC_loss: 0.829319\n",
      "[264]\ttrain's SFC_loss: 0.477043\tvalid's SFC_loss: 0.828864\n",
      "[265]\ttrain's SFC_loss: 0.476196\tvalid's SFC_loss: 0.830218\n",
      "[266]\ttrain's SFC_loss: 0.475328\tvalid's SFC_loss: 0.830236\n",
      "[267]\ttrain's SFC_loss: 0.473836\tvalid's SFC_loss: 0.831655\n",
      "[268]\ttrain's SFC_loss: 0.472257\tvalid's SFC_loss: 0.833845\n",
      "[269]\ttrain's SFC_loss: 0.470959\tvalid's SFC_loss: 0.834554\n",
      "[270]\ttrain's SFC_loss: 0.469785\tvalid's SFC_loss: 0.834991\n",
      "[271]\ttrain's SFC_loss: 0.468806\tvalid's SFC_loss: 0.837526\n",
      "[272]\ttrain's SFC_loss: 0.467681\tvalid's SFC_loss: 0.83932\n",
      "[273]\ttrain's SFC_loss: 0.466562\tvalid's SFC_loss: 0.840346\n",
      "[274]\ttrain's SFC_loss: 0.465504\tvalid's SFC_loss: 0.841048\n",
      "[275]\ttrain's SFC_loss: 0.464578\tvalid's SFC_loss: 0.842323\n",
      "[276]\ttrain's SFC_loss: 0.463331\tvalid's SFC_loss: 0.841931\n",
      "[277]\ttrain's SFC_loss: 0.46239\tvalid's SFC_loss: 0.842991\n",
      "[278]\ttrain's SFC_loss: 0.461063\tvalid's SFC_loss: 0.844319\n",
      "[279]\ttrain's SFC_loss: 0.459774\tvalid's SFC_loss: 0.845659\n",
      "[280]\ttrain's SFC_loss: 0.458878\tvalid's SFC_loss: 0.846737\n",
      "[281]\ttrain's SFC_loss: 0.457869\tvalid's SFC_loss: 0.845144\n",
      "[282]\ttrain's SFC_loss: 0.456753\tvalid's SFC_loss: 0.844387\n",
      "[283]\ttrain's SFC_loss: 0.45576\tvalid's SFC_loss: 0.843562\n",
      "[284]\ttrain's SFC_loss: 0.454845\tvalid's SFC_loss: 0.841313\n",
      "[285]\ttrain's SFC_loss: 0.453899\tvalid's SFC_loss: 0.838388\n",
      "[286]\ttrain's SFC_loss: 0.452575\tvalid's SFC_loss: 0.837229\n",
      "[287]\ttrain's SFC_loss: 0.45124\tvalid's SFC_loss: 0.834426\n",
      "[288]\ttrain's SFC_loss: 0.450035\tvalid's SFC_loss: 0.832851\n",
      "[289]\ttrain's SFC_loss: 0.44914\tvalid's SFC_loss: 0.831343\n",
      "[290]\ttrain's SFC_loss: 0.448136\tvalid's SFC_loss: 0.830586\n",
      "[291]\ttrain's SFC_loss: 0.446865\tvalid's SFC_loss: 0.828659\n",
      "[292]\ttrain's SFC_loss: 0.445962\tvalid's SFC_loss: 0.832001\n",
      "[293]\ttrain's SFC_loss: 0.444881\tvalid's SFC_loss: 0.832476\n",
      "[294]\ttrain's SFC_loss: 0.443904\tvalid's SFC_loss: 0.833516\n",
      "[295]\ttrain's SFC_loss: 0.442937\tvalid's SFC_loss: 0.834869\n",
      "[296]\ttrain's SFC_loss: 0.44212\tvalid's SFC_loss: 0.836524\n",
      "[297]\ttrain's SFC_loss: 0.441627\tvalid's SFC_loss: 0.83798\n",
      "[298]\ttrain's SFC_loss: 0.441051\tvalid's SFC_loss: 0.838045\n",
      "[299]\ttrain's SFC_loss: 0.440518\tvalid's SFC_loss: 0.838139\n",
      "[300]\ttrain's SFC_loss: 0.440084\tvalid's SFC_loss: 0.837534\n",
      "[301]\ttrain's SFC_loss: 0.439429\tvalid's SFC_loss: 0.839444\n",
      "[302]\ttrain's SFC_loss: 0.438684\tvalid's SFC_loss: 0.838848\n",
      "[303]\ttrain's SFC_loss: 0.437877\tvalid's SFC_loss: 0.836751\n",
      "[304]\ttrain's SFC_loss: 0.437374\tvalid's SFC_loss: 0.835\n",
      "[305]\ttrain's SFC_loss: 0.436713\tvalid's SFC_loss: 0.833982\n",
      "[306]\ttrain's SFC_loss: 0.435515\tvalid's SFC_loss: 0.835023\n",
      "[307]\ttrain's SFC_loss: 0.434553\tvalid's SFC_loss: 0.835504\n",
      "[308]\ttrain's SFC_loss: 0.433618\tvalid's SFC_loss: 0.836633\n",
      "[309]\ttrain's SFC_loss: 0.432608\tvalid's SFC_loss: 0.83757\n",
      "[310]\ttrain's SFC_loss: 0.431673\tvalid's SFC_loss: 0.839004\n",
      "[311]\ttrain's SFC_loss: 0.430607\tvalid's SFC_loss: 0.837108\n",
      "[312]\ttrain's SFC_loss: 0.429687\tvalid's SFC_loss: 0.836199\n",
      "[313]\ttrain's SFC_loss: 0.428603\tvalid's SFC_loss: 0.838415\n",
      "[314]\ttrain's SFC_loss: 0.427929\tvalid's SFC_loss: 0.837408\n",
      "[315]\ttrain's SFC_loss: 0.427186\tvalid's SFC_loss: 0.838218\n",
      "[316]\ttrain's SFC_loss: 0.426503\tvalid's SFC_loss: 0.836674\n",
      "[317]\ttrain's SFC_loss: 0.426084\tvalid's SFC_loss: 0.837535\n",
      "[318]\ttrain's SFC_loss: 0.425589\tvalid's SFC_loss: 0.839092\n",
      "[319]\ttrain's SFC_loss: 0.425032\tvalid's SFC_loss: 0.840163\n",
      "[320]\ttrain's SFC_loss: 0.423972\tvalid's SFC_loss: 0.840448\n",
      "[321]\ttrain's SFC_loss: 0.422945\tvalid's SFC_loss: 0.840205\n",
      "[322]\ttrain's SFC_loss: 0.422005\tvalid's SFC_loss: 0.840249\n",
      "[323]\ttrain's SFC_loss: 0.421076\tvalid's SFC_loss: 0.838688\n",
      "[324]\ttrain's SFC_loss: 0.420045\tvalid's SFC_loss: 0.83846\n",
      "[325]\ttrain's SFC_loss: 0.419136\tvalid's SFC_loss: 0.839129\n",
      "[326]\ttrain's SFC_loss: 0.418277\tvalid's SFC_loss: 0.83728\n",
      "[327]\ttrain's SFC_loss: 0.41751\tvalid's SFC_loss: 0.839084\n",
      "[328]\ttrain's SFC_loss: 0.416737\tvalid's SFC_loss: 0.840084\n",
      "[329]\ttrain's SFC_loss: 0.416153\tvalid's SFC_loss: 0.838143\n",
      "[330]\ttrain's SFC_loss: 0.415524\tvalid's SFC_loss: 0.837217\n",
      "[331]\ttrain's SFC_loss: 0.414564\tvalid's SFC_loss: 0.840658\n",
      "[332]\ttrain's SFC_loss: 0.413453\tvalid's SFC_loss: 0.8439\n",
      "[333]\ttrain's SFC_loss: 0.412322\tvalid's SFC_loss: 0.846561\n",
      "[334]\ttrain's SFC_loss: 0.411386\tvalid's SFC_loss: 0.846762\n",
      "[335]\ttrain's SFC_loss: 0.41039\tvalid's SFC_loss: 0.848631\n",
      "[336]\ttrain's SFC_loss: 0.409859\tvalid's SFC_loss: 0.847744\n",
      "[337]\ttrain's SFC_loss: 0.409139\tvalid's SFC_loss: 0.847833\n",
      "[338]\ttrain's SFC_loss: 0.408627\tvalid's SFC_loss: 0.84799\n",
      "[339]\ttrain's SFC_loss: 0.408094\tvalid's SFC_loss: 0.845465\n",
      "[340]\ttrain's SFC_loss: 0.407517\tvalid's SFC_loss: 0.84377\n",
      "[341]\ttrain's SFC_loss: 0.406462\tvalid's SFC_loss: 0.843926\n",
      "[342]\ttrain's SFC_loss: 0.405623\tvalid's SFC_loss: 0.844256\n",
      "[343]\ttrain's SFC_loss: 0.404916\tvalid's SFC_loss: 0.845837\n",
      "[344]\ttrain's SFC_loss: 0.40402\tvalid's SFC_loss: 0.844827\n",
      "[345]\ttrain's SFC_loss: 0.403204\tvalid's SFC_loss: 0.843872\n",
      "[346]\ttrain's SFC_loss: 0.40242\tvalid's SFC_loss: 0.847031\n",
      "[347]\ttrain's SFC_loss: 0.401626\tvalid's SFC_loss: 0.847826\n",
      "[348]\ttrain's SFC_loss: 0.401082\tvalid's SFC_loss: 0.849133\n",
      "[349]\ttrain's SFC_loss: 0.400335\tvalid's SFC_loss: 0.849937\n",
      "[350]\ttrain's SFC_loss: 0.399641\tvalid's SFC_loss: 0.850746\n",
      "[351]\ttrain's SFC_loss: 0.398733\tvalid's SFC_loss: 0.851062\n",
      "[352]\ttrain's SFC_loss: 0.398038\tvalid's SFC_loss: 0.850182\n",
      "[353]\ttrain's SFC_loss: 0.397325\tvalid's SFC_loss: 0.850507\n",
      "[354]\ttrain's SFC_loss: 0.39628\tvalid's SFC_loss: 0.851327\n",
      "[355]\ttrain's SFC_loss: 0.395455\tvalid's SFC_loss: 0.852449\n",
      "[356]\ttrain's SFC_loss: 0.394445\tvalid's SFC_loss: 0.854078\n",
      "[357]\ttrain's SFC_loss: 0.39376\tvalid's SFC_loss: 0.852829\n",
      "[358]\ttrain's SFC_loss: 0.392585\tvalid's SFC_loss: 0.855287\n",
      "[359]\ttrain's SFC_loss: 0.391502\tvalid's SFC_loss: 0.857133\n",
      "[360]\ttrain's SFC_loss: 0.390933\tvalid's SFC_loss: 0.85689\n",
      "[361]\ttrain's SFC_loss: 0.38974\tvalid's SFC_loss: 0.860066\n",
      "[362]\ttrain's SFC_loss: 0.388568\tvalid's SFC_loss: 0.862495\n",
      "[363]\ttrain's SFC_loss: 0.387284\tvalid's SFC_loss: 0.86451\n",
      "[364]\ttrain's SFC_loss: 0.386023\tvalid's SFC_loss: 0.86799\n",
      "[365]\ttrain's SFC_loss: 0.384838\tvalid's SFC_loss: 0.870548\n",
      "[366]\ttrain's SFC_loss: 0.384303\tvalid's SFC_loss: 0.870417\n",
      "[367]\ttrain's SFC_loss: 0.383704\tvalid's SFC_loss: 0.871368\n",
      "[368]\ttrain's SFC_loss: 0.383156\tvalid's SFC_loss: 0.868963\n",
      "[369]\ttrain's SFC_loss: 0.382486\tvalid's SFC_loss: 0.869834\n",
      "[370]\ttrain's SFC_loss: 0.382125\tvalid's SFC_loss: 0.869615\n",
      "[371]\ttrain's SFC_loss: 0.38136\tvalid's SFC_loss: 0.869285\n",
      "[372]\ttrain's SFC_loss: 0.380566\tvalid's SFC_loss: 0.869651\n",
      "[373]\ttrain's SFC_loss: 0.379852\tvalid's SFC_loss: 0.869867\n",
      "[374]\ttrain's SFC_loss: 0.379198\tvalid's SFC_loss: 0.870104\n",
      "[375]\ttrain's SFC_loss: 0.378397\tvalid's SFC_loss: 0.869065\n",
      "[376]\ttrain's SFC_loss: 0.377803\tvalid's SFC_loss: 0.865796\n",
      "[377]\ttrain's SFC_loss: 0.37725\tvalid's SFC_loss: 0.864182\n",
      "[378]\ttrain's SFC_loss: 0.376787\tvalid's SFC_loss: 0.862292\n",
      "[379]\ttrain's SFC_loss: 0.37632\tvalid's SFC_loss: 0.862294\n",
      "[380]\ttrain's SFC_loss: 0.375809\tvalid's SFC_loss: 0.861041\n",
      "[381]\ttrain's SFC_loss: 0.375229\tvalid's SFC_loss: 0.860958\n",
      "[382]\ttrain's SFC_loss: 0.374535\tvalid's SFC_loss: 0.860087\n",
      "[383]\ttrain's SFC_loss: 0.374054\tvalid's SFC_loss: 0.859022\n",
      "[384]\ttrain's SFC_loss: 0.373456\tvalid's SFC_loss: 0.857648\n",
      "[385]\ttrain's SFC_loss: 0.372777\tvalid's SFC_loss: 0.859211\n",
      "[386]\ttrain's SFC_loss: 0.371862\tvalid's SFC_loss: 0.859857\n",
      "[387]\ttrain's SFC_loss: 0.371043\tvalid's SFC_loss: 0.858999\n",
      "[388]\ttrain's SFC_loss: 0.370467\tvalid's SFC_loss: 0.855375\n",
      "[389]\ttrain's SFC_loss: 0.369828\tvalid's SFC_loss: 0.853565\n",
      "[390]\ttrain's SFC_loss: 0.368938\tvalid's SFC_loss: 0.852786\n",
      "[391]\ttrain's SFC_loss: 0.367706\tvalid's SFC_loss: 0.853588\n",
      "Early stopping, best iteration is:\n",
      "[291]\ttrain's SFC_loss: 0.446865\tvalid's SFC_loss: 0.828659\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1   -0.053565\n",
      "1    feature2   -0.006929\n",
      "2    feature3   -0.133639\n",
      "3    feature4    0.057493\n",
      "4    feature5    0.052356\n",
      "5    feature6   -0.071021\n",
      "6    feature7    0.106547\n",
      "7    feature8    0.023593\n",
      "8    feature9   -0.107456\n",
      "9   feature10    0.018156\n",
      "10  feature11   -0.009806\n",
      "11  feature12    0.006138\n",
      "12  feature13    0.060847\n",
      "13  feature14    0.083900\n",
      "14  feature15   -0.078373\n",
      "15  feature16    0.018088\n",
      "16  feature17    0.049800\n",
      "17  feature18   -0.039210\n",
      "18  feature19    0.012922\n",
      "19  feature20    0.010160\n",
      "-------------------- 5 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05847\tvalid's multi_logloss: 1.06404\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05472\tvalid's multi_logloss: 1.06186\n",
      "[3]\ttrain's multi_logloss: 1.05146\tvalid's multi_logloss: 1.05897\n",
      "[4]\ttrain's multi_logloss: 1.04874\tvalid's multi_logloss: 1.05643\n",
      "[5]\ttrain's multi_logloss: 1.04555\tvalid's multi_logloss: 1.05458\n",
      "[6]\ttrain's multi_logloss: 1.04144\tvalid's multi_logloss: 1.05272\n",
      "[7]\ttrain's multi_logloss: 1.03839\tvalid's multi_logloss: 1.05022\n",
      "[8]\ttrain's multi_logloss: 1.0347\tvalid's multi_logloss: 1.04724\n",
      "[9]\ttrain's multi_logloss: 1.03137\tvalid's multi_logloss: 1.04753\n",
      "[10]\ttrain's multi_logloss: 1.02855\tvalid's multi_logloss: 1.04499\n",
      "[11]\ttrain's multi_logloss: 1.0258\tvalid's multi_logloss: 1.04421\n",
      "[12]\ttrain's multi_logloss: 1.02202\tvalid's multi_logloss: 1.04354\n",
      "[13]\ttrain's multi_logloss: 1.019\tvalid's multi_logloss: 1.04442\n",
      "[14]\ttrain's multi_logloss: 1.01611\tvalid's multi_logloss: 1.04099\n",
      "[15]\ttrain's multi_logloss: 1.01348\tvalid's multi_logloss: 1.0387\n",
      "[16]\ttrain's multi_logloss: 1.00957\tvalid's multi_logloss: 1.03502\n",
      "[17]\ttrain's multi_logloss: 1.00589\tvalid's multi_logloss: 1.02952\n",
      "[18]\ttrain's multi_logloss: 1.00264\tvalid's multi_logloss: 1.02659\n",
      "[19]\ttrain's multi_logloss: 0.99891\tvalid's multi_logloss: 1.02634\n",
      "[20]\ttrain's multi_logloss: 0.995881\tvalid's multi_logloss: 1.02111\n",
      "[21]\ttrain's multi_logloss: 0.992276\tvalid's multi_logloss: 1.01628\n",
      "[22]\ttrain's multi_logloss: 0.988749\tvalid's multi_logloss: 1.01166\n",
      "[23]\ttrain's multi_logloss: 0.985828\tvalid's multi_logloss: 1.00716\n",
      "[24]\ttrain's multi_logloss: 0.982945\tvalid's multi_logloss: 1.00372\n",
      "[25]\ttrain's multi_logloss: 0.980112\tvalid's multi_logloss: 1.00011\n",
      "[26]\ttrain's multi_logloss: 0.976967\tvalid's multi_logloss: 0.997939\n",
      "[27]\ttrain's multi_logloss: 0.973886\tvalid's multi_logloss: 0.997317\n",
      "[28]\ttrain's multi_logloss: 0.971157\tvalid's multi_logloss: 0.994276\n",
      "[29]\ttrain's multi_logloss: 0.968285\tvalid's multi_logloss: 0.990294\n",
      "[30]\ttrain's multi_logloss: 0.965156\tvalid's multi_logloss: 0.986443\n",
      "[31]\ttrain's multi_logloss: 0.962612\tvalid's multi_logloss: 0.984306\n",
      "[32]\ttrain's multi_logloss: 0.959383\tvalid's multi_logloss: 0.982266\n",
      "[33]\ttrain's multi_logloss: 0.955731\tvalid's multi_logloss: 0.981331\n",
      "[34]\ttrain's multi_logloss: 0.952226\tvalid's multi_logloss: 0.97954\n",
      "[35]\ttrain's multi_logloss: 0.949736\tvalid's multi_logloss: 0.977429\n",
      "[36]\ttrain's multi_logloss: 0.947502\tvalid's multi_logloss: 0.975362\n",
      "[37]\ttrain's multi_logloss: 0.94474\tvalid's multi_logloss: 0.973772\n",
      "[38]\ttrain's multi_logloss: 0.941934\tvalid's multi_logloss: 0.971467\n",
      "[39]\ttrain's multi_logloss: 0.939269\tvalid's multi_logloss: 0.969217\n",
      "[40]\ttrain's multi_logloss: 0.936502\tvalid's multi_logloss: 0.967133\n",
      "[41]\ttrain's multi_logloss: 0.933967\tvalid's multi_logloss: 0.964497\n",
      "[42]\ttrain's multi_logloss: 0.931344\tvalid's multi_logloss: 0.963533\n",
      "[43]\ttrain's multi_logloss: 0.929447\tvalid's multi_logloss: 0.960392\n",
      "[44]\ttrain's multi_logloss: 0.926669\tvalid's multi_logloss: 0.959424\n",
      "[45]\ttrain's multi_logloss: 0.924233\tvalid's multi_logloss: 0.959451\n",
      "[46]\ttrain's multi_logloss: 0.921768\tvalid's multi_logloss: 0.958822\n",
      "[47]\ttrain's multi_logloss: 0.919161\tvalid's multi_logloss: 0.956989\n",
      "[48]\ttrain's multi_logloss: 0.916448\tvalid's multi_logloss: 0.955702\n",
      "[49]\ttrain's multi_logloss: 0.914242\tvalid's multi_logloss: 0.955486\n",
      "[50]\ttrain's multi_logloss: 0.911944\tvalid's multi_logloss: 0.954808\n",
      "[51]\ttrain's multi_logloss: 0.90951\tvalid's multi_logloss: 0.954005\n",
      "[52]\ttrain's multi_logloss: 0.907575\tvalid's multi_logloss: 0.953557\n",
      "[53]\ttrain's multi_logloss: 0.90487\tvalid's multi_logloss: 0.949684\n",
      "[54]\ttrain's multi_logloss: 0.902663\tvalid's multi_logloss: 0.948457\n",
      "[55]\ttrain's multi_logloss: 0.90022\tvalid's multi_logloss: 0.947269\n",
      "[56]\ttrain's multi_logloss: 0.898849\tvalid's multi_logloss: 0.946801\n",
      "[57]\ttrain's multi_logloss: 0.896775\tvalid's multi_logloss: 0.943995\n",
      "[58]\ttrain's multi_logloss: 0.894481\tvalid's multi_logloss: 0.940098\n",
      "[59]\ttrain's multi_logloss: 0.892316\tvalid's multi_logloss: 0.936362\n",
      "[60]\ttrain's multi_logloss: 0.889929\tvalid's multi_logloss: 0.93275\n",
      "[61]\ttrain's multi_logloss: 0.886245\tvalid's multi_logloss: 0.933126\n",
      "[62]\ttrain's multi_logloss: 0.88342\tvalid's multi_logloss: 0.931011\n",
      "[63]\ttrain's multi_logloss: 0.880922\tvalid's multi_logloss: 0.93087\n",
      "[64]\ttrain's multi_logloss: 0.87821\tvalid's multi_logloss: 0.929654\n",
      "[65]\ttrain's multi_logloss: 0.875768\tvalid's multi_logloss: 0.926272\n",
      "[66]\ttrain's multi_logloss: 0.87234\tvalid's multi_logloss: 0.927263\n",
      "[67]\ttrain's multi_logloss: 0.869121\tvalid's multi_logloss: 0.927922\n",
      "[68]\ttrain's multi_logloss: 0.866298\tvalid's multi_logloss: 0.927133\n",
      "[69]\ttrain's multi_logloss: 0.863366\tvalid's multi_logloss: 0.926334\n",
      "[70]\ttrain's multi_logloss: 0.86065\tvalid's multi_logloss: 0.926147\n",
      "[71]\ttrain's multi_logloss: 0.858698\tvalid's multi_logloss: 0.923134\n",
      "[72]\ttrain's multi_logloss: 0.856482\tvalid's multi_logloss: 0.920359\n",
      "[73]\ttrain's multi_logloss: 0.853918\tvalid's multi_logloss: 0.918308\n",
      "[74]\ttrain's multi_logloss: 0.851657\tvalid's multi_logloss: 0.915721\n",
      "[75]\ttrain's multi_logloss: 0.850347\tvalid's multi_logloss: 0.913881\n",
      "[76]\ttrain's multi_logloss: 0.848146\tvalid's multi_logloss: 0.913181\n",
      "[77]\ttrain's multi_logloss: 0.845904\tvalid's multi_logloss: 0.91365\n",
      "[78]\ttrain's multi_logloss: 0.843712\tvalid's multi_logloss: 0.910561\n",
      "[79]\ttrain's multi_logloss: 0.841519\tvalid's multi_logloss: 0.909852\n",
      "[80]\ttrain's multi_logloss: 0.839384\tvalid's multi_logloss: 0.908455\n",
      "[81]\ttrain's multi_logloss: 0.837792\tvalid's multi_logloss: 0.90802\n",
      "[82]\ttrain's multi_logloss: 0.835761\tvalid's multi_logloss: 0.909514\n",
      "[83]\ttrain's multi_logloss: 0.833158\tvalid's multi_logloss: 0.910026\n",
      "[84]\ttrain's multi_logloss: 0.830631\tvalid's multi_logloss: 0.909624\n",
      "[85]\ttrain's multi_logloss: 0.828168\tvalid's multi_logloss: 0.908619\n",
      "[86]\ttrain's multi_logloss: 0.825686\tvalid's multi_logloss: 0.908572\n",
      "[87]\ttrain's multi_logloss: 0.823602\tvalid's multi_logloss: 0.909301\n",
      "[88]\ttrain's multi_logloss: 0.820985\tvalid's multi_logloss: 0.907941\n",
      "[89]\ttrain's multi_logloss: 0.818346\tvalid's multi_logloss: 0.907723\n",
      "[90]\ttrain's multi_logloss: 0.816268\tvalid's multi_logloss: 0.908001\n",
      "[91]\ttrain's multi_logloss: 0.814337\tvalid's multi_logloss: 0.905337\n",
      "[92]\ttrain's multi_logloss: 0.812177\tvalid's multi_logloss: 0.903854\n",
      "[93]\ttrain's multi_logloss: 0.810144\tvalid's multi_logloss: 0.903151\n",
      "[94]\ttrain's multi_logloss: 0.808273\tvalid's multi_logloss: 0.902429\n",
      "[95]\ttrain's multi_logloss: 0.805914\tvalid's multi_logloss: 0.901178\n",
      "[96]\ttrain's multi_logloss: 0.803038\tvalid's multi_logloss: 0.898\n",
      "[97]\ttrain's multi_logloss: 0.800508\tvalid's multi_logloss: 0.895442\n",
      "[98]\ttrain's multi_logloss: 0.798009\tvalid's multi_logloss: 0.893742\n",
      "[99]\ttrain's multi_logloss: 0.795966\tvalid's multi_logloss: 0.891354\n",
      "[100]\ttrain's multi_logloss: 0.794026\tvalid's multi_logloss: 0.891838\n",
      "[101]\ttrain's multi_logloss: 0.791178\tvalid's multi_logloss: 0.891886\n",
      "[102]\ttrain's multi_logloss: 0.788714\tvalid's multi_logloss: 0.888907\n",
      "[103]\ttrain's multi_logloss: 0.786563\tvalid's multi_logloss: 0.885196\n",
      "[104]\ttrain's multi_logloss: 0.784235\tvalid's multi_logloss: 0.886701\n",
      "[105]\ttrain's multi_logloss: 0.781559\tvalid's multi_logloss: 0.883736\n",
      "[106]\ttrain's multi_logloss: 0.780173\tvalid's multi_logloss: 0.882456\n",
      "[107]\ttrain's multi_logloss: 0.778925\tvalid's multi_logloss: 0.881897\n",
      "[108]\ttrain's multi_logloss: 0.777223\tvalid's multi_logloss: 0.880347\n",
      "[109]\ttrain's multi_logloss: 0.775039\tvalid's multi_logloss: 0.880372\n",
      "[110]\ttrain's multi_logloss: 0.773614\tvalid's multi_logloss: 0.879037\n",
      "[111]\ttrain's multi_logloss: 0.772328\tvalid's multi_logloss: 0.877205\n",
      "[112]\ttrain's multi_logloss: 0.771026\tvalid's multi_logloss: 0.875818\n",
      "[113]\ttrain's multi_logloss: 0.769247\tvalid's multi_logloss: 0.875864\n",
      "[114]\ttrain's multi_logloss: 0.767919\tvalid's multi_logloss: 0.873995\n",
      "[115]\ttrain's multi_logloss: 0.76596\tvalid's multi_logloss: 0.872652\n",
      "[116]\ttrain's multi_logloss: 0.763989\tvalid's multi_logloss: 0.869583\n",
      "[117]\ttrain's multi_logloss: 0.76166\tvalid's multi_logloss: 0.868507\n",
      "[118]\ttrain's multi_logloss: 0.759549\tvalid's multi_logloss: 0.868882\n",
      "[119]\ttrain's multi_logloss: 0.757364\tvalid's multi_logloss: 0.869101\n",
      "[120]\ttrain's multi_logloss: 0.755267\tvalid's multi_logloss: 0.869102\n",
      "[121]\ttrain's multi_logloss: 0.753052\tvalid's multi_logloss: 0.867478\n",
      "[122]\ttrain's multi_logloss: 0.75144\tvalid's multi_logloss: 0.867427\n",
      "[123]\ttrain's multi_logloss: 0.749465\tvalid's multi_logloss: 0.864826\n",
      "[124]\ttrain's multi_logloss: 0.747371\tvalid's multi_logloss: 0.863655\n",
      "[125]\ttrain's multi_logloss: 0.745109\tvalid's multi_logloss: 0.86192\n",
      "[126]\ttrain's multi_logloss: 0.743693\tvalid's multi_logloss: 0.861007\n",
      "[127]\ttrain's multi_logloss: 0.742533\tvalid's multi_logloss: 0.859748\n",
      "[128]\ttrain's multi_logloss: 0.741009\tvalid's multi_logloss: 0.859615\n",
      "[129]\ttrain's multi_logloss: 0.740525\tvalid's multi_logloss: 0.858088\n",
      "[130]\ttrain's multi_logloss: 0.739611\tvalid's multi_logloss: 0.859181\n",
      "[131]\ttrain's multi_logloss: 0.738166\tvalid's multi_logloss: 0.857214\n",
      "[132]\ttrain's multi_logloss: 0.73692\tvalid's multi_logloss: 0.855083\n",
      "[133]\ttrain's multi_logloss: 0.735263\tvalid's multi_logloss: 0.85239\n",
      "[134]\ttrain's multi_logloss: 0.733806\tvalid's multi_logloss: 0.850668\n",
      "[135]\ttrain's multi_logloss: 0.732139\tvalid's multi_logloss: 0.848329\n",
      "[136]\ttrain's multi_logloss: 0.730429\tvalid's multi_logloss: 0.84734\n",
      "[137]\ttrain's multi_logloss: 0.728411\tvalid's multi_logloss: 0.846267\n",
      "[138]\ttrain's multi_logloss: 0.726915\tvalid's multi_logloss: 0.844404\n",
      "[139]\ttrain's multi_logloss: 0.725032\tvalid's multi_logloss: 0.840839\n",
      "[140]\ttrain's multi_logloss: 0.723201\tvalid's multi_logloss: 0.83833\n",
      "[141]\ttrain's multi_logloss: 0.721589\tvalid's multi_logloss: 0.837248\n",
      "[142]\ttrain's multi_logloss: 0.719621\tvalid's multi_logloss: 0.836514\n",
      "[143]\ttrain's multi_logloss: 0.718178\tvalid's multi_logloss: 0.834982\n",
      "[144]\ttrain's multi_logloss: 0.716507\tvalid's multi_logloss: 0.833303\n",
      "[145]\ttrain's multi_logloss: 0.715013\tvalid's multi_logloss: 0.832653\n",
      "[146]\ttrain's multi_logloss: 0.712976\tvalid's multi_logloss: 0.829595\n",
      "[147]\ttrain's multi_logloss: 0.710323\tvalid's multi_logloss: 0.829781\n",
      "[148]\ttrain's multi_logloss: 0.708009\tvalid's multi_logloss: 0.830227\n",
      "[149]\ttrain's multi_logloss: 0.705787\tvalid's multi_logloss: 0.830713\n",
      "[150]\ttrain's multi_logloss: 0.703079\tvalid's multi_logloss: 0.831779\n",
      "[151]\ttrain's multi_logloss: 0.701233\tvalid's multi_logloss: 0.831939\n",
      "[152]\ttrain's multi_logloss: 0.699812\tvalid's multi_logloss: 0.831982\n",
      "[153]\ttrain's multi_logloss: 0.698067\tvalid's multi_logloss: 0.832364\n",
      "[154]\ttrain's multi_logloss: 0.697065\tvalid's multi_logloss: 0.832858\n",
      "[155]\ttrain's multi_logloss: 0.694795\tvalid's multi_logloss: 0.83404\n",
      "[156]\ttrain's multi_logloss: 0.693412\tvalid's multi_logloss: 0.83492\n",
      "[157]\ttrain's multi_logloss: 0.691645\tvalid's multi_logloss: 0.833526\n",
      "[158]\ttrain's multi_logloss: 0.690117\tvalid's multi_logloss: 0.832541\n",
      "[159]\ttrain's multi_logloss: 0.688325\tvalid's multi_logloss: 0.828971\n",
      "[160]\ttrain's multi_logloss: 0.686768\tvalid's multi_logloss: 0.826554\n",
      "[161]\ttrain's multi_logloss: 0.68502\tvalid's multi_logloss: 0.825557\n",
      "[162]\ttrain's multi_logloss: 0.683399\tvalid's multi_logloss: 0.825587\n",
      "[163]\ttrain's multi_logloss: 0.681861\tvalid's multi_logloss: 0.825571\n",
      "[164]\ttrain's multi_logloss: 0.680204\tvalid's multi_logloss: 0.825071\n",
      "[165]\ttrain's multi_logloss: 0.678644\tvalid's multi_logloss: 0.82503\n",
      "[166]\ttrain's multi_logloss: 0.677009\tvalid's multi_logloss: 0.825592\n",
      "[167]\ttrain's multi_logloss: 0.675155\tvalid's multi_logloss: 0.826767\n",
      "[168]\ttrain's multi_logloss: 0.673439\tvalid's multi_logloss: 0.827169\n",
      "[169]\ttrain's multi_logloss: 0.671762\tvalid's multi_logloss: 0.828378\n",
      "[170]\ttrain's multi_logloss: 0.670364\tvalid's multi_logloss: 0.829894\n",
      "[171]\ttrain's multi_logloss: 0.669118\tvalid's multi_logloss: 0.830861\n",
      "[172]\ttrain's multi_logloss: 0.667626\tvalid's multi_logloss: 0.830694\n",
      "[173]\ttrain's multi_logloss: 0.666434\tvalid's multi_logloss: 0.831699\n",
      "[174]\ttrain's multi_logloss: 0.665381\tvalid's multi_logloss: 0.833006\n",
      "[175]\ttrain's multi_logloss: 0.664247\tvalid's multi_logloss: 0.834052\n",
      "[176]\ttrain's multi_logloss: 0.662842\tvalid's multi_logloss: 0.835299\n",
      "[177]\ttrain's multi_logloss: 0.661277\tvalid's multi_logloss: 0.833424\n",
      "[178]\ttrain's multi_logloss: 0.659916\tvalid's multi_logloss: 0.833291\n",
      "[179]\ttrain's multi_logloss: 0.658867\tvalid's multi_logloss: 0.834263\n",
      "[180]\ttrain's multi_logloss: 0.657574\tvalid's multi_logloss: 0.836166\n",
      "[181]\ttrain's multi_logloss: 0.65607\tvalid's multi_logloss: 0.835115\n",
      "[182]\ttrain's multi_logloss: 0.654555\tvalid's multi_logloss: 0.833416\n",
      "[183]\ttrain's multi_logloss: 0.652694\tvalid's multi_logloss: 0.833533\n",
      "[184]\ttrain's multi_logloss: 0.650784\tvalid's multi_logloss: 0.83318\n",
      "[185]\ttrain's multi_logloss: 0.649075\tvalid's multi_logloss: 0.834685\n",
      "[186]\ttrain's multi_logloss: 0.64752\tvalid's multi_logloss: 0.832772\n",
      "[187]\ttrain's multi_logloss: 0.645945\tvalid's multi_logloss: 0.829876\n",
      "[188]\ttrain's multi_logloss: 0.644627\tvalid's multi_logloss: 0.829998\n",
      "[189]\ttrain's multi_logloss: 0.642842\tvalid's multi_logloss: 0.828158\n",
      "[190]\ttrain's multi_logloss: 0.641213\tvalid's multi_logloss: 0.825882\n",
      "[191]\ttrain's multi_logloss: 0.639673\tvalid's multi_logloss: 0.826264\n",
      "[192]\ttrain's multi_logloss: 0.638181\tvalid's multi_logloss: 0.827971\n",
      "[193]\ttrain's multi_logloss: 0.636657\tvalid's multi_logloss: 0.82644\n",
      "[194]\ttrain's multi_logloss: 0.635182\tvalid's multi_logloss: 0.826407\n",
      "[195]\ttrain's multi_logloss: 0.633619\tvalid's multi_logloss: 0.826473\n",
      "[196]\ttrain's multi_logloss: 0.632142\tvalid's multi_logloss: 0.824107\n",
      "[197]\ttrain's multi_logloss: 0.630743\tvalid's multi_logloss: 0.822267\n",
      "[198]\ttrain's multi_logloss: 0.629311\tvalid's multi_logloss: 0.820526\n",
      "[199]\ttrain's multi_logloss: 0.62802\tvalid's multi_logloss: 0.819704\n",
      "[200]\ttrain's multi_logloss: 0.626442\tvalid's multi_logloss: 0.817876\n",
      "[201]\ttrain's multi_logloss: 0.624429\tvalid's multi_logloss: 0.817982\n",
      "[202]\ttrain's multi_logloss: 0.622549\tvalid's multi_logloss: 0.81834\n",
      "[203]\ttrain's multi_logloss: 0.620445\tvalid's multi_logloss: 0.819087\n",
      "[204]\ttrain's multi_logloss: 0.618315\tvalid's multi_logloss: 0.81766\n",
      "[205]\ttrain's multi_logloss: 0.616088\tvalid's multi_logloss: 0.816598\n",
      "[206]\ttrain's multi_logloss: 0.614821\tvalid's multi_logloss: 0.816732\n",
      "[207]\ttrain's multi_logloss: 0.613523\tvalid's multi_logloss: 0.816481\n",
      "[208]\ttrain's multi_logloss: 0.612333\tvalid's multi_logloss: 0.819114\n",
      "[209]\ttrain's multi_logloss: 0.611333\tvalid's multi_logloss: 0.818903\n",
      "[210]\ttrain's multi_logloss: 0.610053\tvalid's multi_logloss: 0.818935\n",
      "[211]\ttrain's multi_logloss: 0.608963\tvalid's multi_logloss: 0.820167\n",
      "[212]\ttrain's multi_logloss: 0.607858\tvalid's multi_logloss: 0.820101\n",
      "[213]\ttrain's multi_logloss: 0.606782\tvalid's multi_logloss: 0.819662\n",
      "[214]\ttrain's multi_logloss: 0.605936\tvalid's multi_logloss: 0.820579\n",
      "[215]\ttrain's multi_logloss: 0.60496\tvalid's multi_logloss: 0.820913\n",
      "[216]\ttrain's multi_logloss: 0.603694\tvalid's multi_logloss: 0.820663\n",
      "[217]\ttrain's multi_logloss: 0.602445\tvalid's multi_logloss: 0.819969\n",
      "[218]\ttrain's multi_logloss: 0.600791\tvalid's multi_logloss: 0.819443\n",
      "[219]\ttrain's multi_logloss: 0.599629\tvalid's multi_logloss: 0.819864\n",
      "[220]\ttrain's multi_logloss: 0.597775\tvalid's multi_logloss: 0.821676\n",
      "[221]\ttrain's multi_logloss: 0.596338\tvalid's multi_logloss: 0.821283\n",
      "[222]\ttrain's multi_logloss: 0.594858\tvalid's multi_logloss: 0.82035\n",
      "[223]\ttrain's multi_logloss: 0.593392\tvalid's multi_logloss: 0.819477\n",
      "[224]\ttrain's multi_logloss: 0.592111\tvalid's multi_logloss: 0.820128\n",
      "[225]\ttrain's multi_logloss: 0.590726\tvalid's multi_logloss: 0.820206\n",
      "[226]\ttrain's multi_logloss: 0.589406\tvalid's multi_logloss: 0.818885\n",
      "[227]\ttrain's multi_logloss: 0.588273\tvalid's multi_logloss: 0.817238\n",
      "[228]\ttrain's multi_logloss: 0.587108\tvalid's multi_logloss: 0.816295\n",
      "[229]\ttrain's multi_logloss: 0.585925\tvalid's multi_logloss: 0.814474\n",
      "[230]\ttrain's multi_logloss: 0.584631\tvalid's multi_logloss: 0.813402\n",
      "[231]\ttrain's multi_logloss: 0.583566\tvalid's multi_logloss: 0.812309\n",
      "[232]\ttrain's multi_logloss: 0.582528\tvalid's multi_logloss: 0.811247\n",
      "[233]\ttrain's multi_logloss: 0.581514\tvalid's multi_logloss: 0.810216\n",
      "[234]\ttrain's multi_logloss: 0.580544\tvalid's multi_logloss: 0.809891\n",
      "[235]\ttrain's multi_logloss: 0.579943\tvalid's multi_logloss: 0.809456\n",
      "[236]\ttrain's multi_logloss: 0.57886\tvalid's multi_logloss: 0.80975\n",
      "[237]\ttrain's multi_logloss: 0.577582\tvalid's multi_logloss: 0.809893\n",
      "[238]\ttrain's multi_logloss: 0.576476\tvalid's multi_logloss: 0.809491\n",
      "[239]\ttrain's multi_logloss: 0.575206\tvalid's multi_logloss: 0.80833\n",
      "[240]\ttrain's multi_logloss: 0.57378\tvalid's multi_logloss: 0.806772\n",
      "[241]\ttrain's multi_logloss: 0.572366\tvalid's multi_logloss: 0.806799\n",
      "[242]\ttrain's multi_logloss: 0.571391\tvalid's multi_logloss: 0.804732\n",
      "[243]\ttrain's multi_logloss: 0.570509\tvalid's multi_logloss: 0.803281\n",
      "[244]\ttrain's multi_logloss: 0.569515\tvalid's multi_logloss: 0.801362\n",
      "[245]\ttrain's multi_logloss: 0.568234\tvalid's multi_logloss: 0.800051\n",
      "[246]\ttrain's multi_logloss: 0.566762\tvalid's multi_logloss: 0.801768\n",
      "[247]\ttrain's multi_logloss: 0.565547\tvalid's multi_logloss: 0.802941\n",
      "[248]\ttrain's multi_logloss: 0.564\tvalid's multi_logloss: 0.8046\n",
      "[249]\ttrain's multi_logloss: 0.562823\tvalid's multi_logloss: 0.805266\n",
      "[250]\ttrain's multi_logloss: 0.561424\tvalid's multi_logloss: 0.806297\n",
      "[251]\ttrain's multi_logloss: 0.560161\tvalid's multi_logloss: 0.806419\n",
      "[252]\ttrain's multi_logloss: 0.55876\tvalid's multi_logloss: 0.807434\n",
      "[253]\ttrain's multi_logloss: 0.557646\tvalid's multi_logloss: 0.807546\n",
      "[254]\ttrain's multi_logloss: 0.556558\tvalid's multi_logloss: 0.807671\n",
      "[255]\ttrain's multi_logloss: 0.555447\tvalid's multi_logloss: 0.807409\n",
      "[256]\ttrain's multi_logloss: 0.554427\tvalid's multi_logloss: 0.806949\n",
      "[257]\ttrain's multi_logloss: 0.553355\tvalid's multi_logloss: 0.806271\n",
      "[258]\ttrain's multi_logloss: 0.552385\tvalid's multi_logloss: 0.806219\n",
      "[259]\ttrain's multi_logloss: 0.551337\tvalid's multi_logloss: 0.806262\n",
      "[260]\ttrain's multi_logloss: 0.550091\tvalid's multi_logloss: 0.803874\n",
      "[261]\ttrain's multi_logloss: 0.548749\tvalid's multi_logloss: 0.801131\n",
      "[262]\ttrain's multi_logloss: 0.548009\tvalid's multi_logloss: 0.799523\n",
      "[263]\ttrain's multi_logloss: 0.547249\tvalid's multi_logloss: 0.798899\n",
      "[264]\ttrain's multi_logloss: 0.546254\tvalid's multi_logloss: 0.797063\n",
      "[265]\ttrain's multi_logloss: 0.545295\tvalid's multi_logloss: 0.794631\n",
      "[266]\ttrain's multi_logloss: 0.544073\tvalid's multi_logloss: 0.793765\n",
      "[267]\ttrain's multi_logloss: 0.542852\tvalid's multi_logloss: 0.793273\n",
      "[268]\ttrain's multi_logloss: 0.541377\tvalid's multi_logloss: 0.792833\n",
      "[269]\ttrain's multi_logloss: 0.540234\tvalid's multi_logloss: 0.793241\n",
      "[270]\ttrain's multi_logloss: 0.539261\tvalid's multi_logloss: 0.792564\n",
      "[271]\ttrain's multi_logloss: 0.538479\tvalid's multi_logloss: 0.792565\n",
      "[272]\ttrain's multi_logloss: 0.53765\tvalid's multi_logloss: 0.791894\n",
      "[273]\ttrain's multi_logloss: 0.536757\tvalid's multi_logloss: 0.79199\n",
      "[274]\ttrain's multi_logloss: 0.535885\tvalid's multi_logloss: 0.792097\n",
      "[275]\ttrain's multi_logloss: 0.534919\tvalid's multi_logloss: 0.790966\n",
      "[276]\ttrain's multi_logloss: 0.533549\tvalid's multi_logloss: 0.791798\n",
      "[277]\ttrain's multi_logloss: 0.532297\tvalid's multi_logloss: 0.791705\n",
      "[278]\ttrain's multi_logloss: 0.531162\tvalid's multi_logloss: 0.792531\n",
      "[279]\ttrain's multi_logloss: 0.529834\tvalid's multi_logloss: 0.791601\n",
      "[280]\ttrain's multi_logloss: 0.528502\tvalid's multi_logloss: 0.790956\n",
      "[281]\ttrain's multi_logloss: 0.52738\tvalid's multi_logloss: 0.791468\n",
      "[282]\ttrain's multi_logloss: 0.52637\tvalid's multi_logloss: 0.789321\n",
      "[283]\ttrain's multi_logloss: 0.525471\tvalid's multi_logloss: 0.787491\n",
      "[284]\ttrain's multi_logloss: 0.524388\tvalid's multi_logloss: 0.785311\n",
      "[285]\ttrain's multi_logloss: 0.523516\tvalid's multi_logloss: 0.784036\n",
      "[286]\ttrain's multi_logloss: 0.522614\tvalid's multi_logloss: 0.783845\n",
      "[287]\ttrain's multi_logloss: 0.521564\tvalid's multi_logloss: 0.784107\n",
      "[288]\ttrain's multi_logloss: 0.520526\tvalid's multi_logloss: 0.78334\n",
      "[289]\ttrain's multi_logloss: 0.519317\tvalid's multi_logloss: 0.783653\n",
      "[290]\ttrain's multi_logloss: 0.51849\tvalid's multi_logloss: 0.783298\n",
      "[291]\ttrain's multi_logloss: 0.51731\tvalid's multi_logloss: 0.784348\n",
      "[292]\ttrain's multi_logloss: 0.516483\tvalid's multi_logloss: 0.785204\n",
      "[293]\ttrain's multi_logloss: 0.515574\tvalid's multi_logloss: 0.786308\n",
      "[294]\ttrain's multi_logloss: 0.514338\tvalid's multi_logloss: 0.787991\n",
      "[295]\ttrain's multi_logloss: 0.513247\tvalid's multi_logloss: 0.789059\n",
      "[296]\ttrain's multi_logloss: 0.512496\tvalid's multi_logloss: 0.790182\n",
      "[297]\ttrain's multi_logloss: 0.511659\tvalid's multi_logloss: 0.790346\n",
      "[298]\ttrain's multi_logloss: 0.510772\tvalid's multi_logloss: 0.790333\n",
      "[299]\ttrain's multi_logloss: 0.509745\tvalid's multi_logloss: 0.790339\n",
      "[300]\ttrain's multi_logloss: 0.509097\tvalid's multi_logloss: 0.790615\n",
      "[301]\ttrain's multi_logloss: 0.508583\tvalid's multi_logloss: 0.790412\n",
      "[302]\ttrain's multi_logloss: 0.507462\tvalid's multi_logloss: 0.790463\n",
      "[303]\ttrain's multi_logloss: 0.506674\tvalid's multi_logloss: 0.79185\n",
      "[304]\ttrain's multi_logloss: 0.505702\tvalid's multi_logloss: 0.793289\n",
      "[305]\ttrain's multi_logloss: 0.505117\tvalid's multi_logloss: 0.794139\n",
      "[306]\ttrain's multi_logloss: 0.504226\tvalid's multi_logloss: 0.795021\n",
      "[307]\ttrain's multi_logloss: 0.503175\tvalid's multi_logloss: 0.796904\n",
      "[308]\ttrain's multi_logloss: 0.502147\tvalid's multi_logloss: 0.798784\n",
      "[309]\ttrain's multi_logloss: 0.501132\tvalid's multi_logloss: 0.799226\n",
      "[310]\ttrain's multi_logloss: 0.500177\tvalid's multi_logloss: 0.800599\n",
      "[311]\ttrain's multi_logloss: 0.499251\tvalid's multi_logloss: 0.801381\n",
      "[312]\ttrain's multi_logloss: 0.498424\tvalid's multi_logloss: 0.803138\n",
      "[313]\ttrain's multi_logloss: 0.497488\tvalid's multi_logloss: 0.804078\n",
      "[314]\ttrain's multi_logloss: 0.496607\tvalid's multi_logloss: 0.805518\n",
      "[315]\ttrain's multi_logloss: 0.495646\tvalid's multi_logloss: 0.80631\n",
      "[316]\ttrain's multi_logloss: 0.494325\tvalid's multi_logloss: 0.806919\n",
      "[317]\ttrain's multi_logloss: 0.493155\tvalid's multi_logloss: 0.80739\n",
      "[318]\ttrain's multi_logloss: 0.492524\tvalid's multi_logloss: 0.807388\n",
      "[319]\ttrain's multi_logloss: 0.491261\tvalid's multi_logloss: 0.808034\n",
      "[320]\ttrain's multi_logloss: 0.490129\tvalid's multi_logloss: 0.809415\n",
      "[321]\ttrain's multi_logloss: 0.488944\tvalid's multi_logloss: 0.808952\n",
      "[322]\ttrain's multi_logloss: 0.488241\tvalid's multi_logloss: 0.80817\n",
      "[323]\ttrain's multi_logloss: 0.487313\tvalid's multi_logloss: 0.8059\n",
      "[324]\ttrain's multi_logloss: 0.486293\tvalid's multi_logloss: 0.805224\n",
      "[325]\ttrain's multi_logloss: 0.485499\tvalid's multi_logloss: 0.804407\n",
      "[326]\ttrain's multi_logloss: 0.484409\tvalid's multi_logloss: 0.804924\n",
      "[327]\ttrain's multi_logloss: 0.483735\tvalid's multi_logloss: 0.805677\n",
      "[328]\ttrain's multi_logloss: 0.482784\tvalid's multi_logloss: 0.803502\n",
      "[329]\ttrain's multi_logloss: 0.481913\tvalid's multi_logloss: 0.802494\n",
      "[330]\ttrain's multi_logloss: 0.48103\tvalid's multi_logloss: 0.801789\n",
      "[331]\ttrain's multi_logloss: 0.479784\tvalid's multi_logloss: 0.802122\n",
      "[332]\ttrain's multi_logloss: 0.4787\tvalid's multi_logloss: 0.803113\n",
      "[333]\ttrain's multi_logloss: 0.4778\tvalid's multi_logloss: 0.804285\n",
      "[334]\ttrain's multi_logloss: 0.47679\tvalid's multi_logloss: 0.804779\n",
      "[335]\ttrain's multi_logloss: 0.4758\tvalid's multi_logloss: 0.805729\n",
      "[336]\ttrain's multi_logloss: 0.474888\tvalid's multi_logloss: 0.806171\n",
      "[337]\ttrain's multi_logloss: 0.474061\tvalid's multi_logloss: 0.80873\n",
      "[338]\ttrain's multi_logloss: 0.473167\tvalid's multi_logloss: 0.811278\n",
      "[339]\ttrain's multi_logloss: 0.472701\tvalid's multi_logloss: 0.812518\n",
      "[340]\ttrain's multi_logloss: 0.472172\tvalid's multi_logloss: 0.814223\n",
      "[341]\ttrain's multi_logloss: 0.471316\tvalid's multi_logloss: 0.814796\n",
      "[342]\ttrain's multi_logloss: 0.47061\tvalid's multi_logloss: 0.814146\n",
      "[343]\ttrain's multi_logloss: 0.470101\tvalid's multi_logloss: 0.814322\n",
      "[344]\ttrain's multi_logloss: 0.469241\tvalid's multi_logloss: 0.815663\n",
      "[345]\ttrain's multi_logloss: 0.468771\tvalid's multi_logloss: 0.815463\n",
      "[346]\ttrain's multi_logloss: 0.467945\tvalid's multi_logloss: 0.814642\n",
      "[347]\ttrain's multi_logloss: 0.467353\tvalid's multi_logloss: 0.814443\n",
      "[348]\ttrain's multi_logloss: 0.466728\tvalid's multi_logloss: 0.814627\n",
      "[349]\ttrain's multi_logloss: 0.466234\tvalid's multi_logloss: 0.814697\n",
      "[350]\ttrain's multi_logloss: 0.465675\tvalid's multi_logloss: 0.815409\n",
      "[351]\ttrain's multi_logloss: 0.464875\tvalid's multi_logloss: 0.813039\n",
      "[352]\ttrain's multi_logloss: 0.464182\tvalid's multi_logloss: 0.813879\n",
      "[353]\ttrain's multi_logloss: 0.463326\tvalid's multi_logloss: 0.812439\n",
      "[354]\ttrain's multi_logloss: 0.462627\tvalid's multi_logloss: 0.81035\n",
      "[355]\ttrain's multi_logloss: 0.461818\tvalid's multi_logloss: 0.811042\n",
      "[356]\ttrain's multi_logloss: 0.460969\tvalid's multi_logloss: 0.810551\n",
      "[357]\ttrain's multi_logloss: 0.460059\tvalid's multi_logloss: 0.810445\n",
      "[358]\ttrain's multi_logloss: 0.459248\tvalid's multi_logloss: 0.810734\n",
      "[359]\ttrain's multi_logloss: 0.458253\tvalid's multi_logloss: 0.811164\n",
      "[360]\ttrain's multi_logloss: 0.457403\tvalid's multi_logloss: 0.811277\n",
      "[361]\ttrain's multi_logloss: 0.456071\tvalid's multi_logloss: 0.810655\n",
      "[362]\ttrain's multi_logloss: 0.454722\tvalid's multi_logloss: 0.810342\n",
      "[363]\ttrain's multi_logloss: 0.453339\tvalid's multi_logloss: 0.810832\n",
      "[364]\ttrain's multi_logloss: 0.45192\tvalid's multi_logloss: 0.810908\n",
      "[365]\ttrain's multi_logloss: 0.450673\tvalid's multi_logloss: 0.809252\n",
      "[366]\ttrain's multi_logloss: 0.450081\tvalid's multi_logloss: 0.810298\n",
      "[367]\ttrain's multi_logloss: 0.449507\tvalid's multi_logloss: 0.811357\n",
      "[368]\ttrain's multi_logloss: 0.448766\tvalid's multi_logloss: 0.812113\n",
      "[369]\ttrain's multi_logloss: 0.448141\tvalid's multi_logloss: 0.81216\n",
      "[370]\ttrain's multi_logloss: 0.447577\tvalid's multi_logloss: 0.812365\n",
      "[371]\ttrain's multi_logloss: 0.447061\tvalid's multi_logloss: 0.812754\n",
      "[372]\ttrain's multi_logloss: 0.446424\tvalid's multi_logloss: 0.812797\n",
      "[373]\ttrain's multi_logloss: 0.445815\tvalid's multi_logloss: 0.812754\n",
      "[374]\ttrain's multi_logloss: 0.445078\tvalid's multi_logloss: 0.813397\n",
      "[375]\ttrain's multi_logloss: 0.444409\tvalid's multi_logloss: 0.815118\n",
      "[376]\ttrain's multi_logloss: 0.443749\tvalid's multi_logloss: 0.815095\n",
      "[377]\ttrain's multi_logloss: 0.44315\tvalid's multi_logloss: 0.816055\n",
      "[378]\ttrain's multi_logloss: 0.442467\tvalid's multi_logloss: 0.816615\n",
      "[379]\ttrain's multi_logloss: 0.441682\tvalid's multi_logloss: 0.816627\n",
      "[380]\ttrain's multi_logloss: 0.441021\tvalid's multi_logloss: 0.8172\n",
      "[381]\ttrain's multi_logloss: 0.440229\tvalid's multi_logloss: 0.817326\n",
      "[382]\ttrain's multi_logloss: 0.439429\tvalid's multi_logloss: 0.818743\n",
      "[383]\ttrain's multi_logloss: 0.438862\tvalid's multi_logloss: 0.820843\n",
      "[384]\ttrain's multi_logloss: 0.438077\tvalid's multi_logloss: 0.822057\n",
      "[385]\ttrain's multi_logloss: 0.437185\tvalid's multi_logloss: 0.822818\n",
      "[386]\ttrain's multi_logloss: 0.436428\tvalid's multi_logloss: 0.822326\n",
      "[387]\ttrain's multi_logloss: 0.435407\tvalid's multi_logloss: 0.822515\n",
      "[388]\ttrain's multi_logloss: 0.43459\tvalid's multi_logloss: 0.821024\n",
      "[389]\ttrain's multi_logloss: 0.433858\tvalid's multi_logloss: 0.820014\n",
      "[390]\ttrain's multi_logloss: 0.43287\tvalid's multi_logloss: 0.820239\n",
      "Early stopping, best iteration is:\n",
      "[290]\ttrain's multi_logloss: 0.51849\tvalid's multi_logloss: 0.783298\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09331\tvalid's SFC_loss: 1.09668\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08901\tvalid's SFC_loss: 1.09358\n",
      "[3]\ttrain's SFC_loss: 1.08257\tvalid's SFC_loss: 1.08825\n",
      "[4]\ttrain's SFC_loss: 1.07744\tvalid's SFC_loss: 1.08479\n",
      "[5]\ttrain's SFC_loss: 1.07293\tvalid's SFC_loss: 1.08469\n",
      "[6]\ttrain's SFC_loss: 1.06623\tvalid's SFC_loss: 1.08408\n",
      "[7]\ttrain's SFC_loss: 1.06123\tvalid's SFC_loss: 1.07789\n",
      "[8]\ttrain's SFC_loss: 1.05489\tvalid's SFC_loss: 1.07385\n",
      "[9]\ttrain's SFC_loss: 1.04982\tvalid's SFC_loss: 1.06962\n",
      "[10]\ttrain's SFC_loss: 1.04369\tvalid's SFC_loss: 1.06547\n",
      "[11]\ttrain's SFC_loss: 1.03831\tvalid's SFC_loss: 1.06475\n",
      "[12]\ttrain's SFC_loss: 1.03343\tvalid's SFC_loss: 1.06358\n",
      "[13]\ttrain's SFC_loss: 1.0287\tvalid's SFC_loss: 1.06251\n",
      "[14]\ttrain's SFC_loss: 1.02392\tvalid's SFC_loss: 1.05888\n",
      "[15]\ttrain's SFC_loss: 1.01972\tvalid's SFC_loss: 1.05652\n",
      "[16]\ttrain's SFC_loss: 1.01443\tvalid's SFC_loss: 1.0507\n",
      "[17]\ttrain's SFC_loss: 1.0092\tvalid's SFC_loss: 1.04438\n",
      "[18]\ttrain's SFC_loss: 1.00416\tvalid's SFC_loss: 1.03869\n",
      "[19]\ttrain's SFC_loss: 0.999325\tvalid's SFC_loss: 1.03418\n",
      "[20]\ttrain's SFC_loss: 0.994606\tvalid's SFC_loss: 1.02859\n",
      "[21]\ttrain's SFC_loss: 0.989267\tvalid's SFC_loss: 1.02437\n",
      "[22]\ttrain's SFC_loss: 0.983245\tvalid's SFC_loss: 1.01948\n",
      "[23]\ttrain's SFC_loss: 0.978466\tvalid's SFC_loss: 1.01389\n",
      "[24]\ttrain's SFC_loss: 0.974305\tvalid's SFC_loss: 1.00623\n",
      "[25]\ttrain's SFC_loss: 0.96883\tvalid's SFC_loss: 0.99947\n",
      "[26]\ttrain's SFC_loss: 0.964047\tvalid's SFC_loss: 0.995671\n",
      "[27]\ttrain's SFC_loss: 0.95922\tvalid's SFC_loss: 0.992393\n",
      "[28]\ttrain's SFC_loss: 0.95472\tvalid's SFC_loss: 0.988639\n",
      "[29]\ttrain's SFC_loss: 0.950619\tvalid's SFC_loss: 0.985544\n",
      "[30]\ttrain's SFC_loss: 0.945919\tvalid's SFC_loss: 0.979934\n",
      "[31]\ttrain's SFC_loss: 0.941616\tvalid's SFC_loss: 0.978826\n",
      "[32]\ttrain's SFC_loss: 0.936742\tvalid's SFC_loss: 0.97641\n",
      "[33]\ttrain's SFC_loss: 0.932343\tvalid's SFC_loss: 0.974757\n",
      "[34]\ttrain's SFC_loss: 0.928014\tvalid's SFC_loss: 0.973696\n",
      "[35]\ttrain's SFC_loss: 0.924191\tvalid's SFC_loss: 0.972722\n",
      "[36]\ttrain's SFC_loss: 0.920358\tvalid's SFC_loss: 0.968724\n",
      "[37]\ttrain's SFC_loss: 0.91643\tvalid's SFC_loss: 0.964866\n",
      "[38]\ttrain's SFC_loss: 0.912526\tvalid's SFC_loss: 0.960809\n",
      "[39]\ttrain's SFC_loss: 0.909115\tvalid's SFC_loss: 0.957014\n",
      "[40]\ttrain's SFC_loss: 0.904859\tvalid's SFC_loss: 0.957469\n",
      "[41]\ttrain's SFC_loss: 0.900913\tvalid's SFC_loss: 0.956635\n",
      "[42]\ttrain's SFC_loss: 0.897962\tvalid's SFC_loss: 0.956643\n",
      "[43]\ttrain's SFC_loss: 0.895543\tvalid's SFC_loss: 0.953434\n",
      "[44]\ttrain's SFC_loss: 0.892334\tvalid's SFC_loss: 0.952738\n",
      "[45]\ttrain's SFC_loss: 0.888836\tvalid's SFC_loss: 0.951551\n",
      "[46]\ttrain's SFC_loss: 0.885291\tvalid's SFC_loss: 0.947275\n",
      "[47]\ttrain's SFC_loss: 0.881702\tvalid's SFC_loss: 0.944985\n",
      "[48]\ttrain's SFC_loss: 0.878358\tvalid's SFC_loss: 0.943736\n",
      "[49]\ttrain's SFC_loss: 0.874852\tvalid's SFC_loss: 0.942857\n",
      "[50]\ttrain's SFC_loss: 0.871909\tvalid's SFC_loss: 0.942377\n",
      "[51]\ttrain's SFC_loss: 0.869158\tvalid's SFC_loss: 0.9414\n",
      "[52]\ttrain's SFC_loss: 0.865995\tvalid's SFC_loss: 0.940717\n",
      "[53]\ttrain's SFC_loss: 0.862959\tvalid's SFC_loss: 0.937164\n",
      "[54]\ttrain's SFC_loss: 0.860724\tvalid's SFC_loss: 0.935724\n",
      "[55]\ttrain's SFC_loss: 0.857833\tvalid's SFC_loss: 0.934663\n",
      "[56]\ttrain's SFC_loss: 0.854912\tvalid's SFC_loss: 0.930373\n",
      "[57]\ttrain's SFC_loss: 0.852184\tvalid's SFC_loss: 0.927187\n",
      "[58]\ttrain's SFC_loss: 0.849391\tvalid's SFC_loss: 0.920323\n",
      "[59]\ttrain's SFC_loss: 0.846378\tvalid's SFC_loss: 0.91577\n",
      "[60]\ttrain's SFC_loss: 0.844009\tvalid's SFC_loss: 0.911488\n",
      "[61]\ttrain's SFC_loss: 0.840028\tvalid's SFC_loss: 0.909976\n",
      "[62]\ttrain's SFC_loss: 0.837042\tvalid's SFC_loss: 0.907731\n",
      "[63]\ttrain's SFC_loss: 0.832909\tvalid's SFC_loss: 0.908432\n",
      "[64]\ttrain's SFC_loss: 0.829931\tvalid's SFC_loss: 0.905307\n",
      "[65]\ttrain's SFC_loss: 0.82578\tvalid's SFC_loss: 0.904911\n",
      "[66]\ttrain's SFC_loss: 0.822451\tvalid's SFC_loss: 0.902966\n",
      "[67]\ttrain's SFC_loss: 0.818689\tvalid's SFC_loss: 0.903656\n",
      "[68]\ttrain's SFC_loss: 0.815\tvalid's SFC_loss: 0.902969\n",
      "[69]\ttrain's SFC_loss: 0.8112\tvalid's SFC_loss: 0.904019\n",
      "[70]\ttrain's SFC_loss: 0.807768\tvalid's SFC_loss: 0.905594\n",
      "[71]\ttrain's SFC_loss: 0.805356\tvalid's SFC_loss: 0.902352\n",
      "[72]\ttrain's SFC_loss: 0.802408\tvalid's SFC_loss: 0.899438\n",
      "[73]\ttrain's SFC_loss: 0.800421\tvalid's SFC_loss: 0.896398\n",
      "[74]\ttrain's SFC_loss: 0.797323\tvalid's SFC_loss: 0.895435\n",
      "[75]\ttrain's SFC_loss: 0.793711\tvalid's SFC_loss: 0.894334\n",
      "[76]\ttrain's SFC_loss: 0.790946\tvalid's SFC_loss: 0.893696\n",
      "[77]\ttrain's SFC_loss: 0.788434\tvalid's SFC_loss: 0.89314\n",
      "[78]\ttrain's SFC_loss: 0.785745\tvalid's SFC_loss: 0.890042\n",
      "[79]\ttrain's SFC_loss: 0.783251\tvalid's SFC_loss: 0.889643\n",
      "[80]\ttrain's SFC_loss: 0.780839\tvalid's SFC_loss: 0.889298\n",
      "[81]\ttrain's SFC_loss: 0.77856\tvalid's SFC_loss: 0.891322\n",
      "[82]\ttrain's SFC_loss: 0.776535\tvalid's SFC_loss: 0.891235\n",
      "[83]\ttrain's SFC_loss: 0.774636\tvalid's SFC_loss: 0.891872\n",
      "[84]\ttrain's SFC_loss: 0.772064\tvalid's SFC_loss: 0.892987\n",
      "[85]\ttrain's SFC_loss: 0.769573\tvalid's SFC_loss: 0.894151\n",
      "[86]\ttrain's SFC_loss: 0.767538\tvalid's SFC_loss: 0.894608\n",
      "[87]\ttrain's SFC_loss: 0.765531\tvalid's SFC_loss: 0.895992\n",
      "[88]\ttrain's SFC_loss: 0.763312\tvalid's SFC_loss: 0.895915\n",
      "[89]\ttrain's SFC_loss: 0.760605\tvalid's SFC_loss: 0.897604\n",
      "[90]\ttrain's SFC_loss: 0.758227\tvalid's SFC_loss: 0.897876\n",
      "[91]\ttrain's SFC_loss: 0.755053\tvalid's SFC_loss: 0.897316\n",
      "[92]\ttrain's SFC_loss: 0.751848\tvalid's SFC_loss: 0.897478\n",
      "[93]\ttrain's SFC_loss: 0.748887\tvalid's SFC_loss: 0.897167\n",
      "[94]\ttrain's SFC_loss: 0.746079\tvalid's SFC_loss: 0.895423\n",
      "[95]\ttrain's SFC_loss: 0.744001\tvalid's SFC_loss: 0.893556\n",
      "[96]\ttrain's SFC_loss: 0.741015\tvalid's SFC_loss: 0.891233\n",
      "[97]\ttrain's SFC_loss: 0.738869\tvalid's SFC_loss: 0.891659\n",
      "[98]\ttrain's SFC_loss: 0.736004\tvalid's SFC_loss: 0.890291\n",
      "[99]\ttrain's SFC_loss: 0.733489\tvalid's SFC_loss: 0.890464\n",
      "[100]\ttrain's SFC_loss: 0.730716\tvalid's SFC_loss: 0.892199\n",
      "[101]\ttrain's SFC_loss: 0.72854\tvalid's SFC_loss: 0.891076\n",
      "[102]\ttrain's SFC_loss: 0.725397\tvalid's SFC_loss: 0.887069\n",
      "[103]\ttrain's SFC_loss: 0.722253\tvalid's SFC_loss: 0.886636\n",
      "[104]\ttrain's SFC_loss: 0.719906\tvalid's SFC_loss: 0.887361\n",
      "[105]\ttrain's SFC_loss: 0.71749\tvalid's SFC_loss: 0.887211\n",
      "[106]\ttrain's SFC_loss: 0.716281\tvalid's SFC_loss: 0.885529\n",
      "[107]\ttrain's SFC_loss: 0.713976\tvalid's SFC_loss: 0.887742\n",
      "[108]\ttrain's SFC_loss: 0.712181\tvalid's SFC_loss: 0.887514\n",
      "[109]\ttrain's SFC_loss: 0.710473\tvalid's SFC_loss: 0.888508\n",
      "[110]\ttrain's SFC_loss: 0.708969\tvalid's SFC_loss: 0.886388\n",
      "[111]\ttrain's SFC_loss: 0.707381\tvalid's SFC_loss: 0.884424\n",
      "[112]\ttrain's SFC_loss: 0.705697\tvalid's SFC_loss: 0.882406\n",
      "[113]\ttrain's SFC_loss: 0.703626\tvalid's SFC_loss: 0.879619\n",
      "[114]\ttrain's SFC_loss: 0.702201\tvalid's SFC_loss: 0.877594\n",
      "[115]\ttrain's SFC_loss: 0.700574\tvalid's SFC_loss: 0.875642\n",
      "[116]\ttrain's SFC_loss: 0.698044\tvalid's SFC_loss: 0.874666\n",
      "[117]\ttrain's SFC_loss: 0.695511\tvalid's SFC_loss: 0.874299\n",
      "[118]\ttrain's SFC_loss: 0.693346\tvalid's SFC_loss: 0.873704\n",
      "[119]\ttrain's SFC_loss: 0.690837\tvalid's SFC_loss: 0.872985\n",
      "[120]\ttrain's SFC_loss: 0.688975\tvalid's SFC_loss: 0.871178\n",
      "[121]\ttrain's SFC_loss: 0.68707\tvalid's SFC_loss: 0.871655\n",
      "[122]\ttrain's SFC_loss: 0.684885\tvalid's SFC_loss: 0.87005\n",
      "[123]\ttrain's SFC_loss: 0.683108\tvalid's SFC_loss: 0.870124\n",
      "[124]\ttrain's SFC_loss: 0.681025\tvalid's SFC_loss: 0.869501\n",
      "[125]\ttrain's SFC_loss: 0.678445\tvalid's SFC_loss: 0.867001\n",
      "[126]\ttrain's SFC_loss: 0.676306\tvalid's SFC_loss: 0.865988\n",
      "[127]\ttrain's SFC_loss: 0.674069\tvalid's SFC_loss: 0.865795\n",
      "[128]\ttrain's SFC_loss: 0.67264\tvalid's SFC_loss: 0.863493\n",
      "[129]\ttrain's SFC_loss: 0.670535\tvalid's SFC_loss: 0.861251\n",
      "[130]\ttrain's SFC_loss: 0.668994\tvalid's SFC_loss: 0.860986\n",
      "[131]\ttrain's SFC_loss: 0.667253\tvalid's SFC_loss: 0.859753\n",
      "[132]\ttrain's SFC_loss: 0.666041\tvalid's SFC_loss: 0.858002\n",
      "[133]\ttrain's SFC_loss: 0.664259\tvalid's SFC_loss: 0.85503\n",
      "[134]\ttrain's SFC_loss: 0.663107\tvalid's SFC_loss: 0.853361\n",
      "[135]\ttrain's SFC_loss: 0.661635\tvalid's SFC_loss: 0.851996\n",
      "[136]\ttrain's SFC_loss: 0.659319\tvalid's SFC_loss: 0.85158\n",
      "[137]\ttrain's SFC_loss: 0.657008\tvalid's SFC_loss: 0.849339\n",
      "[138]\ttrain's SFC_loss: 0.65533\tvalid's SFC_loss: 0.847712\n",
      "[139]\ttrain's SFC_loss: 0.653164\tvalid's SFC_loss: 0.847729\n",
      "[140]\ttrain's SFC_loss: 0.651947\tvalid's SFC_loss: 0.847546\n",
      "[141]\ttrain's SFC_loss: 0.650523\tvalid's SFC_loss: 0.846292\n",
      "[142]\ttrain's SFC_loss: 0.64897\tvalid's SFC_loss: 0.848025\n",
      "[143]\ttrain's SFC_loss: 0.647426\tvalid's SFC_loss: 0.846608\n",
      "[144]\ttrain's SFC_loss: 0.645619\tvalid's SFC_loss: 0.846694\n",
      "[145]\ttrain's SFC_loss: 0.643868\tvalid's SFC_loss: 0.846828\n",
      "[146]\ttrain's SFC_loss: 0.640862\tvalid's SFC_loss: 0.848825\n",
      "[147]\ttrain's SFC_loss: 0.638337\tvalid's SFC_loss: 0.850339\n",
      "[148]\ttrain's SFC_loss: 0.635899\tvalid's SFC_loss: 0.850904\n",
      "[149]\ttrain's SFC_loss: 0.63386\tvalid's SFC_loss: 0.851439\n",
      "[150]\ttrain's SFC_loss: 0.631446\tvalid's SFC_loss: 0.854008\n",
      "[151]\ttrain's SFC_loss: 0.62923\tvalid's SFC_loss: 0.852536\n",
      "[152]\ttrain's SFC_loss: 0.62748\tvalid's SFC_loss: 0.851059\n",
      "[153]\ttrain's SFC_loss: 0.62537\tvalid's SFC_loss: 0.850815\n",
      "[154]\ttrain's SFC_loss: 0.624381\tvalid's SFC_loss: 0.849667\n",
      "[155]\ttrain's SFC_loss: 0.621939\tvalid's SFC_loss: 0.85046\n",
      "[156]\ttrain's SFC_loss: 0.620184\tvalid's SFC_loss: 0.848547\n",
      "[157]\ttrain's SFC_loss: 0.618473\tvalid's SFC_loss: 0.849903\n",
      "[158]\ttrain's SFC_loss: 0.61675\tvalid's SFC_loss: 0.849542\n",
      "[159]\ttrain's SFC_loss: 0.615129\tvalid's SFC_loss: 0.846376\n",
      "[160]\ttrain's SFC_loss: 0.613491\tvalid's SFC_loss: 0.84649\n",
      "[161]\ttrain's SFC_loss: 0.611769\tvalid's SFC_loss: 0.846882\n",
      "[162]\ttrain's SFC_loss: 0.61017\tvalid's SFC_loss: 0.847155\n",
      "[163]\ttrain's SFC_loss: 0.608617\tvalid's SFC_loss: 0.847465\n",
      "[164]\ttrain's SFC_loss: 0.606992\tvalid's SFC_loss: 0.84895\n",
      "[165]\ttrain's SFC_loss: 0.605415\tvalid's SFC_loss: 0.84823\n",
      "[166]\ttrain's SFC_loss: 0.603794\tvalid's SFC_loss: 0.849539\n",
      "[167]\ttrain's SFC_loss: 0.602127\tvalid's SFC_loss: 0.851007\n",
      "[168]\ttrain's SFC_loss: 0.600353\tvalid's SFC_loss: 0.851326\n",
      "[169]\ttrain's SFC_loss: 0.598721\tvalid's SFC_loss: 0.85215\n",
      "[170]\ttrain's SFC_loss: 0.597219\tvalid's SFC_loss: 0.855302\n",
      "[171]\ttrain's SFC_loss: 0.596017\tvalid's SFC_loss: 0.854971\n",
      "[172]\ttrain's SFC_loss: 0.594851\tvalid's SFC_loss: 0.856676\n",
      "[173]\ttrain's SFC_loss: 0.594101\tvalid's SFC_loss: 0.858716\n",
      "[174]\ttrain's SFC_loss: 0.593013\tvalid's SFC_loss: 0.860476\n",
      "[175]\ttrain's SFC_loss: 0.592443\tvalid's SFC_loss: 0.860957\n",
      "[176]\ttrain's SFC_loss: 0.590553\tvalid's SFC_loss: 0.863131\n",
      "[177]\ttrain's SFC_loss: 0.589104\tvalid's SFC_loss: 0.86412\n",
      "[178]\ttrain's SFC_loss: 0.587298\tvalid's SFC_loss: 0.866311\n",
      "[179]\ttrain's SFC_loss: 0.585822\tvalid's SFC_loss: 0.868396\n",
      "[180]\ttrain's SFC_loss: 0.584204\tvalid's SFC_loss: 0.868454\n",
      "[181]\ttrain's SFC_loss: 0.582471\tvalid's SFC_loss: 0.868126\n",
      "[182]\ttrain's SFC_loss: 0.580854\tvalid's SFC_loss: 0.868055\n",
      "[183]\ttrain's SFC_loss: 0.579221\tvalid's SFC_loss: 0.864145\n",
      "[184]\ttrain's SFC_loss: 0.57767\tvalid's SFC_loss: 0.86317\n",
      "[185]\ttrain's SFC_loss: 0.576081\tvalid's SFC_loss: 0.860585\n",
      "[186]\ttrain's SFC_loss: 0.574094\tvalid's SFC_loss: 0.858521\n",
      "[187]\ttrain's SFC_loss: 0.572194\tvalid's SFC_loss: 0.855094\n",
      "[188]\ttrain's SFC_loss: 0.570534\tvalid's SFC_loss: 0.852974\n",
      "[189]\ttrain's SFC_loss: 0.56897\tvalid's SFC_loss: 0.854022\n",
      "[190]\ttrain's SFC_loss: 0.567468\tvalid's SFC_loss: 0.850686\n",
      "[191]\ttrain's SFC_loss: 0.566046\tvalid's SFC_loss: 0.852535\n",
      "[192]\ttrain's SFC_loss: 0.564611\tvalid's SFC_loss: 0.852496\n",
      "[193]\ttrain's SFC_loss: 0.563377\tvalid's SFC_loss: 0.853575\n",
      "[194]\ttrain's SFC_loss: 0.561594\tvalid's SFC_loss: 0.855314\n",
      "[195]\ttrain's SFC_loss: 0.560121\tvalid's SFC_loss: 0.855416\n",
      "[196]\ttrain's SFC_loss: 0.558527\tvalid's SFC_loss: 0.854228\n",
      "[197]\ttrain's SFC_loss: 0.556821\tvalid's SFC_loss: 0.851191\n",
      "[198]\ttrain's SFC_loss: 0.554988\tvalid's SFC_loss: 0.849043\n",
      "[199]\ttrain's SFC_loss: 0.553488\tvalid's SFC_loss: 0.846146\n",
      "[200]\ttrain's SFC_loss: 0.551858\tvalid's SFC_loss: 0.844448\n",
      "[201]\ttrain's SFC_loss: 0.549643\tvalid's SFC_loss: 0.844454\n",
      "[202]\ttrain's SFC_loss: 0.546933\tvalid's SFC_loss: 0.843403\n",
      "[203]\ttrain's SFC_loss: 0.544916\tvalid's SFC_loss: 0.843052\n",
      "[204]\ttrain's SFC_loss: 0.543371\tvalid's SFC_loss: 0.841743\n",
      "[205]\ttrain's SFC_loss: 0.540793\tvalid's SFC_loss: 0.840747\n",
      "[206]\ttrain's SFC_loss: 0.539286\tvalid's SFC_loss: 0.843281\n",
      "[207]\ttrain's SFC_loss: 0.537984\tvalid's SFC_loss: 0.843624\n",
      "[208]\ttrain's SFC_loss: 0.536598\tvalid's SFC_loss: 0.844074\n",
      "[209]\ttrain's SFC_loss: 0.535221\tvalid's SFC_loss: 0.845844\n",
      "[210]\ttrain's SFC_loss: 0.534041\tvalid's SFC_loss: 0.846813\n",
      "[211]\ttrain's SFC_loss: 0.53336\tvalid's SFC_loss: 0.847281\n",
      "[212]\ttrain's SFC_loss: 0.532087\tvalid's SFC_loss: 0.848059\n",
      "[213]\ttrain's SFC_loss: 0.531209\tvalid's SFC_loss: 0.849625\n",
      "[214]\ttrain's SFC_loss: 0.530074\tvalid's SFC_loss: 0.84881\n",
      "[215]\ttrain's SFC_loss: 0.52901\tvalid's SFC_loss: 0.85028\n",
      "[216]\ttrain's SFC_loss: 0.527354\tvalid's SFC_loss: 0.851098\n",
      "[217]\ttrain's SFC_loss: 0.525747\tvalid's SFC_loss: 0.852212\n",
      "[218]\ttrain's SFC_loss: 0.524126\tvalid's SFC_loss: 0.854798\n",
      "[219]\ttrain's SFC_loss: 0.522819\tvalid's SFC_loss: 0.855251\n",
      "[220]\ttrain's SFC_loss: 0.521401\tvalid's SFC_loss: 0.857265\n",
      "[221]\ttrain's SFC_loss: 0.519685\tvalid's SFC_loss: 0.855316\n",
      "[222]\ttrain's SFC_loss: 0.518266\tvalid's SFC_loss: 0.853888\n",
      "[223]\ttrain's SFC_loss: 0.51695\tvalid's SFC_loss: 0.853032\n",
      "[224]\ttrain's SFC_loss: 0.51539\tvalid's SFC_loss: 0.851451\n",
      "[225]\ttrain's SFC_loss: 0.514008\tvalid's SFC_loss: 0.850823\n",
      "[226]\ttrain's SFC_loss: 0.512805\tvalid's SFC_loss: 0.849048\n",
      "[227]\ttrain's SFC_loss: 0.51146\tvalid's SFC_loss: 0.847624\n",
      "[228]\ttrain's SFC_loss: 0.510296\tvalid's SFC_loss: 0.845371\n",
      "[229]\ttrain's SFC_loss: 0.509181\tvalid's SFC_loss: 0.843498\n",
      "[230]\ttrain's SFC_loss: 0.508159\tvalid's SFC_loss: 0.841597\n",
      "[231]\ttrain's SFC_loss: 0.507136\tvalid's SFC_loss: 0.840506\n",
      "[232]\ttrain's SFC_loss: 0.50632\tvalid's SFC_loss: 0.838897\n",
      "[233]\ttrain's SFC_loss: 0.50527\tvalid's SFC_loss: 0.839205\n",
      "[234]\ttrain's SFC_loss: 0.504517\tvalid's SFC_loss: 0.838904\n",
      "[235]\ttrain's SFC_loss: 0.503304\tvalid's SFC_loss: 0.838846\n",
      "[236]\ttrain's SFC_loss: 0.502184\tvalid's SFC_loss: 0.84129\n",
      "[237]\ttrain's SFC_loss: 0.501121\tvalid's SFC_loss: 0.840323\n",
      "[238]\ttrain's SFC_loss: 0.499928\tvalid's SFC_loss: 0.841148\n",
      "[239]\ttrain's SFC_loss: 0.498459\tvalid's SFC_loss: 0.841318\n",
      "[240]\ttrain's SFC_loss: 0.497083\tvalid's SFC_loss: 0.840941\n",
      "[241]\ttrain's SFC_loss: 0.495988\tvalid's SFC_loss: 0.841953\n",
      "[242]\ttrain's SFC_loss: 0.494826\tvalid's SFC_loss: 0.843686\n",
      "[243]\ttrain's SFC_loss: 0.49355\tvalid's SFC_loss: 0.845991\n",
      "[244]\ttrain's SFC_loss: 0.492568\tvalid's SFC_loss: 0.845526\n",
      "[245]\ttrain's SFC_loss: 0.491413\tvalid's SFC_loss: 0.844425\n",
      "[246]\ttrain's SFC_loss: 0.490487\tvalid's SFC_loss: 0.845421\n",
      "[247]\ttrain's SFC_loss: 0.489437\tvalid's SFC_loss: 0.845312\n",
      "[248]\ttrain's SFC_loss: 0.488064\tvalid's SFC_loss: 0.846218\n",
      "[249]\ttrain's SFC_loss: 0.487007\tvalid's SFC_loss: 0.847228\n",
      "[250]\ttrain's SFC_loss: 0.486112\tvalid's SFC_loss: 0.848334\n",
      "[251]\ttrain's SFC_loss: 0.484854\tvalid's SFC_loss: 0.848424\n",
      "[252]\ttrain's SFC_loss: 0.483815\tvalid's SFC_loss: 0.849277\n",
      "[253]\ttrain's SFC_loss: 0.482359\tvalid's SFC_loss: 0.851082\n",
      "[254]\ttrain's SFC_loss: 0.481494\tvalid's SFC_loss: 0.85171\n",
      "[255]\ttrain's SFC_loss: 0.480408\tvalid's SFC_loss: 0.852657\n",
      "[256]\ttrain's SFC_loss: 0.479577\tvalid's SFC_loss: 0.85137\n",
      "[257]\ttrain's SFC_loss: 0.478346\tvalid's SFC_loss: 0.846805\n",
      "[258]\ttrain's SFC_loss: 0.477068\tvalid's SFC_loss: 0.844354\n",
      "[259]\ttrain's SFC_loss: 0.475946\tvalid's SFC_loss: 0.840117\n",
      "[260]\ttrain's SFC_loss: 0.474682\tvalid's SFC_loss: 0.837794\n",
      "[261]\ttrain's SFC_loss: 0.473853\tvalid's SFC_loss: 0.835219\n",
      "[262]\ttrain's SFC_loss: 0.472977\tvalid's SFC_loss: 0.833357\n",
      "[263]\ttrain's SFC_loss: 0.472163\tvalid's SFC_loss: 0.831881\n",
      "[264]\ttrain's SFC_loss: 0.471006\tvalid's SFC_loss: 0.833156\n",
      "[265]\ttrain's SFC_loss: 0.469968\tvalid's SFC_loss: 0.834278\n",
      "[266]\ttrain's SFC_loss: 0.468554\tvalid's SFC_loss: 0.834023\n",
      "[267]\ttrain's SFC_loss: 0.467606\tvalid's SFC_loss: 0.834137\n",
      "[268]\ttrain's SFC_loss: 0.466262\tvalid's SFC_loss: 0.832416\n",
      "[269]\ttrain's SFC_loss: 0.465162\tvalid's SFC_loss: 0.832915\n",
      "[270]\ttrain's SFC_loss: 0.464393\tvalid's SFC_loss: 0.833297\n",
      "[271]\ttrain's SFC_loss: 0.463512\tvalid's SFC_loss: 0.834542\n",
      "[272]\ttrain's SFC_loss: 0.462324\tvalid's SFC_loss: 0.83205\n",
      "[273]\ttrain's SFC_loss: 0.461566\tvalid's SFC_loss: 0.832354\n",
      "[274]\ttrain's SFC_loss: 0.460747\tvalid's SFC_loss: 0.831867\n",
      "[275]\ttrain's SFC_loss: 0.459637\tvalid's SFC_loss: 0.829471\n",
      "[276]\ttrain's SFC_loss: 0.458341\tvalid's SFC_loss: 0.830359\n",
      "[277]\ttrain's SFC_loss: 0.456918\tvalid's SFC_loss: 0.829611\n",
      "[278]\ttrain's SFC_loss: 0.455606\tvalid's SFC_loss: 0.828817\n",
      "[279]\ttrain's SFC_loss: 0.454645\tvalid's SFC_loss: 0.829755\n",
      "[280]\ttrain's SFC_loss: 0.45331\tvalid's SFC_loss: 0.834313\n",
      "[281]\ttrain's SFC_loss: 0.452264\tvalid's SFC_loss: 0.833986\n",
      "[282]\ttrain's SFC_loss: 0.451047\tvalid's SFC_loss: 0.833987\n",
      "[283]\ttrain's SFC_loss: 0.449992\tvalid's SFC_loss: 0.833543\n",
      "[284]\ttrain's SFC_loss: 0.448794\tvalid's SFC_loss: 0.833772\n",
      "[285]\ttrain's SFC_loss: 0.447844\tvalid's SFC_loss: 0.832635\n",
      "[286]\ttrain's SFC_loss: 0.446661\tvalid's SFC_loss: 0.829682\n",
      "[287]\ttrain's SFC_loss: 0.445609\tvalid's SFC_loss: 0.827242\n",
      "[288]\ttrain's SFC_loss: 0.444451\tvalid's SFC_loss: 0.82487\n",
      "[289]\ttrain's SFC_loss: 0.443669\tvalid's SFC_loss: 0.825114\n",
      "[290]\ttrain's SFC_loss: 0.442385\tvalid's SFC_loss: 0.822432\n",
      "[291]\ttrain's SFC_loss: 0.441405\tvalid's SFC_loss: 0.824699\n",
      "[292]\ttrain's SFC_loss: 0.440557\tvalid's SFC_loss: 0.827359\n",
      "[293]\ttrain's SFC_loss: 0.439769\tvalid's SFC_loss: 0.828142\n",
      "[294]\ttrain's SFC_loss: 0.438922\tvalid's SFC_loss: 0.829637\n",
      "[295]\ttrain's SFC_loss: 0.437972\tvalid's SFC_loss: 0.831693\n",
      "[296]\ttrain's SFC_loss: 0.437188\tvalid's SFC_loss: 0.831521\n",
      "[297]\ttrain's SFC_loss: 0.436155\tvalid's SFC_loss: 0.831588\n",
      "[298]\ttrain's SFC_loss: 0.435515\tvalid's SFC_loss: 0.831041\n",
      "[299]\ttrain's SFC_loss: 0.434674\tvalid's SFC_loss: 0.83012\n",
      "[300]\ttrain's SFC_loss: 0.434096\tvalid's SFC_loss: 0.831134\n",
      "[301]\ttrain's SFC_loss: 0.433373\tvalid's SFC_loss: 0.830728\n",
      "[302]\ttrain's SFC_loss: 0.4329\tvalid's SFC_loss: 0.831587\n",
      "[303]\ttrain's SFC_loss: 0.432382\tvalid's SFC_loss: 0.832986\n",
      "[304]\ttrain's SFC_loss: 0.431538\tvalid's SFC_loss: 0.833956\n",
      "[305]\ttrain's SFC_loss: 0.431045\tvalid's SFC_loss: 0.835377\n",
      "[306]\ttrain's SFC_loss: 0.429997\tvalid's SFC_loss: 0.836875\n",
      "[307]\ttrain's SFC_loss: 0.429192\tvalid's SFC_loss: 0.837508\n",
      "[308]\ttrain's SFC_loss: 0.428456\tvalid's SFC_loss: 0.839377\n",
      "[309]\ttrain's SFC_loss: 0.427775\tvalid's SFC_loss: 0.840796\n",
      "[310]\ttrain's SFC_loss: 0.426888\tvalid's SFC_loss: 0.840771\n",
      "[311]\ttrain's SFC_loss: 0.425824\tvalid's SFC_loss: 0.844452\n",
      "[312]\ttrain's SFC_loss: 0.424735\tvalid's SFC_loss: 0.848008\n",
      "[313]\ttrain's SFC_loss: 0.423664\tvalid's SFC_loss: 0.848092\n",
      "[314]\ttrain's SFC_loss: 0.422889\tvalid's SFC_loss: 0.850386\n",
      "[315]\ttrain's SFC_loss: 0.421934\tvalid's SFC_loss: 0.851922\n",
      "[316]\ttrain's SFC_loss: 0.420942\tvalid's SFC_loss: 0.853165\n",
      "[317]\ttrain's SFC_loss: 0.419972\tvalid's SFC_loss: 0.855424\n",
      "[318]\ttrain's SFC_loss: 0.41906\tvalid's SFC_loss: 0.857517\n",
      "[319]\ttrain's SFC_loss: 0.418189\tvalid's SFC_loss: 0.858666\n",
      "[320]\ttrain's SFC_loss: 0.417572\tvalid's SFC_loss: 0.860139\n",
      "[321]\ttrain's SFC_loss: 0.416741\tvalid's SFC_loss: 0.857223\n",
      "[322]\ttrain's SFC_loss: 0.415749\tvalid's SFC_loss: 0.855051\n",
      "[323]\ttrain's SFC_loss: 0.414709\tvalid's SFC_loss: 0.852006\n",
      "[324]\ttrain's SFC_loss: 0.413854\tvalid's SFC_loss: 0.84869\n",
      "[325]\ttrain's SFC_loss: 0.413277\tvalid's SFC_loss: 0.846915\n",
      "[326]\ttrain's SFC_loss: 0.41226\tvalid's SFC_loss: 0.84449\n",
      "[327]\ttrain's SFC_loss: 0.411553\tvalid's SFC_loss: 0.844211\n",
      "[328]\ttrain's SFC_loss: 0.410642\tvalid's SFC_loss: 0.846096\n",
      "[329]\ttrain's SFC_loss: 0.410034\tvalid's SFC_loss: 0.844828\n",
      "[330]\ttrain's SFC_loss: 0.409391\tvalid's SFC_loss: 0.843862\n",
      "[331]\ttrain's SFC_loss: 0.40806\tvalid's SFC_loss: 0.844826\n",
      "[332]\ttrain's SFC_loss: 0.407063\tvalid's SFC_loss: 0.844743\n",
      "[333]\ttrain's SFC_loss: 0.40613\tvalid's SFC_loss: 0.844769\n",
      "[334]\ttrain's SFC_loss: 0.405488\tvalid's SFC_loss: 0.844059\n",
      "[335]\ttrain's SFC_loss: 0.404422\tvalid's SFC_loss: 0.844619\n",
      "[336]\ttrain's SFC_loss: 0.403643\tvalid's SFC_loss: 0.846326\n",
      "[337]\ttrain's SFC_loss: 0.402534\tvalid's SFC_loss: 0.850225\n",
      "[338]\ttrain's SFC_loss: 0.401463\tvalid's SFC_loss: 0.854111\n",
      "[339]\ttrain's SFC_loss: 0.400861\tvalid's SFC_loss: 0.855986\n",
      "[340]\ttrain's SFC_loss: 0.40025\tvalid's SFC_loss: 0.857476\n",
      "[341]\ttrain's SFC_loss: 0.399795\tvalid's SFC_loss: 0.857366\n",
      "[342]\ttrain's SFC_loss: 0.398985\tvalid's SFC_loss: 0.855371\n",
      "[343]\ttrain's SFC_loss: 0.398257\tvalid's SFC_loss: 0.85478\n",
      "[344]\ttrain's SFC_loss: 0.397546\tvalid's SFC_loss: 0.855635\n",
      "[345]\ttrain's SFC_loss: 0.396786\tvalid's SFC_loss: 0.854607\n",
      "[346]\ttrain's SFC_loss: 0.396026\tvalid's SFC_loss: 0.853747\n",
      "[347]\ttrain's SFC_loss: 0.395472\tvalid's SFC_loss: 0.854305\n",
      "[348]\ttrain's SFC_loss: 0.394808\tvalid's SFC_loss: 0.851968\n",
      "[349]\ttrain's SFC_loss: 0.394295\tvalid's SFC_loss: 0.851834\n",
      "[350]\ttrain's SFC_loss: 0.393503\tvalid's SFC_loss: 0.851455\n",
      "[351]\ttrain's SFC_loss: 0.39234\tvalid's SFC_loss: 0.850499\n",
      "[352]\ttrain's SFC_loss: 0.391551\tvalid's SFC_loss: 0.848628\n",
      "[353]\ttrain's SFC_loss: 0.390786\tvalid's SFC_loss: 0.846328\n",
      "[354]\ttrain's SFC_loss: 0.390085\tvalid's SFC_loss: 0.843587\n",
      "[355]\ttrain's SFC_loss: 0.389174\tvalid's SFC_loss: 0.841921\n",
      "[356]\ttrain's SFC_loss: 0.388079\tvalid's SFC_loss: 0.84412\n",
      "[357]\ttrain's SFC_loss: 0.3874\tvalid's SFC_loss: 0.845178\n",
      "[358]\ttrain's SFC_loss: 0.386466\tvalid's SFC_loss: 0.846067\n",
      "[359]\ttrain's SFC_loss: 0.385605\tvalid's SFC_loss: 0.845897\n",
      "[360]\ttrain's SFC_loss: 0.384843\tvalid's SFC_loss: 0.847263\n",
      "[361]\ttrain's SFC_loss: 0.383499\tvalid's SFC_loss: 0.850677\n",
      "[362]\ttrain's SFC_loss: 0.382158\tvalid's SFC_loss: 0.850531\n",
      "[363]\ttrain's SFC_loss: 0.380631\tvalid's SFC_loss: 0.851317\n",
      "[364]\ttrain's SFC_loss: 0.37965\tvalid's SFC_loss: 0.854581\n",
      "[365]\ttrain's SFC_loss: 0.378519\tvalid's SFC_loss: 0.857665\n",
      "[366]\ttrain's SFC_loss: 0.378102\tvalid's SFC_loss: 0.858976\n",
      "[367]\ttrain's SFC_loss: 0.377656\tvalid's SFC_loss: 0.860627\n",
      "[368]\ttrain's SFC_loss: 0.377335\tvalid's SFC_loss: 0.861787\n",
      "[369]\ttrain's SFC_loss: 0.376975\tvalid's SFC_loss: 0.863979\n",
      "[370]\ttrain's SFC_loss: 0.376653\tvalid's SFC_loss: 0.865775\n",
      "[371]\ttrain's SFC_loss: 0.375928\tvalid's SFC_loss: 0.870252\n",
      "[372]\ttrain's SFC_loss: 0.375233\tvalid's SFC_loss: 0.872077\n",
      "[373]\ttrain's SFC_loss: 0.374677\tvalid's SFC_loss: 0.872453\n",
      "[374]\ttrain's SFC_loss: 0.373972\tvalid's SFC_loss: 0.873761\n",
      "[375]\ttrain's SFC_loss: 0.373251\tvalid's SFC_loss: 0.874741\n",
      "[376]\ttrain's SFC_loss: 0.372727\tvalid's SFC_loss: 0.875097\n",
      "[377]\ttrain's SFC_loss: 0.371773\tvalid's SFC_loss: 0.875964\n",
      "[378]\ttrain's SFC_loss: 0.371262\tvalid's SFC_loss: 0.875877\n",
      "[379]\ttrain's SFC_loss: 0.370467\tvalid's SFC_loss: 0.876856\n",
      "[380]\ttrain's SFC_loss: 0.370003\tvalid's SFC_loss: 0.877261\n",
      "[381]\ttrain's SFC_loss: 0.369111\tvalid's SFC_loss: 0.878368\n",
      "[382]\ttrain's SFC_loss: 0.368256\tvalid's SFC_loss: 0.878036\n",
      "[383]\ttrain's SFC_loss: 0.367797\tvalid's SFC_loss: 0.877695\n",
      "[384]\ttrain's SFC_loss: 0.367018\tvalid's SFC_loss: 0.880887\n",
      "[385]\ttrain's SFC_loss: 0.366485\tvalid's SFC_loss: 0.880349\n",
      "[386]\ttrain's SFC_loss: 0.36587\tvalid's SFC_loss: 0.879794\n",
      "[387]\ttrain's SFC_loss: 0.365293\tvalid's SFC_loss: 0.879586\n",
      "[388]\ttrain's SFC_loss: 0.364508\tvalid's SFC_loss: 0.880691\n",
      "[389]\ttrain's SFC_loss: 0.363454\tvalid's SFC_loss: 0.881843\n",
      "[390]\ttrain's SFC_loss: 0.362484\tvalid's SFC_loss: 0.881165\n",
      "Early stopping, best iteration is:\n",
      "[290]\ttrain's SFC_loss: 0.442385\tvalid's SFC_loss: 0.822432\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1    0.021418\n",
      "1    feature2   -0.022814\n",
      "2    feature3   -0.051966\n",
      "3    feature4    0.015200\n",
      "4    feature5    0.014624\n",
      "5    feature6   -0.038805\n",
      "6    feature7    0.075140\n",
      "7    feature8   -0.026885\n",
      "8    feature9   -0.063124\n",
      "9   feature10    0.080599\n",
      "10  feature11    0.040992\n",
      "11  feature12   -0.031397\n",
      "12  feature13    0.062554\n",
      "13  feature14    0.078593\n",
      "14  feature15   -0.129424\n",
      "15  feature16   -0.021039\n",
      "16  feature17    0.032677\n",
      "17  feature18   -0.018504\n",
      "18  feature19    0.078203\n",
      "19  feature20   -0.096041\n",
      "-------------------- 6 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05765\tvalid's multi_logloss: 1.06532\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05264\tvalid's multi_logloss: 1.06593\n",
      "[3]\ttrain's multi_logloss: 1.04756\tvalid's multi_logloss: 1.06612\n",
      "[4]\ttrain's multi_logloss: 1.04374\tvalid's multi_logloss: 1.06501\n",
      "[5]\ttrain's multi_logloss: 1.03881\tvalid's multi_logloss: 1.06647\n",
      "[6]\ttrain's multi_logloss: 1.03393\tvalid's multi_logloss: 1.0648\n",
      "[7]\ttrain's multi_logloss: 1.02931\tvalid's multi_logloss: 1.06411\n",
      "[8]\ttrain's multi_logloss: 1.02532\tvalid's multi_logloss: 1.06249\n",
      "[9]\ttrain's multi_logloss: 1.0218\tvalid's multi_logloss: 1.06173\n",
      "[10]\ttrain's multi_logloss: 1.01788\tvalid's multi_logloss: 1.06019\n",
      "[11]\ttrain's multi_logloss: 1.0141\tvalid's multi_logloss: 1.05899\n",
      "[12]\ttrain's multi_logloss: 1.00983\tvalid's multi_logloss: 1.05852\n",
      "[13]\ttrain's multi_logloss: 1.00654\tvalid's multi_logloss: 1.05724\n",
      "[14]\ttrain's multi_logloss: 1.00303\tvalid's multi_logloss: 1.05651\n",
      "[15]\ttrain's multi_logloss: 1.00026\tvalid's multi_logloss: 1.05513\n",
      "[16]\ttrain's multi_logloss: 0.995082\tvalid's multi_logloss: 1.05312\n",
      "[17]\ttrain's multi_logloss: 0.989624\tvalid's multi_logloss: 1.0537\n",
      "[18]\ttrain's multi_logloss: 0.98518\tvalid's multi_logloss: 1.05521\n",
      "[19]\ttrain's multi_logloss: 0.981952\tvalid's multi_logloss: 1.05304\n",
      "[20]\ttrain's multi_logloss: 0.977143\tvalid's multi_logloss: 1.05399\n",
      "[21]\ttrain's multi_logloss: 0.973577\tvalid's multi_logloss: 1.05514\n",
      "[22]\ttrain's multi_logloss: 0.968856\tvalid's multi_logloss: 1.05488\n",
      "[23]\ttrain's multi_logloss: 0.965102\tvalid's multi_logloss: 1.05684\n",
      "[24]\ttrain's multi_logloss: 0.960655\tvalid's multi_logloss: 1.05643\n",
      "[25]\ttrain's multi_logloss: 0.956152\tvalid's multi_logloss: 1.05848\n",
      "[26]\ttrain's multi_logloss: 0.952715\tvalid's multi_logloss: 1.05719\n",
      "[27]\ttrain's multi_logloss: 0.949199\tvalid's multi_logloss: 1.05633\n",
      "[28]\ttrain's multi_logloss: 0.945892\tvalid's multi_logloss: 1.05532\n",
      "[29]\ttrain's multi_logloss: 0.943298\tvalid's multi_logloss: 1.05427\n",
      "[30]\ttrain's multi_logloss: 0.939963\tvalid's multi_logloss: 1.05235\n",
      "[31]\ttrain's multi_logloss: 0.936971\tvalid's multi_logloss: 1.05107\n",
      "[32]\ttrain's multi_logloss: 0.933724\tvalid's multi_logloss: 1.05134\n",
      "[33]\ttrain's multi_logloss: 0.930786\tvalid's multi_logloss: 1.04843\n",
      "[34]\ttrain's multi_logloss: 0.927621\tvalid's multi_logloss: 1.04873\n",
      "[35]\ttrain's multi_logloss: 0.924558\tvalid's multi_logloss: 1.04876\n",
      "[36]\ttrain's multi_logloss: 0.920861\tvalid's multi_logloss: 1.0483\n",
      "[37]\ttrain's multi_logloss: 0.918157\tvalid's multi_logloss: 1.04772\n",
      "[38]\ttrain's multi_logloss: 0.915007\tvalid's multi_logloss: 1.0448\n",
      "[39]\ttrain's multi_logloss: 0.912438\tvalid's multi_logloss: 1.04406\n",
      "[40]\ttrain's multi_logloss: 0.909564\tvalid's multi_logloss: 1.04327\n",
      "[41]\ttrain's multi_logloss: 0.906931\tvalid's multi_logloss: 1.04184\n",
      "[42]\ttrain's multi_logloss: 0.904023\tvalid's multi_logloss: 1.04188\n",
      "[43]\ttrain's multi_logloss: 0.901025\tvalid's multi_logloss: 1.04189\n",
      "[44]\ttrain's multi_logloss: 0.898979\tvalid's multi_logloss: 1.04193\n",
      "[45]\ttrain's multi_logloss: 0.896238\tvalid's multi_logloss: 1.04201\n",
      "[46]\ttrain's multi_logloss: 0.893056\tvalid's multi_logloss: 1.04007\n",
      "[47]\ttrain's multi_logloss: 0.890087\tvalid's multi_logloss: 1.03794\n",
      "[48]\ttrain's multi_logloss: 0.88708\tvalid's multi_logloss: 1.03852\n",
      "[49]\ttrain's multi_logloss: 0.884669\tvalid's multi_logloss: 1.0401\n",
      "[50]\ttrain's multi_logloss: 0.882769\tvalid's multi_logloss: 1.0389\n",
      "[51]\ttrain's multi_logloss: 0.880347\tvalid's multi_logloss: 1.03822\n",
      "[52]\ttrain's multi_logloss: 0.877331\tvalid's multi_logloss: 1.03801\n",
      "[53]\ttrain's multi_logloss: 0.875608\tvalid's multi_logloss: 1.03924\n",
      "[54]\ttrain's multi_logloss: 0.87397\tvalid's multi_logloss: 1.04018\n",
      "[55]\ttrain's multi_logloss: 0.871598\tvalid's multi_logloss: 1.0396\n",
      "[56]\ttrain's multi_logloss: 0.868817\tvalid's multi_logloss: 1.03987\n",
      "[57]\ttrain's multi_logloss: 0.866007\tvalid's multi_logloss: 1.04102\n",
      "[58]\ttrain's multi_logloss: 0.863539\tvalid's multi_logloss: 1.04049\n",
      "[59]\ttrain's multi_logloss: 0.861059\tvalid's multi_logloss: 1.03956\n",
      "[60]\ttrain's multi_logloss: 0.858508\tvalid's multi_logloss: 1.0393\n",
      "[61]\ttrain's multi_logloss: 0.855978\tvalid's multi_logloss: 1.03876\n",
      "[62]\ttrain's multi_logloss: 0.853177\tvalid's multi_logloss: 1.03922\n",
      "[63]\ttrain's multi_logloss: 0.851077\tvalid's multi_logloss: 1.03854\n",
      "[64]\ttrain's multi_logloss: 0.848599\tvalid's multi_logloss: 1.03975\n",
      "[65]\ttrain's multi_logloss: 0.845879\tvalid's multi_logloss: 1.03933\n",
      "[66]\ttrain's multi_logloss: 0.843484\tvalid's multi_logloss: 1.03879\n",
      "[67]\ttrain's multi_logloss: 0.840846\tvalid's multi_logloss: 1.03901\n",
      "[68]\ttrain's multi_logloss: 0.838866\tvalid's multi_logloss: 1.03962\n",
      "[69]\ttrain's multi_logloss: 0.836401\tvalid's multi_logloss: 1.03962\n",
      "[70]\ttrain's multi_logloss: 0.834176\tvalid's multi_logloss: 1.03935\n",
      "[71]\ttrain's multi_logloss: 0.83153\tvalid's multi_logloss: 1.03632\n",
      "[72]\ttrain's multi_logloss: 0.828634\tvalid's multi_logloss: 1.03417\n",
      "[73]\ttrain's multi_logloss: 0.826778\tvalid's multi_logloss: 1.03405\n",
      "[74]\ttrain's multi_logloss: 0.824966\tvalid's multi_logloss: 1.0344\n",
      "[75]\ttrain's multi_logloss: 0.823177\tvalid's multi_logloss: 1.03472\n",
      "[76]\ttrain's multi_logloss: 0.82057\tvalid's multi_logloss: 1.03469\n",
      "[77]\ttrain's multi_logloss: 0.818063\tvalid's multi_logloss: 1.03467\n",
      "[78]\ttrain's multi_logloss: 0.815996\tvalid's multi_logloss: 1.03466\n",
      "[79]\ttrain's multi_logloss: 0.813409\tvalid's multi_logloss: 1.03486\n",
      "[80]\ttrain's multi_logloss: 0.811426\tvalid's multi_logloss: 1.03506\n",
      "[81]\ttrain's multi_logloss: 0.809119\tvalid's multi_logloss: 1.0326\n",
      "[82]\ttrain's multi_logloss: 0.806895\tvalid's multi_logloss: 1.03228\n",
      "[83]\ttrain's multi_logloss: 0.804224\tvalid's multi_logloss: 1.03446\n",
      "[84]\ttrain's multi_logloss: 0.801315\tvalid's multi_logloss: 1.03548\n",
      "[85]\ttrain's multi_logloss: 0.799082\tvalid's multi_logloss: 1.03178\n",
      "[86]\ttrain's multi_logloss: 0.796816\tvalid's multi_logloss: 1.02978\n",
      "[87]\ttrain's multi_logloss: 0.79442\tvalid's multi_logloss: 1.03042\n",
      "[88]\ttrain's multi_logloss: 0.792166\tvalid's multi_logloss: 1.03152\n",
      "[89]\ttrain's multi_logloss: 0.79075\tvalid's multi_logloss: 1.02962\n",
      "[90]\ttrain's multi_logloss: 0.788815\tvalid's multi_logloss: 1.02769\n",
      "[91]\ttrain's multi_logloss: 0.786685\tvalid's multi_logloss: 1.02786\n",
      "[92]\ttrain's multi_logloss: 0.784835\tvalid's multi_logloss: 1.02726\n",
      "[93]\ttrain's multi_logloss: 0.782517\tvalid's multi_logloss: 1.02704\n",
      "[94]\ttrain's multi_logloss: 0.780036\tvalid's multi_logloss: 1.02673\n",
      "[95]\ttrain's multi_logloss: 0.778108\tvalid's multi_logloss: 1.02751\n",
      "[96]\ttrain's multi_logloss: 0.775295\tvalid's multi_logloss: 1.02695\n",
      "[97]\ttrain's multi_logloss: 0.772702\tvalid's multi_logloss: 1.02805\n",
      "[98]\ttrain's multi_logloss: 0.77034\tvalid's multi_logloss: 1.02682\n",
      "[99]\ttrain's multi_logloss: 0.768085\tvalid's multi_logloss: 1.02646\n",
      "[100]\ttrain's multi_logloss: 0.765927\tvalid's multi_logloss: 1.02621\n",
      "[101]\ttrain's multi_logloss: 0.763633\tvalid's multi_logloss: 1.02794\n",
      "[102]\ttrain's multi_logloss: 0.761355\tvalid's multi_logloss: 1.02729\n",
      "[103]\ttrain's multi_logloss: 0.759165\tvalid's multi_logloss: 1.02858\n",
      "[104]\ttrain's multi_logloss: 0.757432\tvalid's multi_logloss: 1.0291\n",
      "[105]\ttrain's multi_logloss: 0.755346\tvalid's multi_logloss: 1.02757\n",
      "[106]\ttrain's multi_logloss: 0.75391\tvalid's multi_logloss: 1.02755\n",
      "[107]\ttrain's multi_logloss: 0.752715\tvalid's multi_logloss: 1.02564\n",
      "[108]\ttrain's multi_logloss: 0.75148\tvalid's multi_logloss: 1.0256\n",
      "[109]\ttrain's multi_logloss: 0.750103\tvalid's multi_logloss: 1.02589\n",
      "[110]\ttrain's multi_logloss: 0.748743\tvalid's multi_logloss: 1.02591\n",
      "[111]\ttrain's multi_logloss: 0.746996\tvalid's multi_logloss: 1.02423\n",
      "[112]\ttrain's multi_logloss: 0.745595\tvalid's multi_logloss: 1.02174\n",
      "[113]\ttrain's multi_logloss: 0.743963\tvalid's multi_logloss: 1.01929\n",
      "[114]\ttrain's multi_logloss: 0.74251\tvalid's multi_logloss: 1.01602\n",
      "[115]\ttrain's multi_logloss: 0.740986\tvalid's multi_logloss: 1.01308\n",
      "[116]\ttrain's multi_logloss: 0.739652\tvalid's multi_logloss: 1.0134\n",
      "[117]\ttrain's multi_logloss: 0.738136\tvalid's multi_logloss: 1.01315\n",
      "[118]\ttrain's multi_logloss: 0.736202\tvalid's multi_logloss: 1.01301\n",
      "[119]\ttrain's multi_logloss: 0.734399\tvalid's multi_logloss: 1.013\n",
      "[120]\ttrain's multi_logloss: 0.733299\tvalid's multi_logloss: 1.01365\n",
      "[121]\ttrain's multi_logloss: 0.731449\tvalid's multi_logloss: 1.0163\n",
      "[122]\ttrain's multi_logloss: 0.729965\tvalid's multi_logloss: 1.01683\n",
      "[123]\ttrain's multi_logloss: 0.728339\tvalid's multi_logloss: 1.01882\n",
      "[124]\ttrain's multi_logloss: 0.726441\tvalid's multi_logloss: 1.02106\n",
      "[125]\ttrain's multi_logloss: 0.724573\tvalid's multi_logloss: 1.02331\n",
      "[126]\ttrain's multi_logloss: 0.723529\tvalid's multi_logloss: 1.0213\n",
      "[127]\ttrain's multi_logloss: 0.722193\tvalid's multi_logloss: 1.02095\n",
      "[128]\ttrain's multi_logloss: 0.720626\tvalid's multi_logloss: 1.02099\n",
      "[129]\ttrain's multi_logloss: 0.719087\tvalid's multi_logloss: 1.01972\n",
      "[130]\ttrain's multi_logloss: 0.717713\tvalid's multi_logloss: 1.01845\n",
      "[131]\ttrain's multi_logloss: 0.716151\tvalid's multi_logloss: 1.01904\n",
      "[132]\ttrain's multi_logloss: 0.714621\tvalid's multi_logloss: 1.01961\n",
      "[133]\ttrain's multi_logloss: 0.713055\tvalid's multi_logloss: 1.02079\n",
      "[134]\ttrain's multi_logloss: 0.711418\tvalid's multi_logloss: 1.02186\n",
      "[135]\ttrain's multi_logloss: 0.709985\tvalid's multi_logloss: 1.02239\n",
      "[136]\ttrain's multi_logloss: 0.708829\tvalid's multi_logloss: 1.02161\n",
      "[137]\ttrain's multi_logloss: 0.707425\tvalid's multi_logloss: 1.02093\n",
      "[138]\ttrain's multi_logloss: 0.706145\tvalid's multi_logloss: 1.01886\n",
      "[139]\ttrain's multi_logloss: 0.704903\tvalid's multi_logloss: 1.01682\n",
      "[140]\ttrain's multi_logloss: 0.703581\tvalid's multi_logloss: 1.01661\n",
      "[141]\ttrain's multi_logloss: 0.702053\tvalid's multi_logloss: 1.01751\n",
      "[142]\ttrain's multi_logloss: 0.700282\tvalid's multi_logloss: 1.01816\n",
      "[143]\ttrain's multi_logloss: 0.699421\tvalid's multi_logloss: 1.0184\n",
      "[144]\ttrain's multi_logloss: 0.697636\tvalid's multi_logloss: 1.01768\n",
      "[145]\ttrain's multi_logloss: 0.696128\tvalid's multi_logloss: 1.01723\n",
      "[146]\ttrain's multi_logloss: 0.694748\tvalid's multi_logloss: 1.0168\n",
      "[147]\ttrain's multi_logloss: 0.692903\tvalid's multi_logloss: 1.01791\n",
      "[148]\ttrain's multi_logloss: 0.691703\tvalid's multi_logloss: 1.01846\n",
      "[149]\ttrain's multi_logloss: 0.690481\tvalid's multi_logloss: 1.01848\n",
      "[150]\ttrain's multi_logloss: 0.688244\tvalid's multi_logloss: 1.01857\n",
      "[151]\ttrain's multi_logloss: 0.686071\tvalid's multi_logloss: 1.02037\n",
      "[152]\ttrain's multi_logloss: 0.684234\tvalid's multi_logloss: 1.02032\n",
      "[153]\ttrain's multi_logloss: 0.68235\tvalid's multi_logloss: 1.02043\n",
      "[154]\ttrain's multi_logloss: 0.680407\tvalid's multi_logloss: 1.02107\n",
      "[155]\ttrain's multi_logloss: 0.678276\tvalid's multi_logloss: 1.02219\n",
      "[156]\ttrain's multi_logloss: 0.67671\tvalid's multi_logloss: 1.02358\n",
      "[157]\ttrain's multi_logloss: 0.674658\tvalid's multi_logloss: 1.02466\n",
      "[158]\ttrain's multi_logloss: 0.672942\tvalid's multi_logloss: 1.02554\n",
      "[159]\ttrain's multi_logloss: 0.670996\tvalid's multi_logloss: 1.02576\n",
      "[160]\ttrain's multi_logloss: 0.669629\tvalid's multi_logloss: 1.0265\n",
      "[161]\ttrain's multi_logloss: 0.668255\tvalid's multi_logloss: 1.02627\n",
      "[162]\ttrain's multi_logloss: 0.666866\tvalid's multi_logloss: 1.02731\n",
      "[163]\ttrain's multi_logloss: 0.665261\tvalid's multi_logloss: 1.02783\n",
      "[164]\ttrain's multi_logloss: 0.663685\tvalid's multi_logloss: 1.02837\n",
      "[165]\ttrain's multi_logloss: 0.662137\tvalid's multi_logloss: 1.02893\n",
      "[166]\ttrain's multi_logloss: 0.661061\tvalid's multi_logloss: 1.02933\n",
      "[167]\ttrain's multi_logloss: 0.659043\tvalid's multi_logloss: 1.03032\n",
      "[168]\ttrain's multi_logloss: 0.657062\tvalid's multi_logloss: 1.03132\n",
      "[169]\ttrain's multi_logloss: 0.655037\tvalid's multi_logloss: 1.03164\n",
      "[170]\ttrain's multi_logloss: 0.653051\tvalid's multi_logloss: 1.03197\n",
      "[171]\ttrain's multi_logloss: 0.651204\tvalid's multi_logloss: 1.03282\n",
      "[172]\ttrain's multi_logloss: 0.649637\tvalid's multi_logloss: 1.0322\n",
      "[173]\ttrain's multi_logloss: 0.6481\tvalid's multi_logloss: 1.03305\n",
      "[174]\ttrain's multi_logloss: 0.64677\tvalid's multi_logloss: 1.03389\n",
      "[175]\ttrain's multi_logloss: 0.645463\tvalid's multi_logloss: 1.03575\n",
      "[176]\ttrain's multi_logloss: 0.644121\tvalid's multi_logloss: 1.03359\n",
      "[177]\ttrain's multi_logloss: 0.642718\tvalid's multi_logloss: 1.03213\n",
      "[178]\ttrain's multi_logloss: 0.641031\tvalid's multi_logloss: 1.03148\n",
      "[179]\ttrain's multi_logloss: 0.639519\tvalid's multi_logloss: 1.03085\n",
      "[180]\ttrain's multi_logloss: 0.637899\tvalid's multi_logloss: 1.02887\n",
      "[181]\ttrain's multi_logloss: 0.635904\tvalid's multi_logloss: 1.02851\n",
      "[182]\ttrain's multi_logloss: 0.634591\tvalid's multi_logloss: 1.0277\n",
      "[183]\ttrain's multi_logloss: 0.632979\tvalid's multi_logloss: 1.02786\n",
      "[184]\ttrain's multi_logloss: 0.631607\tvalid's multi_logloss: 1.02828\n",
      "[185]\ttrain's multi_logloss: 0.630273\tvalid's multi_logloss: 1.02826\n",
      "[186]\ttrain's multi_logloss: 0.628548\tvalid's multi_logloss: 1.02944\n",
      "[187]\ttrain's multi_logloss: 0.626789\tvalid's multi_logloss: 1.0275\n",
      "[188]\ttrain's multi_logloss: 0.624961\tvalid's multi_logloss: 1.02645\n",
      "[189]\ttrain's multi_logloss: 0.622953\tvalid's multi_logloss: 1.02521\n",
      "[190]\ttrain's multi_logloss: 0.621313\tvalid's multi_logloss: 1.02617\n",
      "[191]\ttrain's multi_logloss: 0.620107\tvalid's multi_logloss: 1.02483\n",
      "[192]\ttrain's multi_logloss: 0.618892\tvalid's multi_logloss: 1.02441\n",
      "[193]\ttrain's multi_logloss: 0.617547\tvalid's multi_logloss: 1.02344\n",
      "[194]\ttrain's multi_logloss: 0.616677\tvalid's multi_logloss: 1.02204\n",
      "[195]\ttrain's multi_logloss: 0.615653\tvalid's multi_logloss: 1.02153\n",
      "[196]\ttrain's multi_logloss: 0.614723\tvalid's multi_logloss: 1.02095\n",
      "[197]\ttrain's multi_logloss: 0.613402\tvalid's multi_logloss: 1.02037\n",
      "[198]\ttrain's multi_logloss: 0.612209\tvalid's multi_logloss: 1.02149\n",
      "[199]\ttrain's multi_logloss: 0.611209\tvalid's multi_logloss: 1.02301\n",
      "[200]\ttrain's multi_logloss: 0.610008\tvalid's multi_logloss: 1.02147\n",
      "[201]\ttrain's multi_logloss: 0.608056\tvalid's multi_logloss: 1.02324\n",
      "[202]\ttrain's multi_logloss: 0.606399\tvalid's multi_logloss: 1.02174\n",
      "[203]\ttrain's multi_logloss: 0.603946\tvalid's multi_logloss: 1.02252\n",
      "[204]\ttrain's multi_logloss: 0.601869\tvalid's multi_logloss: 1.02296\n",
      "[205]\ttrain's multi_logloss: 0.599994\tvalid's multi_logloss: 1.02336\n",
      "[206]\ttrain's multi_logloss: 0.599176\tvalid's multi_logloss: 1.02315\n",
      "[207]\ttrain's multi_logloss: 0.598369\tvalid's multi_logloss: 1.02245\n",
      "[208]\ttrain's multi_logloss: 0.597581\tvalid's multi_logloss: 1.02176\n",
      "[209]\ttrain's multi_logloss: 0.596849\tvalid's multi_logloss: 1.02073\n",
      "[210]\ttrain's multi_logloss: 0.595991\tvalid's multi_logloss: 1.02087\n",
      "[211]\ttrain's multi_logloss: 0.594392\tvalid's multi_logloss: 1.0203\n",
      "[212]\ttrain's multi_logloss: 0.593434\tvalid's multi_logloss: 1.02014\n",
      "[213]\ttrain's multi_logloss: 0.592398\tvalid's multi_logloss: 1.01914\n",
      "[214]\ttrain's multi_logloss: 0.591297\tvalid's multi_logloss: 1.01927\n",
      "[215]\ttrain's multi_logloss: 0.590253\tvalid's multi_logloss: 1.01854\n",
      "[216]\ttrain's multi_logloss: 0.588819\tvalid's multi_logloss: 1.01743\n",
      "[217]\ttrain's multi_logloss: 0.587014\tvalid's multi_logloss: 1.01764\n",
      "[218]\ttrain's multi_logloss: 0.585507\tvalid's multi_logloss: 1.01849\n",
      "[219]\ttrain's multi_logloss: 0.583891\tvalid's multi_logloss: 1.01885\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttrain's multi_logloss: 0.734399\tvalid's multi_logloss: 1.013\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.45454545454545453\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09092\tvalid's SFC_loss: 1.09725\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08506\tvalid's SFC_loss: 1.09291\n",
      "[3]\ttrain's SFC_loss: 1.07689\tvalid's SFC_loss: 1.09299\n",
      "[4]\ttrain's SFC_loss: 1.07021\tvalid's SFC_loss: 1.09041\n",
      "[5]\ttrain's SFC_loss: 1.06512\tvalid's SFC_loss: 1.087\n",
      "[6]\ttrain's SFC_loss: 1.06012\tvalid's SFC_loss: 1.08613\n",
      "[7]\ttrain's SFC_loss: 1.05284\tvalid's SFC_loss: 1.08526\n",
      "[8]\ttrain's SFC_loss: 1.04534\tvalid's SFC_loss: 1.08466\n",
      "[9]\ttrain's SFC_loss: 1.03829\tvalid's SFC_loss: 1.08299\n",
      "[10]\ttrain's SFC_loss: 1.03153\tvalid's SFC_loss: 1.08169\n",
      "[11]\ttrain's SFC_loss: 1.02597\tvalid's SFC_loss: 1.07823\n",
      "[12]\ttrain's SFC_loss: 1.01976\tvalid's SFC_loss: 1.07536\n",
      "[13]\ttrain's SFC_loss: 1.01423\tvalid's SFC_loss: 1.07393\n",
      "[14]\ttrain's SFC_loss: 1.00778\tvalid's SFC_loss: 1.07329\n",
      "[15]\ttrain's SFC_loss: 1.00185\tvalid's SFC_loss: 1.07246\n",
      "[16]\ttrain's SFC_loss: 0.996503\tvalid's SFC_loss: 1.07299\n",
      "[17]\ttrain's SFC_loss: 0.989579\tvalid's SFC_loss: 1.07043\n",
      "[18]\ttrain's SFC_loss: 0.984309\tvalid's SFC_loss: 1.06764\n",
      "[19]\ttrain's SFC_loss: 0.979748\tvalid's SFC_loss: 1.06504\n",
      "[20]\ttrain's SFC_loss: 0.97467\tvalid's SFC_loss: 1.06643\n",
      "[21]\ttrain's SFC_loss: 0.969915\tvalid's SFC_loss: 1.068\n",
      "[22]\ttrain's SFC_loss: 0.965213\tvalid's SFC_loss: 1.06799\n",
      "[23]\ttrain's SFC_loss: 0.95958\tvalid's SFC_loss: 1.07197\n",
      "[24]\ttrain's SFC_loss: 0.954612\tvalid's SFC_loss: 1.07614\n",
      "[25]\ttrain's SFC_loss: 0.949051\tvalid's SFC_loss: 1.07222\n",
      "[26]\ttrain's SFC_loss: 0.944111\tvalid's SFC_loss: 1.06936\n",
      "[27]\ttrain's SFC_loss: 0.940248\tvalid's SFC_loss: 1.06956\n",
      "[28]\ttrain's SFC_loss: 0.936065\tvalid's SFC_loss: 1.06914\n",
      "[29]\ttrain's SFC_loss: 0.932022\tvalid's SFC_loss: 1.0678\n",
      "[30]\ttrain's SFC_loss: 0.927671\tvalid's SFC_loss: 1.06586\n",
      "[31]\ttrain's SFC_loss: 0.923265\tvalid's SFC_loss: 1.06378\n",
      "[32]\ttrain's SFC_loss: 0.918453\tvalid's SFC_loss: 1.05984\n",
      "[33]\ttrain's SFC_loss: 0.914741\tvalid's SFC_loss: 1.05727\n",
      "[34]\ttrain's SFC_loss: 0.910289\tvalid's SFC_loss: 1.05357\n",
      "[35]\ttrain's SFC_loss: 0.905963\tvalid's SFC_loss: 1.05218\n",
      "[36]\ttrain's SFC_loss: 0.901637\tvalid's SFC_loss: 1.04991\n",
      "[37]\ttrain's SFC_loss: 0.896285\tvalid's SFC_loss: 1.04785\n",
      "[38]\ttrain's SFC_loss: 0.892069\tvalid's SFC_loss: 1.04524\n",
      "[39]\ttrain's SFC_loss: 0.88831\tvalid's SFC_loss: 1.04222\n",
      "[40]\ttrain's SFC_loss: 0.88414\tvalid's SFC_loss: 1.04259\n",
      "[41]\ttrain's SFC_loss: 0.880482\tvalid's SFC_loss: 1.04077\n",
      "[42]\ttrain's SFC_loss: 0.876503\tvalid's SFC_loss: 1.03981\n",
      "[43]\ttrain's SFC_loss: 0.873258\tvalid's SFC_loss: 1.0377\n",
      "[44]\ttrain's SFC_loss: 0.869472\tvalid's SFC_loss: 1.03667\n",
      "[45]\ttrain's SFC_loss: 0.866392\tvalid's SFC_loss: 1.03731\n",
      "[46]\ttrain's SFC_loss: 0.862256\tvalid's SFC_loss: 1.03462\n",
      "[47]\ttrain's SFC_loss: 0.858556\tvalid's SFC_loss: 1.03337\n",
      "[48]\ttrain's SFC_loss: 0.854588\tvalid's SFC_loss: 1.03373\n",
      "[49]\ttrain's SFC_loss: 0.851735\tvalid's SFC_loss: 1.03562\n",
      "[50]\ttrain's SFC_loss: 0.848706\tvalid's SFC_loss: 1.03852\n",
      "[51]\ttrain's SFC_loss: 0.845175\tvalid's SFC_loss: 1.03759\n",
      "[52]\ttrain's SFC_loss: 0.840407\tvalid's SFC_loss: 1.03627\n",
      "[53]\ttrain's SFC_loss: 0.836582\tvalid's SFC_loss: 1.03516\n",
      "[54]\ttrain's SFC_loss: 0.83385\tvalid's SFC_loss: 1.03702\n",
      "[55]\ttrain's SFC_loss: 0.829358\tvalid's SFC_loss: 1.03589\n",
      "[56]\ttrain's SFC_loss: 0.825404\tvalid's SFC_loss: 1.03617\n",
      "[57]\ttrain's SFC_loss: 0.822547\tvalid's SFC_loss: 1.03316\n",
      "[58]\ttrain's SFC_loss: 0.818991\tvalid's SFC_loss: 1.03268\n",
      "[59]\ttrain's SFC_loss: 0.815607\tvalid's SFC_loss: 1.03042\n",
      "[60]\ttrain's SFC_loss: 0.811922\tvalid's SFC_loss: 1.03006\n",
      "[61]\ttrain's SFC_loss: 0.808352\tvalid's SFC_loss: 1.03093\n",
      "[62]\ttrain's SFC_loss: 0.804674\tvalid's SFC_loss: 1.03091\n",
      "[63]\ttrain's SFC_loss: 0.801748\tvalid's SFC_loss: 1.03031\n",
      "[64]\ttrain's SFC_loss: 0.797787\tvalid's SFC_loss: 1.03238\n",
      "[65]\ttrain's SFC_loss: 0.794604\tvalid's SFC_loss: 1.03255\n",
      "[66]\ttrain's SFC_loss: 0.791418\tvalid's SFC_loss: 1.03307\n",
      "[67]\ttrain's SFC_loss: 0.787787\tvalid's SFC_loss: 1.02946\n",
      "[68]\ttrain's SFC_loss: 0.784216\tvalid's SFC_loss: 1.02839\n",
      "[69]\ttrain's SFC_loss: 0.781186\tvalid's SFC_loss: 1.02883\n",
      "[70]\ttrain's SFC_loss: 0.777925\tvalid's SFC_loss: 1.0258\n",
      "[71]\ttrain's SFC_loss: 0.775506\tvalid's SFC_loss: 1.02328\n",
      "[72]\ttrain's SFC_loss: 0.773295\tvalid's SFC_loss: 1.02201\n",
      "[73]\ttrain's SFC_loss: 0.770509\tvalid's SFC_loss: 1.01896\n",
      "[74]\ttrain's SFC_loss: 0.767823\tvalid's SFC_loss: 1.01857\n",
      "[75]\ttrain's SFC_loss: 0.765914\tvalid's SFC_loss: 1.01963\n",
      "[76]\ttrain's SFC_loss: 0.762829\tvalid's SFC_loss: 1.01962\n",
      "[77]\ttrain's SFC_loss: 0.759422\tvalid's SFC_loss: 1.0201\n",
      "[78]\ttrain's SFC_loss: 0.757044\tvalid's SFC_loss: 1.02177\n",
      "[79]\ttrain's SFC_loss: 0.753769\tvalid's SFC_loss: 1.02252\n",
      "[80]\ttrain's SFC_loss: 0.750789\tvalid's SFC_loss: 1.0232\n",
      "[81]\ttrain's SFC_loss: 0.748451\tvalid's SFC_loss: 1.02062\n",
      "[82]\ttrain's SFC_loss: 0.745725\tvalid's SFC_loss: 1.01738\n",
      "[83]\ttrain's SFC_loss: 0.742778\tvalid's SFC_loss: 1.01747\n",
      "[84]\ttrain's SFC_loss: 0.739534\tvalid's SFC_loss: 1.01902\n",
      "[85]\ttrain's SFC_loss: 0.737052\tvalid's SFC_loss: 1.01917\n",
      "[86]\ttrain's SFC_loss: 0.734432\tvalid's SFC_loss: 1.01774\n",
      "[87]\ttrain's SFC_loss: 0.732026\tvalid's SFC_loss: 1.01732\n",
      "[88]\ttrain's SFC_loss: 0.729333\tvalid's SFC_loss: 1.01669\n",
      "[89]\ttrain's SFC_loss: 0.726964\tvalid's SFC_loss: 1.01675\n",
      "[90]\ttrain's SFC_loss: 0.72502\tvalid's SFC_loss: 1.01696\n",
      "[91]\ttrain's SFC_loss: 0.722447\tvalid's SFC_loss: 1.01458\n",
      "[92]\ttrain's SFC_loss: 0.719639\tvalid's SFC_loss: 1.01237\n",
      "[93]\ttrain's SFC_loss: 0.717339\tvalid's SFC_loss: 1.01032\n",
      "[94]\ttrain's SFC_loss: 0.714667\tvalid's SFC_loss: 1.00955\n",
      "[95]\ttrain's SFC_loss: 0.712584\tvalid's SFC_loss: 1.01145\n",
      "[96]\ttrain's SFC_loss: 0.709848\tvalid's SFC_loss: 1.0128\n",
      "[97]\ttrain's SFC_loss: 0.70748\tvalid's SFC_loss: 1.01437\n",
      "[98]\ttrain's SFC_loss: 0.70493\tvalid's SFC_loss: 1.01301\n",
      "[99]\ttrain's SFC_loss: 0.702104\tvalid's SFC_loss: 1.01216\n",
      "[100]\ttrain's SFC_loss: 0.700147\tvalid's SFC_loss: 1.01489\n",
      "[101]\ttrain's SFC_loss: 0.698097\tvalid's SFC_loss: 1.01627\n",
      "[102]\ttrain's SFC_loss: 0.695739\tvalid's SFC_loss: 1.01761\n",
      "[103]\ttrain's SFC_loss: 0.693357\tvalid's SFC_loss: 1.01833\n",
      "[104]\ttrain's SFC_loss: 0.691309\tvalid's SFC_loss: 1.02091\n",
      "[105]\ttrain's SFC_loss: 0.689365\tvalid's SFC_loss: 1.01935\n",
      "[106]\ttrain's SFC_loss: 0.687632\tvalid's SFC_loss: 1.01879\n",
      "[107]\ttrain's SFC_loss: 0.685286\tvalid's SFC_loss: 1.01742\n",
      "[108]\ttrain's SFC_loss: 0.683465\tvalid's SFC_loss: 1.018\n",
      "[109]\ttrain's SFC_loss: 0.681727\tvalid's SFC_loss: 1.01948\n",
      "[110]\ttrain's SFC_loss: 0.680121\tvalid's SFC_loss: 1.02204\n",
      "[111]\ttrain's SFC_loss: 0.677965\tvalid's SFC_loss: 1.02234\n",
      "[112]\ttrain's SFC_loss: 0.675745\tvalid's SFC_loss: 1.02057\n",
      "[113]\ttrain's SFC_loss: 0.673828\tvalid's SFC_loss: 1.02127\n",
      "[114]\ttrain's SFC_loss: 0.672168\tvalid's SFC_loss: 1.01729\n",
      "[115]\ttrain's SFC_loss: 0.670661\tvalid's SFC_loss: 1.01451\n",
      "[116]\ttrain's SFC_loss: 0.669323\tvalid's SFC_loss: 1.01163\n",
      "[117]\ttrain's SFC_loss: 0.667319\tvalid's SFC_loss: 1.01165\n",
      "[118]\ttrain's SFC_loss: 0.665611\tvalid's SFC_loss: 1.00957\n",
      "[119]\ttrain's SFC_loss: 0.663844\tvalid's SFC_loss: 1.00731\n",
      "[120]\ttrain's SFC_loss: 0.662693\tvalid's SFC_loss: 1.00733\n",
      "[121]\ttrain's SFC_loss: 0.661128\tvalid's SFC_loss: 1.00937\n",
      "[122]\ttrain's SFC_loss: 0.658995\tvalid's SFC_loss: 1.01084\n",
      "[123]\ttrain's SFC_loss: 0.657408\tvalid's SFC_loss: 1.01066\n",
      "[124]\ttrain's SFC_loss: 0.656013\tvalid's SFC_loss: 1.01084\n",
      "[125]\ttrain's SFC_loss: 0.65388\tvalid's SFC_loss: 1.01062\n",
      "[126]\ttrain's SFC_loss: 0.652142\tvalid's SFC_loss: 1.0113\n",
      "[127]\ttrain's SFC_loss: 0.650572\tvalid's SFC_loss: 1.00956\n",
      "[128]\ttrain's SFC_loss: 0.649139\tvalid's SFC_loss: 1.01128\n",
      "[129]\ttrain's SFC_loss: 0.647446\tvalid's SFC_loss: 1.01245\n",
      "[130]\ttrain's SFC_loss: 0.646116\tvalid's SFC_loss: 1.01273\n",
      "[131]\ttrain's SFC_loss: 0.644297\tvalid's SFC_loss: 1.01439\n",
      "[132]\ttrain's SFC_loss: 0.642516\tvalid's SFC_loss: 1.0164\n",
      "[133]\ttrain's SFC_loss: 0.640933\tvalid's SFC_loss: 1.01824\n",
      "[134]\ttrain's SFC_loss: 0.639119\tvalid's SFC_loss: 1.01668\n",
      "[135]\ttrain's SFC_loss: 0.637786\tvalid's SFC_loss: 1.01564\n",
      "[136]\ttrain's SFC_loss: 0.635993\tvalid's SFC_loss: 1.01597\n",
      "[137]\ttrain's SFC_loss: 0.634947\tvalid's SFC_loss: 1.0161\n",
      "[138]\ttrain's SFC_loss: 0.633062\tvalid's SFC_loss: 1.01343\n",
      "[139]\ttrain's SFC_loss: 0.631398\tvalid's SFC_loss: 1.01321\n",
      "[140]\ttrain's SFC_loss: 0.629878\tvalid's SFC_loss: 1.01265\n",
      "[141]\ttrain's SFC_loss: 0.628033\tvalid's SFC_loss: 1.01284\n",
      "[142]\ttrain's SFC_loss: 0.626559\tvalid's SFC_loss: 1.01365\n",
      "[143]\ttrain's SFC_loss: 0.625052\tvalid's SFC_loss: 1.01626\n",
      "[144]\ttrain's SFC_loss: 0.623594\tvalid's SFC_loss: 1.01889\n",
      "[145]\ttrain's SFC_loss: 0.62203\tvalid's SFC_loss: 1.02207\n",
      "[146]\ttrain's SFC_loss: 0.619759\tvalid's SFC_loss: 1.02084\n",
      "[147]\ttrain's SFC_loss: 0.617745\tvalid's SFC_loss: 1.01924\n",
      "[148]\ttrain's SFC_loss: 0.615853\tvalid's SFC_loss: 1.02062\n",
      "[149]\ttrain's SFC_loss: 0.613629\tvalid's SFC_loss: 1.02154\n",
      "[150]\ttrain's SFC_loss: 0.611325\tvalid's SFC_loss: 1.01967\n",
      "[151]\ttrain's SFC_loss: 0.609311\tvalid's SFC_loss: 1.02221\n",
      "[152]\ttrain's SFC_loss: 0.607325\tvalid's SFC_loss: 1.02235\n",
      "[153]\ttrain's SFC_loss: 0.605462\tvalid's SFC_loss: 1.02077\n",
      "[154]\ttrain's SFC_loss: 0.60344\tvalid's SFC_loss: 1.02023\n",
      "[155]\ttrain's SFC_loss: 0.601587\tvalid's SFC_loss: 1.01795\n",
      "[156]\ttrain's SFC_loss: 0.599684\tvalid's SFC_loss: 1.0168\n",
      "[157]\ttrain's SFC_loss: 0.597771\tvalid's SFC_loss: 1.0181\n",
      "[158]\ttrain's SFC_loss: 0.595829\tvalid's SFC_loss: 1.0165\n",
      "[159]\ttrain's SFC_loss: 0.593156\tvalid's SFC_loss: 1.01789\n",
      "[160]\ttrain's SFC_loss: 0.591539\tvalid's SFC_loss: 1.02021\n",
      "[161]\ttrain's SFC_loss: 0.590332\tvalid's SFC_loss: 1.01999\n",
      "[162]\ttrain's SFC_loss: 0.588762\tvalid's SFC_loss: 1.02103\n",
      "[163]\ttrain's SFC_loss: 0.587362\tvalid's SFC_loss: 1.02202\n",
      "[164]\ttrain's SFC_loss: 0.585986\tvalid's SFC_loss: 1.02435\n",
      "[165]\ttrain's SFC_loss: 0.584555\tvalid's SFC_loss: 1.02607\n",
      "[166]\ttrain's SFC_loss: 0.583264\tvalid's SFC_loss: 1.02461\n",
      "[167]\ttrain's SFC_loss: 0.581409\tvalid's SFC_loss: 1.02579\n",
      "[168]\ttrain's SFC_loss: 0.579716\tvalid's SFC_loss: 1.02732\n",
      "[169]\ttrain's SFC_loss: 0.577507\tvalid's SFC_loss: 1.02715\n",
      "[170]\ttrain's SFC_loss: 0.575817\tvalid's SFC_loss: 1.02718\n",
      "[171]\ttrain's SFC_loss: 0.574248\tvalid's SFC_loss: 1.02826\n",
      "[172]\ttrain's SFC_loss: 0.572475\tvalid's SFC_loss: 1.02712\n",
      "[173]\ttrain's SFC_loss: 0.570901\tvalid's SFC_loss: 1.03056\n",
      "[174]\ttrain's SFC_loss: 0.569402\tvalid's SFC_loss: 1.03334\n",
      "[175]\ttrain's SFC_loss: 0.567765\tvalid's SFC_loss: 1.03495\n",
      "[176]\ttrain's SFC_loss: 0.566387\tvalid's SFC_loss: 1.03569\n",
      "[177]\ttrain's SFC_loss: 0.564803\tvalid's SFC_loss: 1.03652\n",
      "[178]\ttrain's SFC_loss: 0.562933\tvalid's SFC_loss: 1.03409\n",
      "[179]\ttrain's SFC_loss: 0.561573\tvalid's SFC_loss: 1.0322\n",
      "[180]\ttrain's SFC_loss: 0.560541\tvalid's SFC_loss: 1.033\n",
      "[181]\ttrain's SFC_loss: 0.558736\tvalid's SFC_loss: 1.03168\n",
      "[182]\ttrain's SFC_loss: 0.55727\tvalid's SFC_loss: 1.03249\n",
      "[183]\ttrain's SFC_loss: 0.555953\tvalid's SFC_loss: 1.03204\n",
      "[184]\ttrain's SFC_loss: 0.554629\tvalid's SFC_loss: 1.03133\n",
      "[185]\ttrain's SFC_loss: 0.552895\tvalid's SFC_loss: 1.03034\n",
      "[186]\ttrain's SFC_loss: 0.551066\tvalid's SFC_loss: 1.02885\n",
      "[187]\ttrain's SFC_loss: 0.549246\tvalid's SFC_loss: 1.02975\n",
      "[188]\ttrain's SFC_loss: 0.547625\tvalid's SFC_loss: 1.02824\n",
      "[189]\ttrain's SFC_loss: 0.546359\tvalid's SFC_loss: 1.03005\n",
      "[190]\ttrain's SFC_loss: 0.544481\tvalid's SFC_loss: 1.02841\n",
      "[191]\ttrain's SFC_loss: 0.543688\tvalid's SFC_loss: 1.02956\n",
      "[192]\ttrain's SFC_loss: 0.54271\tvalid's SFC_loss: 1.03076\n",
      "[193]\ttrain's SFC_loss: 0.541611\tvalid's SFC_loss: 1.03031\n",
      "[194]\ttrain's SFC_loss: 0.541016\tvalid's SFC_loss: 1.03108\n",
      "[195]\ttrain's SFC_loss: 0.540042\tvalid's SFC_loss: 1.03017\n",
      "[196]\ttrain's SFC_loss: 0.539162\tvalid's SFC_loss: 1.03154\n",
      "[197]\ttrain's SFC_loss: 0.53777\tvalid's SFC_loss: 1.032\n",
      "[198]\ttrain's SFC_loss: 0.536405\tvalid's SFC_loss: 1.02974\n",
      "[199]\ttrain's SFC_loss: 0.535251\tvalid's SFC_loss: 1.0308\n",
      "[200]\ttrain's SFC_loss: 0.534191\tvalid's SFC_loss: 1.03151\n",
      "[201]\ttrain's SFC_loss: 0.531964\tvalid's SFC_loss: 1.03316\n",
      "[202]\ttrain's SFC_loss: 0.530334\tvalid's SFC_loss: 1.03285\n",
      "[203]\ttrain's SFC_loss: 0.528273\tvalid's SFC_loss: 1.03547\n",
      "[204]\ttrain's SFC_loss: 0.525932\tvalid's SFC_loss: 1.03759\n",
      "[205]\ttrain's SFC_loss: 0.524311\tvalid's SFC_loss: 1.03531\n",
      "[206]\ttrain's SFC_loss: 0.52348\tvalid's SFC_loss: 1.03543\n",
      "[207]\ttrain's SFC_loss: 0.522856\tvalid's SFC_loss: 1.0333\n",
      "[208]\ttrain's SFC_loss: 0.522219\tvalid's SFC_loss: 1.03304\n",
      "[209]\ttrain's SFC_loss: 0.521702\tvalid's SFC_loss: 1.03169\n",
      "[210]\ttrain's SFC_loss: 0.520947\tvalid's SFC_loss: 1.02996\n",
      "[211]\ttrain's SFC_loss: 0.519674\tvalid's SFC_loss: 1.02928\n",
      "[212]\ttrain's SFC_loss: 0.51858\tvalid's SFC_loss: 1.02849\n",
      "[213]\ttrain's SFC_loss: 0.517312\tvalid's SFC_loss: 1.02998\n",
      "[214]\ttrain's SFC_loss: 0.516472\tvalid's SFC_loss: 1.02974\n",
      "[215]\ttrain's SFC_loss: 0.515571\tvalid's SFC_loss: 1.02964\n",
      "[216]\ttrain's SFC_loss: 0.514061\tvalid's SFC_loss: 1.0287\n",
      "[217]\ttrain's SFC_loss: 0.512239\tvalid's SFC_loss: 1.02778\n",
      "[218]\ttrain's SFC_loss: 0.510945\tvalid's SFC_loss: 1.027\n",
      "[219]\ttrain's SFC_loss: 0.509703\tvalid's SFC_loss: 1.02573\n",
      "Early stopping, best iteration is:\n",
      "[119]\ttrain's SFC_loss: 0.663844\tvalid's SFC_loss: 1.00731\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.45454545454545453\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1   -0.029223\n",
      "1    feature2    0.048537\n",
      "2    feature3   -0.342294\n",
      "3    feature4    0.057728\n",
      "4    feature5    0.058484\n",
      "5    feature6   -0.007309\n",
      "6    feature7    0.008264\n",
      "7    feature8    0.042444\n",
      "8    feature9    0.007373\n",
      "9   feature10   -0.007144\n",
      "10  feature11   -0.016532\n",
      "11  feature12    0.053142\n",
      "12  feature13    0.049382\n",
      "13  feature14    0.072592\n",
      "14  feature15    0.009056\n",
      "15  feature16   -0.032110\n",
      "16  feature17    0.072610\n",
      "17  feature18   -0.041098\n",
      "18  feature19   -0.024290\n",
      "19  feature20    0.020389\n",
      "-------------------- 7 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05864\tvalid's multi_logloss: 1.06627\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05481\tvalid's multi_logloss: 1.06613\n",
      "[3]\ttrain's multi_logloss: 1.05143\tvalid's multi_logloss: 1.06345\n",
      "[4]\ttrain's multi_logloss: 1.04733\tvalid's multi_logloss: 1.06205\n",
      "[5]\ttrain's multi_logloss: 1.04365\tvalid's multi_logloss: 1.06197\n",
      "[6]\ttrain's multi_logloss: 1.04049\tvalid's multi_logloss: 1.05854\n",
      "[7]\ttrain's multi_logloss: 1.03684\tvalid's multi_logloss: 1.0572\n",
      "[8]\ttrain's multi_logloss: 1.03392\tvalid's multi_logloss: 1.0544\n",
      "[9]\ttrain's multi_logloss: 1.03133\tvalid's multi_logloss: 1.05109\n",
      "[10]\ttrain's multi_logloss: 1.02856\tvalid's multi_logloss: 1.04788\n",
      "[11]\ttrain's multi_logloss: 1.02506\tvalid's multi_logloss: 1.04616\n",
      "[12]\ttrain's multi_logloss: 1.02228\tvalid's multi_logloss: 1.04455\n",
      "[13]\ttrain's multi_logloss: 1.01895\tvalid's multi_logloss: 1.04146\n",
      "[14]\ttrain's multi_logloss: 1.01538\tvalid's multi_logloss: 1.03986\n",
      "[15]\ttrain's multi_logloss: 1.01181\tvalid's multi_logloss: 1.0372\n",
      "[16]\ttrain's multi_logloss: 1.00841\tvalid's multi_logloss: 1.03671\n",
      "[17]\ttrain's multi_logloss: 1.00532\tvalid's multi_logloss: 1.03752\n",
      "[18]\ttrain's multi_logloss: 1.0025\tvalid's multi_logloss: 1.03907\n",
      "[19]\ttrain's multi_logloss: 0.999256\tvalid's multi_logloss: 1.03887\n",
      "[20]\ttrain's multi_logloss: 0.996448\tvalid's multi_logloss: 1.04062\n",
      "[21]\ttrain's multi_logloss: 0.992509\tvalid's multi_logloss: 1.03598\n",
      "[22]\ttrain's multi_logloss: 0.988986\tvalid's multi_logloss: 1.03328\n",
      "[23]\ttrain's multi_logloss: 0.985482\tvalid's multi_logloss: 1.03032\n",
      "[24]\ttrain's multi_logloss: 0.982057\tvalid's multi_logloss: 1.02743\n",
      "[25]\ttrain's multi_logloss: 0.978109\tvalid's multi_logloss: 1.02527\n",
      "[26]\ttrain's multi_logloss: 0.975538\tvalid's multi_logloss: 1.02454\n",
      "[27]\ttrain's multi_logloss: 0.972405\tvalid's multi_logloss: 1.022\n",
      "[28]\ttrain's multi_logloss: 0.969638\tvalid's multi_logloss: 1.01792\n",
      "[29]\ttrain's multi_logloss: 0.966213\tvalid's multi_logloss: 1.01736\n",
      "[30]\ttrain's multi_logloss: 0.962629\tvalid's multi_logloss: 1.01517\n",
      "[31]\ttrain's multi_logloss: 0.959505\tvalid's multi_logloss: 1.01466\n",
      "[32]\ttrain's multi_logloss: 0.956503\tvalid's multi_logloss: 1.00998\n",
      "[33]\ttrain's multi_logloss: 0.953718\tvalid's multi_logloss: 1.01036\n",
      "[34]\ttrain's multi_logloss: 0.950772\tvalid's multi_logloss: 1.00568\n",
      "[35]\ttrain's multi_logloss: 0.947877\tvalid's multi_logloss: 1.00452\n",
      "[36]\ttrain's multi_logloss: 0.944672\tvalid's multi_logloss: 1.00315\n",
      "[37]\ttrain's multi_logloss: 0.941916\tvalid's multi_logloss: 1.00264\n",
      "[38]\ttrain's multi_logloss: 0.93883\tvalid's multi_logloss: 1.00064\n",
      "[39]\ttrain's multi_logloss: 0.935498\tvalid's multi_logloss: 0.998957\n",
      "[40]\ttrain's multi_logloss: 0.931854\tvalid's multi_logloss: 0.995534\n",
      "[41]\ttrain's multi_logloss: 0.928375\tvalid's multi_logloss: 0.994436\n",
      "[42]\ttrain's multi_logloss: 0.924143\tvalid's multi_logloss: 0.993131\n",
      "[43]\ttrain's multi_logloss: 0.92012\tvalid's multi_logloss: 0.991708\n",
      "[44]\ttrain's multi_logloss: 0.916271\tvalid's multi_logloss: 0.990347\n",
      "[45]\ttrain's multi_logloss: 0.912272\tvalid's multi_logloss: 0.987882\n",
      "[46]\ttrain's multi_logloss: 0.909606\tvalid's multi_logloss: 0.987584\n",
      "[47]\ttrain's multi_logloss: 0.906889\tvalid's multi_logloss: 0.98822\n",
      "[48]\ttrain's multi_logloss: 0.904318\tvalid's multi_logloss: 0.987798\n",
      "[49]\ttrain's multi_logloss: 0.902173\tvalid's multi_logloss: 0.987236\n",
      "[50]\ttrain's multi_logloss: 0.899616\tvalid's multi_logloss: 0.987334\n",
      "[51]\ttrain's multi_logloss: 0.896396\tvalid's multi_logloss: 0.985595\n",
      "[52]\ttrain's multi_logloss: 0.893508\tvalid's multi_logloss: 0.985386\n",
      "[53]\ttrain's multi_logloss: 0.889967\tvalid's multi_logloss: 0.985211\n",
      "[54]\ttrain's multi_logloss: 0.887217\tvalid's multi_logloss: 0.985456\n",
      "[55]\ttrain's multi_logloss: 0.884266\tvalid's multi_logloss: 0.985332\n",
      "[56]\ttrain's multi_logloss: 0.882232\tvalid's multi_logloss: 0.984713\n",
      "[57]\ttrain's multi_logloss: 0.879914\tvalid's multi_logloss: 0.983556\n",
      "[58]\ttrain's multi_logloss: 0.877351\tvalid's multi_logloss: 0.983274\n",
      "[59]\ttrain's multi_logloss: 0.874805\tvalid's multi_logloss: 0.982874\n",
      "[60]\ttrain's multi_logloss: 0.872612\tvalid's multi_logloss: 0.981421\n",
      "[61]\ttrain's multi_logloss: 0.869552\tvalid's multi_logloss: 0.981638\n",
      "[62]\ttrain's multi_logloss: 0.867035\tvalid's multi_logloss: 0.979195\n",
      "[63]\ttrain's multi_logloss: 0.86393\tvalid's multi_logloss: 0.979357\n",
      "[64]\ttrain's multi_logloss: 0.861765\tvalid's multi_logloss: 0.978609\n",
      "[65]\ttrain's multi_logloss: 0.859787\tvalid's multi_logloss: 0.977349\n",
      "[66]\ttrain's multi_logloss: 0.857364\tvalid's multi_logloss: 0.975933\n",
      "[67]\ttrain's multi_logloss: 0.854812\tvalid's multi_logloss: 0.974912\n",
      "[68]\ttrain's multi_logloss: 0.852703\tvalid's multi_logloss: 0.973173\n",
      "[69]\ttrain's multi_logloss: 0.849996\tvalid's multi_logloss: 0.97189\n",
      "[70]\ttrain's multi_logloss: 0.847446\tvalid's multi_logloss: 0.970928\n",
      "[71]\ttrain's multi_logloss: 0.844267\tvalid's multi_logloss: 0.971061\n",
      "[72]\ttrain's multi_logloss: 0.841666\tvalid's multi_logloss: 0.969509\n",
      "[73]\ttrain's multi_logloss: 0.838753\tvalid's multi_logloss: 0.968616\n",
      "[74]\ttrain's multi_logloss: 0.835882\tvalid's multi_logloss: 0.967739\n",
      "[75]\ttrain's multi_logloss: 0.833062\tvalid's multi_logloss: 0.965628\n",
      "[76]\ttrain's multi_logloss: 0.830454\tvalid's multi_logloss: 0.963681\n",
      "[77]\ttrain's multi_logloss: 0.827831\tvalid's multi_logloss: 0.961402\n",
      "[78]\ttrain's multi_logloss: 0.825857\tvalid's multi_logloss: 0.960013\n",
      "[79]\ttrain's multi_logloss: 0.823772\tvalid's multi_logloss: 0.959681\n",
      "[80]\ttrain's multi_logloss: 0.821151\tvalid's multi_logloss: 0.958231\n",
      "[81]\ttrain's multi_logloss: 0.818617\tvalid's multi_logloss: 0.957195\n",
      "[82]\ttrain's multi_logloss: 0.816936\tvalid's multi_logloss: 0.955945\n",
      "[83]\ttrain's multi_logloss: 0.814389\tvalid's multi_logloss: 0.953405\n",
      "[84]\ttrain's multi_logloss: 0.812288\tvalid's multi_logloss: 0.950621\n",
      "[85]\ttrain's multi_logloss: 0.810527\tvalid's multi_logloss: 0.949429\n",
      "[86]\ttrain's multi_logloss: 0.808065\tvalid's multi_logloss: 0.948065\n",
      "[87]\ttrain's multi_logloss: 0.805375\tvalid's multi_logloss: 0.947843\n",
      "[88]\ttrain's multi_logloss: 0.803065\tvalid's multi_logloss: 0.947879\n",
      "[89]\ttrain's multi_logloss: 0.800893\tvalid's multi_logloss: 0.944602\n",
      "[90]\ttrain's multi_logloss: 0.798638\tvalid's multi_logloss: 0.944289\n",
      "[91]\ttrain's multi_logloss: 0.796234\tvalid's multi_logloss: 0.94323\n",
      "[92]\ttrain's multi_logloss: 0.793821\tvalid's multi_logloss: 0.941892\n",
      "[93]\ttrain's multi_logloss: 0.791318\tvalid's multi_logloss: 0.941392\n",
      "[94]\ttrain's multi_logloss: 0.788765\tvalid's multi_logloss: 0.940447\n",
      "[95]\ttrain's multi_logloss: 0.786328\tvalid's multi_logloss: 0.940014\n",
      "[96]\ttrain's multi_logloss: 0.784038\tvalid's multi_logloss: 0.941779\n",
      "[97]\ttrain's multi_logloss: 0.781661\tvalid's multi_logloss: 0.942194\n",
      "[98]\ttrain's multi_logloss: 0.779507\tvalid's multi_logloss: 0.942232\n",
      "[99]\ttrain's multi_logloss: 0.776984\tvalid's multi_logloss: 0.94217\n",
      "[100]\ttrain's multi_logloss: 0.774942\tvalid's multi_logloss: 0.942483\n",
      "[101]\ttrain's multi_logloss: 0.773183\tvalid's multi_logloss: 0.943887\n",
      "[102]\ttrain's multi_logloss: 0.770784\tvalid's multi_logloss: 0.943894\n",
      "[103]\ttrain's multi_logloss: 0.768902\tvalid's multi_logloss: 0.943958\n",
      "[104]\ttrain's multi_logloss: 0.766728\tvalid's multi_logloss: 0.943446\n",
      "[105]\ttrain's multi_logloss: 0.764133\tvalid's multi_logloss: 0.943061\n",
      "[106]\ttrain's multi_logloss: 0.762488\tvalid's multi_logloss: 0.942441\n",
      "[107]\ttrain's multi_logloss: 0.760705\tvalid's multi_logloss: 0.941911\n",
      "[108]\ttrain's multi_logloss: 0.75878\tvalid's multi_logloss: 0.940645\n",
      "[109]\ttrain's multi_logloss: 0.757231\tvalid's multi_logloss: 0.940344\n",
      "[110]\ttrain's multi_logloss: 0.755428\tvalid's multi_logloss: 0.938026\n",
      "[111]\ttrain's multi_logloss: 0.75357\tvalid's multi_logloss: 0.938162\n",
      "[112]\ttrain's multi_logloss: 0.752487\tvalid's multi_logloss: 0.937667\n",
      "[113]\ttrain's multi_logloss: 0.750928\tvalid's multi_logloss: 0.936424\n",
      "[114]\ttrain's multi_logloss: 0.749541\tvalid's multi_logloss: 0.937575\n",
      "[115]\ttrain's multi_logloss: 0.747988\tvalid's multi_logloss: 0.938349\n",
      "[116]\ttrain's multi_logloss: 0.746372\tvalid's multi_logloss: 0.939404\n",
      "[117]\ttrain's multi_logloss: 0.744599\tvalid's multi_logloss: 0.940046\n",
      "[118]\ttrain's multi_logloss: 0.742657\tvalid's multi_logloss: 0.939436\n",
      "[119]\ttrain's multi_logloss: 0.741208\tvalid's multi_logloss: 0.938931\n",
      "[120]\ttrain's multi_logloss: 0.739679\tvalid's multi_logloss: 0.939442\n",
      "[121]\ttrain's multi_logloss: 0.73747\tvalid's multi_logloss: 0.937282\n",
      "[122]\ttrain's multi_logloss: 0.735432\tvalid's multi_logloss: 0.937038\n",
      "[123]\ttrain's multi_logloss: 0.733111\tvalid's multi_logloss: 0.936424\n",
      "[124]\ttrain's multi_logloss: 0.731218\tvalid's multi_logloss: 0.935133\n",
      "[125]\ttrain's multi_logloss: 0.729013\tvalid's multi_logloss: 0.933188\n",
      "[126]\ttrain's multi_logloss: 0.727252\tvalid's multi_logloss: 0.931208\n",
      "[127]\ttrain's multi_logloss: 0.725874\tvalid's multi_logloss: 0.929602\n",
      "[128]\ttrain's multi_logloss: 0.723965\tvalid's multi_logloss: 0.928735\n",
      "[129]\ttrain's multi_logloss: 0.72228\tvalid's multi_logloss: 0.927021\n",
      "[130]\ttrain's multi_logloss: 0.720325\tvalid's multi_logloss: 0.926487\n",
      "[131]\ttrain's multi_logloss: 0.718744\tvalid's multi_logloss: 0.926894\n",
      "[132]\ttrain's multi_logloss: 0.717129\tvalid's multi_logloss: 0.928436\n",
      "[133]\ttrain's multi_logloss: 0.715392\tvalid's multi_logloss: 0.929119\n",
      "[134]\ttrain's multi_logloss: 0.71366\tvalid's multi_logloss: 0.93021\n",
      "[135]\ttrain's multi_logloss: 0.712088\tvalid's multi_logloss: 0.929956\n",
      "[136]\ttrain's multi_logloss: 0.710325\tvalid's multi_logloss: 0.929104\n",
      "[137]\ttrain's multi_logloss: 0.708221\tvalid's multi_logloss: 0.929315\n",
      "[138]\ttrain's multi_logloss: 0.706704\tvalid's multi_logloss: 0.930217\n",
      "[139]\ttrain's multi_logloss: 0.704996\tvalid's multi_logloss: 0.930747\n",
      "[140]\ttrain's multi_logloss: 0.703626\tvalid's multi_logloss: 0.930873\n",
      "[141]\ttrain's multi_logloss: 0.701816\tvalid's multi_logloss: 0.930913\n",
      "[142]\ttrain's multi_logloss: 0.700104\tvalid's multi_logloss: 0.931337\n",
      "[143]\ttrain's multi_logloss: 0.698368\tvalid's multi_logloss: 0.931189\n",
      "[144]\ttrain's multi_logloss: 0.69664\tvalid's multi_logloss: 0.931725\n",
      "[145]\ttrain's multi_logloss: 0.695582\tvalid's multi_logloss: 0.932467\n",
      "[146]\ttrain's multi_logloss: 0.693705\tvalid's multi_logloss: 0.933718\n",
      "[147]\ttrain's multi_logloss: 0.691809\tvalid's multi_logloss: 0.933256\n",
      "[148]\ttrain's multi_logloss: 0.689888\tvalid's multi_logloss: 0.934249\n",
      "[149]\ttrain's multi_logloss: 0.688306\tvalid's multi_logloss: 0.934722\n",
      "[150]\ttrain's multi_logloss: 0.686502\tvalid's multi_logloss: 0.935116\n",
      "[151]\ttrain's multi_logloss: 0.683988\tvalid's multi_logloss: 0.934169\n",
      "[152]\ttrain's multi_logloss: 0.681797\tvalid's multi_logloss: 0.934984\n",
      "[153]\ttrain's multi_logloss: 0.679969\tvalid's multi_logloss: 0.93493\n",
      "[154]\ttrain's multi_logloss: 0.677499\tvalid's multi_logloss: 0.933708\n",
      "[155]\ttrain's multi_logloss: 0.675359\tvalid's multi_logloss: 0.933534\n",
      "[156]\ttrain's multi_logloss: 0.673935\tvalid's multi_logloss: 0.933549\n",
      "[157]\ttrain's multi_logloss: 0.672274\tvalid's multi_logloss: 0.932905\n",
      "[158]\ttrain's multi_logloss: 0.670596\tvalid's multi_logloss: 0.933062\n",
      "[159]\ttrain's multi_logloss: 0.66898\tvalid's multi_logloss: 0.93496\n",
      "[160]\ttrain's multi_logloss: 0.667711\tvalid's multi_logloss: 0.934819\n",
      "[161]\ttrain's multi_logloss: 0.666183\tvalid's multi_logloss: 0.935724\n",
      "[162]\ttrain's multi_logloss: 0.66493\tvalid's multi_logloss: 0.934678\n",
      "[163]\ttrain's multi_logloss: 0.663573\tvalid's multi_logloss: 0.93464\n",
      "[164]\ttrain's multi_logloss: 0.662254\tvalid's multi_logloss: 0.933822\n",
      "[165]\ttrain's multi_logloss: 0.660924\tvalid's multi_logloss: 0.933604\n",
      "[166]\ttrain's multi_logloss: 0.659486\tvalid's multi_logloss: 0.933386\n",
      "[167]\ttrain's multi_logloss: 0.657885\tvalid's multi_logloss: 0.933846\n",
      "[168]\ttrain's multi_logloss: 0.656314\tvalid's multi_logloss: 0.934485\n",
      "[169]\ttrain's multi_logloss: 0.654718\tvalid's multi_logloss: 0.934283\n",
      "[170]\ttrain's multi_logloss: 0.653318\tvalid's multi_logloss: 0.934088\n",
      "[171]\ttrain's multi_logloss: 0.651852\tvalid's multi_logloss: 0.93496\n",
      "[172]\ttrain's multi_logloss: 0.649903\tvalid's multi_logloss: 0.933934\n",
      "[173]\ttrain's multi_logloss: 0.648022\tvalid's multi_logloss: 0.932703\n",
      "[174]\ttrain's multi_logloss: 0.6466\tvalid's multi_logloss: 0.932036\n",
      "[175]\ttrain's multi_logloss: 0.645275\tvalid's multi_logloss: 0.930671\n",
      "[176]\ttrain's multi_logloss: 0.643734\tvalid's multi_logloss: 0.929979\n",
      "[177]\ttrain's multi_logloss: 0.642288\tvalid's multi_logloss: 0.928239\n",
      "[178]\ttrain's multi_logloss: 0.640887\tvalid's multi_logloss: 0.927817\n",
      "[179]\ttrain's multi_logloss: 0.639604\tvalid's multi_logloss: 0.926746\n",
      "[180]\ttrain's multi_logloss: 0.638207\tvalid's multi_logloss: 0.926714\n",
      "[181]\ttrain's multi_logloss: 0.636001\tvalid's multi_logloss: 0.927475\n",
      "[182]\ttrain's multi_logloss: 0.634233\tvalid's multi_logloss: 0.929035\n",
      "[183]\ttrain's multi_logloss: 0.632161\tvalid's multi_logloss: 0.929732\n",
      "[184]\ttrain's multi_logloss: 0.630296\tvalid's multi_logloss: 0.92928\n",
      "[185]\ttrain's multi_logloss: 0.628463\tvalid's multi_logloss: 0.928565\n",
      "[186]\ttrain's multi_logloss: 0.626952\tvalid's multi_logloss: 0.927537\n",
      "[187]\ttrain's multi_logloss: 0.625573\tvalid's multi_logloss: 0.927187\n",
      "[188]\ttrain's multi_logloss: 0.624036\tvalid's multi_logloss: 0.926736\n",
      "[189]\ttrain's multi_logloss: 0.622237\tvalid's multi_logloss: 0.926691\n",
      "[190]\ttrain's multi_logloss: 0.620445\tvalid's multi_logloss: 0.927271\n",
      "[191]\ttrain's multi_logloss: 0.618789\tvalid's multi_logloss: 0.925955\n",
      "[192]\ttrain's multi_logloss: 0.617525\tvalid's multi_logloss: 0.924551\n",
      "[193]\ttrain's multi_logloss: 0.61566\tvalid's multi_logloss: 0.924806\n",
      "[194]\ttrain's multi_logloss: 0.614591\tvalid's multi_logloss: 0.92465\n",
      "[195]\ttrain's multi_logloss: 0.612772\tvalid's multi_logloss: 0.924927\n",
      "[196]\ttrain's multi_logloss: 0.611136\tvalid's multi_logloss: 0.92503\n",
      "[197]\ttrain's multi_logloss: 0.609711\tvalid's multi_logloss: 0.923887\n",
      "[198]\ttrain's multi_logloss: 0.608145\tvalid's multi_logloss: 0.92363\n",
      "[199]\ttrain's multi_logloss: 0.606934\tvalid's multi_logloss: 0.922664\n",
      "[200]\ttrain's multi_logloss: 0.60543\tvalid's multi_logloss: 0.922665\n",
      "[201]\ttrain's multi_logloss: 0.603764\tvalid's multi_logloss: 0.922415\n",
      "[202]\ttrain's multi_logloss: 0.602248\tvalid's multi_logloss: 0.922128\n",
      "[203]\ttrain's multi_logloss: 0.600495\tvalid's multi_logloss: 0.922715\n",
      "[204]\ttrain's multi_logloss: 0.598874\tvalid's multi_logloss: 0.922937\n",
      "[205]\ttrain's multi_logloss: 0.597097\tvalid's multi_logloss: 0.923288\n",
      "[206]\ttrain's multi_logloss: 0.596104\tvalid's multi_logloss: 0.922746\n",
      "[207]\ttrain's multi_logloss: 0.594842\tvalid's multi_logloss: 0.921666\n",
      "[208]\ttrain's multi_logloss: 0.594108\tvalid's multi_logloss: 0.921621\n",
      "[209]\ttrain's multi_logloss: 0.592907\tvalid's multi_logloss: 0.922423\n",
      "[210]\ttrain's multi_logloss: 0.592206\tvalid's multi_logloss: 0.922762\n",
      "[211]\ttrain's multi_logloss: 0.590862\tvalid's multi_logloss: 0.922739\n",
      "[212]\ttrain's multi_logloss: 0.589426\tvalid's multi_logloss: 0.923021\n",
      "[213]\ttrain's multi_logloss: 0.588462\tvalid's multi_logloss: 0.92364\n",
      "[214]\ttrain's multi_logloss: 0.586862\tvalid's multi_logloss: 0.923894\n",
      "[215]\ttrain's multi_logloss: 0.585131\tvalid's multi_logloss: 0.923517\n",
      "[216]\ttrain's multi_logloss: 0.583555\tvalid's multi_logloss: 0.923503\n",
      "[217]\ttrain's multi_logloss: 0.582102\tvalid's multi_logloss: 0.923029\n",
      "[218]\ttrain's multi_logloss: 0.580674\tvalid's multi_logloss: 0.921761\n",
      "[219]\ttrain's multi_logloss: 0.579502\tvalid's multi_logloss: 0.920152\n",
      "[220]\ttrain's multi_logloss: 0.578289\tvalid's multi_logloss: 0.919433\n",
      "[221]\ttrain's multi_logloss: 0.57683\tvalid's multi_logloss: 0.918974\n",
      "[222]\ttrain's multi_logloss: 0.575639\tvalid's multi_logloss: 0.91909\n",
      "[223]\ttrain's multi_logloss: 0.574396\tvalid's multi_logloss: 0.918959\n",
      "[224]\ttrain's multi_logloss: 0.573138\tvalid's multi_logloss: 0.918132\n",
      "[225]\ttrain's multi_logloss: 0.5719\tvalid's multi_logloss: 0.917637\n",
      "[226]\ttrain's multi_logloss: 0.570624\tvalid's multi_logloss: 0.916593\n",
      "[227]\ttrain's multi_logloss: 0.569529\tvalid's multi_logloss: 0.917167\n",
      "[228]\ttrain's multi_logloss: 0.56824\tvalid's multi_logloss: 0.917263\n",
      "[229]\ttrain's multi_logloss: 0.566802\tvalid's multi_logloss: 0.918731\n",
      "[230]\ttrain's multi_logloss: 0.565557\tvalid's multi_logloss: 0.919225\n",
      "[231]\ttrain's multi_logloss: 0.564406\tvalid's multi_logloss: 0.921879\n",
      "[232]\ttrain's multi_logloss: 0.563237\tvalid's multi_logloss: 0.923866\n",
      "[233]\ttrain's multi_logloss: 0.56203\tvalid's multi_logloss: 0.925372\n",
      "[234]\ttrain's multi_logloss: 0.561046\tvalid's multi_logloss: 0.925397\n",
      "[235]\ttrain's multi_logloss: 0.560175\tvalid's multi_logloss: 0.926073\n",
      "[236]\ttrain's multi_logloss: 0.558673\tvalid's multi_logloss: 0.925597\n",
      "[237]\ttrain's multi_logloss: 0.557209\tvalid's multi_logloss: 0.924919\n",
      "[238]\ttrain's multi_logloss: 0.555781\tvalid's multi_logloss: 0.924278\n",
      "[239]\ttrain's multi_logloss: 0.554854\tvalid's multi_logloss: 0.923652\n",
      "[240]\ttrain's multi_logloss: 0.553913\tvalid's multi_logloss: 0.923654\n",
      "[241]\ttrain's multi_logloss: 0.55266\tvalid's multi_logloss: 0.923041\n",
      "[242]\ttrain's multi_logloss: 0.551562\tvalid's multi_logloss: 0.922941\n",
      "[243]\ttrain's multi_logloss: 0.550595\tvalid's multi_logloss: 0.922206\n",
      "[244]\ttrain's multi_logloss: 0.549191\tvalid's multi_logloss: 0.921593\n",
      "[245]\ttrain's multi_logloss: 0.547841\tvalid's multi_logloss: 0.920889\n",
      "[246]\ttrain's multi_logloss: 0.546762\tvalid's multi_logloss: 0.921014\n",
      "[247]\ttrain's multi_logloss: 0.545763\tvalid's multi_logloss: 0.921054\n",
      "[248]\ttrain's multi_logloss: 0.544669\tvalid's multi_logloss: 0.922785\n",
      "[249]\ttrain's multi_logloss: 0.543427\tvalid's multi_logloss: 0.92369\n",
      "[250]\ttrain's multi_logloss: 0.542658\tvalid's multi_logloss: 0.923197\n",
      "[251]\ttrain's multi_logloss: 0.541404\tvalid's multi_logloss: 0.924544\n",
      "[252]\ttrain's multi_logloss: 0.540542\tvalid's multi_logloss: 0.923371\n",
      "[253]\ttrain's multi_logloss: 0.539388\tvalid's multi_logloss: 0.92329\n",
      "[254]\ttrain's multi_logloss: 0.538397\tvalid's multi_logloss: 0.92431\n",
      "[255]\ttrain's multi_logloss: 0.537458\tvalid's multi_logloss: 0.92519\n",
      "[256]\ttrain's multi_logloss: 0.536201\tvalid's multi_logloss: 0.926464\n",
      "[257]\ttrain's multi_logloss: 0.534906\tvalid's multi_logloss: 0.926812\n",
      "[258]\ttrain's multi_logloss: 0.53394\tvalid's multi_logloss: 0.927394\n",
      "[259]\ttrain's multi_logloss: 0.532707\tvalid's multi_logloss: 0.927434\n",
      "[260]\ttrain's multi_logloss: 0.531762\tvalid's multi_logloss: 0.927458\n",
      "[261]\ttrain's multi_logloss: 0.530925\tvalid's multi_logloss: 0.928164\n",
      "[262]\ttrain's multi_logloss: 0.530295\tvalid's multi_logloss: 0.928996\n",
      "[263]\ttrain's multi_logloss: 0.529484\tvalid's multi_logloss: 0.929556\n",
      "[264]\ttrain's multi_logloss: 0.528772\tvalid's multi_logloss: 0.930977\n",
      "[265]\ttrain's multi_logloss: 0.528081\tvalid's multi_logloss: 0.932288\n",
      "[266]\ttrain's multi_logloss: 0.527036\tvalid's multi_logloss: 0.933421\n",
      "[267]\ttrain's multi_logloss: 0.525693\tvalid's multi_logloss: 0.933973\n",
      "[268]\ttrain's multi_logloss: 0.524411\tvalid's multi_logloss: 0.934333\n",
      "[269]\ttrain's multi_logloss: 0.523338\tvalid's multi_logloss: 0.934935\n",
      "[270]\ttrain's multi_logloss: 0.522005\tvalid's multi_logloss: 0.935796\n",
      "[271]\ttrain's multi_logloss: 0.52141\tvalid's multi_logloss: 0.935379\n",
      "[272]\ttrain's multi_logloss: 0.520435\tvalid's multi_logloss: 0.934776\n",
      "[273]\ttrain's multi_logloss: 0.519907\tvalid's multi_logloss: 0.934895\n",
      "[274]\ttrain's multi_logloss: 0.519397\tvalid's multi_logloss: 0.935027\n",
      "[275]\ttrain's multi_logloss: 0.518904\tvalid's multi_logloss: 0.935172\n",
      "[276]\ttrain's multi_logloss: 0.517687\tvalid's multi_logloss: 0.934049\n",
      "[277]\ttrain's multi_logloss: 0.516819\tvalid's multi_logloss: 0.9332\n",
      "[278]\ttrain's multi_logloss: 0.515766\tvalid's multi_logloss: 0.933493\n",
      "[279]\ttrain's multi_logloss: 0.514684\tvalid's multi_logloss: 0.93472\n",
      "[280]\ttrain's multi_logloss: 0.513521\tvalid's multi_logloss: 0.9333\n",
      "[281]\ttrain's multi_logloss: 0.512345\tvalid's multi_logloss: 0.933877\n",
      "[282]\ttrain's multi_logloss: 0.511045\tvalid's multi_logloss: 0.934871\n",
      "[283]\ttrain's multi_logloss: 0.509705\tvalid's multi_logloss: 0.935714\n",
      "[284]\ttrain's multi_logloss: 0.508585\tvalid's multi_logloss: 0.936445\n",
      "[285]\ttrain's multi_logloss: 0.507145\tvalid's multi_logloss: 0.936449\n",
      "[286]\ttrain's multi_logloss: 0.505909\tvalid's multi_logloss: 0.935027\n",
      "[287]\ttrain's multi_logloss: 0.504492\tvalid's multi_logloss: 0.934324\n",
      "[288]\ttrain's multi_logloss: 0.503348\tvalid's multi_logloss: 0.934955\n",
      "[289]\ttrain's multi_logloss: 0.502055\tvalid's multi_logloss: 0.935639\n",
      "[290]\ttrain's multi_logloss: 0.50079\tvalid's multi_logloss: 0.935648\n",
      "[291]\ttrain's multi_logloss: 0.499784\tvalid's multi_logloss: 0.935342\n",
      "[292]\ttrain's multi_logloss: 0.498865\tvalid's multi_logloss: 0.933748\n",
      "[293]\ttrain's multi_logloss: 0.497945\tvalid's multi_logloss: 0.934485\n",
      "[294]\ttrain's multi_logloss: 0.497384\tvalid's multi_logloss: 0.936238\n",
      "[295]\ttrain's multi_logloss: 0.496232\tvalid's multi_logloss: 0.9361\n",
      "[296]\ttrain's multi_logloss: 0.495468\tvalid's multi_logloss: 0.937162\n",
      "[297]\ttrain's multi_logloss: 0.494845\tvalid's multi_logloss: 0.938097\n",
      "[298]\ttrain's multi_logloss: 0.49408\tvalid's multi_logloss: 0.938877\n",
      "[299]\ttrain's multi_logloss: 0.493505\tvalid's multi_logloss: 0.93961\n",
      "[300]\ttrain's multi_logloss: 0.492914\tvalid's multi_logloss: 0.940103\n",
      "[301]\ttrain's multi_logloss: 0.492104\tvalid's multi_logloss: 0.938794\n",
      "[302]\ttrain's multi_logloss: 0.49146\tvalid's multi_logloss: 0.937937\n",
      "[303]\ttrain's multi_logloss: 0.490652\tvalid's multi_logloss: 0.937346\n",
      "[304]\ttrain's multi_logloss: 0.489715\tvalid's multi_logloss: 0.937211\n",
      "[305]\ttrain's multi_logloss: 0.48886\tvalid's multi_logloss: 0.937076\n",
      "[306]\ttrain's multi_logloss: 0.487479\tvalid's multi_logloss: 0.935943\n",
      "[307]\ttrain's multi_logloss: 0.486545\tvalid's multi_logloss: 0.93617\n",
      "[308]\ttrain's multi_logloss: 0.485106\tvalid's multi_logloss: 0.934435\n",
      "[309]\ttrain's multi_logloss: 0.483866\tvalid's multi_logloss: 0.932608\n",
      "[310]\ttrain's multi_logloss: 0.482551\tvalid's multi_logloss: 0.932663\n",
      "[311]\ttrain's multi_logloss: 0.48182\tvalid's multi_logloss: 0.934204\n",
      "[312]\ttrain's multi_logloss: 0.481106\tvalid's multi_logloss: 0.935748\n",
      "[313]\ttrain's multi_logloss: 0.480527\tvalid's multi_logloss: 0.936116\n",
      "[314]\ttrain's multi_logloss: 0.47981\tvalid's multi_logloss: 0.936666\n",
      "[315]\ttrain's multi_logloss: 0.478865\tvalid's multi_logloss: 0.937195\n",
      "[316]\ttrain's multi_logloss: 0.478119\tvalid's multi_logloss: 0.938409\n",
      "[317]\ttrain's multi_logloss: 0.477518\tvalid's multi_logloss: 0.939391\n",
      "[318]\ttrain's multi_logloss: 0.476787\tvalid's multi_logloss: 0.939586\n",
      "[319]\ttrain's multi_logloss: 0.476076\tvalid's multi_logloss: 0.940798\n",
      "[320]\ttrain's multi_logloss: 0.475669\tvalid's multi_logloss: 0.940836\n",
      "[321]\ttrain's multi_logloss: 0.474693\tvalid's multi_logloss: 0.941552\n",
      "[322]\ttrain's multi_logloss: 0.473516\tvalid's multi_logloss: 0.94009\n",
      "[323]\ttrain's multi_logloss: 0.472584\tvalid's multi_logloss: 0.939121\n",
      "[324]\ttrain's multi_logloss: 0.471514\tvalid's multi_logloss: 0.93979\n",
      "[325]\ttrain's multi_logloss: 0.470645\tvalid's multi_logloss: 0.940571\n",
      "[326]\ttrain's multi_logloss: 0.470149\tvalid's multi_logloss: 0.941035\n",
      "Early stopping, best iteration is:\n",
      "[226]\ttrain's multi_logloss: 0.570624\tvalid's multi_logloss: 0.916593\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09106\tvalid's SFC_loss: 1.09444\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08547\tvalid's SFC_loss: 1.09225\n",
      "[3]\ttrain's SFC_loss: 1.07881\tvalid's SFC_loss: 1.08935\n",
      "[4]\ttrain's SFC_loss: 1.07165\tvalid's SFC_loss: 1.08657\n",
      "[5]\ttrain's SFC_loss: 1.06522\tvalid's SFC_loss: 1.08339\n",
      "[6]\ttrain's SFC_loss: 1.06047\tvalid's SFC_loss: 1.0804\n",
      "[7]\ttrain's SFC_loss: 1.05557\tvalid's SFC_loss: 1.0761\n",
      "[8]\ttrain's SFC_loss: 1.05099\tvalid's SFC_loss: 1.07001\n",
      "[9]\ttrain's SFC_loss: 1.04632\tvalid's SFC_loss: 1.06699\n",
      "[10]\ttrain's SFC_loss: 1.04183\tvalid's SFC_loss: 1.06264\n",
      "[11]\ttrain's SFC_loss: 1.03639\tvalid's SFC_loss: 1.05944\n",
      "[12]\ttrain's SFC_loss: 1.0326\tvalid's SFC_loss: 1.05706\n",
      "[13]\ttrain's SFC_loss: 1.02834\tvalid's SFC_loss: 1.05411\n",
      "[14]\ttrain's SFC_loss: 1.02316\tvalid's SFC_loss: 1.05108\n",
      "[15]\ttrain's SFC_loss: 1.01749\tvalid's SFC_loss: 1.05154\n",
      "[16]\ttrain's SFC_loss: 1.0135\tvalid's SFC_loss: 1.05243\n",
      "[17]\ttrain's SFC_loss: 1.00788\tvalid's SFC_loss: 1.0507\n",
      "[18]\ttrain's SFC_loss: 1.00328\tvalid's SFC_loss: 1.05115\n",
      "[19]\ttrain's SFC_loss: 0.999377\tvalid's SFC_loss: 1.05262\n",
      "[20]\ttrain's SFC_loss: 0.995024\tvalid's SFC_loss: 1.05403\n",
      "[21]\ttrain's SFC_loss: 0.989763\tvalid's SFC_loss: 1.04958\n",
      "[22]\ttrain's SFC_loss: 0.98434\tvalid's SFC_loss: 1.04761\n",
      "[23]\ttrain's SFC_loss: 0.97874\tvalid's SFC_loss: 1.03979\n",
      "[24]\ttrain's SFC_loss: 0.973705\tvalid's SFC_loss: 1.03707\n",
      "[25]\ttrain's SFC_loss: 0.969031\tvalid's SFC_loss: 1.03308\n",
      "[26]\ttrain's SFC_loss: 0.96347\tvalid's SFC_loss: 1.02769\n",
      "[27]\ttrain's SFC_loss: 0.959263\tvalid's SFC_loss: 1.02717\n",
      "[28]\ttrain's SFC_loss: 0.954488\tvalid's SFC_loss: 1.02336\n",
      "[29]\ttrain's SFC_loss: 0.949659\tvalid's SFC_loss: 1.02\n",
      "[30]\ttrain's SFC_loss: 0.944863\tvalid's SFC_loss: 1.01426\n",
      "[31]\ttrain's SFC_loss: 0.940207\tvalid's SFC_loss: 1.01265\n",
      "[32]\ttrain's SFC_loss: 0.935024\tvalid's SFC_loss: 1.00935\n",
      "[33]\ttrain's SFC_loss: 0.930483\tvalid's SFC_loss: 1.00816\n",
      "[34]\ttrain's SFC_loss: 0.925849\tvalid's SFC_loss: 1.00493\n",
      "[35]\ttrain's SFC_loss: 0.921374\tvalid's SFC_loss: 0.999525\n",
      "[36]\ttrain's SFC_loss: 0.915332\tvalid's SFC_loss: 0.999103\n",
      "[37]\ttrain's SFC_loss: 0.910304\tvalid's SFC_loss: 0.995577\n",
      "[38]\ttrain's SFC_loss: 0.905336\tvalid's SFC_loss: 0.992238\n",
      "[39]\ttrain's SFC_loss: 0.900994\tvalid's SFC_loss: 0.990537\n",
      "[40]\ttrain's SFC_loss: 0.896637\tvalid's SFC_loss: 0.987093\n",
      "[41]\ttrain's SFC_loss: 0.892298\tvalid's SFC_loss: 0.98463\n",
      "[42]\ttrain's SFC_loss: 0.88736\tvalid's SFC_loss: 0.982505\n",
      "[43]\ttrain's SFC_loss: 0.884075\tvalid's SFC_loss: 0.982933\n",
      "[44]\ttrain's SFC_loss: 0.880212\tvalid's SFC_loss: 0.979844\n",
      "[45]\ttrain's SFC_loss: 0.875545\tvalid's SFC_loss: 0.977791\n",
      "[46]\ttrain's SFC_loss: 0.87232\tvalid's SFC_loss: 0.976222\n",
      "[47]\ttrain's SFC_loss: 0.868644\tvalid's SFC_loss: 0.977288\n",
      "[48]\ttrain's SFC_loss: 0.865434\tvalid's SFC_loss: 0.97544\n",
      "[49]\ttrain's SFC_loss: 0.862143\tvalid's SFC_loss: 0.974861\n",
      "[50]\ttrain's SFC_loss: 0.859615\tvalid's SFC_loss: 0.974121\n",
      "[51]\ttrain's SFC_loss: 0.857329\tvalid's SFC_loss: 0.973297\n",
      "[52]\ttrain's SFC_loss: 0.853859\tvalid's SFC_loss: 0.972598\n",
      "[53]\ttrain's SFC_loss: 0.850916\tvalid's SFC_loss: 0.97076\n",
      "[54]\ttrain's SFC_loss: 0.847784\tvalid's SFC_loss: 0.968775\n",
      "[55]\ttrain's SFC_loss: 0.844276\tvalid's SFC_loss: 0.968809\n",
      "[56]\ttrain's SFC_loss: 0.841376\tvalid's SFC_loss: 0.966769\n",
      "[57]\ttrain's SFC_loss: 0.838185\tvalid's SFC_loss: 0.965521\n",
      "[58]\ttrain's SFC_loss: 0.835138\tvalid's SFC_loss: 0.965793\n",
      "[59]\ttrain's SFC_loss: 0.831962\tvalid's SFC_loss: 0.964883\n",
      "[60]\ttrain's SFC_loss: 0.828982\tvalid's SFC_loss: 0.963754\n",
      "[61]\ttrain's SFC_loss: 0.825665\tvalid's SFC_loss: 0.962776\n",
      "[62]\ttrain's SFC_loss: 0.82242\tvalid's SFC_loss: 0.962632\n",
      "[63]\ttrain's SFC_loss: 0.819061\tvalid's SFC_loss: 0.961483\n",
      "[64]\ttrain's SFC_loss: 0.815042\tvalid's SFC_loss: 0.9613\n",
      "[65]\ttrain's SFC_loss: 0.811917\tvalid's SFC_loss: 0.959464\n",
      "[66]\ttrain's SFC_loss: 0.808986\tvalid's SFC_loss: 0.957975\n",
      "[67]\ttrain's SFC_loss: 0.805482\tvalid's SFC_loss: 0.956407\n",
      "[68]\ttrain's SFC_loss: 0.802003\tvalid's SFC_loss: 0.955625\n",
      "[69]\ttrain's SFC_loss: 0.798687\tvalid's SFC_loss: 0.954671\n",
      "[70]\ttrain's SFC_loss: 0.795474\tvalid's SFC_loss: 0.953319\n",
      "[71]\ttrain's SFC_loss: 0.791727\tvalid's SFC_loss: 0.953912\n",
      "[72]\ttrain's SFC_loss: 0.78837\tvalid's SFC_loss: 0.955151\n",
      "[73]\ttrain's SFC_loss: 0.784469\tvalid's SFC_loss: 0.953753\n",
      "[74]\ttrain's SFC_loss: 0.78103\tvalid's SFC_loss: 0.952257\n",
      "[75]\ttrain's SFC_loss: 0.777912\tvalid's SFC_loss: 0.950629\n",
      "[76]\ttrain's SFC_loss: 0.77522\tvalid's SFC_loss: 0.949676\n",
      "[77]\ttrain's SFC_loss: 0.771546\tvalid's SFC_loss: 0.947104\n",
      "[78]\ttrain's SFC_loss: 0.768857\tvalid's SFC_loss: 0.948787\n",
      "[79]\ttrain's SFC_loss: 0.766301\tvalid's SFC_loss: 0.947653\n",
      "[80]\ttrain's SFC_loss: 0.764163\tvalid's SFC_loss: 0.946987\n",
      "[81]\ttrain's SFC_loss: 0.76144\tvalid's SFC_loss: 0.946155\n",
      "[82]\ttrain's SFC_loss: 0.758342\tvalid's SFC_loss: 0.943735\n",
      "[83]\ttrain's SFC_loss: 0.755309\tvalid's SFC_loss: 0.940232\n",
      "[84]\ttrain's SFC_loss: 0.752941\tvalid's SFC_loss: 0.938298\n",
      "[85]\ttrain's SFC_loss: 0.749971\tvalid's SFC_loss: 0.937023\n",
      "[86]\ttrain's SFC_loss: 0.747155\tvalid's SFC_loss: 0.933846\n",
      "[87]\ttrain's SFC_loss: 0.743872\tvalid's SFC_loss: 0.933475\n",
      "[88]\ttrain's SFC_loss: 0.741551\tvalid's SFC_loss: 0.930995\n",
      "[89]\ttrain's SFC_loss: 0.739255\tvalid's SFC_loss: 0.929246\n",
      "[90]\ttrain's SFC_loss: 0.736652\tvalid's SFC_loss: 0.926553\n",
      "[91]\ttrain's SFC_loss: 0.733871\tvalid's SFC_loss: 0.926642\n",
      "[92]\ttrain's SFC_loss: 0.731167\tvalid's SFC_loss: 0.925754\n",
      "[93]\ttrain's SFC_loss: 0.72863\tvalid's SFC_loss: 0.924408\n",
      "[94]\ttrain's SFC_loss: 0.72565\tvalid's SFC_loss: 0.921426\n",
      "[95]\ttrain's SFC_loss: 0.722839\tvalid's SFC_loss: 0.920777\n",
      "[96]\ttrain's SFC_loss: 0.720447\tvalid's SFC_loss: 0.921575\n",
      "[97]\ttrain's SFC_loss: 0.717702\tvalid's SFC_loss: 0.923886\n",
      "[98]\ttrain's SFC_loss: 0.714567\tvalid's SFC_loss: 0.923446\n",
      "[99]\ttrain's SFC_loss: 0.712293\tvalid's SFC_loss: 0.923104\n",
      "[100]\ttrain's SFC_loss: 0.710244\tvalid's SFC_loss: 0.920718\n",
      "[101]\ttrain's SFC_loss: 0.707315\tvalid's SFC_loss: 0.92142\n",
      "[102]\ttrain's SFC_loss: 0.704475\tvalid's SFC_loss: 0.921999\n",
      "[103]\ttrain's SFC_loss: 0.701962\tvalid's SFC_loss: 0.922965\n",
      "[104]\ttrain's SFC_loss: 0.699545\tvalid's SFC_loss: 0.920931\n",
      "[105]\ttrain's SFC_loss: 0.697136\tvalid's SFC_loss: 0.923654\n",
      "[106]\ttrain's SFC_loss: 0.69509\tvalid's SFC_loss: 0.923281\n",
      "[107]\ttrain's SFC_loss: 0.692927\tvalid's SFC_loss: 0.922938\n",
      "[108]\ttrain's SFC_loss: 0.690992\tvalid's SFC_loss: 0.921263\n",
      "[109]\ttrain's SFC_loss: 0.68946\tvalid's SFC_loss: 0.920219\n",
      "[110]\ttrain's SFC_loss: 0.687621\tvalid's SFC_loss: 0.921388\n",
      "[111]\ttrain's SFC_loss: 0.685166\tvalid's SFC_loss: 0.919857\n",
      "[112]\ttrain's SFC_loss: 0.683192\tvalid's SFC_loss: 0.918568\n",
      "[113]\ttrain's SFC_loss: 0.681931\tvalid's SFC_loss: 0.918314\n",
      "[114]\ttrain's SFC_loss: 0.680764\tvalid's SFC_loss: 0.919494\n",
      "[115]\ttrain's SFC_loss: 0.67965\tvalid's SFC_loss: 0.920699\n",
      "[116]\ttrain's SFC_loss: 0.677673\tvalid's SFC_loss: 0.922206\n",
      "[117]\ttrain's SFC_loss: 0.675751\tvalid's SFC_loss: 0.923752\n",
      "[118]\ttrain's SFC_loss: 0.673807\tvalid's SFC_loss: 0.925347\n",
      "[119]\ttrain's SFC_loss: 0.672341\tvalid's SFC_loss: 0.928216\n",
      "[120]\ttrain's SFC_loss: 0.670957\tvalid's SFC_loss: 0.926566\n",
      "[121]\ttrain's SFC_loss: 0.668852\tvalid's SFC_loss: 0.926916\n",
      "[122]\ttrain's SFC_loss: 0.666592\tvalid's SFC_loss: 0.926088\n",
      "[123]\ttrain's SFC_loss: 0.664781\tvalid's SFC_loss: 0.926316\n",
      "[124]\ttrain's SFC_loss: 0.662845\tvalid's SFC_loss: 0.926238\n",
      "[125]\ttrain's SFC_loss: 0.660484\tvalid's SFC_loss: 0.925059\n",
      "[126]\ttrain's SFC_loss: 0.659052\tvalid's SFC_loss: 0.925053\n",
      "[127]\ttrain's SFC_loss: 0.657017\tvalid's SFC_loss: 0.924135\n",
      "[128]\ttrain's SFC_loss: 0.655447\tvalid's SFC_loss: 0.922769\n",
      "[129]\ttrain's SFC_loss: 0.653984\tvalid's SFC_loss: 0.921778\n",
      "[130]\ttrain's SFC_loss: 0.652246\tvalid's SFC_loss: 0.921307\n",
      "[131]\ttrain's SFC_loss: 0.650317\tvalid's SFC_loss: 0.92197\n",
      "[132]\ttrain's SFC_loss: 0.648491\tvalid's SFC_loss: 0.922229\n",
      "[133]\ttrain's SFC_loss: 0.646504\tvalid's SFC_loss: 0.922252\n",
      "[134]\ttrain's SFC_loss: 0.644422\tvalid's SFC_loss: 0.922318\n",
      "[135]\ttrain's SFC_loss: 0.642896\tvalid's SFC_loss: 0.91957\n",
      "[136]\ttrain's SFC_loss: 0.640764\tvalid's SFC_loss: 0.917541\n",
      "[137]\ttrain's SFC_loss: 0.638585\tvalid's SFC_loss: 0.917603\n",
      "[138]\ttrain's SFC_loss: 0.636735\tvalid's SFC_loss: 0.917403\n",
      "[139]\ttrain's SFC_loss: 0.634577\tvalid's SFC_loss: 0.916617\n",
      "[140]\ttrain's SFC_loss: 0.632836\tvalid's SFC_loss: 0.916175\n",
      "[141]\ttrain's SFC_loss: 0.630338\tvalid's SFC_loss: 0.917632\n",
      "[142]\ttrain's SFC_loss: 0.628989\tvalid's SFC_loss: 0.919679\n",
      "[143]\ttrain's SFC_loss: 0.626568\tvalid's SFC_loss: 0.919727\n",
      "[144]\ttrain's SFC_loss: 0.624301\tvalid's SFC_loss: 0.919389\n",
      "[145]\ttrain's SFC_loss: 0.622099\tvalid's SFC_loss: 0.919097\n",
      "[146]\ttrain's SFC_loss: 0.619974\tvalid's SFC_loss: 0.917859\n",
      "[147]\ttrain's SFC_loss: 0.61784\tvalid's SFC_loss: 0.918533\n",
      "[148]\ttrain's SFC_loss: 0.615746\tvalid's SFC_loss: 0.917585\n",
      "[149]\ttrain's SFC_loss: 0.613623\tvalid's SFC_loss: 0.917523\n",
      "[150]\ttrain's SFC_loss: 0.611645\tvalid's SFC_loss: 0.918331\n",
      "[151]\ttrain's SFC_loss: 0.609106\tvalid's SFC_loss: 0.918647\n",
      "[152]\ttrain's SFC_loss: 0.606593\tvalid's SFC_loss: 0.919056\n",
      "[153]\ttrain's SFC_loss: 0.603935\tvalid's SFC_loss: 0.917684\n",
      "[154]\ttrain's SFC_loss: 0.600753\tvalid's SFC_loss: 0.915538\n",
      "[155]\ttrain's SFC_loss: 0.598617\tvalid's SFC_loss: 0.915215\n",
      "[156]\ttrain's SFC_loss: 0.596877\tvalid's SFC_loss: 0.917439\n",
      "[157]\ttrain's SFC_loss: 0.595274\tvalid's SFC_loss: 0.917896\n",
      "[158]\ttrain's SFC_loss: 0.593128\tvalid's SFC_loss: 0.918007\n",
      "[159]\ttrain's SFC_loss: 0.5913\tvalid's SFC_loss: 0.920833\n",
      "[160]\ttrain's SFC_loss: 0.589364\tvalid's SFC_loss: 0.921869\n",
      "[161]\ttrain's SFC_loss: 0.587992\tvalid's SFC_loss: 0.918817\n",
      "[162]\ttrain's SFC_loss: 0.586703\tvalid's SFC_loss: 0.921172\n",
      "[163]\ttrain's SFC_loss: 0.585083\tvalid's SFC_loss: 0.921775\n",
      "[164]\ttrain's SFC_loss: 0.583629\tvalid's SFC_loss: 0.92039\n",
      "[165]\ttrain's SFC_loss: 0.58207\tvalid's SFC_loss: 0.922626\n",
      "[166]\ttrain's SFC_loss: 0.580505\tvalid's SFC_loss: 0.924247\n",
      "[167]\ttrain's SFC_loss: 0.579045\tvalid's SFC_loss: 0.925553\n",
      "[168]\ttrain's SFC_loss: 0.577709\tvalid's SFC_loss: 0.927385\n",
      "[169]\ttrain's SFC_loss: 0.575802\tvalid's SFC_loss: 0.929704\n",
      "[170]\ttrain's SFC_loss: 0.574376\tvalid's SFC_loss: 0.930927\n",
      "[171]\ttrain's SFC_loss: 0.572329\tvalid's SFC_loss: 0.930142\n",
      "[172]\ttrain's SFC_loss: 0.570346\tvalid's SFC_loss: 0.92942\n",
      "[173]\ttrain's SFC_loss: 0.568935\tvalid's SFC_loss: 0.930541\n",
      "[174]\ttrain's SFC_loss: 0.567615\tvalid's SFC_loss: 0.932011\n",
      "[175]\ttrain's SFC_loss: 0.566089\tvalid's SFC_loss: 0.932624\n",
      "[176]\ttrain's SFC_loss: 0.564639\tvalid's SFC_loss: 0.933081\n",
      "[177]\ttrain's SFC_loss: 0.562484\tvalid's SFC_loss: 0.929471\n",
      "[178]\ttrain's SFC_loss: 0.561282\tvalid's SFC_loss: 0.929003\n",
      "[179]\ttrain's SFC_loss: 0.559709\tvalid's SFC_loss: 0.927944\n",
      "[180]\ttrain's SFC_loss: 0.558113\tvalid's SFC_loss: 0.927352\n",
      "[181]\ttrain's SFC_loss: 0.556093\tvalid's SFC_loss: 0.930092\n",
      "[182]\ttrain's SFC_loss: 0.553912\tvalid's SFC_loss: 0.929444\n",
      "[183]\ttrain's SFC_loss: 0.552071\tvalid's SFC_loss: 0.932334\n",
      "[184]\ttrain's SFC_loss: 0.550101\tvalid's SFC_loss: 0.932622\n",
      "[185]\ttrain's SFC_loss: 0.548213\tvalid's SFC_loss: 0.931153\n",
      "[186]\ttrain's SFC_loss: 0.545734\tvalid's SFC_loss: 0.933059\n",
      "[187]\ttrain's SFC_loss: 0.543603\tvalid's SFC_loss: 0.933432\n",
      "[188]\ttrain's SFC_loss: 0.541843\tvalid's SFC_loss: 0.933042\n",
      "[189]\ttrain's SFC_loss: 0.540418\tvalid's SFC_loss: 0.933524\n",
      "[190]\ttrain's SFC_loss: 0.538897\tvalid's SFC_loss: 0.934083\n",
      "[191]\ttrain's SFC_loss: 0.537709\tvalid's SFC_loss: 0.932729\n",
      "[192]\ttrain's SFC_loss: 0.536391\tvalid's SFC_loss: 0.929902\n",
      "[193]\ttrain's SFC_loss: 0.534697\tvalid's SFC_loss: 0.931253\n",
      "[194]\ttrain's SFC_loss: 0.533339\tvalid's SFC_loss: 0.930105\n",
      "[195]\ttrain's SFC_loss: 0.532007\tvalid's SFC_loss: 0.929563\n",
      "[196]\ttrain's SFC_loss: 0.531085\tvalid's SFC_loss: 0.927459\n",
      "[197]\ttrain's SFC_loss: 0.52948\tvalid's SFC_loss: 0.927803\n",
      "[198]\ttrain's SFC_loss: 0.527713\tvalid's SFC_loss: 0.929669\n",
      "[199]\ttrain's SFC_loss: 0.526228\tvalid's SFC_loss: 0.929535\n",
      "[200]\ttrain's SFC_loss: 0.524791\tvalid's SFC_loss: 0.928958\n",
      "[201]\ttrain's SFC_loss: 0.522697\tvalid's SFC_loss: 0.927018\n",
      "[202]\ttrain's SFC_loss: 0.520541\tvalid's SFC_loss: 0.927432\n",
      "[203]\ttrain's SFC_loss: 0.518734\tvalid's SFC_loss: 0.926859\n",
      "[204]\ttrain's SFC_loss: 0.517011\tvalid's SFC_loss: 0.929548\n",
      "[205]\ttrain's SFC_loss: 0.515138\tvalid's SFC_loss: 0.93023\n",
      "[206]\ttrain's SFC_loss: 0.514139\tvalid's SFC_loss: 0.929981\n",
      "[207]\ttrain's SFC_loss: 0.51318\tvalid's SFC_loss: 0.929763\n",
      "[208]\ttrain's SFC_loss: 0.512035\tvalid's SFC_loss: 0.928999\n",
      "[209]\ttrain's SFC_loss: 0.511149\tvalid's SFC_loss: 0.928841\n",
      "[210]\ttrain's SFC_loss: 0.5103\tvalid's SFC_loss: 0.92871\n",
      "[211]\ttrain's SFC_loss: 0.508542\tvalid's SFC_loss: 0.929703\n",
      "[212]\ttrain's SFC_loss: 0.506874\tvalid's SFC_loss: 0.930464\n",
      "[213]\ttrain's SFC_loss: 0.505019\tvalid's SFC_loss: 0.931506\n",
      "[214]\ttrain's SFC_loss: 0.50334\tvalid's SFC_loss: 0.933024\n",
      "[215]\ttrain's SFC_loss: 0.50226\tvalid's SFC_loss: 0.934217\n",
      "[216]\ttrain's SFC_loss: 0.500914\tvalid's SFC_loss: 0.932932\n",
      "[217]\ttrain's SFC_loss: 0.499743\tvalid's SFC_loss: 0.931151\n",
      "[218]\ttrain's SFC_loss: 0.498422\tvalid's SFC_loss: 0.929924\n",
      "[219]\ttrain's SFC_loss: 0.497009\tvalid's SFC_loss: 0.929883\n",
      "[220]\ttrain's SFC_loss: 0.495606\tvalid's SFC_loss: 0.929574\n",
      "[221]\ttrain's SFC_loss: 0.494105\tvalid's SFC_loss: 0.928709\n",
      "[222]\ttrain's SFC_loss: 0.492623\tvalid's SFC_loss: 0.929608\n",
      "[223]\ttrain's SFC_loss: 0.491601\tvalid's SFC_loss: 0.930905\n",
      "[224]\ttrain's SFC_loss: 0.490466\tvalid's SFC_loss: 0.930321\n",
      "[225]\ttrain's SFC_loss: 0.489085\tvalid's SFC_loss: 0.928923\n",
      "[226]\ttrain's SFC_loss: 0.487872\tvalid's SFC_loss: 0.929405\n",
      "[227]\ttrain's SFC_loss: 0.486544\tvalid's SFC_loss: 0.931573\n",
      "[228]\ttrain's SFC_loss: 0.485083\tvalid's SFC_loss: 0.933703\n",
      "[229]\ttrain's SFC_loss: 0.483647\tvalid's SFC_loss: 0.935492\n",
      "[230]\ttrain's SFC_loss: 0.482303\tvalid's SFC_loss: 0.937193\n",
      "[231]\ttrain's SFC_loss: 0.481222\tvalid's SFC_loss: 0.940332\n",
      "[232]\ttrain's SFC_loss: 0.480209\tvalid's SFC_loss: 0.943497\n",
      "[233]\ttrain's SFC_loss: 0.479297\tvalid's SFC_loss: 0.945134\n",
      "[234]\ttrain's SFC_loss: 0.478249\tvalid's SFC_loss: 0.948623\n",
      "[235]\ttrain's SFC_loss: 0.477105\tvalid's SFC_loss: 0.951221\n",
      "[236]\ttrain's SFC_loss: 0.475672\tvalid's SFC_loss: 0.95064\n",
      "[237]\ttrain's SFC_loss: 0.474568\tvalid's SFC_loss: 0.952042\n",
      "[238]\ttrain's SFC_loss: 0.473344\tvalid's SFC_loss: 0.951708\n",
      "[239]\ttrain's SFC_loss: 0.472152\tvalid's SFC_loss: 0.952538\n",
      "[240]\ttrain's SFC_loss: 0.470927\tvalid's SFC_loss: 0.952963\n",
      "[241]\ttrain's SFC_loss: 0.469998\tvalid's SFC_loss: 0.952551\n",
      "[242]\ttrain's SFC_loss: 0.468622\tvalid's SFC_loss: 0.951407\n",
      "[243]\ttrain's SFC_loss: 0.467687\tvalid's SFC_loss: 0.950447\n",
      "[244]\ttrain's SFC_loss: 0.466464\tvalid's SFC_loss: 0.949694\n",
      "[245]\ttrain's SFC_loss: 0.465321\tvalid's SFC_loss: 0.94965\n",
      "[246]\ttrain's SFC_loss: 0.464374\tvalid's SFC_loss: 0.951126\n",
      "[247]\ttrain's SFC_loss: 0.463411\tvalid's SFC_loss: 0.952155\n",
      "[248]\ttrain's SFC_loss: 0.462393\tvalid's SFC_loss: 0.951437\n",
      "[249]\ttrain's SFC_loss: 0.461403\tvalid's SFC_loss: 0.952339\n",
      "[250]\ttrain's SFC_loss: 0.46046\tvalid's SFC_loss: 0.953683\n",
      "[251]\ttrain's SFC_loss: 0.459134\tvalid's SFC_loss: 0.952794\n",
      "[252]\ttrain's SFC_loss: 0.457909\tvalid's SFC_loss: 0.952084\n",
      "[253]\ttrain's SFC_loss: 0.456682\tvalid's SFC_loss: 0.951561\n",
      "[254]\ttrain's SFC_loss: 0.455744\tvalid's SFC_loss: 0.953006\n",
      "[255]\ttrain's SFC_loss: 0.454768\tvalid's SFC_loss: 0.952335\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttrain's SFC_loss: 0.598617\tvalid's SFC_loss: 0.915215\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1    0.116352\n",
      "1    feature2    0.058391\n",
      "2    feature3   -0.226493\n",
      "3    feature4   -0.076684\n",
      "4    feature5    0.064251\n",
      "5    feature6   -0.033816\n",
      "6    feature7    0.032527\n",
      "7    feature8    0.040286\n",
      "8    feature9    0.027755\n",
      "9   feature10    0.023795\n",
      "10  feature11    0.011965\n",
      "11  feature12    0.037382\n",
      "12  feature13   -0.011885\n",
      "13  feature14   -0.082200\n",
      "14  feature15    0.000835\n",
      "15  feature16    0.029383\n",
      "16  feature17   -0.012915\n",
      "17  feature18    0.046511\n",
      "18  feature19   -0.056007\n",
      "19  feature20    0.010567\n",
      "-------------------- 8 --------------------\n",
      "(98, 20) (98,)\n",
      "(10, 20) (10,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.06131\tvalid's multi_logloss: 1.04383\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.058\tvalid's multi_logloss: 1.04172\n",
      "[3]\ttrain's multi_logloss: 1.05512\tvalid's multi_logloss: 1.04172\n",
      "[4]\ttrain's multi_logloss: 1.05176\tvalid's multi_logloss: 1.03738\n",
      "[5]\ttrain's multi_logloss: 1.04802\tvalid's multi_logloss: 1.03505\n",
      "[6]\ttrain's multi_logloss: 1.04458\tvalid's multi_logloss: 1.03152\n",
      "[7]\ttrain's multi_logloss: 1.04096\tvalid's multi_logloss: 1.03161\n",
      "[8]\ttrain's multi_logloss: 1.03729\tvalid's multi_logloss: 1.03019\n",
      "[9]\ttrain's multi_logloss: 1.03445\tvalid's multi_logloss: 1.02978\n",
      "[10]\ttrain's multi_logloss: 1.03074\tvalid's multi_logloss: 1.02941\n",
      "[11]\ttrain's multi_logloss: 1.02791\tvalid's multi_logloss: 1.02827\n",
      "[12]\ttrain's multi_logloss: 1.02472\tvalid's multi_logloss: 1.02692\n",
      "[13]\ttrain's multi_logloss: 1.02154\tvalid's multi_logloss: 1.02539\n",
      "[14]\ttrain's multi_logloss: 1.01864\tvalid's multi_logloss: 1.02396\n",
      "[15]\ttrain's multi_logloss: 1.01614\tvalid's multi_logloss: 1.02416\n",
      "[16]\ttrain's multi_logloss: 1.01297\tvalid's multi_logloss: 1.02485\n",
      "[17]\ttrain's multi_logloss: 1.00914\tvalid's multi_logloss: 1.02475\n",
      "[18]\ttrain's multi_logloss: 1.00618\tvalid's multi_logloss: 1.02172\n",
      "[19]\ttrain's multi_logloss: 1.00313\tvalid's multi_logloss: 1.01916\n",
      "[20]\ttrain's multi_logloss: 0.999718\tvalid's multi_logloss: 1.01708\n",
      "[21]\ttrain's multi_logloss: 0.996156\tvalid's multi_logloss: 1.01308\n",
      "[22]\ttrain's multi_logloss: 0.993101\tvalid's multi_logloss: 1.00952\n",
      "[23]\ttrain's multi_logloss: 0.989522\tvalid's multi_logloss: 1.00715\n",
      "[24]\ttrain's multi_logloss: 0.986098\tvalid's multi_logloss: 1.00509\n",
      "[25]\ttrain's multi_logloss: 0.982927\tvalid's multi_logloss: 1.0018\n",
      "[26]\ttrain's multi_logloss: 0.979831\tvalid's multi_logloss: 0.997769\n",
      "[27]\ttrain's multi_logloss: 0.976673\tvalid's multi_logloss: 0.994989\n",
      "[28]\ttrain's multi_logloss: 0.973572\tvalid's multi_logloss: 0.992267\n",
      "[29]\ttrain's multi_logloss: 0.970773\tvalid's multi_logloss: 0.990418\n",
      "[30]\ttrain's multi_logloss: 0.96791\tvalid's multi_logloss: 0.987324\n",
      "[31]\ttrain's multi_logloss: 0.965144\tvalid's multi_logloss: 0.985374\n",
      "[32]\ttrain's multi_logloss: 0.962273\tvalid's multi_logloss: 0.983278\n",
      "[33]\ttrain's multi_logloss: 0.959716\tvalid's multi_logloss: 0.979693\n",
      "[34]\ttrain's multi_logloss: 0.956905\tvalid's multi_logloss: 0.977332\n",
      "[35]\ttrain's multi_logloss: 0.954319\tvalid's multi_logloss: 0.976219\n",
      "[36]\ttrain's multi_logloss: 0.951301\tvalid's multi_logloss: 0.974455\n",
      "[37]\ttrain's multi_logloss: 0.948835\tvalid's multi_logloss: 0.971728\n",
      "[38]\ttrain's multi_logloss: 0.946675\tvalid's multi_logloss: 0.968599\n",
      "[39]\ttrain's multi_logloss: 0.944283\tvalid's multi_logloss: 0.966761\n",
      "[40]\ttrain's multi_logloss: 0.941606\tvalid's multi_logloss: 0.963058\n",
      "[41]\ttrain's multi_logloss: 0.939014\tvalid's multi_logloss: 0.961287\n",
      "[42]\ttrain's multi_logloss: 0.93653\tvalid's multi_logloss: 0.959982\n",
      "[43]\ttrain's multi_logloss: 0.93381\tvalid's multi_logloss: 0.960093\n",
      "[44]\ttrain's multi_logloss: 0.931685\tvalid's multi_logloss: 0.958013\n",
      "[45]\ttrain's multi_logloss: 0.929326\tvalid's multi_logloss: 0.95688\n",
      "[46]\ttrain's multi_logloss: 0.926794\tvalid's multi_logloss: 0.958022\n",
      "[47]\ttrain's multi_logloss: 0.924574\tvalid's multi_logloss: 0.960202\n",
      "[48]\ttrain's multi_logloss: 0.922553\tvalid's multi_logloss: 0.96192\n",
      "[49]\ttrain's multi_logloss: 0.920723\tvalid's multi_logloss: 0.962582\n",
      "[50]\ttrain's multi_logloss: 0.918958\tvalid's multi_logloss: 0.962416\n",
      "[51]\ttrain's multi_logloss: 0.916178\tvalid's multi_logloss: 0.963821\n",
      "[52]\ttrain's multi_logloss: 0.913576\tvalid's multi_logloss: 0.963498\n",
      "[53]\ttrain's multi_logloss: 0.911006\tvalid's multi_logloss: 0.963079\n",
      "[54]\ttrain's multi_logloss: 0.908146\tvalid's multi_logloss: 0.963885\n",
      "[55]\ttrain's multi_logloss: 0.905378\tvalid's multi_logloss: 0.96481\n",
      "[56]\ttrain's multi_logloss: 0.90269\tvalid's multi_logloss: 0.963039\n",
      "[57]\ttrain's multi_logloss: 0.90038\tvalid's multi_logloss: 0.960028\n",
      "[58]\ttrain's multi_logloss: 0.897541\tvalid's multi_logloss: 0.958741\n",
      "[59]\ttrain's multi_logloss: 0.895157\tvalid's multi_logloss: 0.957412\n",
      "[60]\ttrain's multi_logloss: 0.893485\tvalid's multi_logloss: 0.957711\n",
      "[61]\ttrain's multi_logloss: 0.890423\tvalid's multi_logloss: 0.953022\n",
      "[62]\ttrain's multi_logloss: 0.887593\tvalid's multi_logloss: 0.949031\n",
      "[63]\ttrain's multi_logloss: 0.885276\tvalid's multi_logloss: 0.946123\n",
      "[64]\ttrain's multi_logloss: 0.882659\tvalid's multi_logloss: 0.943716\n",
      "[65]\ttrain's multi_logloss: 0.880551\tvalid's multi_logloss: 0.942601\n",
      "[66]\ttrain's multi_logloss: 0.877544\tvalid's multi_logloss: 0.9394\n",
      "[67]\ttrain's multi_logloss: 0.87501\tvalid's multi_logloss: 0.936271\n",
      "[68]\ttrain's multi_logloss: 0.872211\tvalid's multi_logloss: 0.935124\n",
      "[69]\ttrain's multi_logloss: 0.869897\tvalid's multi_logloss: 0.934041\n",
      "[70]\ttrain's multi_logloss: 0.86707\tvalid's multi_logloss: 0.931925\n",
      "[71]\ttrain's multi_logloss: 0.864778\tvalid's multi_logloss: 0.930218\n",
      "[72]\ttrain's multi_logloss: 0.86214\tvalid's multi_logloss: 0.925604\n",
      "[73]\ttrain's multi_logloss: 0.859565\tvalid's multi_logloss: 0.921074\n",
      "[74]\ttrain's multi_logloss: 0.856574\tvalid's multi_logloss: 0.919977\n",
      "[75]\ttrain's multi_logloss: 0.854297\tvalid's multi_logloss: 0.919527\n",
      "[76]\ttrain's multi_logloss: 0.852019\tvalid's multi_logloss: 0.919072\n",
      "[77]\ttrain's multi_logloss: 0.849969\tvalid's multi_logloss: 0.917435\n",
      "[78]\ttrain's multi_logloss: 0.848118\tvalid's multi_logloss: 0.916879\n",
      "[79]\ttrain's multi_logloss: 0.845801\tvalid's multi_logloss: 0.916596\n",
      "[80]\ttrain's multi_logloss: 0.844105\tvalid's multi_logloss: 0.916065\n",
      "[81]\ttrain's multi_logloss: 0.842119\tvalid's multi_logloss: 0.91437\n",
      "[82]\ttrain's multi_logloss: 0.84023\tvalid's multi_logloss: 0.91209\n",
      "[83]\ttrain's multi_logloss: 0.837485\tvalid's multi_logloss: 0.909277\n",
      "[84]\ttrain's multi_logloss: 0.835448\tvalid's multi_logloss: 0.908532\n",
      "[85]\ttrain's multi_logloss: 0.833021\tvalid's multi_logloss: 0.905705\n",
      "[86]\ttrain's multi_logloss: 0.830187\tvalid's multi_logloss: 0.904772\n",
      "[87]\ttrain's multi_logloss: 0.827463\tvalid's multi_logloss: 0.902107\n",
      "[88]\ttrain's multi_logloss: 0.825123\tvalid's multi_logloss: 0.899682\n",
      "[89]\ttrain's multi_logloss: 0.822809\tvalid's multi_logloss: 0.897405\n",
      "[90]\ttrain's multi_logloss: 0.820654\tvalid's multi_logloss: 0.897969\n",
      "[91]\ttrain's multi_logloss: 0.818735\tvalid's multi_logloss: 0.896742\n",
      "[92]\ttrain's multi_logloss: 0.816772\tvalid's multi_logloss: 0.895127\n",
      "[93]\ttrain's multi_logloss: 0.814955\tvalid's multi_logloss: 0.894759\n",
      "[94]\ttrain's multi_logloss: 0.812952\tvalid's multi_logloss: 0.893853\n",
      "[95]\ttrain's multi_logloss: 0.811157\tvalid's multi_logloss: 0.891054\n",
      "[96]\ttrain's multi_logloss: 0.808714\tvalid's multi_logloss: 0.888441\n",
      "[97]\ttrain's multi_logloss: 0.80638\tvalid's multi_logloss: 0.886619\n",
      "[98]\ttrain's multi_logloss: 0.803945\tvalid's multi_logloss: 0.883892\n",
      "[99]\ttrain's multi_logloss: 0.801671\tvalid's multi_logloss: 0.882833\n",
      "[100]\ttrain's multi_logloss: 0.799774\tvalid's multi_logloss: 0.881104\n",
      "[101]\ttrain's multi_logloss: 0.797549\tvalid's multi_logloss: 0.879176\n",
      "[102]\ttrain's multi_logloss: 0.795321\tvalid's multi_logloss: 0.876878\n",
      "[103]\ttrain's multi_logloss: 0.793008\tvalid's multi_logloss: 0.874134\n",
      "[104]\ttrain's multi_logloss: 0.790782\tvalid's multi_logloss: 0.871548\n",
      "[105]\ttrain's multi_logloss: 0.788693\tvalid's multi_logloss: 0.872641\n",
      "[106]\ttrain's multi_logloss: 0.786811\tvalid's multi_logloss: 0.87051\n",
      "[107]\ttrain's multi_logloss: 0.784491\tvalid's multi_logloss: 0.867759\n",
      "[108]\ttrain's multi_logloss: 0.783267\tvalid's multi_logloss: 0.86547\n",
      "[109]\ttrain's multi_logloss: 0.781699\tvalid's multi_logloss: 0.864309\n",
      "[110]\ttrain's multi_logloss: 0.779292\tvalid's multi_logloss: 0.861393\n",
      "[111]\ttrain's multi_logloss: 0.77762\tvalid's multi_logloss: 0.860723\n",
      "[112]\ttrain's multi_logloss: 0.776269\tvalid's multi_logloss: 0.859002\n",
      "[113]\ttrain's multi_logloss: 0.774881\tvalid's multi_logloss: 0.858502\n",
      "[114]\ttrain's multi_logloss: 0.773339\tvalid's multi_logloss: 0.857936\n",
      "[115]\ttrain's multi_logloss: 0.771823\tvalid's multi_logloss: 0.857275\n",
      "[116]\ttrain's multi_logloss: 0.769455\tvalid's multi_logloss: 0.854452\n",
      "[117]\ttrain's multi_logloss: 0.767198\tvalid's multi_logloss: 0.851648\n",
      "[118]\ttrain's multi_logloss: 0.76493\tvalid's multi_logloss: 0.847923\n",
      "[119]\ttrain's multi_logloss: 0.763174\tvalid's multi_logloss: 0.844396\n",
      "[120]\ttrain's multi_logloss: 0.76142\tvalid's multi_logloss: 0.842192\n",
      "[121]\ttrain's multi_logloss: 0.759707\tvalid's multi_logloss: 0.839057\n",
      "[122]\ttrain's multi_logloss: 0.757906\tvalid's multi_logloss: 0.838576\n",
      "[123]\ttrain's multi_logloss: 0.756237\tvalid's multi_logloss: 0.837477\n",
      "[124]\ttrain's multi_logloss: 0.754524\tvalid's multi_logloss: 0.835462\n",
      "[125]\ttrain's multi_logloss: 0.752746\tvalid's multi_logloss: 0.833981\n",
      "[126]\ttrain's multi_logloss: 0.750904\tvalid's multi_logloss: 0.833871\n",
      "[127]\ttrain's multi_logloss: 0.749143\tvalid's multi_logloss: 0.834034\n",
      "[128]\ttrain's multi_logloss: 0.747342\tvalid's multi_logloss: 0.832262\n",
      "[129]\ttrain's multi_logloss: 0.745906\tvalid's multi_logloss: 0.829462\n",
      "[130]\ttrain's multi_logloss: 0.744395\tvalid's multi_logloss: 0.828318\n",
      "[131]\ttrain's multi_logloss: 0.742926\tvalid's multi_logloss: 0.823886\n",
      "[132]\ttrain's multi_logloss: 0.7417\tvalid's multi_logloss: 0.820244\n",
      "[133]\ttrain's multi_logloss: 0.739941\tvalid's multi_logloss: 0.817118\n",
      "[134]\ttrain's multi_logloss: 0.738641\tvalid's multi_logloss: 0.814964\n",
      "[135]\ttrain's multi_logloss: 0.737143\tvalid's multi_logloss: 0.812895\n",
      "[136]\ttrain's multi_logloss: 0.73531\tvalid's multi_logloss: 0.811985\n",
      "[137]\ttrain's multi_logloss: 0.733242\tvalid's multi_logloss: 0.808333\n",
      "[138]\ttrain's multi_logloss: 0.73149\tvalid's multi_logloss: 0.80783\n",
      "[139]\ttrain's multi_logloss: 0.729461\tvalid's multi_logloss: 0.806905\n",
      "[140]\ttrain's multi_logloss: 0.727354\tvalid's multi_logloss: 0.806055\n",
      "[141]\ttrain's multi_logloss: 0.725671\tvalid's multi_logloss: 0.805012\n",
      "[142]\ttrain's multi_logloss: 0.724491\tvalid's multi_logloss: 0.804682\n",
      "[143]\ttrain's multi_logloss: 0.72324\tvalid's multi_logloss: 0.804498\n",
      "[144]\ttrain's multi_logloss: 0.721706\tvalid's multi_logloss: 0.803613\n",
      "[145]\ttrain's multi_logloss: 0.720642\tvalid's multi_logloss: 0.803176\n",
      "[146]\ttrain's multi_logloss: 0.719261\tvalid's multi_logloss: 0.802696\n",
      "[147]\ttrain's multi_logloss: 0.717761\tvalid's multi_logloss: 0.801543\n",
      "[148]\ttrain's multi_logloss: 0.716464\tvalid's multi_logloss: 0.799537\n",
      "[149]\ttrain's multi_logloss: 0.715072\tvalid's multi_logloss: 0.797743\n",
      "[150]\ttrain's multi_logloss: 0.713925\tvalid's multi_logloss: 0.796609\n",
      "[151]\ttrain's multi_logloss: 0.712311\tvalid's multi_logloss: 0.794149\n",
      "[152]\ttrain's multi_logloss: 0.710765\tvalid's multi_logloss: 0.791972\n",
      "[153]\ttrain's multi_logloss: 0.709033\tvalid's multi_logloss: 0.790591\n",
      "[154]\ttrain's multi_logloss: 0.707943\tvalid's multi_logloss: 0.788658\n",
      "[155]\ttrain's multi_logloss: 0.706308\tvalid's multi_logloss: 0.786458\n",
      "[156]\ttrain's multi_logloss: 0.704677\tvalid's multi_logloss: 0.786216\n",
      "[157]\ttrain's multi_logloss: 0.703104\tvalid's multi_logloss: 0.786296\n",
      "[158]\ttrain's multi_logloss: 0.701785\tvalid's multi_logloss: 0.78762\n",
      "[159]\ttrain's multi_logloss: 0.700187\tvalid's multi_logloss: 0.788067\n",
      "[160]\ttrain's multi_logloss: 0.698842\tvalid's multi_logloss: 0.787965\n",
      "[161]\ttrain's multi_logloss: 0.697479\tvalid's multi_logloss: 0.786169\n",
      "[162]\ttrain's multi_logloss: 0.696193\tvalid's multi_logloss: 0.784634\n",
      "[163]\ttrain's multi_logloss: 0.694824\tvalid's multi_logloss: 0.781541\n",
      "[164]\ttrain's multi_logloss: 0.693203\tvalid's multi_logloss: 0.77936\n",
      "[165]\ttrain's multi_logloss: 0.691746\tvalid's multi_logloss: 0.777092\n",
      "[166]\ttrain's multi_logloss: 0.690696\tvalid's multi_logloss: 0.77459\n",
      "[167]\ttrain's multi_logloss: 0.68924\tvalid's multi_logloss: 0.772473\n",
      "[168]\ttrain's multi_logloss: 0.687815\tvalid's multi_logloss: 0.770373\n",
      "[169]\ttrain's multi_logloss: 0.686376\tvalid's multi_logloss: 0.769043\n",
      "[170]\ttrain's multi_logloss: 0.685317\tvalid's multi_logloss: 0.768334\n",
      "[171]\ttrain's multi_logloss: 0.683634\tvalid's multi_logloss: 0.76777\n",
      "[172]\ttrain's multi_logloss: 0.682218\tvalid's multi_logloss: 0.766592\n",
      "[173]\ttrain's multi_logloss: 0.68096\tvalid's multi_logloss: 0.766791\n",
      "[174]\ttrain's multi_logloss: 0.679662\tvalid's multi_logloss: 0.76595\n",
      "[175]\ttrain's multi_logloss: 0.678337\tvalid's multi_logloss: 0.766221\n",
      "[176]\ttrain's multi_logloss: 0.676015\tvalid's multi_logloss: 0.765773\n",
      "[177]\ttrain's multi_logloss: 0.673479\tvalid's multi_logloss: 0.766293\n",
      "[178]\ttrain's multi_logloss: 0.671119\tvalid's multi_logloss: 0.766125\n",
      "[179]\ttrain's multi_logloss: 0.668915\tvalid's multi_logloss: 0.765684\n",
      "[180]\ttrain's multi_logloss: 0.666753\tvalid's multi_logloss: 0.764451\n",
      "[181]\ttrain's multi_logloss: 0.665386\tvalid's multi_logloss: 0.765391\n",
      "[182]\ttrain's multi_logloss: 0.663748\tvalid's multi_logloss: 0.766156\n",
      "[183]\ttrain's multi_logloss: 0.662361\tvalid's multi_logloss: 0.76667\n",
      "[184]\ttrain's multi_logloss: 0.66115\tvalid's multi_logloss: 0.767084\n",
      "[185]\ttrain's multi_logloss: 0.659518\tvalid's multi_logloss: 0.767622\n",
      "[186]\ttrain's multi_logloss: 0.65825\tvalid's multi_logloss: 0.767365\n",
      "[187]\ttrain's multi_logloss: 0.656939\tvalid's multi_logloss: 0.768213\n",
      "[188]\ttrain's multi_logloss: 0.655792\tvalid's multi_logloss: 0.769348\n",
      "[189]\ttrain's multi_logloss: 0.654489\tvalid's multi_logloss: 0.768771\n",
      "[190]\ttrain's multi_logloss: 0.653213\tvalid's multi_logloss: 0.768211\n",
      "[191]\ttrain's multi_logloss: 0.652298\tvalid's multi_logloss: 0.767696\n",
      "[192]\ttrain's multi_logloss: 0.651172\tvalid's multi_logloss: 0.767558\n",
      "[193]\ttrain's multi_logloss: 0.649676\tvalid's multi_logloss: 0.765832\n",
      "[194]\ttrain's multi_logloss: 0.648517\tvalid's multi_logloss: 0.76609\n",
      "[195]\ttrain's multi_logloss: 0.647374\tvalid's multi_logloss: 0.765822\n",
      "[196]\ttrain's multi_logloss: 0.645681\tvalid's multi_logloss: 0.766747\n",
      "[197]\ttrain's multi_logloss: 0.643696\tvalid's multi_logloss: 0.766981\n",
      "[198]\ttrain's multi_logloss: 0.64191\tvalid's multi_logloss: 0.767362\n",
      "[199]\ttrain's multi_logloss: 0.640549\tvalid's multi_logloss: 0.765944\n",
      "[200]\ttrain's multi_logloss: 0.638881\tvalid's multi_logloss: 0.76565\n",
      "[201]\ttrain's multi_logloss: 0.637447\tvalid's multi_logloss: 0.765979\n",
      "[202]\ttrain's multi_logloss: 0.636037\tvalid's multi_logloss: 0.766319\n",
      "[203]\ttrain's multi_logloss: 0.634621\tvalid's multi_logloss: 0.766269\n",
      "[204]\ttrain's multi_logloss: 0.633248\tvalid's multi_logloss: 0.76663\n",
      "[205]\ttrain's multi_logloss: 0.632108\tvalid's multi_logloss: 0.765277\n",
      "[206]\ttrain's multi_logloss: 0.631319\tvalid's multi_logloss: 0.765174\n",
      "[207]\ttrain's multi_logloss: 0.630596\tvalid's multi_logloss: 0.765738\n",
      "[208]\ttrain's multi_logloss: 0.629991\tvalid's multi_logloss: 0.766206\n",
      "[209]\ttrain's multi_logloss: 0.629383\tvalid's multi_logloss: 0.766507\n",
      "[210]\ttrain's multi_logloss: 0.628391\tvalid's multi_logloss: 0.765502\n",
      "[211]\ttrain's multi_logloss: 0.627459\tvalid's multi_logloss: 0.761975\n",
      "[212]\ttrain's multi_logloss: 0.626842\tvalid's multi_logloss: 0.760377\n",
      "[213]\ttrain's multi_logloss: 0.625647\tvalid's multi_logloss: 0.757616\n",
      "[214]\ttrain's multi_logloss: 0.624671\tvalid's multi_logloss: 0.753648\n",
      "[215]\ttrain's multi_logloss: 0.623503\tvalid's multi_logloss: 0.753084\n",
      "[216]\ttrain's multi_logloss: 0.621778\tvalid's multi_logloss: 0.754196\n",
      "[217]\ttrain's multi_logloss: 0.620365\tvalid's multi_logloss: 0.754085\n",
      "[218]\ttrain's multi_logloss: 0.618526\tvalid's multi_logloss: 0.753659\n",
      "[219]\ttrain's multi_logloss: 0.61708\tvalid's multi_logloss: 0.753383\n",
      "[220]\ttrain's multi_logloss: 0.615726\tvalid's multi_logloss: 0.75382\n",
      "[221]\ttrain's multi_logloss: 0.61436\tvalid's multi_logloss: 0.752235\n",
      "[222]\ttrain's multi_logloss: 0.61294\tvalid's multi_logloss: 0.750831\n",
      "[223]\ttrain's multi_logloss: 0.611573\tvalid's multi_logloss: 0.749156\n",
      "[224]\ttrain's multi_logloss: 0.610205\tvalid's multi_logloss: 0.747803\n",
      "[225]\ttrain's multi_logloss: 0.609162\tvalid's multi_logloss: 0.749844\n",
      "[226]\ttrain's multi_logloss: 0.607772\tvalid's multi_logloss: 0.748881\n",
      "[227]\ttrain's multi_logloss: 0.606444\tvalid's multi_logloss: 0.749593\n",
      "[228]\ttrain's multi_logloss: 0.604975\tvalid's multi_logloss: 0.750692\n",
      "[229]\ttrain's multi_logloss: 0.603639\tvalid's multi_logloss: 0.750099\n",
      "[230]\ttrain's multi_logloss: 0.602326\tvalid's multi_logloss: 0.750156\n",
      "[231]\ttrain's multi_logloss: 0.600638\tvalid's multi_logloss: 0.749439\n",
      "[232]\ttrain's multi_logloss: 0.599745\tvalid's multi_logloss: 0.749602\n",
      "[233]\ttrain's multi_logloss: 0.598368\tvalid's multi_logloss: 0.748872\n",
      "[234]\ttrain's multi_logloss: 0.597243\tvalid's multi_logloss: 0.749099\n",
      "[235]\ttrain's multi_logloss: 0.596571\tvalid's multi_logloss: 0.749023\n",
      "[236]\ttrain's multi_logloss: 0.59508\tvalid's multi_logloss: 0.748342\n",
      "[237]\ttrain's multi_logloss: 0.593626\tvalid's multi_logloss: 0.748942\n",
      "[238]\ttrain's multi_logloss: 0.592206\tvalid's multi_logloss: 0.747848\n",
      "[239]\ttrain's multi_logloss: 0.59102\tvalid's multi_logloss: 0.747109\n",
      "[240]\ttrain's multi_logloss: 0.589422\tvalid's multi_logloss: 0.746932\n",
      "[241]\ttrain's multi_logloss: 0.588437\tvalid's multi_logloss: 0.747599\n",
      "[242]\ttrain's multi_logloss: 0.587513\tvalid's multi_logloss: 0.747063\n",
      "[243]\ttrain's multi_logloss: 0.586436\tvalid's multi_logloss: 0.746427\n",
      "[244]\ttrain's multi_logloss: 0.585387\tvalid's multi_logloss: 0.746215\n",
      "[245]\ttrain's multi_logloss: 0.584259\tvalid's multi_logloss: 0.746787\n",
      "[246]\ttrain's multi_logloss: 0.583022\tvalid's multi_logloss: 0.744495\n",
      "[247]\ttrain's multi_logloss: 0.581962\tvalid's multi_logloss: 0.742734\n",
      "[248]\ttrain's multi_logloss: 0.580829\tvalid's multi_logloss: 0.741751\n",
      "[249]\ttrain's multi_logloss: 0.57979\tvalid's multi_logloss: 0.74002\n",
      "[250]\ttrain's multi_logloss: 0.578947\tvalid's multi_logloss: 0.737366\n",
      "[251]\ttrain's multi_logloss: 0.577513\tvalid's multi_logloss: 0.737316\n",
      "[252]\ttrain's multi_logloss: 0.5763\tvalid's multi_logloss: 0.738103\n",
      "[253]\ttrain's multi_logloss: 0.574902\tvalid's multi_logloss: 0.736751\n",
      "[254]\ttrain's multi_logloss: 0.573493\tvalid's multi_logloss: 0.734014\n",
      "[255]\ttrain's multi_logloss: 0.572424\tvalid's multi_logloss: 0.734622\n",
      "[256]\ttrain's multi_logloss: 0.571111\tvalid's multi_logloss: 0.733974\n",
      "[257]\ttrain's multi_logloss: 0.569927\tvalid's multi_logloss: 0.732929\n",
      "[258]\ttrain's multi_logloss: 0.568624\tvalid's multi_logloss: 0.733656\n",
      "[259]\ttrain's multi_logloss: 0.567619\tvalid's multi_logloss: 0.733385\n",
      "[260]\ttrain's multi_logloss: 0.566363\tvalid's multi_logloss: 0.733831\n",
      "[261]\ttrain's multi_logloss: 0.565143\tvalid's multi_logloss: 0.734641\n",
      "[262]\ttrain's multi_logloss: 0.564474\tvalid's multi_logloss: 0.734687\n",
      "[263]\ttrain's multi_logloss: 0.563543\tvalid's multi_logloss: 0.735674\n",
      "[264]\ttrain's multi_logloss: 0.56232\tvalid's multi_logloss: 0.736545\n",
      "[265]\ttrain's multi_logloss: 0.561263\tvalid's multi_logloss: 0.737798\n",
      "[266]\ttrain's multi_logloss: 0.5602\tvalid's multi_logloss: 0.737183\n",
      "[267]\ttrain's multi_logloss: 0.559559\tvalid's multi_logloss: 0.736822\n",
      "[268]\ttrain's multi_logloss: 0.558254\tvalid's multi_logloss: 0.735765\n",
      "[269]\ttrain's multi_logloss: 0.557474\tvalid's multi_logloss: 0.734963\n",
      "[270]\ttrain's multi_logloss: 0.556416\tvalid's multi_logloss: 0.733878\n",
      "[271]\ttrain's multi_logloss: 0.555514\tvalid's multi_logloss: 0.733619\n",
      "[272]\ttrain's multi_logloss: 0.554682\tvalid's multi_logloss: 0.731954\n",
      "[273]\ttrain's multi_logloss: 0.553441\tvalid's multi_logloss: 0.732402\n",
      "[274]\ttrain's multi_logloss: 0.552291\tvalid's multi_logloss: 0.73268\n",
      "[275]\ttrain's multi_logloss: 0.551104\tvalid's multi_logloss: 0.73318\n",
      "[276]\ttrain's multi_logloss: 0.549792\tvalid's multi_logloss: 0.731954\n",
      "[277]\ttrain's multi_logloss: 0.548884\tvalid's multi_logloss: 0.730313\n",
      "[278]\ttrain's multi_logloss: 0.546993\tvalid's multi_logloss: 0.7308\n",
      "[279]\ttrain's multi_logloss: 0.54528\tvalid's multi_logloss: 0.731379\n",
      "[280]\ttrain's multi_logloss: 0.543984\tvalid's multi_logloss: 0.730923\n",
      "[281]\ttrain's multi_logloss: 0.542612\tvalid's multi_logloss: 0.731578\n",
      "[282]\ttrain's multi_logloss: 0.541577\tvalid's multi_logloss: 0.730276\n",
      "[283]\ttrain's multi_logloss: 0.540648\tvalid's multi_logloss: 0.730127\n",
      "[284]\ttrain's multi_logloss: 0.539652\tvalid's multi_logloss: 0.729766\n",
      "[285]\ttrain's multi_logloss: 0.538699\tvalid's multi_logloss: 0.729112\n",
      "[286]\ttrain's multi_logloss: 0.537871\tvalid's multi_logloss: 0.728409\n",
      "[287]\ttrain's multi_logloss: 0.537051\tvalid's multi_logloss: 0.728056\n",
      "[288]\ttrain's multi_logloss: 0.535955\tvalid's multi_logloss: 0.726158\n",
      "[289]\ttrain's multi_logloss: 0.535107\tvalid's multi_logloss: 0.725215\n",
      "[290]\ttrain's multi_logloss: 0.534307\tvalid's multi_logloss: 0.725027\n",
      "[291]\ttrain's multi_logloss: 0.532655\tvalid's multi_logloss: 0.725161\n",
      "[292]\ttrain's multi_logloss: 0.531671\tvalid's multi_logloss: 0.723899\n",
      "[293]\ttrain's multi_logloss: 0.530642\tvalid's multi_logloss: 0.722978\n",
      "[294]\ttrain's multi_logloss: 0.529419\tvalid's multi_logloss: 0.722962\n",
      "[295]\ttrain's multi_logloss: 0.528009\tvalid's multi_logloss: 0.72293\n",
      "[296]\ttrain's multi_logloss: 0.527499\tvalid's multi_logloss: 0.72172\n",
      "[297]\ttrain's multi_logloss: 0.526942\tvalid's multi_logloss: 0.720767\n",
      "[298]\ttrain's multi_logloss: 0.526276\tvalid's multi_logloss: 0.718091\n",
      "[299]\ttrain's multi_logloss: 0.52561\tvalid's multi_logloss: 0.71589\n",
      "[300]\ttrain's multi_logloss: 0.525053\tvalid's multi_logloss: 0.713466\n",
      "[301]\ttrain's multi_logloss: 0.524165\tvalid's multi_logloss: 0.713234\n",
      "[302]\ttrain's multi_logloss: 0.523045\tvalid's multi_logloss: 0.712078\n",
      "[303]\ttrain's multi_logloss: 0.52195\tvalid's multi_logloss: 0.710946\n",
      "[304]\ttrain's multi_logloss: 0.52073\tvalid's multi_logloss: 0.711755\n",
      "[305]\ttrain's multi_logloss: 0.519662\tvalid's multi_logloss: 0.71263\n",
      "[306]\ttrain's multi_logloss: 0.518459\tvalid's multi_logloss: 0.711184\n",
      "[307]\ttrain's multi_logloss: 0.517345\tvalid's multi_logloss: 0.71\n",
      "[308]\ttrain's multi_logloss: 0.516218\tvalid's multi_logloss: 0.707896\n",
      "[309]\ttrain's multi_logloss: 0.515058\tvalid's multi_logloss: 0.70553\n",
      "[310]\ttrain's multi_logloss: 0.513777\tvalid's multi_logloss: 0.705148\n",
      "[311]\ttrain's multi_logloss: 0.51258\tvalid's multi_logloss: 0.704251\n",
      "[312]\ttrain's multi_logloss: 0.511447\tvalid's multi_logloss: 0.705019\n",
      "[313]\ttrain's multi_logloss: 0.510514\tvalid's multi_logloss: 0.70775\n",
      "[314]\ttrain's multi_logloss: 0.509422\tvalid's multi_logloss: 0.705716\n",
      "[315]\ttrain's multi_logloss: 0.508301\tvalid's multi_logloss: 0.70612\n",
      "[316]\ttrain's multi_logloss: 0.507456\tvalid's multi_logloss: 0.707511\n",
      "[317]\ttrain's multi_logloss: 0.506637\tvalid's multi_logloss: 0.707012\n",
      "[318]\ttrain's multi_logloss: 0.505757\tvalid's multi_logloss: 0.708555\n",
      "[319]\ttrain's multi_logloss: 0.504847\tvalid's multi_logloss: 0.710101\n",
      "[320]\ttrain's multi_logloss: 0.504107\tvalid's multi_logloss: 0.711203\n",
      "[321]\ttrain's multi_logloss: 0.502904\tvalid's multi_logloss: 0.710713\n",
      "[322]\ttrain's multi_logloss: 0.501967\tvalid's multi_logloss: 0.711419\n",
      "[323]\ttrain's multi_logloss: 0.501106\tvalid's multi_logloss: 0.711647\n",
      "[324]\ttrain's multi_logloss: 0.500284\tvalid's multi_logloss: 0.710759\n",
      "[325]\ttrain's multi_logloss: 0.499278\tvalid's multi_logloss: 0.712129\n",
      "[326]\ttrain's multi_logloss: 0.498155\tvalid's multi_logloss: 0.710496\n",
      "[327]\ttrain's multi_logloss: 0.497321\tvalid's multi_logloss: 0.71233\n",
      "[328]\ttrain's multi_logloss: 0.496293\tvalid's multi_logloss: 0.71339\n",
      "[329]\ttrain's multi_logloss: 0.495399\tvalid's multi_logloss: 0.712379\n",
      "[330]\ttrain's multi_logloss: 0.494281\tvalid's multi_logloss: 0.710717\n",
      "[331]\ttrain's multi_logloss: 0.493167\tvalid's multi_logloss: 0.712445\n",
      "[332]\ttrain's multi_logloss: 0.492433\tvalid's multi_logloss: 0.713927\n",
      "[333]\ttrain's multi_logloss: 0.491795\tvalid's multi_logloss: 0.716191\n",
      "[334]\ttrain's multi_logloss: 0.49117\tvalid's multi_logloss: 0.718465\n",
      "[335]\ttrain's multi_logloss: 0.490505\tvalid's multi_logloss: 0.718891\n",
      "[336]\ttrain's multi_logloss: 0.48971\tvalid's multi_logloss: 0.719391\n",
      "[337]\ttrain's multi_logloss: 0.488939\tvalid's multi_logloss: 0.718994\n",
      "[338]\ttrain's multi_logloss: 0.488038\tvalid's multi_logloss: 0.720461\n",
      "[339]\ttrain's multi_logloss: 0.487252\tvalid's multi_logloss: 0.719834\n",
      "[340]\ttrain's multi_logloss: 0.486499\tvalid's multi_logloss: 0.719453\n",
      "[341]\ttrain's multi_logloss: 0.485605\tvalid's multi_logloss: 0.719558\n",
      "[342]\ttrain's multi_logloss: 0.485021\tvalid's multi_logloss: 0.719371\n",
      "[343]\ttrain's multi_logloss: 0.484207\tvalid's multi_logloss: 0.719122\n",
      "[344]\ttrain's multi_logloss: 0.48341\tvalid's multi_logloss: 0.72079\n",
      "[345]\ttrain's multi_logloss: 0.482686\tvalid's multi_logloss: 0.720886\n",
      "[346]\ttrain's multi_logloss: 0.481459\tvalid's multi_logloss: 0.718944\n",
      "[347]\ttrain's multi_logloss: 0.480686\tvalid's multi_logloss: 0.717103\n",
      "[348]\ttrain's multi_logloss: 0.479687\tvalid's multi_logloss: 0.717083\n",
      "[349]\ttrain's multi_logloss: 0.478642\tvalid's multi_logloss: 0.715315\n",
      "[350]\ttrain's multi_logloss: 0.477689\tvalid's multi_logloss: 0.714371\n",
      "[351]\ttrain's multi_logloss: 0.476677\tvalid's multi_logloss: 0.714636\n",
      "[352]\ttrain's multi_logloss: 0.475638\tvalid's multi_logloss: 0.713684\n",
      "[353]\ttrain's multi_logloss: 0.474795\tvalid's multi_logloss: 0.714218\n",
      "[354]\ttrain's multi_logloss: 0.473888\tvalid's multi_logloss: 0.714319\n",
      "[355]\ttrain's multi_logloss: 0.472993\tvalid's multi_logloss: 0.714527\n",
      "[356]\ttrain's multi_logloss: 0.471903\tvalid's multi_logloss: 0.713333\n",
      "[357]\ttrain's multi_logloss: 0.470823\tvalid's multi_logloss: 0.713807\n",
      "[358]\ttrain's multi_logloss: 0.469687\tvalid's multi_logloss: 0.712807\n",
      "[359]\ttrain's multi_logloss: 0.468607\tvalid's multi_logloss: 0.714282\n",
      "[360]\ttrain's multi_logloss: 0.46722\tvalid's multi_logloss: 0.712746\n",
      "[361]\ttrain's multi_logloss: 0.466362\tvalid's multi_logloss: 0.71175\n",
      "[362]\ttrain's multi_logloss: 0.465551\tvalid's multi_logloss: 0.712119\n",
      "[363]\ttrain's multi_logloss: 0.464714\tvalid's multi_logloss: 0.711698\n",
      "[364]\ttrain's multi_logloss: 0.463712\tvalid's multi_logloss: 0.711104\n",
      "[365]\ttrain's multi_logloss: 0.462938\tvalid's multi_logloss: 0.710258\n",
      "[366]\ttrain's multi_logloss: 0.461899\tvalid's multi_logloss: 0.709507\n",
      "[367]\ttrain's multi_logloss: 0.461146\tvalid's multi_logloss: 0.708435\n",
      "[368]\ttrain's multi_logloss: 0.459995\tvalid's multi_logloss: 0.70676\n",
      "[369]\ttrain's multi_logloss: 0.458864\tvalid's multi_logloss: 0.705113\n",
      "[370]\ttrain's multi_logloss: 0.458129\tvalid's multi_logloss: 0.704029\n",
      "[371]\ttrain's multi_logloss: 0.457446\tvalid's multi_logloss: 0.70347\n",
      "[372]\ttrain's multi_logloss: 0.456673\tvalid's multi_logloss: 0.703816\n",
      "[373]\ttrain's multi_logloss: 0.455923\tvalid's multi_logloss: 0.702354\n",
      "[374]\ttrain's multi_logloss: 0.455124\tvalid's multi_logloss: 0.701806\n",
      "[375]\ttrain's multi_logloss: 0.454489\tvalid's multi_logloss: 0.702117\n",
      "[376]\ttrain's multi_logloss: 0.453654\tvalid's multi_logloss: 0.703699\n",
      "[377]\ttrain's multi_logloss: 0.453054\tvalid's multi_logloss: 0.702844\n",
      "[378]\ttrain's multi_logloss: 0.452281\tvalid's multi_logloss: 0.703767\n",
      "[379]\ttrain's multi_logloss: 0.45153\tvalid's multi_logloss: 0.702491\n",
      "[380]\ttrain's multi_logloss: 0.450493\tvalid's multi_logloss: 0.700273\n",
      "[381]\ttrain's multi_logloss: 0.449446\tvalid's multi_logloss: 0.699909\n",
      "[382]\ttrain's multi_logloss: 0.448449\tvalid's multi_logloss: 0.698646\n",
      "[383]\ttrain's multi_logloss: 0.4475\tvalid's multi_logloss: 0.69648\n",
      "[384]\ttrain's multi_logloss: 0.446774\tvalid's multi_logloss: 0.696387\n",
      "[385]\ttrain's multi_logloss: 0.445836\tvalid's multi_logloss: 0.696058\n",
      "[386]\ttrain's multi_logloss: 0.445379\tvalid's multi_logloss: 0.695864\n",
      "[387]\ttrain's multi_logloss: 0.44469\tvalid's multi_logloss: 0.696362\n",
      "[388]\ttrain's multi_logloss: 0.444036\tvalid's multi_logloss: 0.696705\n",
      "[389]\ttrain's multi_logloss: 0.443411\tvalid's multi_logloss: 0.696099\n",
      "[390]\ttrain's multi_logloss: 0.442776\tvalid's multi_logloss: 0.696452\n",
      "[391]\ttrain's multi_logloss: 0.442054\tvalid's multi_logloss: 0.696909\n",
      "[392]\ttrain's multi_logloss: 0.441402\tvalid's multi_logloss: 0.697871\n",
      "[393]\ttrain's multi_logloss: 0.440867\tvalid's multi_logloss: 0.699402\n",
      "[394]\ttrain's multi_logloss: 0.440506\tvalid's multi_logloss: 0.699246\n",
      "[395]\ttrain's multi_logloss: 0.43997\tvalid's multi_logloss: 0.701026\n",
      "[396]\ttrain's multi_logloss: 0.439019\tvalid's multi_logloss: 0.698987\n",
      "[397]\ttrain's multi_logloss: 0.43824\tvalid's multi_logloss: 0.697188\n",
      "[398]\ttrain's multi_logloss: 0.437446\tvalid's multi_logloss: 0.696617\n",
      "[399]\ttrain's multi_logloss: 0.436657\tvalid's multi_logloss: 0.695056\n",
      "[400]\ttrain's multi_logloss: 0.435892\tvalid's multi_logloss: 0.694225\n",
      "[401]\ttrain's multi_logloss: 0.434977\tvalid's multi_logloss: 0.693817\n",
      "[402]\ttrain's multi_logloss: 0.434271\tvalid's multi_logloss: 0.692732\n",
      "[403]\ttrain's multi_logloss: 0.433445\tvalid's multi_logloss: 0.691252\n",
      "[404]\ttrain's multi_logloss: 0.432668\tvalid's multi_logloss: 0.690073\n",
      "[405]\ttrain's multi_logloss: 0.431906\tvalid's multi_logloss: 0.688195\n",
      "[406]\ttrain's multi_logloss: 0.430988\tvalid's multi_logloss: 0.6854\n",
      "[407]\ttrain's multi_logloss: 0.430068\tvalid's multi_logloss: 0.685117\n",
      "[408]\ttrain's multi_logloss: 0.429063\tvalid's multi_logloss: 0.684778\n",
      "[409]\ttrain's multi_logloss: 0.42794\tvalid's multi_logloss: 0.682871\n",
      "[410]\ttrain's multi_logloss: 0.427023\tvalid's multi_logloss: 0.68191\n",
      "[411]\ttrain's multi_logloss: 0.426226\tvalid's multi_logloss: 0.683128\n",
      "[412]\ttrain's multi_logloss: 0.42555\tvalid's multi_logloss: 0.684468\n",
      "[413]\ttrain's multi_logloss: 0.424836\tvalid's multi_logloss: 0.685895\n",
      "[414]\ttrain's multi_logloss: 0.423974\tvalid's multi_logloss: 0.68571\n",
      "[415]\ttrain's multi_logloss: 0.423137\tvalid's multi_logloss: 0.685755\n",
      "[416]\ttrain's multi_logloss: 0.422424\tvalid's multi_logloss: 0.682219\n",
      "[417]\ttrain's multi_logloss: 0.421771\tvalid's multi_logloss: 0.679713\n",
      "[418]\ttrain's multi_logloss: 0.420885\tvalid's multi_logloss: 0.675672\n",
      "[419]\ttrain's multi_logloss: 0.420161\tvalid's multi_logloss: 0.672339\n",
      "[420]\ttrain's multi_logloss: 0.419307\tvalid's multi_logloss: 0.668235\n",
      "[421]\ttrain's multi_logloss: 0.418366\tvalid's multi_logloss: 0.667211\n",
      "[422]\ttrain's multi_logloss: 0.417809\tvalid's multi_logloss: 0.664496\n",
      "[423]\ttrain's multi_logloss: 0.416971\tvalid's multi_logloss: 0.66408\n",
      "[424]\ttrain's multi_logloss: 0.416099\tvalid's multi_logloss: 0.663862\n",
      "[425]\ttrain's multi_logloss: 0.415278\tvalid's multi_logloss: 0.663553\n",
      "[426]\ttrain's multi_logloss: 0.414105\tvalid's multi_logloss: 0.662531\n",
      "[427]\ttrain's multi_logloss: 0.41307\tvalid's multi_logloss: 0.661455\n",
      "[428]\ttrain's multi_logloss: 0.412229\tvalid's multi_logloss: 0.659675\n",
      "[429]\ttrain's multi_logloss: 0.411361\tvalid's multi_logloss: 0.659502\n",
      "[430]\ttrain's multi_logloss: 0.410206\tvalid's multi_logloss: 0.659226\n",
      "[431]\ttrain's multi_logloss: 0.409454\tvalid's multi_logloss: 0.659898\n",
      "[432]\ttrain's multi_logloss: 0.408867\tvalid's multi_logloss: 0.660359\n",
      "[433]\ttrain's multi_logloss: 0.408125\tvalid's multi_logloss: 0.660878\n",
      "[434]\ttrain's multi_logloss: 0.407641\tvalid's multi_logloss: 0.661572\n",
      "[435]\ttrain's multi_logloss: 0.406863\tvalid's multi_logloss: 0.661708\n",
      "[436]\ttrain's multi_logloss: 0.406247\tvalid's multi_logloss: 0.662151\n",
      "[437]\ttrain's multi_logloss: 0.405659\tvalid's multi_logloss: 0.663074\n",
      "[438]\ttrain's multi_logloss: 0.405172\tvalid's multi_logloss: 0.663097\n",
      "[439]\ttrain's multi_logloss: 0.404525\tvalid's multi_logloss: 0.663167\n",
      "[440]\ttrain's multi_logloss: 0.403965\tvalid's multi_logloss: 0.662767\n",
      "[441]\ttrain's multi_logloss: 0.403285\tvalid's multi_logloss: 0.660406\n",
      "[442]\ttrain's multi_logloss: 0.402625\tvalid's multi_logloss: 0.658079\n",
      "[443]\ttrain's multi_logloss: 0.401895\tvalid's multi_logloss: 0.656988\n",
      "[444]\ttrain's multi_logloss: 0.401015\tvalid's multi_logloss: 0.654282\n",
      "[445]\ttrain's multi_logloss: 0.400338\tvalid's multi_logloss: 0.653122\n",
      "[446]\ttrain's multi_logloss: 0.39994\tvalid's multi_logloss: 0.651841\n",
      "[447]\ttrain's multi_logloss: 0.399375\tvalid's multi_logloss: 0.648871\n",
      "[448]\ttrain's multi_logloss: 0.398742\tvalid's multi_logloss: 0.647846\n",
      "[449]\ttrain's multi_logloss: 0.398215\tvalid's multi_logloss: 0.646898\n",
      "[450]\ttrain's multi_logloss: 0.397766\tvalid's multi_logloss: 0.644116\n",
      "[451]\ttrain's multi_logloss: 0.397164\tvalid's multi_logloss: 0.645821\n",
      "[452]\ttrain's multi_logloss: 0.396497\tvalid's multi_logloss: 0.647892\n",
      "[453]\ttrain's multi_logloss: 0.395509\tvalid's multi_logloss: 0.648896\n",
      "[454]\ttrain's multi_logloss: 0.394718\tvalid's multi_logloss: 0.649786\n",
      "[455]\ttrain's multi_logloss: 0.39408\tvalid's multi_logloss: 0.651788\n",
      "[456]\ttrain's multi_logloss: 0.393257\tvalid's multi_logloss: 0.652551\n",
      "[457]\ttrain's multi_logloss: 0.392651\tvalid's multi_logloss: 0.654108\n",
      "[458]\ttrain's multi_logloss: 0.392163\tvalid's multi_logloss: 0.654192\n",
      "[459]\ttrain's multi_logloss: 0.391599\tvalid's multi_logloss: 0.655651\n",
      "[460]\ttrain's multi_logloss: 0.39071\tvalid's multi_logloss: 0.656536\n",
      "[461]\ttrain's multi_logloss: 0.390004\tvalid's multi_logloss: 0.656785\n",
      "[462]\ttrain's multi_logloss: 0.389456\tvalid's multi_logloss: 0.657393\n",
      "[463]\ttrain's multi_logloss: 0.388714\tvalid's multi_logloss: 0.657548\n",
      "[464]\ttrain's multi_logloss: 0.388166\tvalid's multi_logloss: 0.657988\n",
      "[465]\ttrain's multi_logloss: 0.387644\tvalid's multi_logloss: 0.65909\n",
      "[466]\ttrain's multi_logloss: 0.386953\tvalid's multi_logloss: 0.660605\n",
      "[467]\ttrain's multi_logloss: 0.386407\tvalid's multi_logloss: 0.661882\n",
      "[468]\ttrain's multi_logloss: 0.385877\tvalid's multi_logloss: 0.662821\n",
      "[469]\ttrain's multi_logloss: 0.385167\tvalid's multi_logloss: 0.663686\n",
      "[470]\ttrain's multi_logloss: 0.384468\tvalid's multi_logloss: 0.66512\n",
      "[471]\ttrain's multi_logloss: 0.383941\tvalid's multi_logloss: 0.66634\n",
      "[472]\ttrain's multi_logloss: 0.383514\tvalid's multi_logloss: 0.668762\n",
      "[473]\ttrain's multi_logloss: 0.383042\tvalid's multi_logloss: 0.668377\n",
      "[474]\ttrain's multi_logloss: 0.382564\tvalid's multi_logloss: 0.669325\n",
      "[475]\ttrain's multi_logloss: 0.381985\tvalid's multi_logloss: 0.669252\n",
      "[476]\ttrain's multi_logloss: 0.381007\tvalid's multi_logloss: 0.669869\n",
      "[477]\ttrain's multi_logloss: 0.380063\tvalid's multi_logloss: 0.670112\n",
      "[478]\ttrain's multi_logloss: 0.379119\tvalid's multi_logloss: 0.669355\n",
      "[479]\ttrain's multi_logloss: 0.378375\tvalid's multi_logloss: 0.66912\n",
      "[480]\ttrain's multi_logloss: 0.377633\tvalid's multi_logloss: 0.668864\n",
      "[481]\ttrain's multi_logloss: 0.376699\tvalid's multi_logloss: 0.667945\n",
      "[482]\ttrain's multi_logloss: 0.376228\tvalid's multi_logloss: 0.666262\n",
      "[483]\ttrain's multi_logloss: 0.375767\tvalid's multi_logloss: 0.664594\n",
      "[484]\ttrain's multi_logloss: 0.375013\tvalid's multi_logloss: 0.662874\n",
      "[485]\ttrain's multi_logloss: 0.374447\tvalid's multi_logloss: 0.661585\n",
      "[486]\ttrain's multi_logloss: 0.374117\tvalid's multi_logloss: 0.663466\n",
      "[487]\ttrain's multi_logloss: 0.373509\tvalid's multi_logloss: 0.665042\n",
      "[488]\ttrain's multi_logloss: 0.373123\tvalid's multi_logloss: 0.665445\n",
      "[489]\ttrain's multi_logloss: 0.372781\tvalid's multi_logloss: 0.666526\n",
      "[490]\ttrain's multi_logloss: 0.372194\tvalid's multi_logloss: 0.667168\n",
      "[491]\ttrain's multi_logloss: 0.371766\tvalid's multi_logloss: 0.668073\n",
      "[492]\ttrain's multi_logloss: 0.371182\tvalid's multi_logloss: 0.668993\n",
      "[493]\ttrain's multi_logloss: 0.370659\tvalid's multi_logloss: 0.668799\n",
      "[494]\ttrain's multi_logloss: 0.369901\tvalid's multi_logloss: 0.669164\n",
      "[495]\ttrain's multi_logloss: 0.369304\tvalid's multi_logloss: 0.669624\n",
      "[496]\ttrain's multi_logloss: 0.368804\tvalid's multi_logloss: 0.668718\n",
      "[497]\ttrain's multi_logloss: 0.368433\tvalid's multi_logloss: 0.66905\n",
      "[498]\ttrain's multi_logloss: 0.368127\tvalid's multi_logloss: 0.669057\n",
      "[499]\ttrain's multi_logloss: 0.367666\tvalid's multi_logloss: 0.668645\n",
      "[500]\ttrain's multi_logloss: 0.367275\tvalid's multi_logloss: 0.667932\n",
      "[501]\ttrain's multi_logloss: 0.366474\tvalid's multi_logloss: 0.669531\n",
      "[502]\ttrain's multi_logloss: 0.365666\tvalid's multi_logloss: 0.670102\n",
      "[503]\ttrain's multi_logloss: 0.365352\tvalid's multi_logloss: 0.670084\n",
      "[504]\ttrain's multi_logloss: 0.364523\tvalid's multi_logloss: 0.671696\n",
      "[505]\ttrain's multi_logloss: 0.363963\tvalid's multi_logloss: 0.672459\n",
      "[506]\ttrain's multi_logloss: 0.363295\tvalid's multi_logloss: 0.672521\n",
      "[507]\ttrain's multi_logloss: 0.362768\tvalid's multi_logloss: 0.672475\n",
      "[508]\ttrain's multi_logloss: 0.362044\tvalid's multi_logloss: 0.671886\n",
      "[509]\ttrain's multi_logloss: 0.361536\tvalid's multi_logloss: 0.672419\n",
      "[510]\ttrain's multi_logloss: 0.360918\tvalid's multi_logloss: 0.671381\n",
      "[511]\ttrain's multi_logloss: 0.360387\tvalid's multi_logloss: 0.672258\n",
      "[512]\ttrain's multi_logloss: 0.359736\tvalid's multi_logloss: 0.672346\n",
      "[513]\ttrain's multi_logloss: 0.359116\tvalid's multi_logloss: 0.671534\n",
      "[514]\ttrain's multi_logloss: 0.358486\tvalid's multi_logloss: 0.671991\n",
      "[515]\ttrain's multi_logloss: 0.357865\tvalid's multi_logloss: 0.672104\n",
      "[516]\ttrain's multi_logloss: 0.357586\tvalid's multi_logloss: 0.670878\n",
      "[517]\ttrain's multi_logloss: 0.35725\tvalid's multi_logloss: 0.670266\n",
      "[518]\ttrain's multi_logloss: 0.356974\tvalid's multi_logloss: 0.669587\n",
      "[519]\ttrain's multi_logloss: 0.356713\tvalid's multi_logloss: 0.668377\n",
      "[520]\ttrain's multi_logloss: 0.356467\tvalid's multi_logloss: 0.667193\n",
      "[521]\ttrain's multi_logloss: 0.355748\tvalid's multi_logloss: 0.666957\n",
      "[522]\ttrain's multi_logloss: 0.35522\tvalid's multi_logloss: 0.66694\n",
      "[523]\ttrain's multi_logloss: 0.354518\tvalid's multi_logloss: 0.666508\n",
      "[524]\ttrain's multi_logloss: 0.35387\tvalid's multi_logloss: 0.666278\n",
      "[525]\ttrain's multi_logloss: 0.353316\tvalid's multi_logloss: 0.665285\n",
      "[526]\ttrain's multi_logloss: 0.352892\tvalid's multi_logloss: 0.66506\n",
      "[527]\ttrain's multi_logloss: 0.352416\tvalid's multi_logloss: 0.665478\n",
      "[528]\ttrain's multi_logloss: 0.351987\tvalid's multi_logloss: 0.665864\n",
      "[529]\ttrain's multi_logloss: 0.35154\tvalid's multi_logloss: 0.666276\n",
      "[530]\ttrain's multi_logloss: 0.351108\tvalid's multi_logloss: 0.666381\n",
      "[531]\ttrain's multi_logloss: 0.350441\tvalid's multi_logloss: 0.665697\n",
      "[532]\ttrain's multi_logloss: 0.349694\tvalid's multi_logloss: 0.664862\n",
      "[533]\ttrain's multi_logloss: 0.348994\tvalid's multi_logloss: 0.664209\n",
      "[534]\ttrain's multi_logloss: 0.348271\tvalid's multi_logloss: 0.663463\n",
      "[535]\ttrain's multi_logloss: 0.347601\tvalid's multi_logloss: 0.663586\n",
      "[536]\ttrain's multi_logloss: 0.347277\tvalid's multi_logloss: 0.663634\n",
      "[537]\ttrain's multi_logloss: 0.346915\tvalid's multi_logloss: 0.663886\n",
      "[538]\ttrain's multi_logloss: 0.346549\tvalid's multi_logloss: 0.66414\n",
      "[539]\ttrain's multi_logloss: 0.346162\tvalid's multi_logloss: 0.663135\n",
      "[540]\ttrain's multi_logloss: 0.34589\tvalid's multi_logloss: 0.662626\n",
      "[541]\ttrain's multi_logloss: 0.34539\tvalid's multi_logloss: 0.662349\n",
      "[542]\ttrain's multi_logloss: 0.34475\tvalid's multi_logloss: 0.66168\n",
      "[543]\ttrain's multi_logloss: 0.343982\tvalid's multi_logloss: 0.660303\n",
      "[544]\ttrain's multi_logloss: 0.343438\tvalid's multi_logloss: 0.659661\n",
      "[545]\ttrain's multi_logloss: 0.342757\tvalid's multi_logloss: 0.65805\n",
      "[546]\ttrain's multi_logloss: 0.342236\tvalid's multi_logloss: 0.658036\n",
      "[547]\ttrain's multi_logloss: 0.341764\tvalid's multi_logloss: 0.658088\n",
      "[548]\ttrain's multi_logloss: 0.341267\tvalid's multi_logloss: 0.656971\n",
      "[549]\ttrain's multi_logloss: 0.340857\tvalid's multi_logloss: 0.655919\n",
      "[550]\ttrain's multi_logloss: 0.340363\tvalid's multi_logloss: 0.655022\n",
      "Early stopping, best iteration is:\n",
      "[450]\ttrain's multi_logloss: 0.397766\tvalid's multi_logloss: 0.644116\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09295\tvalid's SFC_loss: 1.08922\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08838\tvalid's SFC_loss: 1.0811\n",
      "[3]\ttrain's SFC_loss: 1.08285\tvalid's SFC_loss: 1.08065\n",
      "[4]\ttrain's SFC_loss: 1.0769\tvalid's SFC_loss: 1.07895\n",
      "[5]\ttrain's SFC_loss: 1.07247\tvalid's SFC_loss: 1.0727\n",
      "[6]\ttrain's SFC_loss: 1.06815\tvalid's SFC_loss: 1.06963\n",
      "[7]\ttrain's SFC_loss: 1.06235\tvalid's SFC_loss: 1.06778\n",
      "[8]\ttrain's SFC_loss: 1.05606\tvalid's SFC_loss: 1.06343\n",
      "[9]\ttrain's SFC_loss: 1.05071\tvalid's SFC_loss: 1.06186\n",
      "[10]\ttrain's SFC_loss: 1.04556\tvalid's SFC_loss: 1.06048\n",
      "[11]\ttrain's SFC_loss: 1.04146\tvalid's SFC_loss: 1.06017\n",
      "[12]\ttrain's SFC_loss: 1.03744\tvalid's SFC_loss: 1.05704\n",
      "[13]\ttrain's SFC_loss: 1.03297\tvalid's SFC_loss: 1.05268\n",
      "[14]\ttrain's SFC_loss: 1.02746\tvalid's SFC_loss: 1.04892\n",
      "[15]\ttrain's SFC_loss: 1.02345\tvalid's SFC_loss: 1.04418\n",
      "[16]\ttrain's SFC_loss: 1.01809\tvalid's SFC_loss: 1.03879\n",
      "[17]\ttrain's SFC_loss: 1.0125\tvalid's SFC_loss: 1.03743\n",
      "[18]\ttrain's SFC_loss: 1.00775\tvalid's SFC_loss: 1.03354\n",
      "[19]\ttrain's SFC_loss: 1.00396\tvalid's SFC_loss: 1.02818\n",
      "[20]\ttrain's SFC_loss: 0.998892\tvalid's SFC_loss: 1.02719\n",
      "[21]\ttrain's SFC_loss: 0.994581\tvalid's SFC_loss: 1.02616\n",
      "[22]\ttrain's SFC_loss: 0.990424\tvalid's SFC_loss: 1.02377\n",
      "[23]\ttrain's SFC_loss: 0.98547\tvalid's SFC_loss: 1.0206\n",
      "[24]\ttrain's SFC_loss: 0.980767\tvalid's SFC_loss: 1.01582\n",
      "[25]\ttrain's SFC_loss: 0.977208\tvalid's SFC_loss: 1.01568\n",
      "[26]\ttrain's SFC_loss: 0.972273\tvalid's SFC_loss: 1.00751\n",
      "[27]\ttrain's SFC_loss: 0.968199\tvalid's SFC_loss: 1.00345\n",
      "[28]\ttrain's SFC_loss: 0.963806\tvalid's SFC_loss: 0.99591\n",
      "[29]\ttrain's SFC_loss: 0.95998\tvalid's SFC_loss: 0.990618\n",
      "[30]\ttrain's SFC_loss: 0.955484\tvalid's SFC_loss: 0.986052\n",
      "[31]\ttrain's SFC_loss: 0.950961\tvalid's SFC_loss: 0.983649\n",
      "[32]\ttrain's SFC_loss: 0.946005\tvalid's SFC_loss: 0.97876\n",
      "[33]\ttrain's SFC_loss: 0.942004\tvalid's SFC_loss: 0.972876\n",
      "[34]\ttrain's SFC_loss: 0.93731\tvalid's SFC_loss: 0.967845\n",
      "[35]\ttrain's SFC_loss: 0.932743\tvalid's SFC_loss: 0.964752\n",
      "[36]\ttrain's SFC_loss: 0.928549\tvalid's SFC_loss: 0.963951\n",
      "[37]\ttrain's SFC_loss: 0.92451\tvalid's SFC_loss: 0.9584\n",
      "[38]\ttrain's SFC_loss: 0.920352\tvalid's SFC_loss: 0.956973\n",
      "[39]\ttrain's SFC_loss: 0.91696\tvalid's SFC_loss: 0.951448\n",
      "[40]\ttrain's SFC_loss: 0.913462\tvalid's SFC_loss: 0.946854\n",
      "[41]\ttrain's SFC_loss: 0.910322\tvalid's SFC_loss: 0.947377\n",
      "[42]\ttrain's SFC_loss: 0.906577\tvalid's SFC_loss: 0.94408\n",
      "[43]\ttrain's SFC_loss: 0.902985\tvalid's SFC_loss: 0.942845\n",
      "[44]\ttrain's SFC_loss: 0.899166\tvalid's SFC_loss: 0.939448\n",
      "[45]\ttrain's SFC_loss: 0.895403\tvalid's SFC_loss: 0.936693\n",
      "[46]\ttrain's SFC_loss: 0.892457\tvalid's SFC_loss: 0.939241\n",
      "[47]\ttrain's SFC_loss: 0.889374\tvalid's SFC_loss: 0.936994\n",
      "[48]\ttrain's SFC_loss: 0.88673\tvalid's SFC_loss: 0.937956\n",
      "[49]\ttrain's SFC_loss: 0.884032\tvalid's SFC_loss: 0.940423\n",
      "[50]\ttrain's SFC_loss: 0.881016\tvalid's SFC_loss: 0.942246\n",
      "[51]\ttrain's SFC_loss: 0.877892\tvalid's SFC_loss: 0.942225\n",
      "[52]\ttrain's SFC_loss: 0.873514\tvalid's SFC_loss: 0.942866\n",
      "[53]\ttrain's SFC_loss: 0.87062\tvalid's SFC_loss: 0.943379\n",
      "[54]\ttrain's SFC_loss: 0.866806\tvalid's SFC_loss: 0.942964\n",
      "[55]\ttrain's SFC_loss: 0.863803\tvalid's SFC_loss: 0.944493\n",
      "[56]\ttrain's SFC_loss: 0.86073\tvalid's SFC_loss: 0.941945\n",
      "[57]\ttrain's SFC_loss: 0.857341\tvalid's SFC_loss: 0.938353\n",
      "[58]\ttrain's SFC_loss: 0.85445\tvalid's SFC_loss: 0.93586\n",
      "[59]\ttrain's SFC_loss: 0.851517\tvalid's SFC_loss: 0.932077\n",
      "[60]\ttrain's SFC_loss: 0.848687\tvalid's SFC_loss: 0.929169\n",
      "[61]\ttrain's SFC_loss: 0.845201\tvalid's SFC_loss: 0.924146\n",
      "[62]\ttrain's SFC_loss: 0.84172\tvalid's SFC_loss: 0.921164\n",
      "[63]\ttrain's SFC_loss: 0.838311\tvalid's SFC_loss: 0.916985\n",
      "[64]\ttrain's SFC_loss: 0.835134\tvalid's SFC_loss: 0.914121\n",
      "[65]\ttrain's SFC_loss: 0.8323\tvalid's SFC_loss: 0.910116\n",
      "[66]\ttrain's SFC_loss: 0.828876\tvalid's SFC_loss: 0.911562\n",
      "[67]\ttrain's SFC_loss: 0.825411\tvalid's SFC_loss: 0.907376\n",
      "[68]\ttrain's SFC_loss: 0.822304\tvalid's SFC_loss: 0.905288\n",
      "[69]\ttrain's SFC_loss: 0.818194\tvalid's SFC_loss: 0.904164\n",
      "[70]\ttrain's SFC_loss: 0.815285\tvalid's SFC_loss: 0.900766\n",
      "[71]\ttrain's SFC_loss: 0.812485\tvalid's SFC_loss: 0.898433\n",
      "[72]\ttrain's SFC_loss: 0.808858\tvalid's SFC_loss: 0.896082\n",
      "[73]\ttrain's SFC_loss: 0.806283\tvalid's SFC_loss: 0.895708\n",
      "[74]\ttrain's SFC_loss: 0.803107\tvalid's SFC_loss: 0.893219\n",
      "[75]\ttrain's SFC_loss: 0.800023\tvalid's SFC_loss: 0.889426\n",
      "[76]\ttrain's SFC_loss: 0.797819\tvalid's SFC_loss: 0.889046\n",
      "[77]\ttrain's SFC_loss: 0.795275\tvalid's SFC_loss: 0.888429\n",
      "[78]\ttrain's SFC_loss: 0.792987\tvalid's SFC_loss: 0.888398\n",
      "[79]\ttrain's SFC_loss: 0.790739\tvalid's SFC_loss: 0.886703\n",
      "[80]\ttrain's SFC_loss: 0.788643\tvalid's SFC_loss: 0.886422\n",
      "[81]\ttrain's SFC_loss: 0.785927\tvalid's SFC_loss: 0.88372\n",
      "[82]\ttrain's SFC_loss: 0.783379\tvalid's SFC_loss: 0.882539\n",
      "[83]\ttrain's SFC_loss: 0.780773\tvalid's SFC_loss: 0.881387\n",
      "[84]\ttrain's SFC_loss: 0.778186\tvalid's SFC_loss: 0.878407\n",
      "[85]\ttrain's SFC_loss: 0.775485\tvalid's SFC_loss: 0.873745\n",
      "[86]\ttrain's SFC_loss: 0.772748\tvalid's SFC_loss: 0.874577\n",
      "[87]\ttrain's SFC_loss: 0.770275\tvalid's SFC_loss: 0.873518\n",
      "[88]\ttrain's SFC_loss: 0.767154\tvalid's SFC_loss: 0.871578\n",
      "[89]\ttrain's SFC_loss: 0.764551\tvalid's SFC_loss: 0.871393\n",
      "[90]\ttrain's SFC_loss: 0.761619\tvalid's SFC_loss: 0.871784\n",
      "[91]\ttrain's SFC_loss: 0.758955\tvalid's SFC_loss: 0.87154\n",
      "[92]\ttrain's SFC_loss: 0.756646\tvalid's SFC_loss: 0.867029\n",
      "[93]\ttrain's SFC_loss: 0.753991\tvalid's SFC_loss: 0.863618\n",
      "[94]\ttrain's SFC_loss: 0.752151\tvalid's SFC_loss: 0.862732\n",
      "[95]\ttrain's SFC_loss: 0.74973\tvalid's SFC_loss: 0.858551\n",
      "[96]\ttrain's SFC_loss: 0.746915\tvalid's SFC_loss: 0.853171\n",
      "[97]\ttrain's SFC_loss: 0.74423\tvalid's SFC_loss: 0.848913\n",
      "[98]\ttrain's SFC_loss: 0.741682\tvalid's SFC_loss: 0.844304\n",
      "[99]\ttrain's SFC_loss: 0.739081\tvalid's SFC_loss: 0.839719\n",
      "[100]\ttrain's SFC_loss: 0.736761\tvalid's SFC_loss: 0.836984\n",
      "[101]\ttrain's SFC_loss: 0.734455\tvalid's SFC_loss: 0.835008\n",
      "[102]\ttrain's SFC_loss: 0.731696\tvalid's SFC_loss: 0.833434\n",
      "[103]\ttrain's SFC_loss: 0.729523\tvalid's SFC_loss: 0.832694\n",
      "[104]\ttrain's SFC_loss: 0.727714\tvalid's SFC_loss: 0.831119\n",
      "[105]\ttrain's SFC_loss: 0.725676\tvalid's SFC_loss: 0.829805\n",
      "[106]\ttrain's SFC_loss: 0.722847\tvalid's SFC_loss: 0.826253\n",
      "[107]\ttrain's SFC_loss: 0.719839\tvalid's SFC_loss: 0.82166\n",
      "[108]\ttrain's SFC_loss: 0.717685\tvalid's SFC_loss: 0.818374\n",
      "[109]\ttrain's SFC_loss: 0.715632\tvalid's SFC_loss: 0.815961\n",
      "[110]\ttrain's SFC_loss: 0.712996\tvalid's SFC_loss: 0.81327\n",
      "[111]\ttrain's SFC_loss: 0.71121\tvalid's SFC_loss: 0.812887\n",
      "[112]\ttrain's SFC_loss: 0.709836\tvalid's SFC_loss: 0.811503\n",
      "[113]\ttrain's SFC_loss: 0.708535\tvalid's SFC_loss: 0.810184\n",
      "[114]\ttrain's SFC_loss: 0.706965\tvalid's SFC_loss: 0.809977\n",
      "[115]\ttrain's SFC_loss: 0.705372\tvalid's SFC_loss: 0.809751\n",
      "[116]\ttrain's SFC_loss: 0.703141\tvalid's SFC_loss: 0.807939\n",
      "[117]\ttrain's SFC_loss: 0.700633\tvalid's SFC_loss: 0.803781\n",
      "[118]\ttrain's SFC_loss: 0.698602\tvalid's SFC_loss: 0.799676\n",
      "[119]\ttrain's SFC_loss: 0.696148\tvalid's SFC_loss: 0.795813\n",
      "[120]\ttrain's SFC_loss: 0.693767\tvalid's SFC_loss: 0.794165\n",
      "[121]\ttrain's SFC_loss: 0.691874\tvalid's SFC_loss: 0.794369\n",
      "[122]\ttrain's SFC_loss: 0.690286\tvalid's SFC_loss: 0.793241\n",
      "[123]\ttrain's SFC_loss: 0.688567\tvalid's SFC_loss: 0.793729\n",
      "[124]\ttrain's SFC_loss: 0.686549\tvalid's SFC_loss: 0.791487\n",
      "[125]\ttrain's SFC_loss: 0.685086\tvalid's SFC_loss: 0.790477\n",
      "[126]\ttrain's SFC_loss: 0.683716\tvalid's SFC_loss: 0.787441\n",
      "[127]\ttrain's SFC_loss: 0.681992\tvalid's SFC_loss: 0.785751\n",
      "[128]\ttrain's SFC_loss: 0.680313\tvalid's SFC_loss: 0.782117\n",
      "[129]\ttrain's SFC_loss: 0.67871\tvalid's SFC_loss: 0.778595\n",
      "[130]\ttrain's SFC_loss: 0.676977\tvalid's SFC_loss: 0.775622\n",
      "[131]\ttrain's SFC_loss: 0.675666\tvalid's SFC_loss: 0.770812\n",
      "[132]\ttrain's SFC_loss: 0.674156\tvalid's SFC_loss: 0.765904\n",
      "[133]\ttrain's SFC_loss: 0.672573\tvalid's SFC_loss: 0.762417\n",
      "[134]\ttrain's SFC_loss: 0.671175\tvalid's SFC_loss: 0.762035\n",
      "[135]\ttrain's SFC_loss: 0.669539\tvalid's SFC_loss: 0.759636\n",
      "[136]\ttrain's SFC_loss: 0.667137\tvalid's SFC_loss: 0.757931\n",
      "[137]\ttrain's SFC_loss: 0.664728\tvalid's SFC_loss: 0.755641\n",
      "[138]\ttrain's SFC_loss: 0.662448\tvalid's SFC_loss: 0.755011\n",
      "[139]\ttrain's SFC_loss: 0.660056\tvalid's SFC_loss: 0.753258\n",
      "[140]\ttrain's SFC_loss: 0.657913\tvalid's SFC_loss: 0.751244\n",
      "[141]\ttrain's SFC_loss: 0.656971\tvalid's SFC_loss: 0.751632\n",
      "[142]\ttrain's SFC_loss: 0.655527\tvalid's SFC_loss: 0.749133\n",
      "[143]\ttrain's SFC_loss: 0.653966\tvalid's SFC_loss: 0.750082\n",
      "[144]\ttrain's SFC_loss: 0.652652\tvalid's SFC_loss: 0.749781\n",
      "[145]\ttrain's SFC_loss: 0.650979\tvalid's SFC_loss: 0.7484\n",
      "[146]\ttrain's SFC_loss: 0.649126\tvalid's SFC_loss: 0.747785\n",
      "[147]\ttrain's SFC_loss: 0.647883\tvalid's SFC_loss: 0.746883\n",
      "[148]\ttrain's SFC_loss: 0.646276\tvalid's SFC_loss: 0.74649\n",
      "[149]\ttrain's SFC_loss: 0.644811\tvalid's SFC_loss: 0.743869\n",
      "[150]\ttrain's SFC_loss: 0.643171\tvalid's SFC_loss: 0.742318\n",
      "[151]\ttrain's SFC_loss: 0.64125\tvalid's SFC_loss: 0.740565\n",
      "[152]\ttrain's SFC_loss: 0.639646\tvalid's SFC_loss: 0.738811\n",
      "[153]\ttrain's SFC_loss: 0.637785\tvalid's SFC_loss: 0.738363\n",
      "[154]\ttrain's SFC_loss: 0.636289\tvalid's SFC_loss: 0.737797\n",
      "[155]\ttrain's SFC_loss: 0.63516\tvalid's SFC_loss: 0.736748\n",
      "[156]\ttrain's SFC_loss: 0.633834\tvalid's SFC_loss: 0.738057\n",
      "[157]\ttrain's SFC_loss: 0.632366\tvalid's SFC_loss: 0.740329\n",
      "[158]\ttrain's SFC_loss: 0.630858\tvalid's SFC_loss: 0.740933\n",
      "[159]\ttrain's SFC_loss: 0.629119\tvalid's SFC_loss: 0.743508\n",
      "[160]\ttrain's SFC_loss: 0.627655\tvalid's SFC_loss: 0.744543\n",
      "[161]\ttrain's SFC_loss: 0.626013\tvalid's SFC_loss: 0.743027\n",
      "[162]\ttrain's SFC_loss: 0.624548\tvalid's SFC_loss: 0.743109\n",
      "[163]\ttrain's SFC_loss: 0.622963\tvalid's SFC_loss: 0.74174\n",
      "[164]\ttrain's SFC_loss: 0.621508\tvalid's SFC_loss: 0.741933\n",
      "[165]\ttrain's SFC_loss: 0.620003\tvalid's SFC_loss: 0.741119\n",
      "[166]\ttrain's SFC_loss: 0.618879\tvalid's SFC_loss: 0.737325\n",
      "[167]\ttrain's SFC_loss: 0.617457\tvalid's SFC_loss: 0.737015\n",
      "[168]\ttrain's SFC_loss: 0.615935\tvalid's SFC_loss: 0.733436\n",
      "[169]\ttrain's SFC_loss: 0.614401\tvalid's SFC_loss: 0.730301\n",
      "[170]\ttrain's SFC_loss: 0.612946\tvalid's SFC_loss: 0.730034\n",
      "[171]\ttrain's SFC_loss: 0.611659\tvalid's SFC_loss: 0.728548\n",
      "[172]\ttrain's SFC_loss: 0.610266\tvalid's SFC_loss: 0.726178\n",
      "[173]\ttrain's SFC_loss: 0.608874\tvalid's SFC_loss: 0.722919\n",
      "[174]\ttrain's SFC_loss: 0.607713\tvalid's SFC_loss: 0.720795\n",
      "[175]\ttrain's SFC_loss: 0.60663\tvalid's SFC_loss: 0.720168\n",
      "[176]\ttrain's SFC_loss: 0.603906\tvalid's SFC_loss: 0.720576\n",
      "[177]\ttrain's SFC_loss: 0.601674\tvalid's SFC_loss: 0.719588\n",
      "[178]\ttrain's SFC_loss: 0.599713\tvalid's SFC_loss: 0.71754\n",
      "[179]\ttrain's SFC_loss: 0.597426\tvalid's SFC_loss: 0.715777\n",
      "[180]\ttrain's SFC_loss: 0.59451\tvalid's SFC_loss: 0.715386\n",
      "[181]\ttrain's SFC_loss: 0.592919\tvalid's SFC_loss: 0.715423\n",
      "[182]\ttrain's SFC_loss: 0.590773\tvalid's SFC_loss: 0.717411\n",
      "[183]\ttrain's SFC_loss: 0.588736\tvalid's SFC_loss: 0.718929\n",
      "[184]\ttrain's SFC_loss: 0.586269\tvalid's SFC_loss: 0.722287\n",
      "[185]\ttrain's SFC_loss: 0.584346\tvalid's SFC_loss: 0.725076\n",
      "[186]\ttrain's SFC_loss: 0.582587\tvalid's SFC_loss: 0.724611\n",
      "[187]\ttrain's SFC_loss: 0.580995\tvalid's SFC_loss: 0.723233\n",
      "[188]\ttrain's SFC_loss: 0.579975\tvalid's SFC_loss: 0.721638\n",
      "[189]\ttrain's SFC_loss: 0.578982\tvalid's SFC_loss: 0.720285\n",
      "[190]\ttrain's SFC_loss: 0.577656\tvalid's SFC_loss: 0.721996\n",
      "[191]\ttrain's SFC_loss: 0.576774\tvalid's SFC_loss: 0.722516\n",
      "[192]\ttrain's SFC_loss: 0.576287\tvalid's SFC_loss: 0.722703\n",
      "[193]\ttrain's SFC_loss: 0.575232\tvalid's SFC_loss: 0.720441\n",
      "[194]\ttrain's SFC_loss: 0.57479\tvalid's SFC_loss: 0.719433\n",
      "[195]\ttrain's SFC_loss: 0.573335\tvalid's SFC_loss: 0.7181\n",
      "[196]\ttrain's SFC_loss: 0.571732\tvalid's SFC_loss: 0.718485\n",
      "[197]\ttrain's SFC_loss: 0.569459\tvalid's SFC_loss: 0.720973\n",
      "[198]\ttrain's SFC_loss: 0.567685\tvalid's SFC_loss: 0.721142\n",
      "[199]\ttrain's SFC_loss: 0.566219\tvalid's SFC_loss: 0.723076\n",
      "[200]\ttrain's SFC_loss: 0.564891\tvalid's SFC_loss: 0.722699\n",
      "[201]\ttrain's SFC_loss: 0.563375\tvalid's SFC_loss: 0.723127\n",
      "[202]\ttrain's SFC_loss: 0.562119\tvalid's SFC_loss: 0.724043\n",
      "[203]\ttrain's SFC_loss: 0.560809\tvalid's SFC_loss: 0.725268\n",
      "[204]\ttrain's SFC_loss: 0.559185\tvalid's SFC_loss: 0.725448\n",
      "[205]\ttrain's SFC_loss: 0.557502\tvalid's SFC_loss: 0.725552\n",
      "[206]\ttrain's SFC_loss: 0.556905\tvalid's SFC_loss: 0.726561\n",
      "[207]\ttrain's SFC_loss: 0.556521\tvalid's SFC_loss: 0.726731\n",
      "[208]\ttrain's SFC_loss: 0.556022\tvalid's SFC_loss: 0.728048\n",
      "[209]\ttrain's SFC_loss: 0.555349\tvalid's SFC_loss: 0.729335\n",
      "[210]\ttrain's SFC_loss: 0.554948\tvalid's SFC_loss: 0.730752\n",
      "[211]\ttrain's SFC_loss: 0.553965\tvalid's SFC_loss: 0.727282\n",
      "[212]\ttrain's SFC_loss: 0.552638\tvalid's SFC_loss: 0.727267\n",
      "[213]\ttrain's SFC_loss: 0.551776\tvalid's SFC_loss: 0.727423\n",
      "[214]\ttrain's SFC_loss: 0.550664\tvalid's SFC_loss: 0.725009\n",
      "[215]\ttrain's SFC_loss: 0.54968\tvalid's SFC_loss: 0.723866\n",
      "[216]\ttrain's SFC_loss: 0.547787\tvalid's SFC_loss: 0.724257\n",
      "[217]\ttrain's SFC_loss: 0.546062\tvalid's SFC_loss: 0.723788\n",
      "[218]\ttrain's SFC_loss: 0.544798\tvalid's SFC_loss: 0.723921\n",
      "[219]\ttrain's SFC_loss: 0.543308\tvalid's SFC_loss: 0.72261\n",
      "[220]\ttrain's SFC_loss: 0.541652\tvalid's SFC_loss: 0.725342\n",
      "[221]\ttrain's SFC_loss: 0.539964\tvalid's SFC_loss: 0.726528\n",
      "[222]\ttrain's SFC_loss: 0.538251\tvalid's SFC_loss: 0.725127\n",
      "[223]\ttrain's SFC_loss: 0.536985\tvalid's SFC_loss: 0.723242\n",
      "[224]\ttrain's SFC_loss: 0.535389\tvalid's SFC_loss: 0.722699\n",
      "[225]\ttrain's SFC_loss: 0.533699\tvalid's SFC_loss: 0.722156\n",
      "[226]\ttrain's SFC_loss: 0.532485\tvalid's SFC_loss: 0.72158\n",
      "[227]\ttrain's SFC_loss: 0.531147\tvalid's SFC_loss: 0.722477\n",
      "[228]\ttrain's SFC_loss: 0.52959\tvalid's SFC_loss: 0.723298\n",
      "[229]\ttrain's SFC_loss: 0.528279\tvalid's SFC_loss: 0.723554\n",
      "[230]\ttrain's SFC_loss: 0.526747\tvalid's SFC_loss: 0.724767\n",
      "[231]\ttrain's SFC_loss: 0.525914\tvalid's SFC_loss: 0.725445\n",
      "[232]\ttrain's SFC_loss: 0.524731\tvalid's SFC_loss: 0.724838\n",
      "[233]\ttrain's SFC_loss: 0.523726\tvalid's SFC_loss: 0.724355\n",
      "[234]\ttrain's SFC_loss: 0.522739\tvalid's SFC_loss: 0.723397\n",
      "[235]\ttrain's SFC_loss: 0.521914\tvalid's SFC_loss: 0.72298\n",
      "[236]\ttrain's SFC_loss: 0.52058\tvalid's SFC_loss: 0.720745\n",
      "[237]\ttrain's SFC_loss: 0.519002\tvalid's SFC_loss: 0.717968\n",
      "[238]\ttrain's SFC_loss: 0.517545\tvalid's SFC_loss: 0.71618\n",
      "[239]\ttrain's SFC_loss: 0.516005\tvalid's SFC_loss: 0.713255\n",
      "[240]\ttrain's SFC_loss: 0.514727\tvalid's SFC_loss: 0.712142\n",
      "[241]\ttrain's SFC_loss: 0.513428\tvalid's SFC_loss: 0.714388\n",
      "[242]\ttrain's SFC_loss: 0.512425\tvalid's SFC_loss: 0.715793\n",
      "[243]\ttrain's SFC_loss: 0.511494\tvalid's SFC_loss: 0.714775\n",
      "[244]\ttrain's SFC_loss: 0.510128\tvalid's SFC_loss: 0.715904\n",
      "[245]\ttrain's SFC_loss: 0.509037\tvalid's SFC_loss: 0.715318\n",
      "[246]\ttrain's SFC_loss: 0.508121\tvalid's SFC_loss: 0.712761\n",
      "[247]\ttrain's SFC_loss: 0.507324\tvalid's SFC_loss: 0.708761\n",
      "[248]\ttrain's SFC_loss: 0.506299\tvalid's SFC_loss: 0.705744\n",
      "[249]\ttrain's SFC_loss: 0.505263\tvalid's SFC_loss: 0.702661\n",
      "[250]\ttrain's SFC_loss: 0.504413\tvalid's SFC_loss: 0.699653\n",
      "[251]\ttrain's SFC_loss: 0.503134\tvalid's SFC_loss: 0.699296\n",
      "[252]\ttrain's SFC_loss: 0.50183\tvalid's SFC_loss: 0.698341\n",
      "[253]\ttrain's SFC_loss: 0.50047\tvalid's SFC_loss: 0.697231\n",
      "[254]\ttrain's SFC_loss: 0.499229\tvalid's SFC_loss: 0.696966\n",
      "[255]\ttrain's SFC_loss: 0.497939\tvalid's SFC_loss: 0.695925\n",
      "[256]\ttrain's SFC_loss: 0.496854\tvalid's SFC_loss: 0.696955\n",
      "[257]\ttrain's SFC_loss: 0.495239\tvalid's SFC_loss: 0.69977\n",
      "[258]\ttrain's SFC_loss: 0.493794\tvalid's SFC_loss: 0.699497\n",
      "[259]\ttrain's SFC_loss: 0.492437\tvalid's SFC_loss: 0.699946\n",
      "[260]\ttrain's SFC_loss: 0.490947\tvalid's SFC_loss: 0.702\n",
      "[261]\ttrain's SFC_loss: 0.490331\tvalid's SFC_loss: 0.703417\n",
      "[262]\ttrain's SFC_loss: 0.489547\tvalid's SFC_loss: 0.705615\n",
      "[263]\ttrain's SFC_loss: 0.488935\tvalid's SFC_loss: 0.705967\n",
      "[264]\ttrain's SFC_loss: 0.488237\tvalid's SFC_loss: 0.707569\n",
      "[265]\ttrain's SFC_loss: 0.487538\tvalid's SFC_loss: 0.709656\n",
      "[266]\ttrain's SFC_loss: 0.486533\tvalid's SFC_loss: 0.709191\n",
      "[267]\ttrain's SFC_loss: 0.485695\tvalid's SFC_loss: 0.708004\n",
      "[268]\ttrain's SFC_loss: 0.484667\tvalid's SFC_loss: 0.708508\n",
      "[269]\ttrain's SFC_loss: 0.483708\tvalid's SFC_loss: 0.70674\n",
      "[270]\ttrain's SFC_loss: 0.482711\tvalid's SFC_loss: 0.707238\n",
      "[271]\ttrain's SFC_loss: 0.481743\tvalid's SFC_loss: 0.706356\n",
      "[272]\ttrain's SFC_loss: 0.48069\tvalid's SFC_loss: 0.706224\n",
      "[273]\ttrain's SFC_loss: 0.47967\tvalid's SFC_loss: 0.706809\n",
      "[274]\ttrain's SFC_loss: 0.478899\tvalid's SFC_loss: 0.707473\n",
      "[275]\ttrain's SFC_loss: 0.477921\tvalid's SFC_loss: 0.707155\n",
      "[276]\ttrain's SFC_loss: 0.476708\tvalid's SFC_loss: 0.707613\n",
      "[277]\ttrain's SFC_loss: 0.475316\tvalid's SFC_loss: 0.705437\n",
      "[278]\ttrain's SFC_loss: 0.473682\tvalid's SFC_loss: 0.703347\n",
      "[279]\ttrain's SFC_loss: 0.472904\tvalid's SFC_loss: 0.70056\n",
      "[280]\ttrain's SFC_loss: 0.471662\tvalid's SFC_loss: 0.699842\n",
      "[281]\ttrain's SFC_loss: 0.470441\tvalid's SFC_loss: 0.700122\n",
      "[282]\ttrain's SFC_loss: 0.469743\tvalid's SFC_loss: 0.698034\n",
      "[283]\ttrain's SFC_loss: 0.469032\tvalid's SFC_loss: 0.696688\n",
      "[284]\ttrain's SFC_loss: 0.468398\tvalid's SFC_loss: 0.696522\n",
      "[285]\ttrain's SFC_loss: 0.467844\tvalid's SFC_loss: 0.6959\n",
      "[286]\ttrain's SFC_loss: 0.466903\tvalid's SFC_loss: 0.69589\n",
      "[287]\ttrain's SFC_loss: 0.465819\tvalid's SFC_loss: 0.696643\n",
      "[288]\ttrain's SFC_loss: 0.464747\tvalid's SFC_loss: 0.696633\n",
      "[289]\ttrain's SFC_loss: 0.463868\tvalid's SFC_loss: 0.695134\n",
      "[290]\ttrain's SFC_loss: 0.46293\tvalid's SFC_loss: 0.694208\n",
      "[291]\ttrain's SFC_loss: 0.46187\tvalid's SFC_loss: 0.694141\n",
      "[292]\ttrain's SFC_loss: 0.460852\tvalid's SFC_loss: 0.692392\n",
      "[293]\ttrain's SFC_loss: 0.45997\tvalid's SFC_loss: 0.691353\n",
      "[294]\ttrain's SFC_loss: 0.459179\tvalid's SFC_loss: 0.690247\n",
      "[295]\ttrain's SFC_loss: 0.457684\tvalid's SFC_loss: 0.690202\n",
      "[296]\ttrain's SFC_loss: 0.45724\tvalid's SFC_loss: 0.687849\n",
      "[297]\ttrain's SFC_loss: 0.456857\tvalid's SFC_loss: 0.686632\n",
      "[298]\ttrain's SFC_loss: 0.456367\tvalid's SFC_loss: 0.68539\n",
      "[299]\ttrain's SFC_loss: 0.455895\tvalid's SFC_loss: 0.684172\n",
      "[300]\ttrain's SFC_loss: 0.455206\tvalid's SFC_loss: 0.683265\n",
      "[301]\ttrain's SFC_loss: 0.454089\tvalid's SFC_loss: 0.68222\n",
      "[302]\ttrain's SFC_loss: 0.453161\tvalid's SFC_loss: 0.682828\n",
      "[303]\ttrain's SFC_loss: 0.452075\tvalid's SFC_loss: 0.685838\n",
      "[304]\ttrain's SFC_loss: 0.450851\tvalid's SFC_loss: 0.688782\n",
      "[305]\ttrain's SFC_loss: 0.449781\tvalid's SFC_loss: 0.69127\n",
      "[306]\ttrain's SFC_loss: 0.448381\tvalid's SFC_loss: 0.689913\n",
      "[307]\ttrain's SFC_loss: 0.447173\tvalid's SFC_loss: 0.688782\n",
      "[308]\ttrain's SFC_loss: 0.445935\tvalid's SFC_loss: 0.688162\n",
      "[309]\ttrain's SFC_loss: 0.444741\tvalid's SFC_loss: 0.687583\n",
      "[310]\ttrain's SFC_loss: 0.443747\tvalid's SFC_loss: 0.685861\n",
      "[311]\ttrain's SFC_loss: 0.443019\tvalid's SFC_loss: 0.684111\n",
      "[312]\ttrain's SFC_loss: 0.441983\tvalid's SFC_loss: 0.680834\n",
      "[313]\ttrain's SFC_loss: 0.441294\tvalid's SFC_loss: 0.682707\n",
      "[314]\ttrain's SFC_loss: 0.440328\tvalid's SFC_loss: 0.680261\n",
      "[315]\ttrain's SFC_loss: 0.439451\tvalid's SFC_loss: 0.678367\n",
      "[316]\ttrain's SFC_loss: 0.438777\tvalid's SFC_loss: 0.677825\n",
      "[317]\ttrain's SFC_loss: 0.437815\tvalid's SFC_loss: 0.677696\n",
      "[318]\ttrain's SFC_loss: 0.437244\tvalid's SFC_loss: 0.678149\n",
      "[319]\ttrain's SFC_loss: 0.436396\tvalid's SFC_loss: 0.677908\n",
      "[320]\ttrain's SFC_loss: 0.4357\tvalid's SFC_loss: 0.679865\n",
      "[321]\ttrain's SFC_loss: 0.434579\tvalid's SFC_loss: 0.680677\n",
      "[322]\ttrain's SFC_loss: 0.43344\tvalid's SFC_loss: 0.68115\n",
      "[323]\ttrain's SFC_loss: 0.432486\tvalid's SFC_loss: 0.681348\n",
      "[324]\ttrain's SFC_loss: 0.431468\tvalid's SFC_loss: 0.683159\n",
      "[325]\ttrain's SFC_loss: 0.430342\tvalid's SFC_loss: 0.685377\n",
      "[326]\ttrain's SFC_loss: 0.429438\tvalid's SFC_loss: 0.687945\n",
      "[327]\ttrain's SFC_loss: 0.428539\tvalid's SFC_loss: 0.689141\n",
      "[328]\ttrain's SFC_loss: 0.427361\tvalid's SFC_loss: 0.689668\n",
      "[329]\ttrain's SFC_loss: 0.426223\tvalid's SFC_loss: 0.690239\n",
      "[330]\ttrain's SFC_loss: 0.425076\tvalid's SFC_loss: 0.690819\n",
      "[331]\ttrain's SFC_loss: 0.424449\tvalid's SFC_loss: 0.694526\n",
      "[332]\ttrain's SFC_loss: 0.423705\tvalid's SFC_loss: 0.696778\n",
      "[333]\ttrain's SFC_loss: 0.422973\tvalid's SFC_loss: 0.69845\n",
      "[334]\ttrain's SFC_loss: 0.421887\tvalid's SFC_loss: 0.700936\n",
      "[335]\ttrain's SFC_loss: 0.421291\tvalid's SFC_loss: 0.704581\n",
      "[336]\ttrain's SFC_loss: 0.420603\tvalid's SFC_loss: 0.702763\n",
      "[337]\ttrain's SFC_loss: 0.419902\tvalid's SFC_loss: 0.700811\n",
      "[338]\ttrain's SFC_loss: 0.418999\tvalid's SFC_loss: 0.701864\n",
      "[339]\ttrain's SFC_loss: 0.418338\tvalid's SFC_loss: 0.703032\n",
      "[340]\ttrain's SFC_loss: 0.417481\tvalid's SFC_loss: 0.702742\n",
      "[341]\ttrain's SFC_loss: 0.416931\tvalid's SFC_loss: 0.70307\n",
      "[342]\ttrain's SFC_loss: 0.416111\tvalid's SFC_loss: 0.703306\n",
      "[343]\ttrain's SFC_loss: 0.415188\tvalid's SFC_loss: 0.70542\n",
      "[344]\ttrain's SFC_loss: 0.41448\tvalid's SFC_loss: 0.706452\n",
      "[345]\ttrain's SFC_loss: 0.41369\tvalid's SFC_loss: 0.706936\n",
      "[346]\ttrain's SFC_loss: 0.412875\tvalid's SFC_loss: 0.705099\n",
      "[347]\ttrain's SFC_loss: 0.411972\tvalid's SFC_loss: 0.703218\n",
      "[348]\ttrain's SFC_loss: 0.41105\tvalid's SFC_loss: 0.699984\n",
      "[349]\ttrain's SFC_loss: 0.410209\tvalid's SFC_loss: 0.69734\n",
      "[350]\ttrain's SFC_loss: 0.409345\tvalid's SFC_loss: 0.694203\n",
      "[351]\ttrain's SFC_loss: 0.40846\tvalid's SFC_loss: 0.695095\n",
      "[352]\ttrain's SFC_loss: 0.40783\tvalid's SFC_loss: 0.695306\n",
      "[353]\ttrain's SFC_loss: 0.406994\tvalid's SFC_loss: 0.695259\n",
      "[354]\ttrain's SFC_loss: 0.405958\tvalid's SFC_loss: 0.695827\n",
      "[355]\ttrain's SFC_loss: 0.405296\tvalid's SFC_loss: 0.694503\n",
      "[356]\ttrain's SFC_loss: 0.404215\tvalid's SFC_loss: 0.692094\n",
      "[357]\ttrain's SFC_loss: 0.403309\tvalid's SFC_loss: 0.693556\n",
      "[358]\ttrain's SFC_loss: 0.402124\tvalid's SFC_loss: 0.693161\n",
      "[359]\ttrain's SFC_loss: 0.400711\tvalid's SFC_loss: 0.691779\n",
      "[360]\ttrain's SFC_loss: 0.399488\tvalid's SFC_loss: 0.691684\n",
      "[361]\ttrain's SFC_loss: 0.398715\tvalid's SFC_loss: 0.692372\n",
      "[362]\ttrain's SFC_loss: 0.397907\tvalid's SFC_loss: 0.694027\n",
      "[363]\ttrain's SFC_loss: 0.397045\tvalid's SFC_loss: 0.694329\n",
      "[364]\ttrain's SFC_loss: 0.39641\tvalid's SFC_loss: 0.694913\n",
      "[365]\ttrain's SFC_loss: 0.395703\tvalid's SFC_loss: 0.694188\n",
      "[366]\ttrain's SFC_loss: 0.394627\tvalid's SFC_loss: 0.691412\n",
      "[367]\ttrain's SFC_loss: 0.393442\tvalid's SFC_loss: 0.690039\n",
      "[368]\ttrain's SFC_loss: 0.392431\tvalid's SFC_loss: 0.690414\n",
      "[369]\ttrain's SFC_loss: 0.391905\tvalid's SFC_loss: 0.690402\n",
      "[370]\ttrain's SFC_loss: 0.391052\tvalid's SFC_loss: 0.690271\n",
      "[371]\ttrain's SFC_loss: 0.39052\tvalid's SFC_loss: 0.689039\n",
      "[372]\ttrain's SFC_loss: 0.389796\tvalid's SFC_loss: 0.686994\n",
      "[373]\ttrain's SFC_loss: 0.389172\tvalid's SFC_loss: 0.689315\n",
      "[374]\ttrain's SFC_loss: 0.388646\tvalid's SFC_loss: 0.688851\n",
      "[375]\ttrain's SFC_loss: 0.387977\tvalid's SFC_loss: 0.688168\n",
      "[376]\ttrain's SFC_loss: 0.387278\tvalid's SFC_loss: 0.689024\n",
      "[377]\ttrain's SFC_loss: 0.386644\tvalid's SFC_loss: 0.688935\n",
      "[378]\ttrain's SFC_loss: 0.385762\tvalid's SFC_loss: 0.689576\n",
      "[379]\ttrain's SFC_loss: 0.385192\tvalid's SFC_loss: 0.688632\n",
      "[380]\ttrain's SFC_loss: 0.384309\tvalid's SFC_loss: 0.69129\n",
      "[381]\ttrain's SFC_loss: 0.383428\tvalid's SFC_loss: 0.69084\n",
      "[382]\ttrain's SFC_loss: 0.382558\tvalid's SFC_loss: 0.689721\n",
      "[383]\ttrain's SFC_loss: 0.381733\tvalid's SFC_loss: 0.688539\n",
      "[384]\ttrain's SFC_loss: 0.381208\tvalid's SFC_loss: 0.687156\n",
      "[385]\ttrain's SFC_loss: 0.38047\tvalid's SFC_loss: 0.686976\n",
      "[386]\ttrain's SFC_loss: 0.379627\tvalid's SFC_loss: 0.688188\n",
      "[387]\ttrain's SFC_loss: 0.378776\tvalid's SFC_loss: 0.689932\n",
      "[388]\ttrain's SFC_loss: 0.378128\tvalid's SFC_loss: 0.690672\n",
      "[389]\ttrain's SFC_loss: 0.377329\tvalid's SFC_loss: 0.691014\n",
      "[390]\ttrain's SFC_loss: 0.376611\tvalid's SFC_loss: 0.692746\n",
      "[391]\ttrain's SFC_loss: 0.375953\tvalid's SFC_loss: 0.691163\n",
      "[392]\ttrain's SFC_loss: 0.375526\tvalid's SFC_loss: 0.690214\n",
      "[393]\ttrain's SFC_loss: 0.374871\tvalid's SFC_loss: 0.691043\n",
      "[394]\ttrain's SFC_loss: 0.37444\tvalid's SFC_loss: 0.693234\n",
      "[395]\ttrain's SFC_loss: 0.373866\tvalid's SFC_loss: 0.695188\n",
      "[396]\ttrain's SFC_loss: 0.372972\tvalid's SFC_loss: 0.693858\n",
      "[397]\ttrain's SFC_loss: 0.372384\tvalid's SFC_loss: 0.693377\n",
      "[398]\ttrain's SFC_loss: 0.371526\tvalid's SFC_loss: 0.69209\n",
      "[399]\ttrain's SFC_loss: 0.370849\tvalid's SFC_loss: 0.690344\n",
      "[400]\ttrain's SFC_loss: 0.369997\tvalid's SFC_loss: 0.690141\n",
      "[401]\ttrain's SFC_loss: 0.369326\tvalid's SFC_loss: 0.690978\n",
      "[402]\ttrain's SFC_loss: 0.368246\tvalid's SFC_loss: 0.690564\n",
      "[403]\ttrain's SFC_loss: 0.36752\tvalid's SFC_loss: 0.688878\n",
      "[404]\ttrain's SFC_loss: 0.366724\tvalid's SFC_loss: 0.688209\n",
      "[405]\ttrain's SFC_loss: 0.366053\tvalid's SFC_loss: 0.685741\n",
      "[406]\ttrain's SFC_loss: 0.36496\tvalid's SFC_loss: 0.685093\n",
      "[407]\ttrain's SFC_loss: 0.363945\tvalid's SFC_loss: 0.685062\n",
      "[408]\ttrain's SFC_loss: 0.36269\tvalid's SFC_loss: 0.685428\n",
      "[409]\ttrain's SFC_loss: 0.361705\tvalid's SFC_loss: 0.682598\n",
      "[410]\ttrain's SFC_loss: 0.360712\tvalid's SFC_loss: 0.681939\n",
      "[411]\ttrain's SFC_loss: 0.360062\tvalid's SFC_loss: 0.68333\n",
      "[412]\ttrain's SFC_loss: 0.359366\tvalid's SFC_loss: 0.685918\n",
      "[413]\ttrain's SFC_loss: 0.358721\tvalid's SFC_loss: 0.688449\n",
      "[414]\ttrain's SFC_loss: 0.358086\tvalid's SFC_loss: 0.688467\n",
      "[415]\ttrain's SFC_loss: 0.357391\tvalid's SFC_loss: 0.690645\n",
      "[416]\ttrain's SFC_loss: 0.356756\tvalid's SFC_loss: 0.68683\n",
      "[417]\ttrain's SFC_loss: 0.356215\tvalid's SFC_loss: 0.686849\n",
      "Early stopping, best iteration is:\n",
      "[317]\ttrain's SFC_loss: 0.437815\tvalid's SFC_loss: 0.677696\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1   -0.031263\n",
      "1    feature2    0.022843\n",
      "2    feature3    0.096259\n",
      "3    feature4   -0.055642\n",
      "4    feature5   -0.004299\n",
      "5    feature6    0.082594\n",
      "6    feature7   -0.088263\n",
      "7    feature8    0.023190\n",
      "8    feature9    0.016968\n",
      "9   feature10   -0.076359\n",
      "10  feature11    0.094605\n",
      "11  feature12   -0.136402\n",
      "12  feature13   -0.060622\n",
      "13  feature14    0.008249\n",
      "14  feature15    0.004143\n",
      "15  feature16    0.026350\n",
      "16  feature17   -0.047148\n",
      "17  feature18    0.024316\n",
      "18  feature19    0.099454\n",
      "19  feature20    0.001030\n",
      "-------------------- 9 --------------------\n",
      "(98, 20) (98,)\n",
      "(10, 20) (10,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.06053\tvalid's multi_logloss: 1.04294\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05678\tvalid's multi_logloss: 1.04091\n",
      "[3]\ttrain's multi_logloss: 1.05278\tvalid's multi_logloss: 1.03907\n",
      "[4]\ttrain's multi_logloss: 1.04899\tvalid's multi_logloss: 1.03694\n",
      "[5]\ttrain's multi_logloss: 1.04515\tvalid's multi_logloss: 1.03642\n",
      "[6]\ttrain's multi_logloss: 1.04194\tvalid's multi_logloss: 1.03402\n",
      "[7]\ttrain's multi_logloss: 1.03886\tvalid's multi_logloss: 1.03205\n",
      "[8]\ttrain's multi_logloss: 1.03609\tvalid's multi_logloss: 1.03008\n",
      "[9]\ttrain's multi_logloss: 1.03337\tvalid's multi_logloss: 1.02748\n",
      "[10]\ttrain's multi_logloss: 1.03069\tvalid's multi_logloss: 1.02558\n",
      "[11]\ttrain's multi_logloss: 1.02673\tvalid's multi_logloss: 1.02509\n",
      "[12]\ttrain's multi_logloss: 1.02311\tvalid's multi_logloss: 1.02159\n",
      "[13]\ttrain's multi_logloss: 1.01991\tvalid's multi_logloss: 1.0196\n",
      "[14]\ttrain's multi_logloss: 1.01671\tvalid's multi_logloss: 1.01721\n",
      "[15]\ttrain's multi_logloss: 1.01286\tvalid's multi_logloss: 1.01453\n",
      "[16]\ttrain's multi_logloss: 1.00936\tvalid's multi_logloss: 1.0106\n",
      "[17]\ttrain's multi_logloss: 1.00526\tvalid's multi_logloss: 1.00854\n",
      "[18]\ttrain's multi_logloss: 1.00148\tvalid's multi_logloss: 1.00476\n",
      "[19]\ttrain's multi_logloss: 0.99745\tvalid's multi_logloss: 1.00174\n",
      "[20]\ttrain's multi_logloss: 0.993836\tvalid's multi_logloss: 1.00014\n",
      "[21]\ttrain's multi_logloss: 0.99013\tvalid's multi_logloss: 1.00025\n",
      "[22]\ttrain's multi_logloss: 0.986637\tvalid's multi_logloss: 1.00172\n",
      "[23]\ttrain's multi_logloss: 0.983186\tvalid's multi_logloss: 1.00217\n",
      "[24]\ttrain's multi_logloss: 0.97982\tvalid's multi_logloss: 1.00059\n",
      "[25]\ttrain's multi_logloss: 0.976434\tvalid's multi_logloss: 0.998328\n",
      "[26]\ttrain's multi_logloss: 0.972527\tvalid's multi_logloss: 0.995982\n",
      "[27]\ttrain's multi_logloss: 0.969117\tvalid's multi_logloss: 0.991994\n",
      "[28]\ttrain's multi_logloss: 0.965395\tvalid's multi_logloss: 0.989548\n",
      "[29]\ttrain's multi_logloss: 0.962303\tvalid's multi_logloss: 0.987813\n",
      "[30]\ttrain's multi_logloss: 0.959145\tvalid's multi_logloss: 0.986247\n",
      "[31]\ttrain's multi_logloss: 0.956304\tvalid's multi_logloss: 0.98423\n",
      "[32]\ttrain's multi_logloss: 0.953429\tvalid's multi_logloss: 0.982293\n",
      "[33]\ttrain's multi_logloss: 0.950882\tvalid's multi_logloss: 0.980989\n",
      "[34]\ttrain's multi_logloss: 0.948304\tvalid's multi_logloss: 0.979773\n",
      "[35]\ttrain's multi_logloss: 0.946019\tvalid's multi_logloss: 0.979428\n",
      "[36]\ttrain's multi_logloss: 0.942625\tvalid's multi_logloss: 0.979143\n",
      "[37]\ttrain's multi_logloss: 0.940067\tvalid's multi_logloss: 0.973546\n",
      "[38]\ttrain's multi_logloss: 0.937689\tvalid's multi_logloss: 0.973442\n",
      "[39]\ttrain's multi_logloss: 0.934719\tvalid's multi_logloss: 0.971624\n",
      "[40]\ttrain's multi_logloss: 0.931791\tvalid's multi_logloss: 0.968975\n",
      "[41]\ttrain's multi_logloss: 0.928547\tvalid's multi_logloss: 0.966924\n",
      "[42]\ttrain's multi_logloss: 0.925807\tvalid's multi_logloss: 0.963651\n",
      "[43]\ttrain's multi_logloss: 0.923297\tvalid's multi_logloss: 0.961976\n",
      "[44]\ttrain's multi_logloss: 0.92018\tvalid's multi_logloss: 0.959419\n",
      "[45]\ttrain's multi_logloss: 0.916898\tvalid's multi_logloss: 0.957623\n",
      "[46]\ttrain's multi_logloss: 0.914708\tvalid's multi_logloss: 0.956786\n",
      "[47]\ttrain's multi_logloss: 0.912568\tvalid's multi_logloss: 0.955983\n",
      "[48]\ttrain's multi_logloss: 0.910478\tvalid's multi_logloss: 0.955214\n",
      "[49]\ttrain's multi_logloss: 0.908674\tvalid's multi_logloss: 0.955272\n",
      "[50]\ttrain's multi_logloss: 0.906817\tvalid's multi_logloss: 0.953295\n",
      "[51]\ttrain's multi_logloss: 0.903541\tvalid's multi_logloss: 0.947091\n",
      "[52]\ttrain's multi_logloss: 0.900478\tvalid's multi_logloss: 0.943107\n",
      "[53]\ttrain's multi_logloss: 0.897886\tvalid's multi_logloss: 0.942415\n",
      "[54]\ttrain's multi_logloss: 0.895673\tvalid's multi_logloss: 0.94015\n",
      "[55]\ttrain's multi_logloss: 0.892582\tvalid's multi_logloss: 0.934221\n",
      "[56]\ttrain's multi_logloss: 0.88978\tvalid's multi_logloss: 0.931438\n",
      "[57]\ttrain's multi_logloss: 0.887097\tvalid's multi_logloss: 0.928772\n",
      "[58]\ttrain's multi_logloss: 0.884593\tvalid's multi_logloss: 0.925506\n",
      "[59]\ttrain's multi_logloss: 0.881961\tvalid's multi_logloss: 0.921659\n",
      "[60]\ttrain's multi_logloss: 0.879432\tvalid's multi_logloss: 0.91973\n",
      "[61]\ttrain's multi_logloss: 0.87677\tvalid's multi_logloss: 0.917177\n",
      "[62]\ttrain's multi_logloss: 0.873942\tvalid's multi_logloss: 0.916083\n",
      "[63]\ttrain's multi_logloss: 0.871439\tvalid's multi_logloss: 0.914694\n",
      "[64]\ttrain's multi_logloss: 0.869126\tvalid's multi_logloss: 0.91189\n",
      "[65]\ttrain's multi_logloss: 0.866982\tvalid's multi_logloss: 0.909719\n",
      "[66]\ttrain's multi_logloss: 0.864385\tvalid's multi_logloss: 0.907834\n",
      "[67]\ttrain's multi_logloss: 0.861466\tvalid's multi_logloss: 0.907362\n",
      "[68]\ttrain's multi_logloss: 0.859024\tvalid's multi_logloss: 0.906145\n",
      "[69]\ttrain's multi_logloss: 0.856636\tvalid's multi_logloss: 0.904573\n",
      "[70]\ttrain's multi_logloss: 0.853894\tvalid's multi_logloss: 0.903337\n",
      "[71]\ttrain's multi_logloss: 0.85128\tvalid's multi_logloss: 0.90283\n",
      "[72]\ttrain's multi_logloss: 0.848695\tvalid's multi_logloss: 0.901578\n",
      "[73]\ttrain's multi_logloss: 0.846104\tvalid's multi_logloss: 0.902024\n",
      "[74]\ttrain's multi_logloss: 0.843557\tvalid's multi_logloss: 0.90128\n",
      "[75]\ttrain's multi_logloss: 0.840842\tvalid's multi_logloss: 0.901677\n",
      "[76]\ttrain's multi_logloss: 0.838698\tvalid's multi_logloss: 0.901013\n",
      "[77]\ttrain's multi_logloss: 0.836728\tvalid's multi_logloss: 0.902118\n",
      "[78]\ttrain's multi_logloss: 0.835181\tvalid's multi_logloss: 0.901059\n",
      "[79]\ttrain's multi_logloss: 0.83284\tvalid's multi_logloss: 0.901654\n",
      "[80]\ttrain's multi_logloss: 0.831343\tvalid's multi_logloss: 0.900634\n",
      "[81]\ttrain's multi_logloss: 0.829364\tvalid's multi_logloss: 0.899523\n",
      "[82]\ttrain's multi_logloss: 0.827704\tvalid's multi_logloss: 0.900673\n",
      "[83]\ttrain's multi_logloss: 0.825679\tvalid's multi_logloss: 0.899228\n",
      "[84]\ttrain's multi_logloss: 0.82356\tvalid's multi_logloss: 0.898985\n",
      "[85]\ttrain's multi_logloss: 0.821226\tvalid's multi_logloss: 0.898121\n",
      "[86]\ttrain's multi_logloss: 0.818265\tvalid's multi_logloss: 0.894602\n",
      "[87]\ttrain's multi_logloss: 0.815465\tvalid's multi_logloss: 0.89205\n",
      "[88]\ttrain's multi_logloss: 0.812703\tvalid's multi_logloss: 0.889538\n",
      "[89]\ttrain's multi_logloss: 0.810113\tvalid's multi_logloss: 0.887422\n",
      "[90]\ttrain's multi_logloss: 0.807623\tvalid's multi_logloss: 0.884878\n",
      "[91]\ttrain's multi_logloss: 0.805511\tvalid's multi_logloss: 0.883596\n",
      "[92]\ttrain's multi_logloss: 0.803768\tvalid's multi_logloss: 0.88226\n",
      "[93]\ttrain's multi_logloss: 0.801684\tvalid's multi_logloss: 0.881178\n",
      "[94]\ttrain's multi_logloss: 0.799182\tvalid's multi_logloss: 0.879622\n",
      "[95]\ttrain's multi_logloss: 0.797052\tvalid's multi_logloss: 0.878208\n",
      "[96]\ttrain's multi_logloss: 0.795042\tvalid's multi_logloss: 0.876027\n",
      "[97]\ttrain's multi_logloss: 0.793282\tvalid's multi_logloss: 0.874218\n",
      "[98]\ttrain's multi_logloss: 0.791043\tvalid's multi_logloss: 0.871045\n",
      "[99]\ttrain's multi_logloss: 0.78891\tvalid's multi_logloss: 0.869986\n",
      "[100]\ttrain's multi_logloss: 0.78659\tvalid's multi_logloss: 0.86713\n",
      "[101]\ttrain's multi_logloss: 0.784565\tvalid's multi_logloss: 0.864487\n",
      "[102]\ttrain's multi_logloss: 0.782559\tvalid's multi_logloss: 0.86319\n",
      "[103]\ttrain's multi_logloss: 0.780545\tvalid's multi_logloss: 0.862504\n",
      "[104]\ttrain's multi_logloss: 0.778422\tvalid's multi_logloss: 0.860526\n",
      "[105]\ttrain's multi_logloss: 0.776299\tvalid's multi_logloss: 0.858147\n",
      "[106]\ttrain's multi_logloss: 0.774978\tvalid's multi_logloss: 0.857966\n",
      "[107]\ttrain's multi_logloss: 0.773636\tvalid's multi_logloss: 0.85803\n",
      "[108]\ttrain's multi_logloss: 0.772556\tvalid's multi_logloss: 0.85606\n",
      "[109]\ttrain's multi_logloss: 0.771157\tvalid's multi_logloss: 0.854074\n",
      "[110]\ttrain's multi_logloss: 0.770125\tvalid's multi_logloss: 0.853187\n",
      "[111]\ttrain's multi_logloss: 0.768813\tvalid's multi_logloss: 0.853713\n",
      "[112]\ttrain's multi_logloss: 0.767335\tvalid's multi_logloss: 0.852664\n",
      "[113]\ttrain's multi_logloss: 0.766379\tvalid's multi_logloss: 0.852347\n",
      "[114]\ttrain's multi_logloss: 0.765308\tvalid's multi_logloss: 0.852229\n",
      "[115]\ttrain's multi_logloss: 0.764092\tvalid's multi_logloss: 0.851603\n",
      "[116]\ttrain's multi_logloss: 0.762086\tvalid's multi_logloss: 0.851952\n",
      "[117]\ttrain's multi_logloss: 0.760045\tvalid's multi_logloss: 0.850725\n",
      "[118]\ttrain's multi_logloss: 0.757791\tvalid's multi_logloss: 0.852069\n",
      "[119]\ttrain's multi_logloss: 0.755716\tvalid's multi_logloss: 0.851205\n",
      "[120]\ttrain's multi_logloss: 0.753531\tvalid's multi_logloss: 0.852219\n",
      "[121]\ttrain's multi_logloss: 0.751593\tvalid's multi_logloss: 0.851676\n",
      "[122]\ttrain's multi_logloss: 0.749464\tvalid's multi_logloss: 0.851189\n",
      "[123]\ttrain's multi_logloss: 0.748175\tvalid's multi_logloss: 0.851111\n",
      "[124]\ttrain's multi_logloss: 0.746621\tvalid's multi_logloss: 0.849596\n",
      "[125]\ttrain's multi_logloss: 0.744759\tvalid's multi_logloss: 0.847684\n",
      "[126]\ttrain's multi_logloss: 0.743091\tvalid's multi_logloss: 0.845629\n",
      "[127]\ttrain's multi_logloss: 0.741192\tvalid's multi_logloss: 0.842919\n",
      "[128]\ttrain's multi_logloss: 0.739068\tvalid's multi_logloss: 0.840149\n",
      "[129]\ttrain's multi_logloss: 0.737335\tvalid's multi_logloss: 0.838334\n",
      "[130]\ttrain's multi_logloss: 0.735574\tvalid's multi_logloss: 0.835072\n",
      "[131]\ttrain's multi_logloss: 0.734319\tvalid's multi_logloss: 0.834083\n",
      "[132]\ttrain's multi_logloss: 0.732586\tvalid's multi_logloss: 0.832359\n",
      "[133]\ttrain's multi_logloss: 0.730858\tvalid's multi_logloss: 0.828962\n",
      "[134]\ttrain's multi_logloss: 0.729006\tvalid's multi_logloss: 0.82751\n",
      "[135]\ttrain's multi_logloss: 0.727371\tvalid's multi_logloss: 0.826389\n",
      "[136]\ttrain's multi_logloss: 0.72526\tvalid's multi_logloss: 0.825842\n",
      "[137]\ttrain's multi_logloss: 0.722793\tvalid's multi_logloss: 0.825309\n",
      "[138]\ttrain's multi_logloss: 0.721255\tvalid's multi_logloss: 0.825011\n",
      "[139]\ttrain's multi_logloss: 0.719396\tvalid's multi_logloss: 0.823115\n",
      "[140]\ttrain's multi_logloss: 0.717385\tvalid's multi_logloss: 0.822131\n",
      "[141]\ttrain's multi_logloss: 0.716214\tvalid's multi_logloss: 0.821495\n",
      "[142]\ttrain's multi_logloss: 0.715402\tvalid's multi_logloss: 0.822271\n",
      "[143]\ttrain's multi_logloss: 0.714419\tvalid's multi_logloss: 0.821999\n",
      "[144]\ttrain's multi_logloss: 0.713006\tvalid's multi_logloss: 0.821119\n",
      "[145]\ttrain's multi_logloss: 0.712159\tvalid's multi_logloss: 0.822114\n",
      "[146]\ttrain's multi_logloss: 0.711091\tvalid's multi_logloss: 0.822322\n",
      "[147]\ttrain's multi_logloss: 0.709618\tvalid's multi_logloss: 0.819662\n",
      "[148]\ttrain's multi_logloss: 0.708394\tvalid's multi_logloss: 0.820301\n",
      "[149]\ttrain's multi_logloss: 0.706722\tvalid's multi_logloss: 0.818011\n",
      "[150]\ttrain's multi_logloss: 0.705217\tvalid's multi_logloss: 0.815721\n",
      "[151]\ttrain's multi_logloss: 0.703522\tvalid's multi_logloss: 0.814227\n",
      "[152]\ttrain's multi_logloss: 0.701838\tvalid's multi_logloss: 0.812995\n",
      "[153]\ttrain's multi_logloss: 0.700262\tvalid's multi_logloss: 0.811403\n",
      "[154]\ttrain's multi_logloss: 0.69858\tvalid's multi_logloss: 0.810833\n",
      "[155]\ttrain's multi_logloss: 0.696923\tvalid's multi_logloss: 0.809585\n",
      "[156]\ttrain's multi_logloss: 0.696048\tvalid's multi_logloss: 0.808683\n",
      "[157]\ttrain's multi_logloss: 0.694977\tvalid's multi_logloss: 0.8086\n",
      "[158]\ttrain's multi_logloss: 0.693934\tvalid's multi_logloss: 0.808534\n",
      "[159]\ttrain's multi_logloss: 0.693121\tvalid's multi_logloss: 0.808574\n",
      "[160]\ttrain's multi_logloss: 0.692486\tvalid's multi_logloss: 0.808715\n",
      "[161]\ttrain's multi_logloss: 0.690719\tvalid's multi_logloss: 0.805655\n",
      "[162]\ttrain's multi_logloss: 0.688925\tvalid's multi_logloss: 0.802291\n",
      "[163]\ttrain's multi_logloss: 0.68718\tvalid's multi_logloss: 0.799512\n",
      "[164]\ttrain's multi_logloss: 0.68524\tvalid's multi_logloss: 0.796889\n",
      "[165]\ttrain's multi_logloss: 0.683607\tvalid's multi_logloss: 0.795821\n",
      "[166]\ttrain's multi_logloss: 0.682271\tvalid's multi_logloss: 0.793512\n",
      "[167]\ttrain's multi_logloss: 0.680454\tvalid's multi_logloss: 0.792009\n",
      "[168]\ttrain's multi_logloss: 0.679006\tvalid's multi_logloss: 0.790621\n",
      "[169]\ttrain's multi_logloss: 0.67763\tvalid's multi_logloss: 0.788767\n",
      "[170]\ttrain's multi_logloss: 0.676226\tvalid's multi_logloss: 0.786633\n",
      "[171]\ttrain's multi_logloss: 0.674844\tvalid's multi_logloss: 0.78439\n",
      "[172]\ttrain's multi_logloss: 0.673292\tvalid's multi_logloss: 0.781762\n",
      "[173]\ttrain's multi_logloss: 0.67178\tvalid's multi_logloss: 0.779752\n",
      "[174]\ttrain's multi_logloss: 0.670132\tvalid's multi_logloss: 0.777859\n",
      "[175]\ttrain's multi_logloss: 0.668719\tvalid's multi_logloss: 0.775373\n",
      "[176]\ttrain's multi_logloss: 0.66691\tvalid's multi_logloss: 0.774619\n",
      "[177]\ttrain's multi_logloss: 0.665439\tvalid's multi_logloss: 0.773603\n",
      "[178]\ttrain's multi_logloss: 0.663866\tvalid's multi_logloss: 0.771994\n",
      "[179]\ttrain's multi_logloss: 0.661916\tvalid's multi_logloss: 0.771561\n",
      "[180]\ttrain's multi_logloss: 0.660223\tvalid's multi_logloss: 0.771108\n",
      "[181]\ttrain's multi_logloss: 0.658836\tvalid's multi_logloss: 0.770137\n",
      "[182]\ttrain's multi_logloss: 0.657472\tvalid's multi_logloss: 0.76923\n",
      "[183]\ttrain's multi_logloss: 0.656216\tvalid's multi_logloss: 0.76711\n",
      "[184]\ttrain's multi_logloss: 0.655005\tvalid's multi_logloss: 0.765534\n",
      "[185]\ttrain's multi_logloss: 0.653646\tvalid's multi_logloss: 0.764587\n",
      "[186]\ttrain's multi_logloss: 0.652117\tvalid's multi_logloss: 0.765004\n",
      "[187]\ttrain's multi_logloss: 0.65083\tvalid's multi_logloss: 0.764146\n",
      "[188]\ttrain's multi_logloss: 0.649249\tvalid's multi_logloss: 0.764334\n",
      "[189]\ttrain's multi_logloss: 0.64787\tvalid's multi_logloss: 0.763773\n",
      "[190]\ttrain's multi_logloss: 0.646454\tvalid's multi_logloss: 0.763026\n",
      "[191]\ttrain's multi_logloss: 0.644731\tvalid's multi_logloss: 0.76094\n",
      "[192]\ttrain's multi_logloss: 0.643409\tvalid's multi_logloss: 0.759954\n",
      "[193]\ttrain's multi_logloss: 0.64225\tvalid's multi_logloss: 0.759155\n",
      "[194]\ttrain's multi_logloss: 0.640939\tvalid's multi_logloss: 0.75815\n",
      "[195]\ttrain's multi_logloss: 0.639529\tvalid's multi_logloss: 0.756951\n",
      "[196]\ttrain's multi_logloss: 0.637813\tvalid's multi_logloss: 0.754955\n",
      "[197]\ttrain's multi_logloss: 0.636377\tvalid's multi_logloss: 0.753036\n",
      "[198]\ttrain's multi_logloss: 0.634708\tvalid's multi_logloss: 0.75221\n",
      "[199]\ttrain's multi_logloss: 0.633159\tvalid's multi_logloss: 0.750597\n",
      "[200]\ttrain's multi_logloss: 0.631736\tvalid's multi_logloss: 0.749585\n",
      "[201]\ttrain's multi_logloss: 0.630338\tvalid's multi_logloss: 0.749965\n",
      "[202]\ttrain's multi_logloss: 0.628995\tvalid's multi_logloss: 0.750226\n",
      "[203]\ttrain's multi_logloss: 0.627679\tvalid's multi_logloss: 0.750509\n",
      "[204]\ttrain's multi_logloss: 0.626402\tvalid's multi_logloss: 0.750809\n",
      "[205]\ttrain's multi_logloss: 0.625106\tvalid's multi_logloss: 0.750669\n",
      "[206]\ttrain's multi_logloss: 0.624086\tvalid's multi_logloss: 0.750865\n",
      "[207]\ttrain's multi_logloss: 0.623134\tvalid's multi_logloss: 0.750772\n",
      "[208]\ttrain's multi_logloss: 0.622158\tvalid's multi_logloss: 0.750002\n",
      "[209]\ttrain's multi_logloss: 0.621212\tvalid's multi_logloss: 0.748965\n",
      "[210]\ttrain's multi_logloss: 0.620205\tvalid's multi_logloss: 0.74868\n",
      "[211]\ttrain's multi_logloss: 0.619457\tvalid's multi_logloss: 0.748724\n",
      "[212]\ttrain's multi_logloss: 0.618941\tvalid's multi_logloss: 0.74874\n",
      "[213]\ttrain's multi_logloss: 0.61823\tvalid's multi_logloss: 0.748813\n",
      "[214]\ttrain's multi_logloss: 0.617737\tvalid's multi_logloss: 0.747831\n",
      "[215]\ttrain's multi_logloss: 0.61716\tvalid's multi_logloss: 0.747514\n",
      "[216]\ttrain's multi_logloss: 0.615386\tvalid's multi_logloss: 0.747797\n",
      "[217]\ttrain's multi_logloss: 0.61381\tvalid's multi_logloss: 0.74779\n",
      "[218]\ttrain's multi_logloss: 0.61253\tvalid's multi_logloss: 0.748494\n",
      "[219]\ttrain's multi_logloss: 0.611077\tvalid's multi_logloss: 0.748396\n",
      "[220]\ttrain's multi_logloss: 0.609557\tvalid's multi_logloss: 0.748454\n",
      "[221]\ttrain's multi_logloss: 0.608263\tvalid's multi_logloss: 0.747272\n",
      "[222]\ttrain's multi_logloss: 0.606957\tvalid's multi_logloss: 0.745949\n",
      "[223]\ttrain's multi_logloss: 0.605718\tvalid's multi_logloss: 0.745983\n",
      "[224]\ttrain's multi_logloss: 0.604159\tvalid's multi_logloss: 0.744572\n",
      "[225]\ttrain's multi_logloss: 0.603087\tvalid's multi_logloss: 0.744567\n",
      "[226]\ttrain's multi_logloss: 0.60187\tvalid's multi_logloss: 0.743202\n",
      "[227]\ttrain's multi_logloss: 0.600277\tvalid's multi_logloss: 0.742265\n",
      "[228]\ttrain's multi_logloss: 0.599026\tvalid's multi_logloss: 0.741265\n",
      "[229]\ttrain's multi_logloss: 0.597627\tvalid's multi_logloss: 0.739721\n",
      "[230]\ttrain's multi_logloss: 0.596541\tvalid's multi_logloss: 0.738684\n",
      "[231]\ttrain's multi_logloss: 0.595912\tvalid's multi_logloss: 0.738647\n",
      "[232]\ttrain's multi_logloss: 0.594567\tvalid's multi_logloss: 0.737788\n",
      "[233]\ttrain's multi_logloss: 0.59381\tvalid's multi_logloss: 0.738271\n",
      "[234]\ttrain's multi_logloss: 0.592892\tvalid's multi_logloss: 0.739871\n",
      "[235]\ttrain's multi_logloss: 0.591992\tvalid's multi_logloss: 0.740558\n",
      "[236]\ttrain's multi_logloss: 0.590366\tvalid's multi_logloss: 0.738995\n",
      "[237]\ttrain's multi_logloss: 0.589093\tvalid's multi_logloss: 0.737388\n",
      "[238]\ttrain's multi_logloss: 0.587571\tvalid's multi_logloss: 0.73589\n",
      "[239]\ttrain's multi_logloss: 0.586304\tvalid's multi_logloss: 0.73358\n",
      "[240]\ttrain's multi_logloss: 0.584925\tvalid's multi_logloss: 0.733579\n",
      "[241]\ttrain's multi_logloss: 0.583706\tvalid's multi_logloss: 0.73354\n",
      "[242]\ttrain's multi_logloss: 0.5826\tvalid's multi_logloss: 0.732313\n",
      "[243]\ttrain's multi_logloss: 0.581821\tvalid's multi_logloss: 0.732129\n",
      "[244]\ttrain's multi_logloss: 0.580747\tvalid's multi_logloss: 0.731561\n",
      "[245]\ttrain's multi_logloss: 0.579375\tvalid's multi_logloss: 0.729806\n",
      "[246]\ttrain's multi_logloss: 0.578279\tvalid's multi_logloss: 0.728385\n",
      "[247]\ttrain's multi_logloss: 0.577504\tvalid's multi_logloss: 0.726312\n",
      "[248]\ttrain's multi_logloss: 0.576803\tvalid's multi_logloss: 0.725885\n",
      "[249]\ttrain's multi_logloss: 0.575413\tvalid's multi_logloss: 0.723859\n",
      "[250]\ttrain's multi_logloss: 0.57486\tvalid's multi_logloss: 0.722397\n",
      "[251]\ttrain's multi_logloss: 0.573554\tvalid's multi_logloss: 0.722204\n",
      "[252]\ttrain's multi_logloss: 0.572562\tvalid's multi_logloss: 0.720681\n",
      "[253]\ttrain's multi_logloss: 0.571165\tvalid's multi_logloss: 0.719716\n",
      "[254]\ttrain's multi_logloss: 0.569986\tvalid's multi_logloss: 0.719585\n",
      "[255]\ttrain's multi_logloss: 0.568921\tvalid's multi_logloss: 0.719033\n",
      "[256]\ttrain's multi_logloss: 0.568151\tvalid's multi_logloss: 0.719864\n",
      "[257]\ttrain's multi_logloss: 0.567541\tvalid's multi_logloss: 0.720309\n",
      "[258]\ttrain's multi_logloss: 0.567051\tvalid's multi_logloss: 0.71956\n",
      "[259]\ttrain's multi_logloss: 0.56609\tvalid's multi_logloss: 0.719441\n",
      "[260]\ttrain's multi_logloss: 0.565302\tvalid's multi_logloss: 0.719564\n",
      "[261]\ttrain's multi_logloss: 0.564317\tvalid's multi_logloss: 0.71914\n",
      "[262]\ttrain's multi_logloss: 0.563031\tvalid's multi_logloss: 0.72012\n",
      "[263]\ttrain's multi_logloss: 0.561759\tvalid's multi_logloss: 0.720199\n",
      "[264]\ttrain's multi_logloss: 0.56067\tvalid's multi_logloss: 0.718459\n",
      "[265]\ttrain's multi_logloss: 0.559639\tvalid's multi_logloss: 0.71818\n",
      "[266]\ttrain's multi_logloss: 0.558553\tvalid's multi_logloss: 0.718629\n",
      "[267]\ttrain's multi_logloss: 0.55757\tvalid's multi_logloss: 0.718733\n",
      "[268]\ttrain's multi_logloss: 0.556413\tvalid's multi_logloss: 0.716906\n",
      "[269]\ttrain's multi_logloss: 0.555307\tvalid's multi_logloss: 0.716217\n",
      "[270]\ttrain's multi_logloss: 0.554266\tvalid's multi_logloss: 0.716526\n",
      "[271]\ttrain's multi_logloss: 0.553462\tvalid's multi_logloss: 0.715462\n",
      "[272]\ttrain's multi_logloss: 0.552742\tvalid's multi_logloss: 0.713802\n",
      "[273]\ttrain's multi_logloss: 0.55195\tvalid's multi_logloss: 0.713175\n",
      "[274]\ttrain's multi_logloss: 0.551159\tvalid's multi_logloss: 0.712733\n",
      "[275]\ttrain's multi_logloss: 0.550163\tvalid's multi_logloss: 0.712973\n",
      "[276]\ttrain's multi_logloss: 0.548907\tvalid's multi_logloss: 0.712491\n",
      "[277]\ttrain's multi_logloss: 0.547488\tvalid's multi_logloss: 0.712089\n",
      "[278]\ttrain's multi_logloss: 0.546042\tvalid's multi_logloss: 0.71038\n",
      "[279]\ttrain's multi_logloss: 0.545194\tvalid's multi_logloss: 0.710635\n",
      "[280]\ttrain's multi_logloss: 0.54415\tvalid's multi_logloss: 0.710715\n",
      "[281]\ttrain's multi_logloss: 0.542923\tvalid's multi_logloss: 0.710054\n",
      "[282]\ttrain's multi_logloss: 0.542093\tvalid's multi_logloss: 0.710361\n",
      "[283]\ttrain's multi_logloss: 0.541172\tvalid's multi_logloss: 0.710165\n",
      "[284]\ttrain's multi_logloss: 0.5402\tvalid's multi_logloss: 0.709027\n",
      "[285]\ttrain's multi_logloss: 0.5392\tvalid's multi_logloss: 0.70893\n",
      "[286]\ttrain's multi_logloss: 0.538313\tvalid's multi_logloss: 0.708043\n",
      "[287]\ttrain's multi_logloss: 0.537339\tvalid's multi_logloss: 0.707811\n",
      "[288]\ttrain's multi_logloss: 0.536528\tvalid's multi_logloss: 0.706959\n",
      "[289]\ttrain's multi_logloss: 0.535506\tvalid's multi_logloss: 0.707478\n",
      "[290]\ttrain's multi_logloss: 0.534321\tvalid's multi_logloss: 0.707184\n",
      "[291]\ttrain's multi_logloss: 0.533195\tvalid's multi_logloss: 0.70799\n",
      "[292]\ttrain's multi_logloss: 0.53214\tvalid's multi_logloss: 0.708173\n",
      "[293]\ttrain's multi_logloss: 0.531056\tvalid's multi_logloss: 0.709006\n",
      "[294]\ttrain's multi_logloss: 0.53065\tvalid's multi_logloss: 0.708768\n",
      "[295]\ttrain's multi_logloss: 0.529574\tvalid's multi_logloss: 0.707754\n",
      "[296]\ttrain's multi_logloss: 0.529113\tvalid's multi_logloss: 0.707574\n",
      "[297]\ttrain's multi_logloss: 0.528849\tvalid's multi_logloss: 0.708534\n",
      "[298]\ttrain's multi_logloss: 0.528486\tvalid's multi_logloss: 0.709515\n",
      "[299]\ttrain's multi_logloss: 0.527696\tvalid's multi_logloss: 0.710003\n",
      "[300]\ttrain's multi_logloss: 0.527198\tvalid's multi_logloss: 0.71108\n",
      "[301]\ttrain's multi_logloss: 0.526297\tvalid's multi_logloss: 0.708959\n",
      "[302]\ttrain's multi_logloss: 0.525131\tvalid's multi_logloss: 0.709989\n",
      "[303]\ttrain's multi_logloss: 0.523984\tvalid's multi_logloss: 0.707521\n",
      "[304]\ttrain's multi_logloss: 0.523156\tvalid's multi_logloss: 0.705812\n",
      "[305]\ttrain's multi_logloss: 0.522163\tvalid's multi_logloss: 0.706619\n",
      "[306]\ttrain's multi_logloss: 0.521005\tvalid's multi_logloss: 0.706142\n",
      "[307]\ttrain's multi_logloss: 0.520033\tvalid's multi_logloss: 0.705188\n",
      "[308]\ttrain's multi_logloss: 0.51884\tvalid's multi_logloss: 0.704574\n",
      "[309]\ttrain's multi_logloss: 0.517638\tvalid's multi_logloss: 0.704772\n",
      "[310]\ttrain's multi_logloss: 0.516722\tvalid's multi_logloss: 0.703826\n",
      "[311]\ttrain's multi_logloss: 0.515528\tvalid's multi_logloss: 0.703537\n",
      "[312]\ttrain's multi_logloss: 0.514499\tvalid's multi_logloss: 0.703852\n",
      "[313]\ttrain's multi_logloss: 0.51348\tvalid's multi_logloss: 0.703757\n",
      "[314]\ttrain's multi_logloss: 0.512568\tvalid's multi_logloss: 0.70316\n",
      "[315]\ttrain's multi_logloss: 0.511626\tvalid's multi_logloss: 0.702716\n",
      "[316]\ttrain's multi_logloss: 0.510782\tvalid's multi_logloss: 0.702801\n",
      "[317]\ttrain's multi_logloss: 0.510139\tvalid's multi_logloss: 0.700821\n",
      "[318]\ttrain's multi_logloss: 0.509331\tvalid's multi_logloss: 0.69893\n",
      "[319]\ttrain's multi_logloss: 0.5084\tvalid's multi_logloss: 0.698153\n",
      "[320]\ttrain's multi_logloss: 0.507609\tvalid's multi_logloss: 0.697121\n",
      "[321]\ttrain's multi_logloss: 0.506718\tvalid's multi_logloss: 0.69497\n",
      "[322]\ttrain's multi_logloss: 0.505872\tvalid's multi_logloss: 0.695415\n",
      "[323]\ttrain's multi_logloss: 0.505104\tvalid's multi_logloss: 0.69508\n",
      "[324]\ttrain's multi_logloss: 0.504173\tvalid's multi_logloss: 0.694793\n",
      "[325]\ttrain's multi_logloss: 0.503336\tvalid's multi_logloss: 0.693955\n",
      "[326]\ttrain's multi_logloss: 0.502499\tvalid's multi_logloss: 0.693257\n",
      "[327]\ttrain's multi_logloss: 0.501717\tvalid's multi_logloss: 0.692186\n",
      "[328]\ttrain's multi_logloss: 0.500825\tvalid's multi_logloss: 0.691676\n",
      "[329]\ttrain's multi_logloss: 0.500051\tvalid's multi_logloss: 0.691381\n",
      "[330]\ttrain's multi_logloss: 0.499281\tvalid's multi_logloss: 0.69105\n",
      "[331]\ttrain's multi_logloss: 0.498689\tvalid's multi_logloss: 0.690111\n",
      "[332]\ttrain's multi_logloss: 0.497951\tvalid's multi_logloss: 0.689954\n",
      "[333]\ttrain's multi_logloss: 0.497211\tvalid's multi_logloss: 0.690418\n",
      "[334]\ttrain's multi_logloss: 0.496566\tvalid's multi_logloss: 0.690813\n",
      "[335]\ttrain's multi_logloss: 0.495976\tvalid's multi_logloss: 0.690835\n",
      "[336]\ttrain's multi_logloss: 0.495134\tvalid's multi_logloss: 0.69029\n",
      "[337]\ttrain's multi_logloss: 0.494338\tvalid's multi_logloss: 0.690156\n",
      "[338]\ttrain's multi_logloss: 0.493519\tvalid's multi_logloss: 0.68973\n",
      "[339]\ttrain's multi_logloss: 0.492929\tvalid's multi_logloss: 0.690766\n",
      "[340]\ttrain's multi_logloss: 0.492143\tvalid's multi_logloss: 0.690268\n",
      "[341]\ttrain's multi_logloss: 0.490869\tvalid's multi_logloss: 0.689592\n",
      "[342]\ttrain's multi_logloss: 0.490209\tvalid's multi_logloss: 0.690762\n",
      "[343]\ttrain's multi_logloss: 0.489031\tvalid's multi_logloss: 0.690945\n",
      "[344]\ttrain's multi_logloss: 0.488472\tvalid's multi_logloss: 0.691623\n",
      "[345]\ttrain's multi_logloss: 0.48759\tvalid's multi_logloss: 0.690518\n",
      "[346]\ttrain's multi_logloss: 0.486748\tvalid's multi_logloss: 0.688052\n",
      "[347]\ttrain's multi_logloss: 0.48612\tvalid's multi_logloss: 0.686816\n",
      "[348]\ttrain's multi_logloss: 0.485214\tvalid's multi_logloss: 0.685568\n",
      "[349]\ttrain's multi_logloss: 0.484332\tvalid's multi_logloss: 0.68473\n",
      "[350]\ttrain's multi_logloss: 0.483312\tvalid's multi_logloss: 0.684601\n",
      "[351]\ttrain's multi_logloss: 0.482334\tvalid's multi_logloss: 0.684462\n",
      "[352]\ttrain's multi_logloss: 0.481586\tvalid's multi_logloss: 0.683063\n",
      "[353]\ttrain's multi_logloss: 0.480927\tvalid's multi_logloss: 0.682832\n",
      "[354]\ttrain's multi_logloss: 0.480089\tvalid's multi_logloss: 0.681495\n",
      "[355]\ttrain's multi_logloss: 0.479319\tvalid's multi_logloss: 0.680804\n",
      "[356]\ttrain's multi_logloss: 0.478146\tvalid's multi_logloss: 0.680911\n",
      "[357]\ttrain's multi_logloss: 0.476981\tvalid's multi_logloss: 0.681724\n",
      "[358]\ttrain's multi_logloss: 0.475729\tvalid's multi_logloss: 0.681525\n",
      "[359]\ttrain's multi_logloss: 0.474824\tvalid's multi_logloss: 0.681385\n",
      "[360]\ttrain's multi_logloss: 0.473747\tvalid's multi_logloss: 0.682866\n",
      "[361]\ttrain's multi_logloss: 0.472837\tvalid's multi_logloss: 0.683963\n",
      "[362]\ttrain's multi_logloss: 0.47182\tvalid's multi_logloss: 0.684601\n",
      "[363]\ttrain's multi_logloss: 0.470825\tvalid's multi_logloss: 0.685258\n",
      "[364]\ttrain's multi_logloss: 0.470145\tvalid's multi_logloss: 0.685457\n",
      "[365]\ttrain's multi_logloss: 0.469215\tvalid's multi_logloss: 0.685313\n",
      "[366]\ttrain's multi_logloss: 0.468201\tvalid's multi_logloss: 0.685099\n",
      "[367]\ttrain's multi_logloss: 0.46714\tvalid's multi_logloss: 0.684731\n",
      "[368]\ttrain's multi_logloss: 0.466038\tvalid's multi_logloss: 0.684763\n",
      "[369]\ttrain's multi_logloss: 0.465138\tvalid's multi_logloss: 0.684761\n",
      "[370]\ttrain's multi_logloss: 0.464215\tvalid's multi_logloss: 0.684923\n",
      "[371]\ttrain's multi_logloss: 0.463673\tvalid's multi_logloss: 0.684269\n",
      "[372]\ttrain's multi_logloss: 0.462656\tvalid's multi_logloss: 0.683713\n",
      "[373]\ttrain's multi_logloss: 0.46162\tvalid's multi_logloss: 0.682923\n",
      "[374]\ttrain's multi_logloss: 0.460624\tvalid's multi_logloss: 0.681311\n",
      "[375]\ttrain's multi_logloss: 0.460051\tvalid's multi_logloss: 0.681378\n",
      "[376]\ttrain's multi_logloss: 0.459493\tvalid's multi_logloss: 0.679662\n",
      "[377]\ttrain's multi_logloss: 0.458713\tvalid's multi_logloss: 0.677188\n",
      "[378]\ttrain's multi_logloss: 0.458116\tvalid's multi_logloss: 0.675782\n",
      "[379]\ttrain's multi_logloss: 0.457529\tvalid's multi_logloss: 0.674391\n",
      "[380]\ttrain's multi_logloss: 0.456591\tvalid's multi_logloss: 0.673407\n",
      "[381]\ttrain's multi_logloss: 0.455693\tvalid's multi_logloss: 0.673597\n",
      "[382]\ttrain's multi_logloss: 0.454968\tvalid's multi_logloss: 0.67438\n",
      "[383]\ttrain's multi_logloss: 0.454299\tvalid's multi_logloss: 0.674344\n",
      "[384]\ttrain's multi_logloss: 0.453189\tvalid's multi_logloss: 0.674827\n",
      "[385]\ttrain's multi_logloss: 0.452222\tvalid's multi_logloss: 0.674409\n",
      "[386]\ttrain's multi_logloss: 0.451645\tvalid's multi_logloss: 0.675057\n",
      "[387]\ttrain's multi_logloss: 0.451065\tvalid's multi_logloss: 0.675598\n",
      "[388]\ttrain's multi_logloss: 0.450386\tvalid's multi_logloss: 0.675425\n",
      "[389]\ttrain's multi_logloss: 0.44954\tvalid's multi_logloss: 0.675596\n",
      "[390]\ttrain's multi_logloss: 0.4488\tvalid's multi_logloss: 0.676226\n",
      "[391]\ttrain's multi_logloss: 0.447681\tvalid's multi_logloss: 0.675927\n",
      "[392]\ttrain's multi_logloss: 0.446529\tvalid's multi_logloss: 0.67511\n",
      "[393]\ttrain's multi_logloss: 0.445975\tvalid's multi_logloss: 0.674026\n",
      "[394]\ttrain's multi_logloss: 0.444985\tvalid's multi_logloss: 0.673739\n",
      "[395]\ttrain's multi_logloss: 0.444279\tvalid's multi_logloss: 0.673998\n",
      "[396]\ttrain's multi_logloss: 0.443679\tvalid's multi_logloss: 0.673273\n",
      "[397]\ttrain's multi_logloss: 0.443025\tvalid's multi_logloss: 0.67389\n",
      "[398]\ttrain's multi_logloss: 0.442155\tvalid's multi_logloss: 0.674482\n",
      "[399]\ttrain's multi_logloss: 0.441169\tvalid's multi_logloss: 0.67545\n",
      "[400]\ttrain's multi_logloss: 0.44046\tvalid's multi_logloss: 0.676357\n",
      "[401]\ttrain's multi_logloss: 0.439953\tvalid's multi_logloss: 0.677241\n",
      "[402]\ttrain's multi_logloss: 0.439465\tvalid's multi_logloss: 0.677676\n",
      "[403]\ttrain's multi_logloss: 0.439119\tvalid's multi_logloss: 0.678097\n",
      "[404]\ttrain's multi_logloss: 0.438772\tvalid's multi_logloss: 0.679507\n",
      "[405]\ttrain's multi_logloss: 0.438293\tvalid's multi_logloss: 0.680503\n",
      "[406]\ttrain's multi_logloss: 0.437313\tvalid's multi_logloss: 0.678107\n",
      "[407]\ttrain's multi_logloss: 0.436418\tvalid's multi_logloss: 0.676619\n",
      "[408]\ttrain's multi_logloss: 0.435659\tvalid's multi_logloss: 0.676112\n",
      "[409]\ttrain's multi_logloss: 0.434545\tvalid's multi_logloss: 0.674171\n",
      "[410]\ttrain's multi_logloss: 0.433592\tvalid's multi_logloss: 0.672583\n",
      "[411]\ttrain's multi_logloss: 0.432958\tvalid's multi_logloss: 0.672345\n",
      "[412]\ttrain's multi_logloss: 0.432206\tvalid's multi_logloss: 0.671191\n",
      "[413]\ttrain's multi_logloss: 0.43154\tvalid's multi_logloss: 0.670751\n",
      "[414]\ttrain's multi_logloss: 0.430798\tvalid's multi_logloss: 0.670893\n",
      "[415]\ttrain's multi_logloss: 0.430211\tvalid's multi_logloss: 0.670427\n",
      "[416]\ttrain's multi_logloss: 0.429172\tvalid's multi_logloss: 0.669905\n",
      "[417]\ttrain's multi_logloss: 0.428432\tvalid's multi_logloss: 0.667924\n",
      "[418]\ttrain's multi_logloss: 0.427413\tvalid's multi_logloss: 0.667376\n",
      "[419]\ttrain's multi_logloss: 0.426501\tvalid's multi_logloss: 0.66599\n",
      "[420]\ttrain's multi_logloss: 0.425667\tvalid's multi_logloss: 0.664943\n",
      "[421]\ttrain's multi_logloss: 0.424712\tvalid's multi_logloss: 0.664106\n",
      "[422]\ttrain's multi_logloss: 0.424033\tvalid's multi_logloss: 0.663621\n",
      "[423]\ttrain's multi_logloss: 0.423281\tvalid's multi_logloss: 0.661959\n",
      "[424]\ttrain's multi_logloss: 0.422466\tvalid's multi_logloss: 0.661189\n",
      "[425]\ttrain's multi_logloss: 0.421719\tvalid's multi_logloss: 0.662104\n",
      "[426]\ttrain's multi_logloss: 0.420918\tvalid's multi_logloss: 0.662312\n",
      "[427]\ttrain's multi_logloss: 0.420069\tvalid's multi_logloss: 0.661269\n",
      "[428]\ttrain's multi_logloss: 0.419324\tvalid's multi_logloss: 0.661108\n",
      "[429]\ttrain's multi_logloss: 0.418558\tvalid's multi_logloss: 0.66096\n",
      "[430]\ttrain's multi_logloss: 0.417674\tvalid's multi_logloss: 0.66063\n",
      "[431]\ttrain's multi_logloss: 0.4172\tvalid's multi_logloss: 0.661076\n",
      "[432]\ttrain's multi_logloss: 0.416694\tvalid's multi_logloss: 0.662254\n",
      "[433]\ttrain's multi_logloss: 0.416164\tvalid's multi_logloss: 0.661333\n",
      "[434]\ttrain's multi_logloss: 0.415617\tvalid's multi_logloss: 0.660677\n",
      "[435]\ttrain's multi_logloss: 0.415072\tvalid's multi_logloss: 0.660761\n",
      "[436]\ttrain's multi_logloss: 0.414328\tvalid's multi_logloss: 0.659703\n",
      "[437]\ttrain's multi_logloss: 0.413759\tvalid's multi_logloss: 0.659228\n",
      "[438]\ttrain's multi_logloss: 0.413132\tvalid's multi_logloss: 0.65841\n",
      "[439]\ttrain's multi_logloss: 0.412513\tvalid's multi_logloss: 0.657661\n",
      "[440]\ttrain's multi_logloss: 0.411977\tvalid's multi_logloss: 0.65722\n",
      "[441]\ttrain's multi_logloss: 0.411315\tvalid's multi_logloss: 0.658354\n",
      "[442]\ttrain's multi_logloss: 0.410661\tvalid's multi_logloss: 0.65766\n",
      "[443]\ttrain's multi_logloss: 0.410183\tvalid's multi_logloss: 0.657518\n",
      "[444]\ttrain's multi_logloss: 0.409657\tvalid's multi_logloss: 0.657765\n",
      "[445]\ttrain's multi_logloss: 0.409047\tvalid's multi_logloss: 0.655758\n",
      "[446]\ttrain's multi_logloss: 0.408235\tvalid's multi_logloss: 0.654374\n",
      "[447]\ttrain's multi_logloss: 0.407592\tvalid's multi_logloss: 0.654081\n",
      "[448]\ttrain's multi_logloss: 0.406859\tvalid's multi_logloss: 0.653617\n",
      "[449]\ttrain's multi_logloss: 0.406169\tvalid's multi_logloss: 0.653358\n",
      "[450]\ttrain's multi_logloss: 0.405568\tvalid's multi_logloss: 0.653715\n",
      "[451]\ttrain's multi_logloss: 0.404885\tvalid's multi_logloss: 0.651957\n",
      "[452]\ttrain's multi_logloss: 0.404223\tvalid's multi_logloss: 0.651456\n",
      "[453]\ttrain's multi_logloss: 0.403582\tvalid's multi_logloss: 0.649516\n",
      "[454]\ttrain's multi_logloss: 0.402972\tvalid's multi_logloss: 0.649404\n",
      "[455]\ttrain's multi_logloss: 0.402353\tvalid's multi_logloss: 0.648893\n",
      "[456]\ttrain's multi_logloss: 0.401735\tvalid's multi_logloss: 0.64769\n",
      "[457]\ttrain's multi_logloss: 0.400841\tvalid's multi_logloss: 0.646133\n",
      "[458]\ttrain's multi_logloss: 0.399874\tvalid's multi_logloss: 0.645192\n",
      "[459]\ttrain's multi_logloss: 0.399055\tvalid's multi_logloss: 0.644154\n",
      "[460]\ttrain's multi_logloss: 0.398324\tvalid's multi_logloss: 0.6426\n",
      "[461]\ttrain's multi_logloss: 0.398102\tvalid's multi_logloss: 0.642101\n",
      "[462]\ttrain's multi_logloss: 0.397945\tvalid's multi_logloss: 0.642906\n",
      "[463]\ttrain's multi_logloss: 0.397357\tvalid's multi_logloss: 0.643053\n",
      "[464]\ttrain's multi_logloss: 0.397151\tvalid's multi_logloss: 0.642239\n",
      "[465]\ttrain's multi_logloss: 0.397019\tvalid's multi_logloss: 0.643049\n",
      "[466]\ttrain's multi_logloss: 0.39613\tvalid's multi_logloss: 0.641947\n",
      "[467]\ttrain's multi_logloss: 0.395377\tvalid's multi_logloss: 0.641538\n",
      "[468]\ttrain's multi_logloss: 0.394469\tvalid's multi_logloss: 0.641237\n",
      "[469]\ttrain's multi_logloss: 0.393743\tvalid's multi_logloss: 0.64132\n",
      "[470]\ttrain's multi_logloss: 0.393088\tvalid's multi_logloss: 0.641737\n",
      "[471]\ttrain's multi_logloss: 0.392435\tvalid's multi_logloss: 0.641968\n",
      "[472]\ttrain's multi_logloss: 0.391674\tvalid's multi_logloss: 0.642174\n",
      "[473]\ttrain's multi_logloss: 0.391085\tvalid's multi_logloss: 0.642217\n",
      "[474]\ttrain's multi_logloss: 0.390431\tvalid's multi_logloss: 0.642771\n",
      "[475]\ttrain's multi_logloss: 0.389736\tvalid's multi_logloss: 0.642952\n",
      "[476]\ttrain's multi_logloss: 0.388862\tvalid's multi_logloss: 0.641393\n",
      "[477]\ttrain's multi_logloss: 0.387908\tvalid's multi_logloss: 0.640961\n",
      "[478]\ttrain's multi_logloss: 0.387117\tvalid's multi_logloss: 0.639984\n",
      "[479]\ttrain's multi_logloss: 0.386221\tvalid's multi_logloss: 0.638933\n",
      "[480]\ttrain's multi_logloss: 0.385176\tvalid's multi_logloss: 0.638202\n",
      "[481]\ttrain's multi_logloss: 0.384702\tvalid's multi_logloss: 0.638471\n",
      "[482]\ttrain's multi_logloss: 0.384237\tvalid's multi_logloss: 0.638742\n",
      "[483]\ttrain's multi_logloss: 0.383575\tvalid's multi_logloss: 0.638609\n",
      "[484]\ttrain's multi_logloss: 0.38319\tvalid's multi_logloss: 0.638813\n",
      "[485]\ttrain's multi_logloss: 0.382813\tvalid's multi_logloss: 0.639167\n",
      "[486]\ttrain's multi_logloss: 0.382225\tvalid's multi_logloss: 0.640181\n",
      "[487]\ttrain's multi_logloss: 0.381685\tvalid's multi_logloss: 0.64005\n",
      "[488]\ttrain's multi_logloss: 0.381207\tvalid's multi_logloss: 0.640829\n",
      "[489]\ttrain's multi_logloss: 0.380758\tvalid's multi_logloss: 0.641455\n",
      "[490]\ttrain's multi_logloss: 0.380224\tvalid's multi_logloss: 0.642209\n",
      "[491]\ttrain's multi_logloss: 0.379696\tvalid's multi_logloss: 0.641373\n",
      "[492]\ttrain's multi_logloss: 0.379143\tvalid's multi_logloss: 0.640443\n",
      "[493]\ttrain's multi_logloss: 0.378454\tvalid's multi_logloss: 0.64036\n",
      "[494]\ttrain's multi_logloss: 0.377777\tvalid's multi_logloss: 0.640282\n",
      "[495]\ttrain's multi_logloss: 0.377291\tvalid's multi_logloss: 0.640208\n",
      "[496]\ttrain's multi_logloss: 0.376833\tvalid's multi_logloss: 0.639176\n",
      "[497]\ttrain's multi_logloss: 0.376163\tvalid's multi_logloss: 0.6392\n",
      "[498]\ttrain's multi_logloss: 0.375604\tvalid's multi_logloss: 0.638964\n",
      "[499]\ttrain's multi_logloss: 0.375094\tvalid's multi_logloss: 0.638303\n",
      "[500]\ttrain's multi_logloss: 0.374771\tvalid's multi_logloss: 0.637981\n",
      "[501]\ttrain's multi_logloss: 0.374388\tvalid's multi_logloss: 0.637997\n",
      "[502]\ttrain's multi_logloss: 0.373793\tvalid's multi_logloss: 0.638367\n",
      "[503]\ttrain's multi_logloss: 0.373239\tvalid's multi_logloss: 0.636936\n",
      "[504]\ttrain's multi_logloss: 0.372872\tvalid's multi_logloss: 0.636967\n",
      "[505]\ttrain's multi_logloss: 0.372181\tvalid's multi_logloss: 0.637025\n",
      "[506]\ttrain's multi_logloss: 0.371599\tvalid's multi_logloss: 0.636719\n",
      "[507]\ttrain's multi_logloss: 0.370939\tvalid's multi_logloss: 0.636551\n",
      "[508]\ttrain's multi_logloss: 0.370384\tvalid's multi_logloss: 0.636377\n",
      "[509]\ttrain's multi_logloss: 0.369799\tvalid's multi_logloss: 0.636385\n",
      "[510]\ttrain's multi_logloss: 0.369092\tvalid's multi_logloss: 0.635543\n",
      "[511]\ttrain's multi_logloss: 0.368544\tvalid's multi_logloss: 0.635174\n",
      "[512]\ttrain's multi_logloss: 0.368135\tvalid's multi_logloss: 0.633974\n",
      "[513]\ttrain's multi_logloss: 0.367695\tvalid's multi_logloss: 0.632859\n",
      "[514]\ttrain's multi_logloss: 0.366944\tvalid's multi_logloss: 0.631968\n",
      "[515]\ttrain's multi_logloss: 0.366172\tvalid's multi_logloss: 0.630168\n",
      "[516]\ttrain's multi_logloss: 0.365692\tvalid's multi_logloss: 0.629362\n",
      "[517]\ttrain's multi_logloss: 0.365123\tvalid's multi_logloss: 0.629545\n",
      "[518]\ttrain's multi_logloss: 0.364674\tvalid's multi_logloss: 0.627121\n",
      "[519]\ttrain's multi_logloss: 0.363952\tvalid's multi_logloss: 0.625744\n",
      "[520]\ttrain's multi_logloss: 0.363324\tvalid's multi_logloss: 0.623964\n",
      "[521]\ttrain's multi_logloss: 0.362676\tvalid's multi_logloss: 0.623036\n",
      "[522]\ttrain's multi_logloss: 0.362055\tvalid's multi_logloss: 0.622328\n",
      "[523]\ttrain's multi_logloss: 0.361205\tvalid's multi_logloss: 0.621669\n",
      "[524]\ttrain's multi_logloss: 0.360508\tvalid's multi_logloss: 0.620758\n",
      "[525]\ttrain's multi_logloss: 0.359757\tvalid's multi_logloss: 0.620228\n",
      "[526]\ttrain's multi_logloss: 0.359273\tvalid's multi_logloss: 0.620275\n",
      "[527]\ttrain's multi_logloss: 0.358759\tvalid's multi_logloss: 0.618998\n",
      "[528]\ttrain's multi_logloss: 0.358171\tvalid's multi_logloss: 0.619042\n",
      "[529]\ttrain's multi_logloss: 0.357633\tvalid's multi_logloss: 0.618946\n",
      "[530]\ttrain's multi_logloss: 0.357079\tvalid's multi_logloss: 0.617713\n",
      "[531]\ttrain's multi_logloss: 0.356527\tvalid's multi_logloss: 0.61536\n",
      "[532]\ttrain's multi_logloss: 0.356043\tvalid's multi_logloss: 0.613928\n",
      "[533]\ttrain's multi_logloss: 0.355457\tvalid's multi_logloss: 0.611306\n",
      "[534]\ttrain's multi_logloss: 0.354935\tvalid's multi_logloss: 0.610318\n",
      "[535]\ttrain's multi_logloss: 0.354355\tvalid's multi_logloss: 0.607668\n",
      "[536]\ttrain's multi_logloss: 0.353981\tvalid's multi_logloss: 0.606591\n",
      "[537]\ttrain's multi_logloss: 0.353531\tvalid's multi_logloss: 0.606244\n",
      "[538]\ttrain's multi_logloss: 0.35332\tvalid's multi_logloss: 0.607386\n",
      "[539]\ttrain's multi_logloss: 0.352986\tvalid's multi_logloss: 0.608275\n",
      "[540]\ttrain's multi_logloss: 0.352712\tvalid's multi_logloss: 0.608274\n",
      "[541]\ttrain's multi_logloss: 0.352218\tvalid's multi_logloss: 0.606995\n",
      "[542]\ttrain's multi_logloss: 0.351745\tvalid's multi_logloss: 0.605846\n",
      "[543]\ttrain's multi_logloss: 0.351222\tvalid's multi_logloss: 0.604751\n",
      "[544]\ttrain's multi_logloss: 0.350654\tvalid's multi_logloss: 0.604629\n",
      "[545]\ttrain's multi_logloss: 0.350196\tvalid's multi_logloss: 0.603378\n",
      "[546]\ttrain's multi_logloss: 0.349433\tvalid's multi_logloss: 0.60264\n",
      "[547]\ttrain's multi_logloss: 0.348554\tvalid's multi_logloss: 0.601335\n",
      "[548]\ttrain's multi_logloss: 0.347739\tvalid's multi_logloss: 0.600308\n",
      "[549]\ttrain's multi_logloss: 0.34695\tvalid's multi_logloss: 0.598426\n",
      "[550]\ttrain's multi_logloss: 0.346298\tvalid's multi_logloss: 0.597307\n",
      "[551]\ttrain's multi_logloss: 0.3456\tvalid's multi_logloss: 0.596403\n",
      "[552]\ttrain's multi_logloss: 0.345028\tvalid's multi_logloss: 0.596585\n",
      "[553]\ttrain's multi_logloss: 0.344253\tvalid's multi_logloss: 0.595636\n",
      "[554]\ttrain's multi_logloss: 0.343359\tvalid's multi_logloss: 0.595944\n",
      "[555]\ttrain's multi_logloss: 0.342677\tvalid's multi_logloss: 0.59479\n",
      "[556]\ttrain's multi_logloss: 0.342349\tvalid's multi_logloss: 0.595271\n",
      "[557]\ttrain's multi_logloss: 0.341767\tvalid's multi_logloss: 0.595081\n",
      "[558]\ttrain's multi_logloss: 0.34122\tvalid's multi_logloss: 0.59431\n",
      "[559]\ttrain's multi_logloss: 0.340783\tvalid's multi_logloss: 0.593516\n",
      "[560]\ttrain's multi_logloss: 0.340277\tvalid's multi_logloss: 0.593352\n",
      "[561]\ttrain's multi_logloss: 0.339578\tvalid's multi_logloss: 0.592717\n",
      "[562]\ttrain's multi_logloss: 0.338922\tvalid's multi_logloss: 0.593825\n",
      "[563]\ttrain's multi_logloss: 0.33829\tvalid's multi_logloss: 0.593784\n",
      "[564]\ttrain's multi_logloss: 0.337685\tvalid's multi_logloss: 0.59359\n",
      "[565]\ttrain's multi_logloss: 0.337305\tvalid's multi_logloss: 0.5923\n",
      "[566]\ttrain's multi_logloss: 0.33677\tvalid's multi_logloss: 0.592656\n",
      "[567]\ttrain's multi_logloss: 0.336058\tvalid's multi_logloss: 0.590228\n",
      "[568]\ttrain's multi_logloss: 0.335514\tvalid's multi_logloss: 0.591258\n",
      "[569]\ttrain's multi_logloss: 0.335018\tvalid's multi_logloss: 0.592203\n",
      "[570]\ttrain's multi_logloss: 0.334487\tvalid's multi_logloss: 0.591749\n",
      "[571]\ttrain's multi_logloss: 0.334218\tvalid's multi_logloss: 0.591332\n",
      "[572]\ttrain's multi_logloss: 0.333607\tvalid's multi_logloss: 0.592289\n",
      "[573]\ttrain's multi_logloss: 0.333196\tvalid's multi_logloss: 0.592491\n",
      "[574]\ttrain's multi_logloss: 0.332593\tvalid's multi_logloss: 0.592666\n",
      "[575]\ttrain's multi_logloss: 0.332156\tvalid's multi_logloss: 0.591393\n",
      "[576]\ttrain's multi_logloss: 0.331606\tvalid's multi_logloss: 0.590515\n",
      "[577]\ttrain's multi_logloss: 0.331098\tvalid's multi_logloss: 0.589725\n",
      "[578]\ttrain's multi_logloss: 0.330663\tvalid's multi_logloss: 0.588979\n",
      "[579]\ttrain's multi_logloss: 0.330353\tvalid's multi_logloss: 0.587838\n",
      "[580]\ttrain's multi_logloss: 0.329811\tvalid's multi_logloss: 0.586811\n",
      "[581]\ttrain's multi_logloss: 0.329197\tvalid's multi_logloss: 0.586163\n",
      "[582]\ttrain's multi_logloss: 0.328861\tvalid's multi_logloss: 0.585958\n",
      "[583]\ttrain's multi_logloss: 0.328227\tvalid's multi_logloss: 0.586191\n",
      "[584]\ttrain's multi_logloss: 0.327637\tvalid's multi_logloss: 0.585829\n",
      "[585]\ttrain's multi_logloss: 0.327142\tvalid's multi_logloss: 0.585874\n",
      "[586]\ttrain's multi_logloss: 0.326564\tvalid's multi_logloss: 0.585281\n",
      "[587]\ttrain's multi_logloss: 0.325946\tvalid's multi_logloss: 0.58532\n",
      "[588]\ttrain's multi_logloss: 0.325365\tvalid's multi_logloss: 0.584585\n",
      "[589]\ttrain's multi_logloss: 0.324703\tvalid's multi_logloss: 0.582796\n",
      "[590]\ttrain's multi_logloss: 0.32394\tvalid's multi_logloss: 0.581619\n",
      "[591]\ttrain's multi_logloss: 0.323575\tvalid's multi_logloss: 0.582698\n",
      "[592]\ttrain's multi_logloss: 0.323164\tvalid's multi_logloss: 0.582968\n",
      "[593]\ttrain's multi_logloss: 0.322732\tvalid's multi_logloss: 0.583253\n",
      "[594]\ttrain's multi_logloss: 0.322328\tvalid's multi_logloss: 0.583724\n",
      "[595]\ttrain's multi_logloss: 0.321854\tvalid's multi_logloss: 0.582834\n",
      "[596]\ttrain's multi_logloss: 0.321012\tvalid's multi_logloss: 0.582492\n",
      "[597]\ttrain's multi_logloss: 0.320599\tvalid's multi_logloss: 0.582592\n",
      "[598]\ttrain's multi_logloss: 0.320149\tvalid's multi_logloss: 0.583634\n",
      "[599]\ttrain's multi_logloss: 0.319667\tvalid's multi_logloss: 0.584078\n",
      "[600]\ttrain's multi_logloss: 0.319267\tvalid's multi_logloss: 0.584386\n",
      "[601]\ttrain's multi_logloss: 0.318765\tvalid's multi_logloss: 0.583904\n",
      "[602]\ttrain's multi_logloss: 0.318314\tvalid's multi_logloss: 0.583458\n",
      "[603]\ttrain's multi_logloss: 0.317719\tvalid's multi_logloss: 0.582292\n",
      "[604]\ttrain's multi_logloss: 0.317192\tvalid's multi_logloss: 0.58269\n",
      "[605]\ttrain's multi_logloss: 0.316672\tvalid's multi_logloss: 0.583159\n",
      "[606]\ttrain's multi_logloss: 0.31628\tvalid's multi_logloss: 0.583592\n",
      "[607]\ttrain's multi_logloss: 0.315796\tvalid's multi_logloss: 0.583632\n",
      "[608]\ttrain's multi_logloss: 0.315414\tvalid's multi_logloss: 0.584578\n",
      "[609]\ttrain's multi_logloss: 0.315005\tvalid's multi_logloss: 0.584962\n",
      "[610]\ttrain's multi_logloss: 0.314425\tvalid's multi_logloss: 0.584103\n",
      "[611]\ttrain's multi_logloss: 0.314062\tvalid's multi_logloss: 0.583229\n",
      "[612]\ttrain's multi_logloss: 0.313653\tvalid's multi_logloss: 0.58353\n",
      "[613]\ttrain's multi_logloss: 0.313196\tvalid's multi_logloss: 0.583556\n",
      "[614]\ttrain's multi_logloss: 0.312748\tvalid's multi_logloss: 0.583588\n",
      "[615]\ttrain's multi_logloss: 0.312364\tvalid's multi_logloss: 0.584302\n",
      "[616]\ttrain's multi_logloss: 0.31189\tvalid's multi_logloss: 0.583863\n",
      "[617]\ttrain's multi_logloss: 0.311374\tvalid's multi_logloss: 0.583275\n",
      "[618]\ttrain's multi_logloss: 0.310953\tvalid's multi_logloss: 0.58316\n",
      "[619]\ttrain's multi_logloss: 0.310557\tvalid's multi_logloss: 0.583238\n",
      "[620]\ttrain's multi_logloss: 0.31008\tvalid's multi_logloss: 0.582977\n",
      "[621]\ttrain's multi_logloss: 0.309485\tvalid's multi_logloss: 0.582031\n",
      "[622]\ttrain's multi_logloss: 0.30901\tvalid's multi_logloss: 0.581977\n",
      "[623]\ttrain's multi_logloss: 0.308615\tvalid's multi_logloss: 0.582522\n",
      "[624]\ttrain's multi_logloss: 0.308092\tvalid's multi_logloss: 0.581781\n",
      "[625]\ttrain's multi_logloss: 0.307563\tvalid's multi_logloss: 0.581189\n",
      "[626]\ttrain's multi_logloss: 0.307261\tvalid's multi_logloss: 0.580722\n",
      "[627]\ttrain's multi_logloss: 0.306839\tvalid's multi_logloss: 0.580065\n",
      "[628]\ttrain's multi_logloss: 0.306521\tvalid's multi_logloss: 0.579865\n",
      "[629]\ttrain's multi_logloss: 0.306203\tvalid's multi_logloss: 0.579956\n",
      "[630]\ttrain's multi_logloss: 0.305896\tvalid's multi_logloss: 0.580061\n",
      "[631]\ttrain's multi_logloss: 0.305403\tvalid's multi_logloss: 0.580044\n",
      "[632]\ttrain's multi_logloss: 0.305037\tvalid's multi_logloss: 0.579841\n",
      "[633]\ttrain's multi_logloss: 0.304589\tvalid's multi_logloss: 0.579983\n",
      "[634]\ttrain's multi_logloss: 0.304169\tvalid's multi_logloss: 0.579975\n",
      "[635]\ttrain's multi_logloss: 0.303703\tvalid's multi_logloss: 0.580545\n",
      "[636]\ttrain's multi_logloss: 0.303129\tvalid's multi_logloss: 0.579554\n",
      "[637]\ttrain's multi_logloss: 0.302614\tvalid's multi_logloss: 0.578648\n",
      "[638]\ttrain's multi_logloss: 0.302116\tvalid's multi_logloss: 0.577185\n",
      "[639]\ttrain's multi_logloss: 0.301645\tvalid's multi_logloss: 0.576702\n",
      "[640]\ttrain's multi_logloss: 0.301076\tvalid's multi_logloss: 0.575306\n",
      "[641]\ttrain's multi_logloss: 0.300643\tvalid's multi_logloss: 0.575479\n",
      "[642]\ttrain's multi_logloss: 0.300288\tvalid's multi_logloss: 0.575492\n",
      "[643]\ttrain's multi_logloss: 0.299991\tvalid's multi_logloss: 0.576038\n",
      "[644]\ttrain's multi_logloss: 0.299596\tvalid's multi_logloss: 0.576259\n",
      "[645]\ttrain's multi_logloss: 0.29907\tvalid's multi_logloss: 0.577079\n",
      "[646]\ttrain's multi_logloss: 0.298751\tvalid's multi_logloss: 0.576442\n",
      "[647]\ttrain's multi_logloss: 0.298353\tvalid's multi_logloss: 0.577286\n",
      "[648]\ttrain's multi_logloss: 0.297977\tvalid's multi_logloss: 0.577353\n",
      "[649]\ttrain's multi_logloss: 0.297594\tvalid's multi_logloss: 0.578202\n",
      "[650]\ttrain's multi_logloss: 0.297183\tvalid's multi_logloss: 0.579012\n",
      "[651]\ttrain's multi_logloss: 0.296715\tvalid's multi_logloss: 0.578329\n",
      "[652]\ttrain's multi_logloss: 0.296403\tvalid's multi_logloss: 0.578713\n",
      "[653]\ttrain's multi_logloss: 0.295953\tvalid's multi_logloss: 0.57805\n",
      "[654]\ttrain's multi_logloss: 0.295468\tvalid's multi_logloss: 0.577938\n",
      "[655]\ttrain's multi_logloss: 0.295178\tvalid's multi_logloss: 0.578334\n",
      "[656]\ttrain's multi_logloss: 0.294519\tvalid's multi_logloss: 0.577851\n",
      "[657]\ttrain's multi_logloss: 0.293885\tvalid's multi_logloss: 0.578265\n",
      "[658]\ttrain's multi_logloss: 0.293339\tvalid's multi_logloss: 0.578069\n",
      "[659]\ttrain's multi_logloss: 0.292818\tvalid's multi_logloss: 0.578286\n",
      "[660]\ttrain's multi_logloss: 0.292225\tvalid's multi_logloss: 0.578063\n",
      "[661]\ttrain's multi_logloss: 0.291711\tvalid's multi_logloss: 0.577471\n",
      "[662]\ttrain's multi_logloss: 0.291248\tvalid's multi_logloss: 0.578168\n",
      "[663]\ttrain's multi_logloss: 0.290676\tvalid's multi_logloss: 0.578617\n",
      "[664]\ttrain's multi_logloss: 0.290061\tvalid's multi_logloss: 0.579081\n",
      "[665]\ttrain's multi_logloss: 0.289474\tvalid's multi_logloss: 0.578757\n",
      "[666]\ttrain's multi_logloss: 0.28882\tvalid's multi_logloss: 0.577464\n",
      "[667]\ttrain's multi_logloss: 0.28839\tvalid's multi_logloss: 0.576357\n",
      "[668]\ttrain's multi_logloss: 0.287912\tvalid's multi_logloss: 0.574629\n",
      "[669]\ttrain's multi_logloss: 0.287543\tvalid's multi_logloss: 0.573994\n",
      "[670]\ttrain's multi_logloss: 0.287198\tvalid's multi_logloss: 0.572146\n",
      "[671]\ttrain's multi_logloss: 0.286674\tvalid's multi_logloss: 0.572561\n",
      "[672]\ttrain's multi_logloss: 0.286314\tvalid's multi_logloss: 0.572704\n",
      "[673]\ttrain's multi_logloss: 0.285649\tvalid's multi_logloss: 0.571619\n",
      "[674]\ttrain's multi_logloss: 0.285325\tvalid's multi_logloss: 0.572708\n",
      "[675]\ttrain's multi_logloss: 0.284863\tvalid's multi_logloss: 0.572808\n",
      "[676]\ttrain's multi_logloss: 0.28428\tvalid's multi_logloss: 0.572274\n",
      "[677]\ttrain's multi_logloss: 0.283691\tvalid's multi_logloss: 0.57185\n",
      "[678]\ttrain's multi_logloss: 0.28312\tvalid's multi_logloss: 0.570677\n",
      "[679]\ttrain's multi_logloss: 0.28243\tvalid's multi_logloss: 0.57047\n",
      "[680]\ttrain's multi_logloss: 0.281809\tvalid's multi_logloss: 0.569625\n",
      "[681]\ttrain's multi_logloss: 0.281437\tvalid's multi_logloss: 0.57056\n",
      "[682]\ttrain's multi_logloss: 0.280937\tvalid's multi_logloss: 0.569587\n",
      "[683]\ttrain's multi_logloss: 0.280385\tvalid's multi_logloss: 0.569402\n",
      "[684]\ttrain's multi_logloss: 0.279912\tvalid's multi_logloss: 0.56879\n",
      "[685]\ttrain's multi_logloss: 0.279512\tvalid's multi_logloss: 0.56787\n",
      "[686]\ttrain's multi_logloss: 0.279122\tvalid's multi_logloss: 0.568236\n",
      "[687]\ttrain's multi_logloss: 0.278629\tvalid's multi_logloss: 0.568618\n",
      "[688]\ttrain's multi_logloss: 0.278177\tvalid's multi_logloss: 0.568701\n",
      "[689]\ttrain's multi_logloss: 0.277735\tvalid's multi_logloss: 0.569073\n",
      "[690]\ttrain's multi_logloss: 0.277288\tvalid's multi_logloss: 0.569379\n",
      "[691]\ttrain's multi_logloss: 0.276774\tvalid's multi_logloss: 0.569521\n",
      "[692]\ttrain's multi_logloss: 0.276429\tvalid's multi_logloss: 0.569274\n",
      "[693]\ttrain's multi_logloss: 0.275995\tvalid's multi_logloss: 0.569503\n",
      "[694]\ttrain's multi_logloss: 0.275663\tvalid's multi_logloss: 0.56988\n",
      "[695]\ttrain's multi_logloss: 0.275266\tvalid's multi_logloss: 0.570067\n",
      "[696]\ttrain's multi_logloss: 0.274916\tvalid's multi_logloss: 0.568553\n",
      "[697]\ttrain's multi_logloss: 0.274443\tvalid's multi_logloss: 0.567683\n",
      "[698]\ttrain's multi_logloss: 0.274172\tvalid's multi_logloss: 0.566058\n",
      "[699]\ttrain's multi_logloss: 0.273637\tvalid's multi_logloss: 0.564837\n",
      "[700]\ttrain's multi_logloss: 0.273119\tvalid's multi_logloss: 0.563644\n",
      "[701]\ttrain's multi_logloss: 0.272754\tvalid's multi_logloss: 0.5632\n",
      "[702]\ttrain's multi_logloss: 0.272407\tvalid's multi_logloss: 0.564076\n",
      "[703]\ttrain's multi_logloss: 0.27202\tvalid's multi_logloss: 0.563619\n",
      "[704]\ttrain's multi_logloss: 0.271736\tvalid's multi_logloss: 0.564618\n",
      "[705]\ttrain's multi_logloss: 0.271293\tvalid's multi_logloss: 0.564604\n",
      "[706]\ttrain's multi_logloss: 0.271077\tvalid's multi_logloss: 0.565096\n",
      "[707]\ttrain's multi_logloss: 0.270703\tvalid's multi_logloss: 0.564603\n",
      "[708]\ttrain's multi_logloss: 0.270463\tvalid's multi_logloss: 0.565587\n",
      "[709]\ttrain's multi_logloss: 0.270186\tvalid's multi_logloss: 0.565689\n",
      "[710]\ttrain's multi_logloss: 0.269887\tvalid's multi_logloss: 0.566582\n",
      "[711]\ttrain's multi_logloss: 0.269395\tvalid's multi_logloss: 0.565076\n",
      "[712]\ttrain's multi_logloss: 0.268928\tvalid's multi_logloss: 0.563808\n",
      "[713]\ttrain's multi_logloss: 0.268536\tvalid's multi_logloss: 0.563245\n",
      "[714]\ttrain's multi_logloss: 0.268058\tvalid's multi_logloss: 0.562445\n",
      "[715]\ttrain's multi_logloss: 0.267703\tvalid's multi_logloss: 0.56163\n",
      "[716]\ttrain's multi_logloss: 0.267113\tvalid's multi_logloss: 0.561\n",
      "[717]\ttrain's multi_logloss: 0.266562\tvalid's multi_logloss: 0.561352\n",
      "[718]\ttrain's multi_logloss: 0.26601\tvalid's multi_logloss: 0.560597\n",
      "[719]\ttrain's multi_logloss: 0.265591\tvalid's multi_logloss: 0.5602\n",
      "[720]\ttrain's multi_logloss: 0.265181\tvalid's multi_logloss: 0.559442\n",
      "[721]\ttrain's multi_logloss: 0.264949\tvalid's multi_logloss: 0.558535\n",
      "[722]\ttrain's multi_logloss: 0.264772\tvalid's multi_logloss: 0.558376\n",
      "[723]\ttrain's multi_logloss: 0.264575\tvalid's multi_logloss: 0.558377\n",
      "[724]\ttrain's multi_logloss: 0.264357\tvalid's multi_logloss: 0.557493\n",
      "[725]\ttrain's multi_logloss: 0.264102\tvalid's multi_logloss: 0.556616\n",
      "[726]\ttrain's multi_logloss: 0.263595\tvalid's multi_logloss: 0.554964\n",
      "[727]\ttrain's multi_logloss: 0.263253\tvalid's multi_logloss: 0.554297\n",
      "[728]\ttrain's multi_logloss: 0.262745\tvalid's multi_logloss: 0.554278\n",
      "[729]\ttrain's multi_logloss: 0.262373\tvalid's multi_logloss: 0.553712\n",
      "[730]\ttrain's multi_logloss: 0.261983\tvalid's multi_logloss: 0.552459\n",
      "[731]\ttrain's multi_logloss: 0.261461\tvalid's multi_logloss: 0.550832\n",
      "[732]\ttrain's multi_logloss: 0.260884\tvalid's multi_logloss: 0.551958\n",
      "[733]\ttrain's multi_logloss: 0.260444\tvalid's multi_logloss: 0.552763\n",
      "[734]\ttrain's multi_logloss: 0.259995\tvalid's multi_logloss: 0.552684\n",
      "[735]\ttrain's multi_logloss: 0.259793\tvalid's multi_logloss: 0.552531\n",
      "[736]\ttrain's multi_logloss: 0.259387\tvalid's multi_logloss: 0.553785\n",
      "[737]\ttrain's multi_logloss: 0.25901\tvalid's multi_logloss: 0.553272\n",
      "[738]\ttrain's multi_logloss: 0.258743\tvalid's multi_logloss: 0.554153\n",
      "[739]\ttrain's multi_logloss: 0.258337\tvalid's multi_logloss: 0.555636\n",
      "[740]\ttrain's multi_logloss: 0.257873\tvalid's multi_logloss: 0.556706\n",
      "[741]\ttrain's multi_logloss: 0.257434\tvalid's multi_logloss: 0.55649\n",
      "[742]\ttrain's multi_logloss: 0.256861\tvalid's multi_logloss: 0.557167\n",
      "[743]\ttrain's multi_logloss: 0.256482\tvalid's multi_logloss: 0.556564\n",
      "[744]\ttrain's multi_logloss: 0.256164\tvalid's multi_logloss: 0.555467\n",
      "[745]\ttrain's multi_logloss: 0.255711\tvalid's multi_logloss: 0.554727\n",
      "[746]\ttrain's multi_logloss: 0.255139\tvalid's multi_logloss: 0.553005\n",
      "[747]\ttrain's multi_logloss: 0.254718\tvalid's multi_logloss: 0.55355\n",
      "[748]\ttrain's multi_logloss: 0.254164\tvalid's multi_logloss: 0.55185\n",
      "[749]\ttrain's multi_logloss: 0.253635\tvalid's multi_logloss: 0.551337\n",
      "[750]\ttrain's multi_logloss: 0.253207\tvalid's multi_logloss: 0.551127\n",
      "[751]\ttrain's multi_logloss: 0.252816\tvalid's multi_logloss: 0.552281\n",
      "[752]\ttrain's multi_logloss: 0.252504\tvalid's multi_logloss: 0.553134\n",
      "[753]\ttrain's multi_logloss: 0.252278\tvalid's multi_logloss: 0.553616\n",
      "[754]\ttrain's multi_logloss: 0.251967\tvalid's multi_logloss: 0.55356\n",
      "[755]\ttrain's multi_logloss: 0.2516\tvalid's multi_logloss: 0.554736\n",
      "[756]\ttrain's multi_logloss: 0.251066\tvalid's multi_logloss: 0.552856\n",
      "[757]\ttrain's multi_logloss: 0.250452\tvalid's multi_logloss: 0.550863\n",
      "[758]\ttrain's multi_logloss: 0.250036\tvalid's multi_logloss: 0.549028\n",
      "[759]\ttrain's multi_logloss: 0.249523\tvalid's multi_logloss: 0.547827\n",
      "[760]\ttrain's multi_logloss: 0.249309\tvalid's multi_logloss: 0.546736\n",
      "[761]\ttrain's multi_logloss: 0.248775\tvalid's multi_logloss: 0.546683\n",
      "[762]\ttrain's multi_logloss: 0.248463\tvalid's multi_logloss: 0.546928\n",
      "[763]\ttrain's multi_logloss: 0.248066\tvalid's multi_logloss: 0.547677\n",
      "[764]\ttrain's multi_logloss: 0.247688\tvalid's multi_logloss: 0.548875\n",
      "[765]\ttrain's multi_logloss: 0.247152\tvalid's multi_logloss: 0.548788\n",
      "[766]\ttrain's multi_logloss: 0.246722\tvalid's multi_logloss: 0.54974\n",
      "[767]\ttrain's multi_logloss: 0.246395\tvalid's multi_logloss: 0.551057\n",
      "[768]\ttrain's multi_logloss: 0.246014\tvalid's multi_logloss: 0.551115\n",
      "[769]\ttrain's multi_logloss: 0.245645\tvalid's multi_logloss: 0.550468\n",
      "[770]\ttrain's multi_logloss: 0.24537\tvalid's multi_logloss: 0.552321\n",
      "[771]\ttrain's multi_logloss: 0.24503\tvalid's multi_logloss: 0.55228\n",
      "[772]\ttrain's multi_logloss: 0.244639\tvalid's multi_logloss: 0.551609\n",
      "[773]\ttrain's multi_logloss: 0.244438\tvalid's multi_logloss: 0.550599\n",
      "[774]\ttrain's multi_logloss: 0.244112\tvalid's multi_logloss: 0.549662\n",
      "[775]\ttrain's multi_logloss: 0.243852\tvalid's multi_logloss: 0.550165\n",
      "[776]\ttrain's multi_logloss: 0.243454\tvalid's multi_logloss: 0.550672\n",
      "[777]\ttrain's multi_logloss: 0.243112\tvalid's multi_logloss: 0.550865\n",
      "[778]\ttrain's multi_logloss: 0.242795\tvalid's multi_logloss: 0.551255\n",
      "[779]\ttrain's multi_logloss: 0.242436\tvalid's multi_logloss: 0.550622\n",
      "[780]\ttrain's multi_logloss: 0.241982\tvalid's multi_logloss: 0.551579\n",
      "[781]\ttrain's multi_logloss: 0.241462\tvalid's multi_logloss: 0.551769\n",
      "[782]\ttrain's multi_logloss: 0.241116\tvalid's multi_logloss: 0.550458\n",
      "[783]\ttrain's multi_logloss: 0.240649\tvalid's multi_logloss: 0.550837\n",
      "[784]\ttrain's multi_logloss: 0.240346\tvalid's multi_logloss: 0.551433\n",
      "[785]\ttrain's multi_logloss: 0.240033\tvalid's multi_logloss: 0.550707\n",
      "[786]\ttrain's multi_logloss: 0.239716\tvalid's multi_logloss: 0.550491\n",
      "[787]\ttrain's multi_logloss: 0.239277\tvalid's multi_logloss: 0.55018\n",
      "[788]\ttrain's multi_logloss: 0.238815\tvalid's multi_logloss: 0.548771\n",
      "[789]\ttrain's multi_logloss: 0.238399\tvalid's multi_logloss: 0.547644\n",
      "[790]\ttrain's multi_logloss: 0.238046\tvalid's multi_logloss: 0.546733\n",
      "[791]\ttrain's multi_logloss: 0.237651\tvalid's multi_logloss: 0.547202\n",
      "[792]\ttrain's multi_logloss: 0.23716\tvalid's multi_logloss: 0.548169\n",
      "[793]\ttrain's multi_logloss: 0.236821\tvalid's multi_logloss: 0.547472\n",
      "[794]\ttrain's multi_logloss: 0.236462\tvalid's multi_logloss: 0.547754\n",
      "[795]\ttrain's multi_logloss: 0.23613\tvalid's multi_logloss: 0.547554\n",
      "[796]\ttrain's multi_logloss: 0.235847\tvalid's multi_logloss: 0.548097\n",
      "[797]\ttrain's multi_logloss: 0.235614\tvalid's multi_logloss: 0.548051\n",
      "[798]\ttrain's multi_logloss: 0.235377\tvalid's multi_logloss: 0.548449\n",
      "[799]\ttrain's multi_logloss: 0.235139\tvalid's multi_logloss: 0.548352\n",
      "[800]\ttrain's multi_logloss: 0.234845\tvalid's multi_logloss: 0.549147\n",
      "[801]\ttrain's multi_logloss: 0.234624\tvalid's multi_logloss: 0.549575\n",
      "[802]\ttrain's multi_logloss: 0.234245\tvalid's multi_logloss: 0.548928\n",
      "[803]\ttrain's multi_logloss: 0.234174\tvalid's multi_logloss: 0.548997\n",
      "[804]\ttrain's multi_logloss: 0.233912\tvalid's multi_logloss: 0.54885\n",
      "[805]\ttrain's multi_logloss: 0.233612\tvalid's multi_logloss: 0.549151\n",
      "[806]\ttrain's multi_logloss: 0.233258\tvalid's multi_logloss: 0.548335\n",
      "[807]\ttrain's multi_logloss: 0.233013\tvalid's multi_logloss: 0.54817\n",
      "[808]\ttrain's multi_logloss: 0.232708\tvalid's multi_logloss: 0.548086\n",
      "[809]\ttrain's multi_logloss: 0.232394\tvalid's multi_logloss: 0.547892\n",
      "[810]\ttrain's multi_logloss: 0.232045\tvalid's multi_logloss: 0.547145\n",
      "[811]\ttrain's multi_logloss: 0.231639\tvalid's multi_logloss: 0.546428\n",
      "[812]\ttrain's multi_logloss: 0.231295\tvalid's multi_logloss: 0.546581\n",
      "[813]\ttrain's multi_logloss: 0.230972\tvalid's multi_logloss: 0.546553\n",
      "[814]\ttrain's multi_logloss: 0.230621\tvalid's multi_logloss: 0.546046\n",
      "[815]\ttrain's multi_logloss: 0.230248\tvalid's multi_logloss: 0.545351\n",
      "[816]\ttrain's multi_logloss: 0.230032\tvalid's multi_logloss: 0.546196\n",
      "[817]\ttrain's multi_logloss: 0.229775\tvalid's multi_logloss: 0.546403\n",
      "[818]\ttrain's multi_logloss: 0.229469\tvalid's multi_logloss: 0.54628\n",
      "[819]\ttrain's multi_logloss: 0.229047\tvalid's multi_logloss: 0.545419\n",
      "[820]\ttrain's multi_logloss: 0.228711\tvalid's multi_logloss: 0.546117\n",
      "[821]\ttrain's multi_logloss: 0.228394\tvalid's multi_logloss: 0.545575\n",
      "[822]\ttrain's multi_logloss: 0.22802\tvalid's multi_logloss: 0.54452\n",
      "[823]\ttrain's multi_logloss: 0.227692\tvalid's multi_logloss: 0.544992\n",
      "[824]\ttrain's multi_logloss: 0.227384\tvalid's multi_logloss: 0.544673\n",
      "[825]\ttrain's multi_logloss: 0.227026\tvalid's multi_logloss: 0.542678\n",
      "[826]\ttrain's multi_logloss: 0.226834\tvalid's multi_logloss: 0.542853\n",
      "[827]\ttrain's multi_logloss: 0.226596\tvalid's multi_logloss: 0.541806\n",
      "[828]\ttrain's multi_logloss: 0.22622\tvalid's multi_logloss: 0.540708\n",
      "[829]\ttrain's multi_logloss: 0.225999\tvalid's multi_logloss: 0.539979\n",
      "[830]\ttrain's multi_logloss: 0.225713\tvalid's multi_logloss: 0.538402\n",
      "[831]\ttrain's multi_logloss: 0.225265\tvalid's multi_logloss: 0.537574\n",
      "[832]\ttrain's multi_logloss: 0.224895\tvalid's multi_logloss: 0.537447\n",
      "[833]\ttrain's multi_logloss: 0.224527\tvalid's multi_logloss: 0.537316\n",
      "[834]\ttrain's multi_logloss: 0.224214\tvalid's multi_logloss: 0.537228\n",
      "[835]\ttrain's multi_logloss: 0.223865\tvalid's multi_logloss: 0.537122\n",
      "[836]\ttrain's multi_logloss: 0.223563\tvalid's multi_logloss: 0.536639\n",
      "[837]\ttrain's multi_logloss: 0.223267\tvalid's multi_logloss: 0.536165\n",
      "[838]\ttrain's multi_logloss: 0.223046\tvalid's multi_logloss: 0.536343\n",
      "[839]\ttrain's multi_logloss: 0.222743\tvalid's multi_logloss: 0.536586\n",
      "[840]\ttrain's multi_logloss: 0.222425\tvalid's multi_logloss: 0.536206\n",
      "[841]\ttrain's multi_logloss: 0.222006\tvalid's multi_logloss: 0.535942\n",
      "[842]\ttrain's multi_logloss: 0.221665\tvalid's multi_logloss: 0.536242\n",
      "[843]\ttrain's multi_logloss: 0.221367\tvalid's multi_logloss: 0.536188\n",
      "[844]\ttrain's multi_logloss: 0.221049\tvalid's multi_logloss: 0.535652\n",
      "[845]\ttrain's multi_logloss: 0.22058\tvalid's multi_logloss: 0.535644\n",
      "[846]\ttrain's multi_logloss: 0.220119\tvalid's multi_logloss: 0.534152\n",
      "[847]\ttrain's multi_logloss: 0.219874\tvalid's multi_logloss: 0.533213\n",
      "[848]\ttrain's multi_logloss: 0.219513\tvalid's multi_logloss: 0.531314\n",
      "[849]\ttrain's multi_logloss: 0.21923\tvalid's multi_logloss: 0.529971\n",
      "[850]\ttrain's multi_logloss: 0.218901\tvalid's multi_logloss: 0.529831\n",
      "[851]\ttrain's multi_logloss: 0.218481\tvalid's multi_logloss: 0.529628\n",
      "[852]\ttrain's multi_logloss: 0.218181\tvalid's multi_logloss: 0.5281\n",
      "[853]\ttrain's multi_logloss: 0.217879\tvalid's multi_logloss: 0.528689\n",
      "[854]\ttrain's multi_logloss: 0.217505\tvalid's multi_logloss: 0.528611\n",
      "[855]\ttrain's multi_logloss: 0.217171\tvalid's multi_logloss: 0.527799\n",
      "[856]\ttrain's multi_logloss: 0.216702\tvalid's multi_logloss: 0.526833\n",
      "[857]\ttrain's multi_logloss: 0.216217\tvalid's multi_logloss: 0.525564\n",
      "[858]\ttrain's multi_logloss: 0.215479\tvalid's multi_logloss: 0.524238\n",
      "[859]\ttrain's multi_logloss: 0.214903\tvalid's multi_logloss: 0.523648\n",
      "[860]\ttrain's multi_logloss: 0.214586\tvalid's multi_logloss: 0.523511\n",
      "[861]\ttrain's multi_logloss: 0.21438\tvalid's multi_logloss: 0.523707\n",
      "[862]\ttrain's multi_logloss: 0.214166\tvalid's multi_logloss: 0.523726\n",
      "[863]\ttrain's multi_logloss: 0.213807\tvalid's multi_logloss: 0.52341\n",
      "[864]\ttrain's multi_logloss: 0.213618\tvalid's multi_logloss: 0.523634\n",
      "[865]\ttrain's multi_logloss: 0.213436\tvalid's multi_logloss: 0.523867\n",
      "[866]\ttrain's multi_logloss: 0.213116\tvalid's multi_logloss: 0.524944\n",
      "[867]\ttrain's multi_logloss: 0.212824\tvalid's multi_logloss: 0.526071\n",
      "[868]\ttrain's multi_logloss: 0.212588\tvalid's multi_logloss: 0.526296\n",
      "[869]\ttrain's multi_logloss: 0.212273\tvalid's multi_logloss: 0.527699\n",
      "[870]\ttrain's multi_logloss: 0.211944\tvalid's multi_logloss: 0.529019\n",
      "[871]\ttrain's multi_logloss: 0.211665\tvalid's multi_logloss: 0.529482\n",
      "[872]\ttrain's multi_logloss: 0.21139\tvalid's multi_logloss: 0.528568\n",
      "[873]\ttrain's multi_logloss: 0.211078\tvalid's multi_logloss: 0.529324\n",
      "[874]\ttrain's multi_logloss: 0.210754\tvalid's multi_logloss: 0.529796\n",
      "[875]\ttrain's multi_logloss: 0.210443\tvalid's multi_logloss: 0.53071\n",
      "[876]\ttrain's multi_logloss: 0.20998\tvalid's multi_logloss: 0.530472\n",
      "[877]\ttrain's multi_logloss: 0.209486\tvalid's multi_logloss: 0.529488\n",
      "[878]\ttrain's multi_logloss: 0.208983\tvalid's multi_logloss: 0.529426\n",
      "[879]\ttrain's multi_logloss: 0.208542\tvalid's multi_logloss: 0.528778\n",
      "[880]\ttrain's multi_logloss: 0.208046\tvalid's multi_logloss: 0.528198\n",
      "[881]\ttrain's multi_logloss: 0.207696\tvalid's multi_logloss: 0.526974\n",
      "[882]\ttrain's multi_logloss: 0.207357\tvalid's multi_logloss: 0.525769\n",
      "[883]\ttrain's multi_logloss: 0.206997\tvalid's multi_logloss: 0.525096\n",
      "[884]\ttrain's multi_logloss: 0.20677\tvalid's multi_logloss: 0.52585\n",
      "[885]\ttrain's multi_logloss: 0.206481\tvalid's multi_logloss: 0.525366\n",
      "[886]\ttrain's multi_logloss: 0.206193\tvalid's multi_logloss: 0.525181\n",
      "[887]\ttrain's multi_logloss: 0.205965\tvalid's multi_logloss: 0.524506\n",
      "[888]\ttrain's multi_logloss: 0.205799\tvalid's multi_logloss: 0.523723\n",
      "[889]\ttrain's multi_logloss: 0.205693\tvalid's multi_logloss: 0.522869\n",
      "[890]\ttrain's multi_logloss: 0.20546\tvalid's multi_logloss: 0.52192\n",
      "[891]\ttrain's multi_logloss: 0.205121\tvalid's multi_logloss: 0.522383\n",
      "[892]\ttrain's multi_logloss: 0.204922\tvalid's multi_logloss: 0.521983\n",
      "[893]\ttrain's multi_logloss: 0.204792\tvalid's multi_logloss: 0.522452\n",
      "[894]\ttrain's multi_logloss: 0.204602\tvalid's multi_logloss: 0.522251\n",
      "[895]\ttrain's multi_logloss: 0.204394\tvalid's multi_logloss: 0.521844\n",
      "[896]\ttrain's multi_logloss: 0.204149\tvalid's multi_logloss: 0.521552\n",
      "[897]\ttrain's multi_logloss: 0.203904\tvalid's multi_logloss: 0.521336\n",
      "[898]\ttrain's multi_logloss: 0.203644\tvalid's multi_logloss: 0.520735\n",
      "[899]\ttrain's multi_logloss: 0.203403\tvalid's multi_logloss: 0.520741\n",
      "[900]\ttrain's multi_logloss: 0.203128\tvalid's multi_logloss: 0.520095\n",
      "[901]\ttrain's multi_logloss: 0.202841\tvalid's multi_logloss: 0.521521\n",
      "[902]\ttrain's multi_logloss: 0.202542\tvalid's multi_logloss: 0.522772\n",
      "[903]\ttrain's multi_logloss: 0.202217\tvalid's multi_logloss: 0.523947\n",
      "[904]\ttrain's multi_logloss: 0.202053\tvalid's multi_logloss: 0.524584\n",
      "[905]\ttrain's multi_logloss: 0.201792\tvalid's multi_logloss: 0.526536\n",
      "[906]\ttrain's multi_logloss: 0.201526\tvalid's multi_logloss: 0.526872\n",
      "[907]\ttrain's multi_logloss: 0.201261\tvalid's multi_logloss: 0.52729\n",
      "[908]\ttrain's multi_logloss: 0.201004\tvalid's multi_logloss: 0.527716\n",
      "[909]\ttrain's multi_logloss: 0.200628\tvalid's multi_logloss: 0.527724\n",
      "[910]\ttrain's multi_logloss: 0.200339\tvalid's multi_logloss: 0.528464\n",
      "[911]\ttrain's multi_logloss: 0.20006\tvalid's multi_logloss: 0.527571\n",
      "[912]\ttrain's multi_logloss: 0.199697\tvalid's multi_logloss: 0.526491\n",
      "[913]\ttrain's multi_logloss: 0.199394\tvalid's multi_logloss: 0.527077\n",
      "[914]\ttrain's multi_logloss: 0.199041\tvalid's multi_logloss: 0.526177\n",
      "[915]\ttrain's multi_logloss: 0.198821\tvalid's multi_logloss: 0.527082\n",
      "[916]\ttrain's multi_logloss: 0.198636\tvalid's multi_logloss: 0.526577\n",
      "[917]\ttrain's multi_logloss: 0.198434\tvalid's multi_logloss: 0.526351\n",
      "[918]\ttrain's multi_logloss: 0.198257\tvalid's multi_logloss: 0.52586\n",
      "[919]\ttrain's multi_logloss: 0.197965\tvalid's multi_logloss: 0.525277\n",
      "[920]\ttrain's multi_logloss: 0.197727\tvalid's multi_logloss: 0.525424\n",
      "[921]\ttrain's multi_logloss: 0.19745\tvalid's multi_logloss: 0.525567\n",
      "[922]\ttrain's multi_logloss: 0.19705\tvalid's multi_logloss: 0.525385\n",
      "[923]\ttrain's multi_logloss: 0.196573\tvalid's multi_logloss: 0.525502\n",
      "[924]\ttrain's multi_logloss: 0.196341\tvalid's multi_logloss: 0.52592\n",
      "[925]\ttrain's multi_logloss: 0.19611\tvalid's multi_logloss: 0.526262\n",
      "[926]\ttrain's multi_logloss: 0.195801\tvalid's multi_logloss: 0.526658\n",
      "[927]\ttrain's multi_logloss: 0.195539\tvalid's multi_logloss: 0.526073\n",
      "[928]\ttrain's multi_logloss: 0.195199\tvalid's multi_logloss: 0.525735\n",
      "[929]\ttrain's multi_logloss: 0.194962\tvalid's multi_logloss: 0.525961\n",
      "[930]\ttrain's multi_logloss: 0.194524\tvalid's multi_logloss: 0.525682\n",
      "[931]\ttrain's multi_logloss: 0.194402\tvalid's multi_logloss: 0.525058\n",
      "[932]\ttrain's multi_logloss: 0.194306\tvalid's multi_logloss: 0.524959\n",
      "[933]\ttrain's multi_logloss: 0.194154\tvalid's multi_logloss: 0.524269\n",
      "[934]\ttrain's multi_logloss: 0.194041\tvalid's multi_logloss: 0.524691\n",
      "[935]\ttrain's multi_logloss: 0.19394\tvalid's multi_logloss: 0.524441\n",
      "[936]\ttrain's multi_logloss: 0.193691\tvalid's multi_logloss: 0.524254\n",
      "[937]\ttrain's multi_logloss: 0.193478\tvalid's multi_logloss: 0.524174\n",
      "[938]\ttrain's multi_logloss: 0.193241\tvalid's multi_logloss: 0.523674\n",
      "[939]\ttrain's multi_logloss: 0.192978\tvalid's multi_logloss: 0.523687\n",
      "[940]\ttrain's multi_logloss: 0.192752\tvalid's multi_logloss: 0.524301\n",
      "[941]\ttrain's multi_logloss: 0.192552\tvalid's multi_logloss: 0.523922\n",
      "[942]\ttrain's multi_logloss: 0.192191\tvalid's multi_logloss: 0.523612\n",
      "[943]\ttrain's multi_logloss: 0.191851\tvalid's multi_logloss: 0.523966\n",
      "[944]\ttrain's multi_logloss: 0.191498\tvalid's multi_logloss: 0.523682\n",
      "[945]\ttrain's multi_logloss: 0.191191\tvalid's multi_logloss: 0.52319\n",
      "[946]\ttrain's multi_logloss: 0.190976\tvalid's multi_logloss: 0.522912\n",
      "[947]\ttrain's multi_logloss: 0.190584\tvalid's multi_logloss: 0.522673\n",
      "[948]\ttrain's multi_logloss: 0.190294\tvalid's multi_logloss: 0.522321\n",
      "[949]\ttrain's multi_logloss: 0.189871\tvalid's multi_logloss: 0.521735\n",
      "[950]\ttrain's multi_logloss: 0.189531\tvalid's multi_logloss: 0.520909\n",
      "[951]\ttrain's multi_logloss: 0.189223\tvalid's multi_logloss: 0.521561\n",
      "[952]\ttrain's multi_logloss: 0.188997\tvalid's multi_logloss: 0.522046\n",
      "[953]\ttrain's multi_logloss: 0.188802\tvalid's multi_logloss: 0.52322\n",
      "[954]\ttrain's multi_logloss: 0.188656\tvalid's multi_logloss: 0.523354\n",
      "[955]\ttrain's multi_logloss: 0.188388\tvalid's multi_logloss: 0.524633\n",
      "[956]\ttrain's multi_logloss: 0.188213\tvalid's multi_logloss: 0.525076\n",
      "[957]\ttrain's multi_logloss: 0.187998\tvalid's multi_logloss: 0.525785\n",
      "[958]\ttrain's multi_logloss: 0.187768\tvalid's multi_logloss: 0.526137\n",
      "[959]\ttrain's multi_logloss: 0.187423\tvalid's multi_logloss: 0.526765\n",
      "[960]\ttrain's multi_logloss: 0.187304\tvalid's multi_logloss: 0.526814\n",
      "[961]\ttrain's multi_logloss: 0.187067\tvalid's multi_logloss: 0.525475\n",
      "[962]\ttrain's multi_logloss: 0.186833\tvalid's multi_logloss: 0.52499\n",
      "[963]\ttrain's multi_logloss: 0.186609\tvalid's multi_logloss: 0.524475\n",
      "[964]\ttrain's multi_logloss: 0.186387\tvalid's multi_logloss: 0.523351\n",
      "[965]\ttrain's multi_logloss: 0.186048\tvalid's multi_logloss: 0.521993\n",
      "[966]\ttrain's multi_logloss: 0.185768\tvalid's multi_logloss: 0.521547\n",
      "[967]\ttrain's multi_logloss: 0.185505\tvalid's multi_logloss: 0.522146\n",
      "[968]\ttrain's multi_logloss: 0.185257\tvalid's multi_logloss: 0.521986\n",
      "[969]\ttrain's multi_logloss: 0.185038\tvalid's multi_logloss: 0.522258\n",
      "[970]\ttrain's multi_logloss: 0.184777\tvalid's multi_logloss: 0.522508\n",
      "[971]\ttrain's multi_logloss: 0.184564\tvalid's multi_logloss: 0.52228\n",
      "[972]\ttrain's multi_logloss: 0.184286\tvalid's multi_logloss: 0.521568\n",
      "[973]\ttrain's multi_logloss: 0.183992\tvalid's multi_logloss: 0.521544\n",
      "[974]\ttrain's multi_logloss: 0.183556\tvalid's multi_logloss: 0.521561\n",
      "[975]\ttrain's multi_logloss: 0.183056\tvalid's multi_logloss: 0.520582\n",
      "[976]\ttrain's multi_logloss: 0.182775\tvalid's multi_logloss: 0.519894\n",
      "[977]\ttrain's multi_logloss: 0.182529\tvalid's multi_logloss: 0.520362\n",
      "[978]\ttrain's multi_logloss: 0.18229\tvalid's multi_logloss: 0.519318\n",
      "[979]\ttrain's multi_logloss: 0.182072\tvalid's multi_logloss: 0.518959\n",
      "[980]\ttrain's multi_logloss: 0.18183\tvalid's multi_logloss: 0.518539\n",
      "[981]\ttrain's multi_logloss: 0.181606\tvalid's multi_logloss: 0.519111\n",
      "[982]\ttrain's multi_logloss: 0.181289\tvalid's multi_logloss: 0.518661\n",
      "[983]\ttrain's multi_logloss: 0.181074\tvalid's multi_logloss: 0.519859\n",
      "[984]\ttrain's multi_logloss: 0.180894\tvalid's multi_logloss: 0.520384\n",
      "[985]\ttrain's multi_logloss: 0.180714\tvalid's multi_logloss: 0.521206\n",
      "[986]\ttrain's multi_logloss: 0.180417\tvalid's multi_logloss: 0.521395\n",
      "[987]\ttrain's multi_logloss: 0.18019\tvalid's multi_logloss: 0.521623\n",
      "[988]\ttrain's multi_logloss: 0.179921\tvalid's multi_logloss: 0.522258\n",
      "[989]\ttrain's multi_logloss: 0.17963\tvalid's multi_logloss: 0.522118\n",
      "[990]\ttrain's multi_logloss: 0.179434\tvalid's multi_logloss: 0.522463\n",
      "[991]\ttrain's multi_logloss: 0.179343\tvalid's multi_logloss: 0.523222\n",
      "[992]\ttrain's multi_logloss: 0.179195\tvalid's multi_logloss: 0.523076\n",
      "[993]\ttrain's multi_logloss: 0.179056\tvalid's multi_logloss: 0.523514\n",
      "[994]\ttrain's multi_logloss: 0.178982\tvalid's multi_logloss: 0.524304\n",
      "[995]\ttrain's multi_logloss: 0.178958\tvalid's multi_logloss: 0.524604\n",
      "[996]\ttrain's multi_logloss: 0.178607\tvalid's multi_logloss: 0.523459\n",
      "[997]\ttrain's multi_logloss: 0.178392\tvalid's multi_logloss: 0.523702\n",
      "[998]\ttrain's multi_logloss: 0.178087\tvalid's multi_logloss: 0.521891\n",
      "[999]\ttrain's multi_logloss: 0.177774\tvalid's multi_logloss: 0.521049\n",
      "[1000]\ttrain's multi_logloss: 0.177472\tvalid's multi_logloss: 0.519507\n",
      "[1001]\ttrain's multi_logloss: 0.177156\tvalid's multi_logloss: 0.518428\n",
      "[1002]\ttrain's multi_logloss: 0.176848\tvalid's multi_logloss: 0.517455\n",
      "[1003]\ttrain's multi_logloss: 0.176459\tvalid's multi_logloss: 0.517134\n",
      "[1004]\ttrain's multi_logloss: 0.176187\tvalid's multi_logloss: 0.516841\n",
      "[1005]\ttrain's multi_logloss: 0.175891\tvalid's multi_logloss: 0.516251\n",
      "[1006]\ttrain's multi_logloss: 0.17566\tvalid's multi_logloss: 0.517452\n",
      "[1007]\ttrain's multi_logloss: 0.175318\tvalid's multi_logloss: 0.516709\n",
      "[1008]\ttrain's multi_logloss: 0.174974\tvalid's multi_logloss: 0.51668\n",
      "[1009]\ttrain's multi_logloss: 0.174736\tvalid's multi_logloss: 0.516751\n",
      "[1010]\ttrain's multi_logloss: 0.174532\tvalid's multi_logloss: 0.517854\n",
      "[1011]\ttrain's multi_logloss: 0.174391\tvalid's multi_logloss: 0.517842\n",
      "[1012]\ttrain's multi_logloss: 0.174256\tvalid's multi_logloss: 0.517833\n",
      "[1013]\ttrain's multi_logloss: 0.174121\tvalid's multi_logloss: 0.518361\n",
      "[1014]\ttrain's multi_logloss: 0.17399\tvalid's multi_logloss: 0.518978\n",
      "[1015]\ttrain's multi_logloss: 0.173872\tvalid's multi_logloss: 0.518032\n",
      "[1016]\ttrain's multi_logloss: 0.173816\tvalid's multi_logloss: 0.517469\n",
      "[1017]\ttrain's multi_logloss: 0.17366\tvalid's multi_logloss: 0.516924\n",
      "[1018]\ttrain's multi_logloss: 0.173589\tvalid's multi_logloss: 0.516771\n",
      "[1019]\ttrain's multi_logloss: 0.173439\tvalid's multi_logloss: 0.516236\n",
      "[1020]\ttrain's multi_logloss: 0.173394\tvalid's multi_logloss: 0.515699\n",
      "[1021]\ttrain's multi_logloss: 0.173098\tvalid's multi_logloss: 0.516117\n",
      "[1022]\ttrain's multi_logloss: 0.172905\tvalid's multi_logloss: 0.516481\n",
      "[1023]\ttrain's multi_logloss: 0.172721\tvalid's multi_logloss: 0.517071\n",
      "[1024]\ttrain's multi_logloss: 0.172513\tvalid's multi_logloss: 0.517709\n",
      "[1025]\ttrain's multi_logloss: 0.172316\tvalid's multi_logloss: 0.518168\n",
      "[1026]\ttrain's multi_logloss: 0.17193\tvalid's multi_logloss: 0.519264\n",
      "[1027]\ttrain's multi_logloss: 0.171514\tvalid's multi_logloss: 0.520841\n",
      "[1028]\ttrain's multi_logloss: 0.1711\tvalid's multi_logloss: 0.520404\n",
      "[1029]\ttrain's multi_logloss: 0.170705\tvalid's multi_logloss: 0.520303\n",
      "[1030]\ttrain's multi_logloss: 0.17027\tvalid's multi_logloss: 0.521295\n",
      "[1031]\ttrain's multi_logloss: 0.169974\tvalid's multi_logloss: 0.522271\n",
      "[1032]\ttrain's multi_logloss: 0.169528\tvalid's multi_logloss: 0.522474\n",
      "[1033]\ttrain's multi_logloss: 0.169044\tvalid's multi_logloss: 0.523511\n",
      "[1034]\ttrain's multi_logloss: 0.168602\tvalid's multi_logloss: 0.523565\n",
      "[1035]\ttrain's multi_logloss: 0.168327\tvalid's multi_logloss: 0.523768\n",
      "[1036]\ttrain's multi_logloss: 0.168114\tvalid's multi_logloss: 0.524259\n",
      "[1037]\ttrain's multi_logloss: 0.167953\tvalid's multi_logloss: 0.525408\n",
      "[1038]\ttrain's multi_logloss: 0.167699\tvalid's multi_logloss: 0.526086\n",
      "[1039]\ttrain's multi_logloss: 0.167453\tvalid's multi_logloss: 0.526766\n",
      "[1040]\ttrain's multi_logloss: 0.167215\tvalid's multi_logloss: 0.527115\n",
      "[1041]\ttrain's multi_logloss: 0.166984\tvalid's multi_logloss: 0.525693\n",
      "[1042]\ttrain's multi_logloss: 0.166717\tvalid's multi_logloss: 0.524876\n",
      "[1043]\ttrain's multi_logloss: 0.166479\tvalid's multi_logloss: 0.523805\n",
      "[1044]\ttrain's multi_logloss: 0.166139\tvalid's multi_logloss: 0.523786\n",
      "[1045]\ttrain's multi_logloss: 0.165905\tvalid's multi_logloss: 0.523459\n",
      "[1046]\ttrain's multi_logloss: 0.165673\tvalid's multi_logloss: 0.523221\n",
      "[1047]\ttrain's multi_logloss: 0.165433\tvalid's multi_logloss: 0.52294\n",
      "[1048]\ttrain's multi_logloss: 0.165249\tvalid's multi_logloss: 0.523957\n",
      "[1049]\ttrain's multi_logloss: 0.165041\tvalid's multi_logloss: 0.525104\n",
      "[1050]\ttrain's multi_logloss: 0.164842\tvalid's multi_logloss: 0.526354\n",
      "[1051]\ttrain's multi_logloss: 0.164587\tvalid's multi_logloss: 0.525638\n",
      "[1052]\ttrain's multi_logloss: 0.16423\tvalid's multi_logloss: 0.525733\n",
      "[1053]\ttrain's multi_logloss: 0.163956\tvalid's multi_logloss: 0.525752\n",
      "[1054]\ttrain's multi_logloss: 0.163706\tvalid's multi_logloss: 0.525931\n",
      "[1055]\ttrain's multi_logloss: 0.163493\tvalid's multi_logloss: 0.526399\n",
      "[1056]\ttrain's multi_logloss: 0.163193\tvalid's multi_logloss: 0.526503\n",
      "[1057]\ttrain's multi_logloss: 0.162963\tvalid's multi_logloss: 0.527276\n",
      "[1058]\ttrain's multi_logloss: 0.162718\tvalid's multi_logloss: 0.527412\n",
      "[1059]\ttrain's multi_logloss: 0.162407\tvalid's multi_logloss: 0.527825\n",
      "[1060]\ttrain's multi_logloss: 0.162129\tvalid's multi_logloss: 0.527242\n",
      "[1061]\ttrain's multi_logloss: 0.161798\tvalid's multi_logloss: 0.526228\n",
      "[1062]\ttrain's multi_logloss: 0.161544\tvalid's multi_logloss: 0.526793\n",
      "[1063]\ttrain's multi_logloss: 0.161194\tvalid's multi_logloss: 0.526363\n",
      "[1064]\ttrain's multi_logloss: 0.160924\tvalid's multi_logloss: 0.525947\n",
      "[1065]\ttrain's multi_logloss: 0.160615\tvalid's multi_logloss: 0.52505\n",
      "[1066]\ttrain's multi_logloss: 0.160482\tvalid's multi_logloss: 0.525107\n",
      "[1067]\ttrain's multi_logloss: 0.160284\tvalid's multi_logloss: 0.524435\n",
      "[1068]\ttrain's multi_logloss: 0.160056\tvalid's multi_logloss: 0.523657\n",
      "[1069]\ttrain's multi_logloss: 0.159808\tvalid's multi_logloss: 0.523887\n",
      "[1070]\ttrain's multi_logloss: 0.159646\tvalid's multi_logloss: 0.523749\n",
      "[1071]\ttrain's multi_logloss: 0.159454\tvalid's multi_logloss: 0.523267\n",
      "[1072]\ttrain's multi_logloss: 0.159249\tvalid's multi_logloss: 0.522481\n",
      "[1073]\ttrain's multi_logloss: 0.159018\tvalid's multi_logloss: 0.521742\n",
      "[1074]\ttrain's multi_logloss: 0.158838\tvalid's multi_logloss: 0.520972\n",
      "[1075]\ttrain's multi_logloss: 0.158635\tvalid's multi_logloss: 0.520793\n",
      "[1076]\ttrain's multi_logloss: 0.158495\tvalid's multi_logloss: 0.520986\n",
      "[1077]\ttrain's multi_logloss: 0.158292\tvalid's multi_logloss: 0.520882\n",
      "[1078]\ttrain's multi_logloss: 0.158145\tvalid's multi_logloss: 0.520482\n",
      "[1079]\ttrain's multi_logloss: 0.158025\tvalid's multi_logloss: 0.520147\n",
      "[1080]\ttrain's multi_logloss: 0.157918\tvalid's multi_logloss: 0.51936\n",
      "[1081]\ttrain's multi_logloss: 0.157702\tvalid's multi_logloss: 0.518689\n",
      "[1082]\ttrain's multi_logloss: 0.157456\tvalid's multi_logloss: 0.518625\n",
      "[1083]\ttrain's multi_logloss: 0.157255\tvalid's multi_logloss: 0.518774\n",
      "[1084]\ttrain's multi_logloss: 0.157009\tvalid's multi_logloss: 0.518785\n",
      "[1085]\ttrain's multi_logloss: 0.156759\tvalid's multi_logloss: 0.519194\n",
      "[1086]\ttrain's multi_logloss: 0.156607\tvalid's multi_logloss: 0.52021\n",
      "[1087]\ttrain's multi_logloss: 0.156524\tvalid's multi_logloss: 0.520408\n",
      "[1088]\ttrain's multi_logloss: 0.156437\tvalid's multi_logloss: 0.520898\n",
      "[1089]\ttrain's multi_logloss: 0.156269\tvalid's multi_logloss: 0.521476\n",
      "[1090]\ttrain's multi_logloss: 0.156133\tvalid's multi_logloss: 0.52249\n",
      "[1091]\ttrain's multi_logloss: 0.155934\tvalid's multi_logloss: 0.52244\n",
      "[1092]\ttrain's multi_logloss: 0.15572\tvalid's multi_logloss: 0.521227\n",
      "[1093]\ttrain's multi_logloss: 0.155462\tvalid's multi_logloss: 0.520746\n",
      "[1094]\ttrain's multi_logloss: 0.155254\tvalid's multi_logloss: 0.51889\n",
      "[1095]\ttrain's multi_logloss: 0.155044\tvalid's multi_logloss: 0.517928\n",
      "[1096]\ttrain's multi_logloss: 0.154761\tvalid's multi_logloss: 0.517323\n",
      "[1097]\ttrain's multi_logloss: 0.154467\tvalid's multi_logloss: 0.517138\n",
      "[1098]\ttrain's multi_logloss: 0.154252\tvalid's multi_logloss: 0.518113\n",
      "[1099]\ttrain's multi_logloss: 0.153966\tvalid's multi_logloss: 0.518092\n",
      "[1100]\ttrain's multi_logloss: 0.153698\tvalid's multi_logloss: 0.518221\n",
      "[1101]\ttrain's multi_logloss: 0.153479\tvalid's multi_logloss: 0.518208\n",
      "[1102]\ttrain's multi_logloss: 0.153098\tvalid's multi_logloss: 0.517399\n",
      "[1103]\ttrain's multi_logloss: 0.152743\tvalid's multi_logloss: 0.516683\n",
      "[1104]\ttrain's multi_logloss: 0.152372\tvalid's multi_logloss: 0.516317\n",
      "[1105]\ttrain's multi_logloss: 0.152148\tvalid's multi_logloss: 0.517313\n",
      "[1106]\ttrain's multi_logloss: 0.151905\tvalid's multi_logloss: 0.518197\n",
      "[1107]\ttrain's multi_logloss: 0.151617\tvalid's multi_logloss: 0.519265\n",
      "[1108]\ttrain's multi_logloss: 0.151307\tvalid's multi_logloss: 0.518842\n",
      "[1109]\ttrain's multi_logloss: 0.151005\tvalid's multi_logloss: 0.519628\n",
      "[1110]\ttrain's multi_logloss: 0.150537\tvalid's multi_logloss: 0.519612\n",
      "[1111]\ttrain's multi_logloss: 0.15026\tvalid's multi_logloss: 0.519962\n",
      "[1112]\ttrain's multi_logloss: 0.150065\tvalid's multi_logloss: 0.519641\n",
      "[1113]\ttrain's multi_logloss: 0.149896\tvalid's multi_logloss: 0.519412\n",
      "[1114]\ttrain's multi_logloss: 0.149743\tvalid's multi_logloss: 0.519978\n",
      "[1115]\ttrain's multi_logloss: 0.149453\tvalid's multi_logloss: 0.520075\n",
      "[1116]\ttrain's multi_logloss: 0.149347\tvalid's multi_logloss: 0.520071\n",
      "[1117]\ttrain's multi_logloss: 0.14924\tvalid's multi_logloss: 0.519757\n",
      "[1118]\ttrain's multi_logloss: 0.149136\tvalid's multi_logloss: 0.519449\n",
      "[1119]\ttrain's multi_logloss: 0.149051\tvalid's multi_logloss: 0.519464\n",
      "[1120]\ttrain's multi_logloss: 0.148914\tvalid's multi_logloss: 0.519635\n",
      "Early stopping, best iteration is:\n",
      "[1020]\ttrain's multi_logloss: 0.173394\tvalid's multi_logloss: 0.515699\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.8\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09176\tvalid's SFC_loss: 1.09131\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08636\tvalid's SFC_loss: 1.08715\n",
      "[3]\ttrain's SFC_loss: 1.07963\tvalid's SFC_loss: 1.082\n",
      "[4]\ttrain's SFC_loss: 1.07289\tvalid's SFC_loss: 1.07754\n",
      "[5]\ttrain's SFC_loss: 1.06697\tvalid's SFC_loss: 1.07298\n",
      "[6]\ttrain's SFC_loss: 1.0624\tvalid's SFC_loss: 1.06543\n",
      "[7]\ttrain's SFC_loss: 1.05794\tvalid's SFC_loss: 1.0614\n",
      "[8]\ttrain's SFC_loss: 1.05339\tvalid's SFC_loss: 1.05417\n",
      "[9]\ttrain's SFC_loss: 1.04857\tvalid's SFC_loss: 1.04683\n",
      "[10]\ttrain's SFC_loss: 1.04403\tvalid's SFC_loss: 1.03943\n",
      "[11]\ttrain's SFC_loss: 1.03948\tvalid's SFC_loss: 1.03682\n",
      "[12]\ttrain's SFC_loss: 1.03354\tvalid's SFC_loss: 1.03271\n",
      "[13]\ttrain's SFC_loss: 1.0277\tvalid's SFC_loss: 1.03014\n",
      "[14]\ttrain's SFC_loss: 1.02095\tvalid's SFC_loss: 1.02426\n",
      "[15]\ttrain's SFC_loss: 1.01631\tvalid's SFC_loss: 1.01999\n",
      "[16]\ttrain's SFC_loss: 1.01107\tvalid's SFC_loss: 1.01548\n",
      "[17]\ttrain's SFC_loss: 1.00596\tvalid's SFC_loss: 1.01086\n",
      "[18]\ttrain's SFC_loss: 1.00114\tvalid's SFC_loss: 1.00881\n",
      "[19]\ttrain's SFC_loss: 0.995569\tvalid's SFC_loss: 1.00567\n",
      "[20]\ttrain's SFC_loss: 0.99067\tvalid's SFC_loss: 1.00159\n",
      "[21]\ttrain's SFC_loss: 0.985222\tvalid's SFC_loss: 1.00315\n",
      "[22]\ttrain's SFC_loss: 0.979949\tvalid's SFC_loss: 1.00611\n",
      "[23]\ttrain's SFC_loss: 0.974549\tvalid's SFC_loss: 1.00616\n",
      "[24]\ttrain's SFC_loss: 0.969588\tvalid's SFC_loss: 1.00532\n",
      "[25]\ttrain's SFC_loss: 0.964647\tvalid's SFC_loss: 1.00294\n",
      "[26]\ttrain's SFC_loss: 0.959712\tvalid's SFC_loss: 0.997267\n",
      "[27]\ttrain's SFC_loss: 0.955162\tvalid's SFC_loss: 0.992752\n",
      "[28]\ttrain's SFC_loss: 0.950903\tvalid's SFC_loss: 0.98829\n",
      "[29]\ttrain's SFC_loss: 0.946808\tvalid's SFC_loss: 0.98594\n",
      "[30]\ttrain's SFC_loss: 0.942722\tvalid's SFC_loss: 0.983845\n",
      "[31]\ttrain's SFC_loss: 0.938653\tvalid's SFC_loss: 0.980547\n",
      "[32]\ttrain's SFC_loss: 0.93426\tvalid's SFC_loss: 0.976998\n",
      "[33]\ttrain's SFC_loss: 0.930554\tvalid's SFC_loss: 0.974727\n",
      "[34]\ttrain's SFC_loss: 0.926017\tvalid's SFC_loss: 0.971914\n",
      "[35]\ttrain's SFC_loss: 0.922167\tvalid's SFC_loss: 0.969756\n",
      "[36]\ttrain's SFC_loss: 0.917901\tvalid's SFC_loss: 0.965624\n",
      "[37]\ttrain's SFC_loss: 0.913124\tvalid's SFC_loss: 0.964802\n",
      "[38]\ttrain's SFC_loss: 0.908961\tvalid's SFC_loss: 0.959344\n",
      "[39]\ttrain's SFC_loss: 0.905115\tvalid's SFC_loss: 0.956212\n",
      "[40]\ttrain's SFC_loss: 0.901212\tvalid's SFC_loss: 0.953152\n",
      "[41]\ttrain's SFC_loss: 0.896894\tvalid's SFC_loss: 0.949052\n",
      "[42]\ttrain's SFC_loss: 0.893129\tvalid's SFC_loss: 0.946263\n",
      "[43]\ttrain's SFC_loss: 0.889314\tvalid's SFC_loss: 0.943492\n",
      "[44]\ttrain's SFC_loss: 0.886354\tvalid's SFC_loss: 0.938628\n",
      "[45]\ttrain's SFC_loss: 0.882901\tvalid's SFC_loss: 0.936451\n",
      "[46]\ttrain's SFC_loss: 0.879855\tvalid's SFC_loss: 0.935233\n",
      "[47]\ttrain's SFC_loss: 0.876984\tvalid's SFC_loss: 0.934422\n",
      "[48]\ttrain's SFC_loss: 0.874137\tvalid's SFC_loss: 0.93334\n",
      "[49]\ttrain's SFC_loss: 0.871231\tvalid's SFC_loss: 0.933016\n",
      "[50]\ttrain's SFC_loss: 0.868363\tvalid's SFC_loss: 0.932971\n",
      "[51]\ttrain's SFC_loss: 0.865534\tvalid's SFC_loss: 0.930762\n",
      "[52]\ttrain's SFC_loss: 0.861673\tvalid's SFC_loss: 0.923138\n",
      "[53]\ttrain's SFC_loss: 0.858553\tvalid's SFC_loss: 0.919579\n",
      "[54]\ttrain's SFC_loss: 0.855099\tvalid's SFC_loss: 0.915079\n",
      "[55]\ttrain's SFC_loss: 0.85179\tvalid's SFC_loss: 0.908917\n",
      "[56]\ttrain's SFC_loss: 0.848237\tvalid's SFC_loss: 0.903367\n",
      "[57]\ttrain's SFC_loss: 0.845004\tvalid's SFC_loss: 0.899063\n",
      "[58]\ttrain's SFC_loss: 0.841598\tvalid's SFC_loss: 0.895582\n",
      "[59]\ttrain's SFC_loss: 0.838203\tvalid's SFC_loss: 0.891615\n",
      "[60]\ttrain's SFC_loss: 0.834983\tvalid's SFC_loss: 0.888241\n",
      "[61]\ttrain's SFC_loss: 0.832173\tvalid's SFC_loss: 0.884616\n",
      "[62]\ttrain's SFC_loss: 0.829535\tvalid's SFC_loss: 0.881502\n",
      "[63]\ttrain's SFC_loss: 0.826094\tvalid's SFC_loss: 0.877591\n",
      "[64]\ttrain's SFC_loss: 0.822756\tvalid's SFC_loss: 0.8738\n",
      "[65]\ttrain's SFC_loss: 0.819512\tvalid's SFC_loss: 0.870107\n",
      "[66]\ttrain's SFC_loss: 0.816975\tvalid's SFC_loss: 0.868356\n",
      "[67]\ttrain's SFC_loss: 0.813281\tvalid's SFC_loss: 0.867325\n",
      "[68]\ttrain's SFC_loss: 0.810135\tvalid's SFC_loss: 0.866325\n",
      "[69]\ttrain's SFC_loss: 0.807231\tvalid's SFC_loss: 0.863463\n",
      "[70]\ttrain's SFC_loss: 0.804048\tvalid's SFC_loss: 0.861642\n",
      "[71]\ttrain's SFC_loss: 0.800893\tvalid's SFC_loss: 0.8616\n",
      "[72]\ttrain's SFC_loss: 0.797363\tvalid's SFC_loss: 0.86087\n",
      "[73]\ttrain's SFC_loss: 0.793955\tvalid's SFC_loss: 0.862657\n",
      "[74]\ttrain's SFC_loss: 0.790496\tvalid's SFC_loss: 0.86161\n",
      "[75]\ttrain's SFC_loss: 0.786666\tvalid's SFC_loss: 0.862399\n",
      "[76]\ttrain's SFC_loss: 0.784293\tvalid's SFC_loss: 0.862624\n",
      "[77]\ttrain's SFC_loss: 0.781414\tvalid's SFC_loss: 0.862238\n",
      "[78]\ttrain's SFC_loss: 0.779714\tvalid's SFC_loss: 0.86072\n",
      "[79]\ttrain's SFC_loss: 0.777537\tvalid's SFC_loss: 0.861083\n",
      "[80]\ttrain's SFC_loss: 0.775453\tvalid's SFC_loss: 0.862903\n",
      "[81]\ttrain's SFC_loss: 0.773165\tvalid's SFC_loss: 0.860038\n",
      "[82]\ttrain's SFC_loss: 0.770365\tvalid's SFC_loss: 0.857398\n",
      "[83]\ttrain's SFC_loss: 0.767804\tvalid's SFC_loss: 0.855659\n",
      "[84]\ttrain's SFC_loss: 0.765673\tvalid's SFC_loss: 0.852872\n",
      "[85]\ttrain's SFC_loss: 0.763135\tvalid's SFC_loss: 0.850802\n",
      "[86]\ttrain's SFC_loss: 0.75977\tvalid's SFC_loss: 0.848177\n",
      "[87]\ttrain's SFC_loss: 0.75729\tvalid's SFC_loss: 0.847203\n",
      "[88]\ttrain's SFC_loss: 0.754861\tvalid's SFC_loss: 0.846694\n",
      "[89]\ttrain's SFC_loss: 0.75224\tvalid's SFC_loss: 0.844792\n",
      "[90]\ttrain's SFC_loss: 0.749069\tvalid's SFC_loss: 0.841565\n",
      "[91]\ttrain's SFC_loss: 0.746222\tvalid's SFC_loss: 0.838543\n",
      "[92]\ttrain's SFC_loss: 0.74349\tvalid's SFC_loss: 0.837077\n",
      "[93]\ttrain's SFC_loss: 0.740693\tvalid's SFC_loss: 0.834171\n",
      "[94]\ttrain's SFC_loss: 0.738184\tvalid's SFC_loss: 0.832857\n",
      "[95]\ttrain's SFC_loss: 0.735844\tvalid's SFC_loss: 0.832374\n",
      "[96]\ttrain's SFC_loss: 0.733255\tvalid's SFC_loss: 0.829873\n",
      "[97]\ttrain's SFC_loss: 0.730967\tvalid's SFC_loss: 0.82842\n",
      "[98]\ttrain's SFC_loss: 0.728361\tvalid's SFC_loss: 0.825417\n",
      "[99]\ttrain's SFC_loss: 0.725821\tvalid's SFC_loss: 0.822479\n",
      "[100]\ttrain's SFC_loss: 0.724056\tvalid's SFC_loss: 0.820618\n",
      "[101]\ttrain's SFC_loss: 0.721377\tvalid's SFC_loss: 0.818044\n",
      "[102]\ttrain's SFC_loss: 0.718635\tvalid's SFC_loss: 0.816588\n",
      "[103]\ttrain's SFC_loss: 0.716328\tvalid's SFC_loss: 0.814099\n",
      "[104]\ttrain's SFC_loss: 0.714094\tvalid's SFC_loss: 0.813716\n",
      "[105]\ttrain's SFC_loss: 0.711475\tvalid's SFC_loss: 0.811247\n",
      "[106]\ttrain's SFC_loss: 0.709987\tvalid's SFC_loss: 0.809986\n",
      "[107]\ttrain's SFC_loss: 0.708825\tvalid's SFC_loss: 0.8087\n",
      "[108]\ttrain's SFC_loss: 0.707463\tvalid's SFC_loss: 0.806011\n",
      "[109]\ttrain's SFC_loss: 0.706059\tvalid's SFC_loss: 0.803981\n",
      "[110]\ttrain's SFC_loss: 0.703983\tvalid's SFC_loss: 0.801922\n",
      "[111]\ttrain's SFC_loss: 0.70302\tvalid's SFC_loss: 0.802738\n",
      "[112]\ttrain's SFC_loss: 0.702017\tvalid's SFC_loss: 0.802639\n",
      "[113]\ttrain's SFC_loss: 0.701319\tvalid's SFC_loss: 0.803862\n",
      "[114]\ttrain's SFC_loss: 0.699697\tvalid's SFC_loss: 0.801737\n",
      "[115]\ttrain's SFC_loss: 0.699156\tvalid's SFC_loss: 0.802326\n",
      "[116]\ttrain's SFC_loss: 0.697049\tvalid's SFC_loss: 0.803588\n",
      "[117]\ttrain's SFC_loss: 0.694945\tvalid's SFC_loss: 0.803375\n",
      "[118]\ttrain's SFC_loss: 0.692372\tvalid's SFC_loss: 0.805253\n",
      "[119]\ttrain's SFC_loss: 0.689867\tvalid's SFC_loss: 0.806576\n",
      "[120]\ttrain's SFC_loss: 0.687245\tvalid's SFC_loss: 0.806515\n",
      "[121]\ttrain's SFC_loss: 0.685895\tvalid's SFC_loss: 0.806926\n",
      "[122]\ttrain's SFC_loss: 0.683849\tvalid's SFC_loss: 0.806163\n",
      "[123]\ttrain's SFC_loss: 0.681968\tvalid's SFC_loss: 0.8083\n",
      "[124]\ttrain's SFC_loss: 0.680078\tvalid's SFC_loss: 0.807393\n",
      "[125]\ttrain's SFC_loss: 0.678004\tvalid's SFC_loss: 0.805593\n",
      "[126]\ttrain's SFC_loss: 0.675899\tvalid's SFC_loss: 0.80459\n",
      "[127]\ttrain's SFC_loss: 0.674028\tvalid's SFC_loss: 0.803235\n",
      "[128]\ttrain's SFC_loss: 0.671789\tvalid's SFC_loss: 0.802265\n",
      "[129]\ttrain's SFC_loss: 0.67002\tvalid's SFC_loss: 0.80014\n",
      "[130]\ttrain's SFC_loss: 0.668627\tvalid's SFC_loss: 0.797158\n",
      "[131]\ttrain's SFC_loss: 0.665897\tvalid's SFC_loss: 0.796328\n",
      "[132]\ttrain's SFC_loss: 0.663773\tvalid's SFC_loss: 0.791763\n",
      "[133]\ttrain's SFC_loss: 0.661801\tvalid's SFC_loss: 0.786622\n",
      "[134]\ttrain's SFC_loss: 0.659852\tvalid's SFC_loss: 0.782448\n",
      "[135]\ttrain's SFC_loss: 0.658115\tvalid's SFC_loss: 0.781715\n",
      "[136]\ttrain's SFC_loss: 0.655399\tvalid's SFC_loss: 0.780682\n",
      "[137]\ttrain's SFC_loss: 0.65341\tvalid's SFC_loss: 0.779509\n",
      "[138]\ttrain's SFC_loss: 0.651107\tvalid's SFC_loss: 0.778664\n",
      "[139]\ttrain's SFC_loss: 0.648322\tvalid's SFC_loss: 0.77803\n",
      "[140]\ttrain's SFC_loss: 0.646223\tvalid's SFC_loss: 0.778278\n",
      "[141]\ttrain's SFC_loss: 0.645174\tvalid's SFC_loss: 0.777647\n",
      "[142]\ttrain's SFC_loss: 0.644205\tvalid's SFC_loss: 0.777073\n",
      "[143]\ttrain's SFC_loss: 0.643266\tvalid's SFC_loss: 0.776517\n",
      "[144]\ttrain's SFC_loss: 0.642278\tvalid's SFC_loss: 0.776413\n",
      "[145]\ttrain's SFC_loss: 0.64132\tvalid's SFC_loss: 0.776329\n",
      "[146]\ttrain's SFC_loss: 0.639361\tvalid's SFC_loss: 0.773829\n",
      "[147]\ttrain's SFC_loss: 0.637401\tvalid's SFC_loss: 0.773234\n",
      "[148]\ttrain's SFC_loss: 0.635805\tvalid's SFC_loss: 0.77235\n",
      "[149]\ttrain's SFC_loss: 0.634176\tvalid's SFC_loss: 0.769864\n",
      "[150]\ttrain's SFC_loss: 0.632266\tvalid's SFC_loss: 0.768581\n",
      "[151]\ttrain's SFC_loss: 0.630677\tvalid's SFC_loss: 0.766929\n",
      "[152]\ttrain's SFC_loss: 0.629058\tvalid's SFC_loss: 0.76699\n",
      "[153]\ttrain's SFC_loss: 0.627459\tvalid's SFC_loss: 0.766094\n",
      "[154]\ttrain's SFC_loss: 0.625795\tvalid's SFC_loss: 0.765311\n",
      "[155]\ttrain's SFC_loss: 0.624161\tvalid's SFC_loss: 0.764332\n",
      "[156]\ttrain's SFC_loss: 0.623118\tvalid's SFC_loss: 0.764492\n",
      "[157]\ttrain's SFC_loss: 0.62225\tvalid's SFC_loss: 0.764577\n",
      "[158]\ttrain's SFC_loss: 0.621424\tvalid's SFC_loss: 0.764683\n",
      "[159]\ttrain's SFC_loss: 0.620463\tvalid's SFC_loss: 0.763392\n",
      "[160]\ttrain's SFC_loss: 0.619264\tvalid's SFC_loss: 0.763655\n",
      "[161]\ttrain's SFC_loss: 0.617483\tvalid's SFC_loss: 0.761455\n",
      "[162]\ttrain's SFC_loss: 0.615536\tvalid's SFC_loss: 0.759375\n",
      "[163]\ttrain's SFC_loss: 0.614193\tvalid's SFC_loss: 0.758643\n",
      "[164]\ttrain's SFC_loss: 0.612895\tvalid's SFC_loss: 0.757908\n",
      "[165]\ttrain's SFC_loss: 0.61116\tvalid's SFC_loss: 0.754379\n",
      "[166]\ttrain's SFC_loss: 0.609686\tvalid's SFC_loss: 0.753869\n",
      "[167]\ttrain's SFC_loss: 0.60852\tvalid's SFC_loss: 0.753809\n",
      "[168]\ttrain's SFC_loss: 0.60723\tvalid's SFC_loss: 0.750988\n",
      "[169]\ttrain's SFC_loss: 0.605979\tvalid's SFC_loss: 0.75014\n",
      "[170]\ttrain's SFC_loss: 0.604708\tvalid's SFC_loss: 0.750259\n",
      "[171]\ttrain's SFC_loss: 0.602821\tvalid's SFC_loss: 0.748236\n",
      "[172]\ttrain's SFC_loss: 0.601183\tvalid's SFC_loss: 0.746454\n",
      "[173]\ttrain's SFC_loss: 0.599741\tvalid's SFC_loss: 0.74348\n",
      "[174]\ttrain's SFC_loss: 0.59839\tvalid's SFC_loss: 0.740217\n",
      "[175]\ttrain's SFC_loss: 0.596953\tvalid's SFC_loss: 0.736961\n",
      "[176]\ttrain's SFC_loss: 0.595214\tvalid's SFC_loss: 0.735585\n",
      "[177]\ttrain's SFC_loss: 0.593268\tvalid's SFC_loss: 0.735385\n",
      "[178]\ttrain's SFC_loss: 0.591561\tvalid's SFC_loss: 0.733574\n",
      "[179]\ttrain's SFC_loss: 0.589903\tvalid's SFC_loss: 0.732125\n",
      "[180]\ttrain's SFC_loss: 0.588527\tvalid's SFC_loss: 0.730883\n",
      "[181]\ttrain's SFC_loss: 0.586856\tvalid's SFC_loss: 0.72832\n",
      "[182]\ttrain's SFC_loss: 0.585441\tvalid's SFC_loss: 0.727382\n",
      "[183]\ttrain's SFC_loss: 0.583833\tvalid's SFC_loss: 0.725241\n",
      "[184]\ttrain's SFC_loss: 0.582666\tvalid's SFC_loss: 0.723016\n",
      "[185]\ttrain's SFC_loss: 0.58149\tvalid's SFC_loss: 0.721385\n",
      "[186]\ttrain's SFC_loss: 0.579909\tvalid's SFC_loss: 0.722481\n",
      "[187]\ttrain's SFC_loss: 0.578367\tvalid's SFC_loss: 0.721918\n",
      "[188]\ttrain's SFC_loss: 0.577149\tvalid's SFC_loss: 0.720736\n",
      "[189]\ttrain's SFC_loss: 0.575484\tvalid's SFC_loss: 0.720727\n",
      "[190]\ttrain's SFC_loss: 0.574152\tvalid's SFC_loss: 0.720999\n",
      "[191]\ttrain's SFC_loss: 0.572951\tvalid's SFC_loss: 0.719473\n",
      "[192]\ttrain's SFC_loss: 0.571733\tvalid's SFC_loss: 0.71872\n",
      "[193]\ttrain's SFC_loss: 0.57059\tvalid's SFC_loss: 0.717349\n",
      "[194]\ttrain's SFC_loss: 0.569127\tvalid's SFC_loss: 0.71629\n",
      "[195]\ttrain's SFC_loss: 0.56801\tvalid's SFC_loss: 0.715388\n",
      "[196]\ttrain's SFC_loss: 0.566119\tvalid's SFC_loss: 0.715052\n",
      "[197]\ttrain's SFC_loss: 0.563995\tvalid's SFC_loss: 0.710006\n",
      "[198]\ttrain's SFC_loss: 0.562043\tvalid's SFC_loss: 0.708858\n",
      "[199]\ttrain's SFC_loss: 0.560138\tvalid's SFC_loss: 0.707932\n",
      "[200]\ttrain's SFC_loss: 0.5579\tvalid's SFC_loss: 0.707293\n",
      "[201]\ttrain's SFC_loss: 0.556394\tvalid's SFC_loss: 0.707583\n",
      "[202]\ttrain's SFC_loss: 0.555097\tvalid's SFC_loss: 0.707817\n",
      "[203]\ttrain's SFC_loss: 0.553869\tvalid's SFC_loss: 0.70828\n",
      "[204]\ttrain's SFC_loss: 0.552827\tvalid's SFC_loss: 0.706863\n",
      "[205]\ttrain's SFC_loss: 0.55146\tvalid's SFC_loss: 0.70825\n",
      "[206]\ttrain's SFC_loss: 0.550339\tvalid's SFC_loss: 0.706867\n",
      "[207]\ttrain's SFC_loss: 0.549294\tvalid's SFC_loss: 0.708201\n",
      "[208]\ttrain's SFC_loss: 0.547924\tvalid's SFC_loss: 0.707274\n",
      "[209]\ttrain's SFC_loss: 0.54695\tvalid's SFC_loss: 0.70659\n",
      "[210]\ttrain's SFC_loss: 0.545598\tvalid's SFC_loss: 0.707437\n",
      "[211]\ttrain's SFC_loss: 0.544833\tvalid's SFC_loss: 0.707381\n",
      "[212]\ttrain's SFC_loss: 0.543842\tvalid's SFC_loss: 0.707687\n",
      "[213]\ttrain's SFC_loss: 0.543217\tvalid's SFC_loss: 0.707796\n",
      "[214]\ttrain's SFC_loss: 0.542544\tvalid's SFC_loss: 0.707815\n",
      "[215]\ttrain's SFC_loss: 0.541861\tvalid's SFC_loss: 0.708454\n",
      "[216]\ttrain's SFC_loss: 0.540177\tvalid's SFC_loss: 0.706687\n",
      "[217]\ttrain's SFC_loss: 0.538816\tvalid's SFC_loss: 0.707088\n",
      "[218]\ttrain's SFC_loss: 0.53744\tvalid's SFC_loss: 0.705342\n",
      "[219]\ttrain's SFC_loss: 0.536018\tvalid's SFC_loss: 0.706196\n",
      "[220]\ttrain's SFC_loss: 0.534728\tvalid's SFC_loss: 0.705446\n",
      "[221]\ttrain's SFC_loss: 0.533407\tvalid's SFC_loss: 0.704684\n",
      "[222]\ttrain's SFC_loss: 0.531917\tvalid's SFC_loss: 0.70334\n",
      "[223]\ttrain's SFC_loss: 0.530434\tvalid's SFC_loss: 0.702622\n",
      "[224]\ttrain's SFC_loss: 0.529321\tvalid's SFC_loss: 0.701209\n",
      "[225]\ttrain's SFC_loss: 0.528188\tvalid's SFC_loss: 0.701395\n",
      "[226]\ttrain's SFC_loss: 0.526657\tvalid's SFC_loss: 0.700535\n",
      "[227]\ttrain's SFC_loss: 0.525114\tvalid's SFC_loss: 0.699526\n",
      "[228]\ttrain's SFC_loss: 0.523898\tvalid's SFC_loss: 0.699516\n",
      "[229]\ttrain's SFC_loss: 0.522585\tvalid's SFC_loss: 0.697233\n",
      "[230]\ttrain's SFC_loss: 0.521609\tvalid's SFC_loss: 0.695351\n",
      "[231]\ttrain's SFC_loss: 0.520717\tvalid's SFC_loss: 0.696957\n",
      "[232]\ttrain's SFC_loss: 0.520062\tvalid's SFC_loss: 0.695495\n",
      "[233]\ttrain's SFC_loss: 0.519394\tvalid's SFC_loss: 0.695946\n",
      "[234]\ttrain's SFC_loss: 0.51833\tvalid's SFC_loss: 0.697367\n",
      "[235]\ttrain's SFC_loss: 0.51712\tvalid's SFC_loss: 0.697268\n",
      "[236]\ttrain's SFC_loss: 0.516059\tvalid's SFC_loss: 0.694963\n",
      "[237]\ttrain's SFC_loss: 0.514985\tvalid's SFC_loss: 0.694236\n",
      "[238]\ttrain's SFC_loss: 0.51374\tvalid's SFC_loss: 0.693722\n",
      "[239]\ttrain's SFC_loss: 0.512562\tvalid's SFC_loss: 0.69067\n",
      "[240]\ttrain's SFC_loss: 0.511183\tvalid's SFC_loss: 0.689119\n",
      "[241]\ttrain's SFC_loss: 0.50985\tvalid's SFC_loss: 0.688202\n",
      "[242]\ttrain's SFC_loss: 0.508475\tvalid's SFC_loss: 0.688231\n",
      "[243]\ttrain's SFC_loss: 0.507439\tvalid's SFC_loss: 0.687655\n",
      "[244]\ttrain's SFC_loss: 0.506339\tvalid's SFC_loss: 0.688574\n",
      "[245]\ttrain's SFC_loss: 0.505283\tvalid's SFC_loss: 0.689821\n",
      "[246]\ttrain's SFC_loss: 0.504128\tvalid's SFC_loss: 0.691053\n",
      "[247]\ttrain's SFC_loss: 0.502825\tvalid's SFC_loss: 0.689489\n",
      "[248]\ttrain's SFC_loss: 0.501485\tvalid's SFC_loss: 0.689051\n",
      "[249]\ttrain's SFC_loss: 0.50049\tvalid's SFC_loss: 0.689673\n",
      "[250]\ttrain's SFC_loss: 0.499247\tvalid's SFC_loss: 0.69008\n",
      "[251]\ttrain's SFC_loss: 0.498474\tvalid's SFC_loss: 0.688496\n",
      "[252]\ttrain's SFC_loss: 0.49755\tvalid's SFC_loss: 0.686734\n",
      "[253]\ttrain's SFC_loss: 0.49669\tvalid's SFC_loss: 0.685666\n",
      "[254]\ttrain's SFC_loss: 0.495821\tvalid's SFC_loss: 0.684039\n",
      "[255]\ttrain's SFC_loss: 0.494855\tvalid's SFC_loss: 0.682185\n",
      "[256]\ttrain's SFC_loss: 0.494097\tvalid's SFC_loss: 0.68292\n",
      "[257]\ttrain's SFC_loss: 0.493238\tvalid's SFC_loss: 0.683859\n",
      "[258]\ttrain's SFC_loss: 0.492473\tvalid's SFC_loss: 0.68452\n",
      "[259]\ttrain's SFC_loss: 0.491275\tvalid's SFC_loss: 0.685082\n",
      "[260]\ttrain's SFC_loss: 0.490159\tvalid's SFC_loss: 0.686143\n",
      "[261]\ttrain's SFC_loss: 0.488628\tvalid's SFC_loss: 0.686098\n",
      "[262]\ttrain's SFC_loss: 0.487663\tvalid's SFC_loss: 0.687803\n",
      "[263]\ttrain's SFC_loss: 0.486483\tvalid's SFC_loss: 0.686323\n",
      "[264]\ttrain's SFC_loss: 0.485077\tvalid's SFC_loss: 0.685277\n",
      "[265]\ttrain's SFC_loss: 0.483933\tvalid's SFC_loss: 0.68517\n",
      "[266]\ttrain's SFC_loss: 0.482629\tvalid's SFC_loss: 0.683771\n",
      "[267]\ttrain's SFC_loss: 0.48179\tvalid's SFC_loss: 0.683779\n",
      "[268]\ttrain's SFC_loss: 0.480446\tvalid's SFC_loss: 0.683655\n",
      "[269]\ttrain's SFC_loss: 0.479531\tvalid's SFC_loss: 0.684173\n",
      "[270]\ttrain's SFC_loss: 0.478479\tvalid's SFC_loss: 0.68455\n",
      "[271]\ttrain's SFC_loss: 0.477459\tvalid's SFC_loss: 0.684936\n",
      "[272]\ttrain's SFC_loss: 0.476529\tvalid's SFC_loss: 0.682967\n",
      "[273]\ttrain's SFC_loss: 0.475661\tvalid's SFC_loss: 0.682469\n",
      "[274]\ttrain's SFC_loss: 0.474866\tvalid's SFC_loss: 0.679585\n",
      "[275]\ttrain's SFC_loss: 0.473944\tvalid's SFC_loss: 0.677602\n",
      "[276]\ttrain's SFC_loss: 0.472906\tvalid's SFC_loss: 0.677122\n",
      "[277]\ttrain's SFC_loss: 0.471855\tvalid's SFC_loss: 0.677099\n",
      "[278]\ttrain's SFC_loss: 0.470843\tvalid's SFC_loss: 0.676634\n",
      "[279]\ttrain's SFC_loss: 0.469759\tvalid's SFC_loss: 0.677388\n",
      "[280]\ttrain's SFC_loss: 0.468621\tvalid's SFC_loss: 0.679362\n",
      "[281]\ttrain's SFC_loss: 0.467764\tvalid's SFC_loss: 0.679757\n",
      "[282]\ttrain's SFC_loss: 0.466966\tvalid's SFC_loss: 0.67963\n",
      "[283]\ttrain's SFC_loss: 0.466189\tvalid's SFC_loss: 0.677934\n",
      "[284]\ttrain's SFC_loss: 0.465083\tvalid's SFC_loss: 0.676414\n",
      "[285]\ttrain's SFC_loss: 0.464225\tvalid's SFC_loss: 0.677025\n",
      "[286]\ttrain's SFC_loss: 0.463235\tvalid's SFC_loss: 0.676973\n",
      "[287]\ttrain's SFC_loss: 0.462316\tvalid's SFC_loss: 0.675228\n",
      "[288]\ttrain's SFC_loss: 0.460995\tvalid's SFC_loss: 0.675584\n",
      "[289]\ttrain's SFC_loss: 0.459908\tvalid's SFC_loss: 0.676603\n",
      "[290]\ttrain's SFC_loss: 0.458989\tvalid's SFC_loss: 0.676979\n",
      "[291]\ttrain's SFC_loss: 0.458374\tvalid's SFC_loss: 0.675208\n",
      "[292]\ttrain's SFC_loss: 0.45766\tvalid's SFC_loss: 0.675381\n",
      "[293]\ttrain's SFC_loss: 0.457032\tvalid's SFC_loss: 0.674035\n",
      "[294]\ttrain's SFC_loss: 0.456233\tvalid's SFC_loss: 0.674638\n",
      "[295]\ttrain's SFC_loss: 0.45565\tvalid's SFC_loss: 0.676182\n",
      "[296]\ttrain's SFC_loss: 0.454989\tvalid's SFC_loss: 0.678402\n",
      "[297]\ttrain's SFC_loss: 0.453965\tvalid's SFC_loss: 0.677627\n",
      "[298]\ttrain's SFC_loss: 0.453588\tvalid's SFC_loss: 0.678444\n",
      "[299]\ttrain's SFC_loss: 0.452826\tvalid's SFC_loss: 0.679505\n",
      "[300]\ttrain's SFC_loss: 0.452441\tvalid's SFC_loss: 0.680753\n",
      "[301]\ttrain's SFC_loss: 0.451368\tvalid's SFC_loss: 0.678915\n",
      "[302]\ttrain's SFC_loss: 0.450811\tvalid's SFC_loss: 0.679604\n",
      "[303]\ttrain's SFC_loss: 0.4499\tvalid's SFC_loss: 0.680526\n",
      "[304]\ttrain's SFC_loss: 0.448947\tvalid's SFC_loss: 0.679081\n",
      "[305]\ttrain's SFC_loss: 0.4479\tvalid's SFC_loss: 0.677756\n",
      "[306]\ttrain's SFC_loss: 0.447053\tvalid's SFC_loss: 0.675855\n",
      "[307]\ttrain's SFC_loss: 0.446095\tvalid's SFC_loss: 0.674775\n",
      "[308]\ttrain's SFC_loss: 0.445002\tvalid's SFC_loss: 0.673198\n",
      "[309]\ttrain's SFC_loss: 0.443917\tvalid's SFC_loss: 0.672248\n",
      "[310]\ttrain's SFC_loss: 0.442931\tvalid's SFC_loss: 0.671484\n",
      "[311]\ttrain's SFC_loss: 0.442203\tvalid's SFC_loss: 0.672446\n",
      "[312]\ttrain's SFC_loss: 0.441105\tvalid's SFC_loss: 0.673073\n",
      "[313]\ttrain's SFC_loss: 0.440401\tvalid's SFC_loss: 0.672931\n",
      "[314]\ttrain's SFC_loss: 0.439479\tvalid's SFC_loss: 0.674172\n",
      "[315]\ttrain's SFC_loss: 0.438827\tvalid's SFC_loss: 0.674852\n",
      "[316]\ttrain's SFC_loss: 0.437859\tvalid's SFC_loss: 0.672848\n",
      "[317]\ttrain's SFC_loss: 0.436948\tvalid's SFC_loss: 0.670422\n",
      "[318]\ttrain's SFC_loss: 0.436057\tvalid's SFC_loss: 0.667808\n",
      "[319]\ttrain's SFC_loss: 0.435369\tvalid's SFC_loss: 0.667741\n",
      "[320]\ttrain's SFC_loss: 0.434567\tvalid's SFC_loss: 0.665603\n",
      "[321]\ttrain's SFC_loss: 0.433723\tvalid's SFC_loss: 0.663041\n",
      "[322]\ttrain's SFC_loss: 0.432932\tvalid's SFC_loss: 0.661891\n",
      "[323]\ttrain's SFC_loss: 0.432128\tvalid's SFC_loss: 0.663054\n",
      "[324]\ttrain's SFC_loss: 0.431285\tvalid's SFC_loss: 0.664066\n",
      "[325]\ttrain's SFC_loss: 0.430778\tvalid's SFC_loss: 0.659751\n",
      "[326]\ttrain's SFC_loss: 0.429798\tvalid's SFC_loss: 0.658547\n",
      "[327]\ttrain's SFC_loss: 0.428524\tvalid's SFC_loss: 0.656636\n",
      "[328]\ttrain's SFC_loss: 0.427538\tvalid's SFC_loss: 0.655808\n",
      "[329]\ttrain's SFC_loss: 0.426569\tvalid's SFC_loss: 0.654846\n",
      "[330]\ttrain's SFC_loss: 0.425581\tvalid's SFC_loss: 0.654112\n",
      "[331]\ttrain's SFC_loss: 0.425011\tvalid's SFC_loss: 0.654763\n",
      "[332]\ttrain's SFC_loss: 0.424331\tvalid's SFC_loss: 0.656168\n",
      "[333]\ttrain's SFC_loss: 0.423739\tvalid's SFC_loss: 0.657278\n",
      "[334]\ttrain's SFC_loss: 0.423017\tvalid's SFC_loss: 0.658804\n",
      "[335]\ttrain's SFC_loss: 0.422385\tvalid's SFC_loss: 0.660224\n",
      "[336]\ttrain's SFC_loss: 0.421607\tvalid's SFC_loss: 0.660419\n",
      "[337]\ttrain's SFC_loss: 0.421149\tvalid's SFC_loss: 0.661262\n",
      "[338]\ttrain's SFC_loss: 0.420168\tvalid's SFC_loss: 0.659528\n",
      "[339]\ttrain's SFC_loss: 0.419475\tvalid's SFC_loss: 0.659464\n",
      "[340]\ttrain's SFC_loss: 0.418823\tvalid's SFC_loss: 0.659439\n",
      "[341]\ttrain's SFC_loss: 0.417904\tvalid's SFC_loss: 0.661569\n",
      "[342]\ttrain's SFC_loss: 0.41702\tvalid's SFC_loss: 0.663713\n",
      "[343]\ttrain's SFC_loss: 0.416171\tvalid's SFC_loss: 0.665872\n",
      "[344]\ttrain's SFC_loss: 0.41544\tvalid's SFC_loss: 0.66445\n",
      "[345]\ttrain's SFC_loss: 0.414636\tvalid's SFC_loss: 0.666627\n",
      "[346]\ttrain's SFC_loss: 0.413628\tvalid's SFC_loss: 0.664895\n",
      "[347]\ttrain's SFC_loss: 0.412654\tvalid's SFC_loss: 0.663193\n",
      "[348]\ttrain's SFC_loss: 0.411712\tvalid's SFC_loss: 0.661522\n",
      "[349]\ttrain's SFC_loss: 0.410802\tvalid's SFC_loss: 0.659881\n",
      "[350]\ttrain's SFC_loss: 0.410036\tvalid's SFC_loss: 0.658831\n",
      "[351]\ttrain's SFC_loss: 0.40945\tvalid's SFC_loss: 0.657024\n",
      "[352]\ttrain's SFC_loss: 0.408933\tvalid's SFC_loss: 0.655581\n",
      "[353]\ttrain's SFC_loss: 0.408386\tvalid's SFC_loss: 0.653834\n",
      "[354]\ttrain's SFC_loss: 0.407956\tvalid's SFC_loss: 0.652\n",
      "[355]\ttrain's SFC_loss: 0.407478\tvalid's SFC_loss: 0.650325\n",
      "[356]\ttrain's SFC_loss: 0.406386\tvalid's SFC_loss: 0.650679\n",
      "[357]\ttrain's SFC_loss: 0.405367\tvalid's SFC_loss: 0.65063\n",
      "[358]\ttrain's SFC_loss: 0.404356\tvalid's SFC_loss: 0.651208\n",
      "[359]\ttrain's SFC_loss: 0.403293\tvalid's SFC_loss: 0.652035\n",
      "[360]\ttrain's SFC_loss: 0.402219\tvalid's SFC_loss: 0.654044\n",
      "[361]\ttrain's SFC_loss: 0.401532\tvalid's SFC_loss: 0.655079\n",
      "[362]\ttrain's SFC_loss: 0.400677\tvalid's SFC_loss: 0.655797\n",
      "[363]\ttrain's SFC_loss: 0.399781\tvalid's SFC_loss: 0.65605\n",
      "[364]\ttrain's SFC_loss: 0.398878\tvalid's SFC_loss: 0.656249\n",
      "[365]\ttrain's SFC_loss: 0.39785\tvalid's SFC_loss: 0.656231\n",
      "[366]\ttrain's SFC_loss: 0.396958\tvalid's SFC_loss: 0.657378\n",
      "[367]\ttrain's SFC_loss: 0.395971\tvalid's SFC_loss: 0.658704\n",
      "[368]\ttrain's SFC_loss: 0.395205\tvalid's SFC_loss: 0.660472\n",
      "[369]\ttrain's SFC_loss: 0.394171\tvalid's SFC_loss: 0.659999\n",
      "[370]\ttrain's SFC_loss: 0.393198\tvalid's SFC_loss: 0.658496\n",
      "[371]\ttrain's SFC_loss: 0.392613\tvalid's SFC_loss: 0.657509\n",
      "[372]\ttrain's SFC_loss: 0.391744\tvalid's SFC_loss: 0.657432\n",
      "[373]\ttrain's SFC_loss: 0.391051\tvalid's SFC_loss: 0.657072\n",
      "[374]\ttrain's SFC_loss: 0.390336\tvalid's SFC_loss: 0.658491\n",
      "[375]\ttrain's SFC_loss: 0.3897\tvalid's SFC_loss: 0.658814\n",
      "[376]\ttrain's SFC_loss: 0.388951\tvalid's SFC_loss: 0.660087\n",
      "[377]\ttrain's SFC_loss: 0.388057\tvalid's SFC_loss: 0.659547\n",
      "[378]\ttrain's SFC_loss: 0.387433\tvalid's SFC_loss: 0.658348\n",
      "[379]\ttrain's SFC_loss: 0.386743\tvalid's SFC_loss: 0.657533\n",
      "[380]\ttrain's SFC_loss: 0.386127\tvalid's SFC_loss: 0.658276\n",
      "[381]\ttrain's SFC_loss: 0.385368\tvalid's SFC_loss: 0.659518\n",
      "[382]\ttrain's SFC_loss: 0.384659\tvalid's SFC_loss: 0.65913\n",
      "[383]\ttrain's SFC_loss: 0.383881\tvalid's SFC_loss: 0.660629\n",
      "[384]\ttrain's SFC_loss: 0.383188\tvalid's SFC_loss: 0.661935\n",
      "[385]\ttrain's SFC_loss: 0.382497\tvalid's SFC_loss: 0.663492\n",
      "[386]\ttrain's SFC_loss: 0.381717\tvalid's SFC_loss: 0.664155\n",
      "[387]\ttrain's SFC_loss: 0.381009\tvalid's SFC_loss: 0.664565\n",
      "[388]\ttrain's SFC_loss: 0.380185\tvalid's SFC_loss: 0.665073\n",
      "[389]\ttrain's SFC_loss: 0.379396\tvalid's SFC_loss: 0.664795\n",
      "[390]\ttrain's SFC_loss: 0.378362\tvalid's SFC_loss: 0.665426\n",
      "[391]\ttrain's SFC_loss: 0.37784\tvalid's SFC_loss: 0.66604\n",
      "[392]\ttrain's SFC_loss: 0.376899\tvalid's SFC_loss: 0.666194\n",
      "[393]\ttrain's SFC_loss: 0.376117\tvalid's SFC_loss: 0.666894\n",
      "[394]\ttrain's SFC_loss: 0.375492\tvalid's SFC_loss: 0.669126\n",
      "[395]\ttrain's SFC_loss: 0.375251\tvalid's SFC_loss: 0.669334\n",
      "[396]\ttrain's SFC_loss: 0.37443\tvalid's SFC_loss: 0.669343\n",
      "[397]\ttrain's SFC_loss: 0.373671\tvalid's SFC_loss: 0.66981\n",
      "[398]\ttrain's SFC_loss: 0.372822\tvalid's SFC_loss: 0.671555\n",
      "[399]\ttrain's SFC_loss: 0.372195\tvalid's SFC_loss: 0.671682\n",
      "[400]\ttrain's SFC_loss: 0.371522\tvalid's SFC_loss: 0.671178\n",
      "[401]\ttrain's SFC_loss: 0.371311\tvalid's SFC_loss: 0.673361\n",
      "[402]\ttrain's SFC_loss: 0.37106\tvalid's SFC_loss: 0.675112\n",
      "[403]\ttrain's SFC_loss: 0.371099\tvalid's SFC_loss: 0.674729\n",
      "[404]\ttrain's SFC_loss: 0.370826\tvalid's SFC_loss: 0.675068\n",
      "[405]\ttrain's SFC_loss: 0.370728\tvalid's SFC_loss: 0.674797\n",
      "[406]\ttrain's SFC_loss: 0.36961\tvalid's SFC_loss: 0.673668\n",
      "[407]\ttrain's SFC_loss: 0.368612\tvalid's SFC_loss: 0.671829\n",
      "[408]\ttrain's SFC_loss: 0.367629\tvalid's SFC_loss: 0.670901\n",
      "[409]\ttrain's SFC_loss: 0.366605\tvalid's SFC_loss: 0.669542\n",
      "[410]\ttrain's SFC_loss: 0.365442\tvalid's SFC_loss: 0.665615\n",
      "[411]\ttrain's SFC_loss: 0.364806\tvalid's SFC_loss: 0.665786\n",
      "[412]\ttrain's SFC_loss: 0.364177\tvalid's SFC_loss: 0.665563\n",
      "[413]\ttrain's SFC_loss: 0.363608\tvalid's SFC_loss: 0.665228\n",
      "[414]\ttrain's SFC_loss: 0.36302\tvalid's SFC_loss: 0.665023\n",
      "[415]\ttrain's SFC_loss: 0.362506\tvalid's SFC_loss: 0.665585\n",
      "[416]\ttrain's SFC_loss: 0.361821\tvalid's SFC_loss: 0.66605\n",
      "[417]\ttrain's SFC_loss: 0.36109\tvalid's SFC_loss: 0.665017\n",
      "[418]\ttrain's SFC_loss: 0.360351\tvalid's SFC_loss: 0.66472\n",
      "[419]\ttrain's SFC_loss: 0.359326\tvalid's SFC_loss: 0.663431\n",
      "[420]\ttrain's SFC_loss: 0.358793\tvalid's SFC_loss: 0.663774\n",
      "[421]\ttrain's SFC_loss: 0.358069\tvalid's SFC_loss: 0.663788\n",
      "[422]\ttrain's SFC_loss: 0.357386\tvalid's SFC_loss: 0.664699\n",
      "[423]\ttrain's SFC_loss: 0.356437\tvalid's SFC_loss: 0.662848\n",
      "[424]\ttrain's SFC_loss: 0.355538\tvalid's SFC_loss: 0.662375\n",
      "[425]\ttrain's SFC_loss: 0.354709\tvalid's SFC_loss: 0.661646\n",
      "[426]\ttrain's SFC_loss: 0.353961\tvalid's SFC_loss: 0.662081\n",
      "[427]\ttrain's SFC_loss: 0.353336\tvalid's SFC_loss: 0.663423\n",
      "[428]\ttrain's SFC_loss: 0.35263\tvalid's SFC_loss: 0.66316\n",
      "[429]\ttrain's SFC_loss: 0.351949\tvalid's SFC_loss: 0.66358\n",
      "[430]\ttrain's SFC_loss: 0.351107\tvalid's SFC_loss: 0.66336\n",
      "[431]\ttrain's SFC_loss: 0.350595\tvalid's SFC_loss: 0.661063\n",
      "[432]\ttrain's SFC_loss: 0.349953\tvalid's SFC_loss: 0.658727\n",
      "[433]\ttrain's SFC_loss: 0.349433\tvalid's SFC_loss: 0.657544\n",
      "[434]\ttrain's SFC_loss: 0.348819\tvalid's SFC_loss: 0.656936\n",
      "[435]\ttrain's SFC_loss: 0.348569\tvalid's SFC_loss: 0.658599\n",
      "[436]\ttrain's SFC_loss: 0.347779\tvalid's SFC_loss: 0.656362\n",
      "[437]\ttrain's SFC_loss: 0.347018\tvalid's SFC_loss: 0.654161\n",
      "[438]\ttrain's SFC_loss: 0.346555\tvalid's SFC_loss: 0.653737\n",
      "[439]\ttrain's SFC_loss: 0.346041\tvalid's SFC_loss: 0.653926\n",
      "[440]\ttrain's SFC_loss: 0.345297\tvalid's SFC_loss: 0.652105\n",
      "[441]\ttrain's SFC_loss: 0.344749\tvalid's SFC_loss: 0.65025\n",
      "[442]\ttrain's SFC_loss: 0.344228\tvalid's SFC_loss: 0.649495\n",
      "[443]\ttrain's SFC_loss: 0.343653\tvalid's SFC_loss: 0.648669\n",
      "[444]\ttrain's SFC_loss: 0.34334\tvalid's SFC_loss: 0.648396\n",
      "[445]\ttrain's SFC_loss: 0.342828\tvalid's SFC_loss: 0.648239\n",
      "[446]\ttrain's SFC_loss: 0.342047\tvalid's SFC_loss: 0.64875\n",
      "[447]\ttrain's SFC_loss: 0.341473\tvalid's SFC_loss: 0.648001\n",
      "[448]\ttrain's SFC_loss: 0.340889\tvalid's SFC_loss: 0.646197\n",
      "[449]\ttrain's SFC_loss: 0.340266\tvalid's SFC_loss: 0.645969\n",
      "[450]\ttrain's SFC_loss: 0.339462\tvalid's SFC_loss: 0.645132\n",
      "[451]\ttrain's SFC_loss: 0.338808\tvalid's SFC_loss: 0.645562\n",
      "[452]\ttrain's SFC_loss: 0.338344\tvalid's SFC_loss: 0.644797\n",
      "[453]\ttrain's SFC_loss: 0.337631\tvalid's SFC_loss: 0.643319\n",
      "[454]\ttrain's SFC_loss: 0.336934\tvalid's SFC_loss: 0.644468\n",
      "[455]\ttrain's SFC_loss: 0.336217\tvalid's SFC_loss: 0.642826\n",
      "[456]\ttrain's SFC_loss: 0.335471\tvalid's SFC_loss: 0.640767\n",
      "[457]\ttrain's SFC_loss: 0.334925\tvalid's SFC_loss: 0.639085\n",
      "[458]\ttrain's SFC_loss: 0.334548\tvalid's SFC_loss: 0.637549\n",
      "[459]\ttrain's SFC_loss: 0.333804\tvalid's SFC_loss: 0.63596\n",
      "[460]\ttrain's SFC_loss: 0.332958\tvalid's SFC_loss: 0.634825\n",
      "[461]\ttrain's SFC_loss: 0.332707\tvalid's SFC_loss: 0.633956\n",
      "[462]\ttrain's SFC_loss: 0.332462\tvalid's SFC_loss: 0.634602\n",
      "[463]\ttrain's SFC_loss: 0.332236\tvalid's SFC_loss: 0.633763\n",
      "[464]\ttrain's SFC_loss: 0.331895\tvalid's SFC_loss: 0.633217\n",
      "[465]\ttrain's SFC_loss: 0.331685\tvalid's SFC_loss: 0.633871\n",
      "[466]\ttrain's SFC_loss: 0.330875\tvalid's SFC_loss: 0.63367\n",
      "[467]\ttrain's SFC_loss: 0.329978\tvalid's SFC_loss: 0.633079\n",
      "[468]\ttrain's SFC_loss: 0.329373\tvalid's SFC_loss: 0.633574\n",
      "[469]\ttrain's SFC_loss: 0.328836\tvalid's SFC_loss: 0.634564\n",
      "[470]\ttrain's SFC_loss: 0.328242\tvalid's SFC_loss: 0.633887\n",
      "[471]\ttrain's SFC_loss: 0.327872\tvalid's SFC_loss: 0.634823\n",
      "[472]\ttrain's SFC_loss: 0.327264\tvalid's SFC_loss: 0.63499\n",
      "[473]\ttrain's SFC_loss: 0.32679\tvalid's SFC_loss: 0.635068\n",
      "[474]\ttrain's SFC_loss: 0.326367\tvalid's SFC_loss: 0.635278\n",
      "[475]\ttrain's SFC_loss: 0.325917\tvalid's SFC_loss: 0.636157\n",
      "[476]\ttrain's SFC_loss: 0.325121\tvalid's SFC_loss: 0.636307\n",
      "[477]\ttrain's SFC_loss: 0.324489\tvalid's SFC_loss: 0.637255\n",
      "[478]\ttrain's SFC_loss: 0.323619\tvalid's SFC_loss: 0.635927\n",
      "[479]\ttrain's SFC_loss: 0.322701\tvalid's SFC_loss: 0.634906\n",
      "[480]\ttrain's SFC_loss: 0.322032\tvalid's SFC_loss: 0.634226\n",
      "[481]\ttrain's SFC_loss: 0.321545\tvalid's SFC_loss: 0.632522\n",
      "[482]\ttrain's SFC_loss: 0.321237\tvalid's SFC_loss: 0.632507\n",
      "[483]\ttrain's SFC_loss: 0.320886\tvalid's SFC_loss: 0.63243\n",
      "[484]\ttrain's SFC_loss: 0.320512\tvalid's SFC_loss: 0.633374\n",
      "[485]\ttrain's SFC_loss: 0.320126\tvalid's SFC_loss: 0.632953\n",
      "[486]\ttrain's SFC_loss: 0.319921\tvalid's SFC_loss: 0.632667\n",
      "[487]\ttrain's SFC_loss: 0.319558\tvalid's SFC_loss: 0.633869\n",
      "[488]\ttrain's SFC_loss: 0.319273\tvalid's SFC_loss: 0.635493\n",
      "[489]\ttrain's SFC_loss: 0.318964\tvalid's SFC_loss: 0.636426\n",
      "[490]\ttrain's SFC_loss: 0.318567\tvalid's SFC_loss: 0.63563\n",
      "[491]\ttrain's SFC_loss: 0.317992\tvalid's SFC_loss: 0.635265\n",
      "[492]\ttrain's SFC_loss: 0.317375\tvalid's SFC_loss: 0.634259\n",
      "[493]\ttrain's SFC_loss: 0.316769\tvalid's SFC_loss: 0.633276\n",
      "[494]\ttrain's SFC_loss: 0.316146\tvalid's SFC_loss: 0.632633\n",
      "[495]\ttrain's SFC_loss: 0.315659\tvalid's SFC_loss: 0.632839\n",
      "[496]\ttrain's SFC_loss: 0.315215\tvalid's SFC_loss: 0.632797\n",
      "[497]\ttrain's SFC_loss: 0.314855\tvalid's SFC_loss: 0.632112\n",
      "[498]\ttrain's SFC_loss: 0.314455\tvalid's SFC_loss: 0.632714\n",
      "[499]\ttrain's SFC_loss: 0.314112\tvalid's SFC_loss: 0.633295\n",
      "[500]\ttrain's SFC_loss: 0.313761\tvalid's SFC_loss: 0.633389\n",
      "[501]\ttrain's SFC_loss: 0.312993\tvalid's SFC_loss: 0.633721\n",
      "[502]\ttrain's SFC_loss: 0.312685\tvalid's SFC_loss: 0.633364\n",
      "[503]\ttrain's SFC_loss: 0.312077\tvalid's SFC_loss: 0.632044\n",
      "[504]\ttrain's SFC_loss: 0.311417\tvalid's SFC_loss: 0.632316\n",
      "[505]\ttrain's SFC_loss: 0.311001\tvalid's SFC_loss: 0.632215\n",
      "[506]\ttrain's SFC_loss: 0.310677\tvalid's SFC_loss: 0.633172\n",
      "[507]\ttrain's SFC_loss: 0.310264\tvalid's SFC_loss: 0.63306\n",
      "[508]\ttrain's SFC_loss: 0.309907\tvalid's SFC_loss: 0.632968\n",
      "[509]\ttrain's SFC_loss: 0.309348\tvalid's SFC_loss: 0.631352\n",
      "[510]\ttrain's SFC_loss: 0.30867\tvalid's SFC_loss: 0.628777\n",
      "[511]\ttrain's SFC_loss: 0.308044\tvalid's SFC_loss: 0.62733\n",
      "[512]\ttrain's SFC_loss: 0.307447\tvalid's SFC_loss: 0.626299\n",
      "[513]\ttrain's SFC_loss: 0.306666\tvalid's SFC_loss: 0.624795\n",
      "[514]\ttrain's SFC_loss: 0.306064\tvalid's SFC_loss: 0.62479\n",
      "[515]\ttrain's SFC_loss: 0.305419\tvalid's SFC_loss: 0.624534\n",
      "[516]\ttrain's SFC_loss: 0.304979\tvalid's SFC_loss: 0.624171\n",
      "[517]\ttrain's SFC_loss: 0.304599\tvalid's SFC_loss: 0.622637\n",
      "[518]\ttrain's SFC_loss: 0.304178\tvalid's SFC_loss: 0.623603\n",
      "[519]\ttrain's SFC_loss: 0.303714\tvalid's SFC_loss: 0.622104\n",
      "[520]\ttrain's SFC_loss: 0.303413\tvalid's SFC_loss: 0.622507\n",
      "[521]\ttrain's SFC_loss: 0.302746\tvalid's SFC_loss: 0.62159\n",
      "[522]\ttrain's SFC_loss: 0.302094\tvalid's SFC_loss: 0.620527\n",
      "[523]\ttrain's SFC_loss: 0.30153\tvalid's SFC_loss: 0.620927\n",
      "[524]\ttrain's SFC_loss: 0.301019\tvalid's SFC_loss: 0.620299\n",
      "[525]\ttrain's SFC_loss: 0.300466\tvalid's SFC_loss: 0.618999\n",
      "[526]\ttrain's SFC_loss: 0.299968\tvalid's SFC_loss: 0.619326\n",
      "[527]\ttrain's SFC_loss: 0.299309\tvalid's SFC_loss: 0.619717\n",
      "[528]\ttrain's SFC_loss: 0.29879\tvalid's SFC_loss: 0.618996\n",
      "[529]\ttrain's SFC_loss: 0.298531\tvalid's SFC_loss: 0.617264\n",
      "[530]\ttrain's SFC_loss: 0.297998\tvalid's SFC_loss: 0.617188\n",
      "[531]\ttrain's SFC_loss: 0.297573\tvalid's SFC_loss: 0.61581\n",
      "[532]\ttrain's SFC_loss: 0.297347\tvalid's SFC_loss: 0.615308\n",
      "[533]\ttrain's SFC_loss: 0.29678\tvalid's SFC_loss: 0.612235\n",
      "[534]\ttrain's SFC_loss: 0.296294\tvalid's SFC_loss: 0.610107\n",
      "[535]\ttrain's SFC_loss: 0.295784\tvalid's SFC_loss: 0.606732\n",
      "[536]\ttrain's SFC_loss: 0.295587\tvalid's SFC_loss: 0.606757\n",
      "[537]\ttrain's SFC_loss: 0.295323\tvalid's SFC_loss: 0.606417\n",
      "[538]\ttrain's SFC_loss: 0.294947\tvalid's SFC_loss: 0.607616\n",
      "[539]\ttrain's SFC_loss: 0.294622\tvalid's SFC_loss: 0.608685\n",
      "[540]\ttrain's SFC_loss: 0.294291\tvalid's SFC_loss: 0.609793\n",
      "[541]\ttrain's SFC_loss: 0.293825\tvalid's SFC_loss: 0.609409\n",
      "[542]\ttrain's SFC_loss: 0.293269\tvalid's SFC_loss: 0.606554\n",
      "[543]\ttrain's SFC_loss: 0.292772\tvalid's SFC_loss: 0.604507\n",
      "[544]\ttrain's SFC_loss: 0.29232\tvalid's SFC_loss: 0.603363\n",
      "[545]\ttrain's SFC_loss: 0.29163\tvalid's SFC_loss: 0.600505\n",
      "[546]\ttrain's SFC_loss: 0.291096\tvalid's SFC_loss: 0.600399\n",
      "[547]\ttrain's SFC_loss: 0.290735\tvalid's SFC_loss: 0.599589\n",
      "[548]\ttrain's SFC_loss: 0.290066\tvalid's SFC_loss: 0.599032\n",
      "[549]\ttrain's SFC_loss: 0.289461\tvalid's SFC_loss: 0.59769\n",
      "[550]\ttrain's SFC_loss: 0.289151\tvalid's SFC_loss: 0.596733\n",
      "[551]\ttrain's SFC_loss: 0.288581\tvalid's SFC_loss: 0.595278\n",
      "[552]\ttrain's SFC_loss: 0.287968\tvalid's SFC_loss: 0.597028\n",
      "[553]\ttrain's SFC_loss: 0.287356\tvalid's SFC_loss: 0.596426\n",
      "[554]\ttrain's SFC_loss: 0.286857\tvalid's SFC_loss: 0.596743\n",
      "[555]\ttrain's SFC_loss: 0.286363\tvalid's SFC_loss: 0.596211\n",
      "[556]\ttrain's SFC_loss: 0.285894\tvalid's SFC_loss: 0.596134\n",
      "[557]\ttrain's SFC_loss: 0.285355\tvalid's SFC_loss: 0.593662\n",
      "[558]\ttrain's SFC_loss: 0.284792\tvalid's SFC_loss: 0.591354\n",
      "[559]\ttrain's SFC_loss: 0.284255\tvalid's SFC_loss: 0.589421\n",
      "[560]\ttrain's SFC_loss: 0.283909\tvalid's SFC_loss: 0.590026\n",
      "[561]\ttrain's SFC_loss: 0.283446\tvalid's SFC_loss: 0.589828\n",
      "[562]\ttrain's SFC_loss: 0.283056\tvalid's SFC_loss: 0.588263\n",
      "[563]\ttrain's SFC_loss: 0.282589\tvalid's SFC_loss: 0.587495\n",
      "[564]\ttrain's SFC_loss: 0.282236\tvalid's SFC_loss: 0.586932\n",
      "[565]\ttrain's SFC_loss: 0.281736\tvalid's SFC_loss: 0.587326\n",
      "[566]\ttrain's SFC_loss: 0.281316\tvalid's SFC_loss: 0.587324\n",
      "[567]\ttrain's SFC_loss: 0.280991\tvalid's SFC_loss: 0.586099\n",
      "[568]\ttrain's SFC_loss: 0.280513\tvalid's SFC_loss: 0.586129\n",
      "[569]\ttrain's SFC_loss: 0.280015\tvalid's SFC_loss: 0.587243\n",
      "[570]\ttrain's SFC_loss: 0.2797\tvalid's SFC_loss: 0.586444\n",
      "[571]\ttrain's SFC_loss: 0.279157\tvalid's SFC_loss: 0.586314\n",
      "[572]\ttrain's SFC_loss: 0.278632\tvalid's SFC_loss: 0.585235\n",
      "[573]\ttrain's SFC_loss: 0.278002\tvalid's SFC_loss: 0.584157\n",
      "[574]\ttrain's SFC_loss: 0.277553\tvalid's SFC_loss: 0.582795\n",
      "[575]\ttrain's SFC_loss: 0.277174\tvalid's SFC_loss: 0.582761\n",
      "[576]\ttrain's SFC_loss: 0.276637\tvalid's SFC_loss: 0.582478\n",
      "[577]\ttrain's SFC_loss: 0.276211\tvalid's SFC_loss: 0.580977\n",
      "[578]\ttrain's SFC_loss: 0.275712\tvalid's SFC_loss: 0.581361\n",
      "[579]\ttrain's SFC_loss: 0.275247\tvalid's SFC_loss: 0.580167\n",
      "[580]\ttrain's SFC_loss: 0.274894\tvalid's SFC_loss: 0.579936\n",
      "[581]\ttrain's SFC_loss: 0.274481\tvalid's SFC_loss: 0.58001\n",
      "[582]\ttrain's SFC_loss: 0.274059\tvalid's SFC_loss: 0.581025\n",
      "[583]\ttrain's SFC_loss: 0.273561\tvalid's SFC_loss: 0.58105\n",
      "[584]\ttrain's SFC_loss: 0.273149\tvalid's SFC_loss: 0.58177\n",
      "[585]\ttrain's SFC_loss: 0.27274\tvalid's SFC_loss: 0.583656\n",
      "[586]\ttrain's SFC_loss: 0.271967\tvalid's SFC_loss: 0.583364\n",
      "[587]\ttrain's SFC_loss: 0.271346\tvalid's SFC_loss: 0.58341\n",
      "[588]\ttrain's SFC_loss: 0.270942\tvalid's SFC_loss: 0.583601\n",
      "[589]\ttrain's SFC_loss: 0.27035\tvalid's SFC_loss: 0.58279\n",
      "[590]\ttrain's SFC_loss: 0.269652\tvalid's SFC_loss: 0.581538\n",
      "[591]\ttrain's SFC_loss: 0.269317\tvalid's SFC_loss: 0.583301\n",
      "[592]\ttrain's SFC_loss: 0.268963\tvalid's SFC_loss: 0.58521\n",
      "[593]\ttrain's SFC_loss: 0.26862\tvalid's SFC_loss: 0.585313\n",
      "[594]\ttrain's SFC_loss: 0.268312\tvalid's SFC_loss: 0.585498\n",
      "[595]\ttrain's SFC_loss: 0.267981\tvalid's SFC_loss: 0.585024\n",
      "[596]\ttrain's SFC_loss: 0.267508\tvalid's SFC_loss: 0.585738\n",
      "[597]\ttrain's SFC_loss: 0.267254\tvalid's SFC_loss: 0.585098\n",
      "[598]\ttrain's SFC_loss: 0.2669\tvalid's SFC_loss: 0.586346\n",
      "[599]\ttrain's SFC_loss: 0.266136\tvalid's SFC_loss: 0.585616\n",
      "[600]\ttrain's SFC_loss: 0.265395\tvalid's SFC_loss: 0.584911\n",
      "[601]\ttrain's SFC_loss: 0.264916\tvalid's SFC_loss: 0.586407\n",
      "[602]\ttrain's SFC_loss: 0.264319\tvalid's SFC_loss: 0.587651\n",
      "[603]\ttrain's SFC_loss: 0.263803\tvalid's SFC_loss: 0.588778\n",
      "[604]\ttrain's SFC_loss: 0.263262\tvalid's SFC_loss: 0.588544\n",
      "[605]\ttrain's SFC_loss: 0.262824\tvalid's SFC_loss: 0.586254\n",
      "[606]\ttrain's SFC_loss: 0.262497\tvalid's SFC_loss: 0.587665\n",
      "[607]\ttrain's SFC_loss: 0.262117\tvalid's SFC_loss: 0.588852\n",
      "[608]\ttrain's SFC_loss: 0.261852\tvalid's SFC_loss: 0.588623\n",
      "[609]\ttrain's SFC_loss: 0.261494\tvalid's SFC_loss: 0.589939\n",
      "[610]\ttrain's SFC_loss: 0.261181\tvalid's SFC_loss: 0.591514\n",
      "[611]\ttrain's SFC_loss: 0.260903\tvalid's SFC_loss: 0.590496\n",
      "[612]\ttrain's SFC_loss: 0.260622\tvalid's SFC_loss: 0.589412\n",
      "[613]\ttrain's SFC_loss: 0.260131\tvalid's SFC_loss: 0.58902\n",
      "[614]\ttrain's SFC_loss: 0.259822\tvalid's SFC_loss: 0.588946\n",
      "[615]\ttrain's SFC_loss: 0.259271\tvalid's SFC_loss: 0.590265\n",
      "[616]\ttrain's SFC_loss: 0.258908\tvalid's SFC_loss: 0.591052\n",
      "[617]\ttrain's SFC_loss: 0.258462\tvalid's SFC_loss: 0.591838\n",
      "[618]\ttrain's SFC_loss: 0.25807\tvalid's SFC_loss: 0.59119\n",
      "[619]\ttrain's SFC_loss: 0.257778\tvalid's SFC_loss: 0.59247\n",
      "[620]\ttrain's SFC_loss: 0.257432\tvalid's SFC_loss: 0.59254\n",
      "[621]\ttrain's SFC_loss: 0.257083\tvalid's SFC_loss: 0.592907\n",
      "[622]\ttrain's SFC_loss: 0.256794\tvalid's SFC_loss: 0.594161\n",
      "[623]\ttrain's SFC_loss: 0.256465\tvalid's SFC_loss: 0.593952\n",
      "[624]\ttrain's SFC_loss: 0.255915\tvalid's SFC_loss: 0.594239\n",
      "[625]\ttrain's SFC_loss: 0.255465\tvalid's SFC_loss: 0.59441\n",
      "[626]\ttrain's SFC_loss: 0.25492\tvalid's SFC_loss: 0.593183\n",
      "[627]\ttrain's SFC_loss: 0.25453\tvalid's SFC_loss: 0.592931\n",
      "[628]\ttrain's SFC_loss: 0.254079\tvalid's SFC_loss: 0.592941\n",
      "[629]\ttrain's SFC_loss: 0.253713\tvalid's SFC_loss: 0.592674\n",
      "[630]\ttrain's SFC_loss: 0.253273\tvalid's SFC_loss: 0.59294\n",
      "[631]\ttrain's SFC_loss: 0.25284\tvalid's SFC_loss: 0.594007\n",
      "[632]\ttrain's SFC_loss: 0.252509\tvalid's SFC_loss: 0.59341\n",
      "[633]\ttrain's SFC_loss: 0.252083\tvalid's SFC_loss: 0.593828\n",
      "[634]\ttrain's SFC_loss: 0.251879\tvalid's SFC_loss: 0.59448\n",
      "[635]\ttrain's SFC_loss: 0.251485\tvalid's SFC_loss: 0.593185\n",
      "[636]\ttrain's SFC_loss: 0.250988\tvalid's SFC_loss: 0.591676\n",
      "[637]\ttrain's SFC_loss: 0.250503\tvalid's SFC_loss: 0.589959\n",
      "[638]\ttrain's SFC_loss: 0.250002\tvalid's SFC_loss: 0.589374\n",
      "[639]\ttrain's SFC_loss: 0.249547\tvalid's SFC_loss: 0.588585\n",
      "[640]\ttrain's SFC_loss: 0.248986\tvalid's SFC_loss: 0.587225\n",
      "[641]\ttrain's SFC_loss: 0.248694\tvalid's SFC_loss: 0.587374\n",
      "[642]\ttrain's SFC_loss: 0.248268\tvalid's SFC_loss: 0.58874\n",
      "[643]\ttrain's SFC_loss: 0.247818\tvalid's SFC_loss: 0.589062\n",
      "[644]\ttrain's SFC_loss: 0.247546\tvalid's SFC_loss: 0.589921\n",
      "[645]\ttrain's SFC_loss: 0.247226\tvalid's SFC_loss: 0.589675\n",
      "[646]\ttrain's SFC_loss: 0.246801\tvalid's SFC_loss: 0.590655\n",
      "[647]\ttrain's SFC_loss: 0.246426\tvalid's SFC_loss: 0.591563\n",
      "[648]\ttrain's SFC_loss: 0.246063\tvalid's SFC_loss: 0.59248\n",
      "[649]\ttrain's SFC_loss: 0.245885\tvalid's SFC_loss: 0.593395\n",
      "[650]\ttrain's SFC_loss: 0.245543\tvalid's SFC_loss: 0.594094\n",
      "[651]\ttrain's SFC_loss: 0.245335\tvalid's SFC_loss: 0.594687\n",
      "[652]\ttrain's SFC_loss: 0.245089\tvalid's SFC_loss: 0.594262\n",
      "[653]\ttrain's SFC_loss: 0.244851\tvalid's SFC_loss: 0.59554\n",
      "[654]\ttrain's SFC_loss: 0.244619\tvalid's SFC_loss: 0.597372\n",
      "[655]\ttrain's SFC_loss: 0.244392\tvalid's SFC_loss: 0.596966\n",
      "[656]\ttrain's SFC_loss: 0.243959\tvalid's SFC_loss: 0.59901\n",
      "[657]\ttrain's SFC_loss: 0.243444\tvalid's SFC_loss: 0.597682\n",
      "[658]\ttrain's SFC_loss: 0.242947\tvalid's SFC_loss: 0.599474\n",
      "[659]\ttrain's SFC_loss: 0.242574\tvalid's SFC_loss: 0.601561\n",
      "[660]\ttrain's SFC_loss: 0.242129\tvalid's SFC_loss: 0.602759\n",
      "[661]\ttrain's SFC_loss: 0.241812\tvalid's SFC_loss: 0.604993\n",
      "[662]\ttrain's SFC_loss: 0.241494\tvalid's SFC_loss: 0.605303\n",
      "[663]\ttrain's SFC_loss: 0.241013\tvalid's SFC_loss: 0.60492\n",
      "[664]\ttrain's SFC_loss: 0.240637\tvalid's SFC_loss: 0.604528\n",
      "[665]\ttrain's SFC_loss: 0.240203\tvalid's SFC_loss: 0.60501\n",
      "[666]\ttrain's SFC_loss: 0.239812\tvalid's SFC_loss: 0.603538\n",
      "[667]\ttrain's SFC_loss: 0.23956\tvalid's SFC_loss: 0.601954\n",
      "[668]\ttrain's SFC_loss: 0.239247\tvalid's SFC_loss: 0.600449\n",
      "[669]\ttrain's SFC_loss: 0.238913\tvalid's SFC_loss: 0.599036\n",
      "[670]\ttrain's SFC_loss: 0.238636\tvalid's SFC_loss: 0.598432\n",
      "[671]\ttrain's SFC_loss: 0.238131\tvalid's SFC_loss: 0.597881\n",
      "[672]\ttrain's SFC_loss: 0.237685\tvalid's SFC_loss: 0.598779\n",
      "[673]\ttrain's SFC_loss: 0.2372\tvalid's SFC_loss: 0.598984\n",
      "[674]\ttrain's SFC_loss: 0.236745\tvalid's SFC_loss: 0.599119\n",
      "[675]\ttrain's SFC_loss: 0.236336\tvalid's SFC_loss: 0.599691\n",
      "[676]\ttrain's SFC_loss: 0.235734\tvalid's SFC_loss: 0.598507\n",
      "[677]\ttrain's SFC_loss: 0.235252\tvalid's SFC_loss: 0.598195\n",
      "[678]\ttrain's SFC_loss: 0.23464\tvalid's SFC_loss: 0.598837\n",
      "[679]\ttrain's SFC_loss: 0.234142\tvalid's SFC_loss: 0.598194\n",
      "[680]\ttrain's SFC_loss: 0.233761\tvalid's SFC_loss: 0.597901\n",
      "Early stopping, best iteration is:\n",
      "[580]\ttrain's SFC_loss: 0.274894\tvalid's SFC_loss: 0.579936\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.8\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1   -0.053991\n",
      "1    feature2   -0.087162\n",
      "2    feature3   -0.053903\n",
      "3    feature4   -0.011884\n",
      "4    feature5    0.017776\n",
      "5    feature6   -0.030136\n",
      "6    feature7    0.007532\n",
      "7    feature8   -0.101787\n",
      "8    feature9    0.074838\n",
      "9   feature10   -0.068404\n",
      "10  feature11    0.038222\n",
      "11  feature12   -0.039170\n",
      "12  feature13    0.035323\n",
      "13  feature14    0.026590\n",
      "14  feature15    0.176842\n",
      "15  feature16    0.065628\n",
      "16  feature17    0.010180\n",
      "17  feature18   -0.019395\n",
      "18  feature19   -0.034167\n",
      "19  feature20    0.047069\n",
      "[[0.975 0.025 0.   ]\n",
      " [0.025 0.95  0.025]\n",
      " [0.    0.025 0.975]]\n",
      "-------------------- 0 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.0579\tvalid's multi_logloss: 1.0629\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05439\tvalid's multi_logloss: 1.06299\n",
      "[3]\ttrain's multi_logloss: 1.05121\tvalid's multi_logloss: 1.05944\n",
      "[4]\ttrain's multi_logloss: 1.04779\tvalid's multi_logloss: 1.05866\n",
      "[5]\ttrain's multi_logloss: 1.04472\tvalid's multi_logloss: 1.05578\n",
      "[6]\ttrain's multi_logloss: 1.04058\tvalid's multi_logloss: 1.05157\n",
      "[7]\ttrain's multi_logloss: 1.03664\tvalid's multi_logloss: 1.0463\n",
      "[8]\ttrain's multi_logloss: 1.03318\tvalid's multi_logloss: 1.04168\n",
      "[9]\ttrain's multi_logloss: 1.03007\tvalid's multi_logloss: 1.04009\n",
      "[10]\ttrain's multi_logloss: 1.02676\tvalid's multi_logloss: 1.03542\n",
      "[11]\ttrain's multi_logloss: 1.02308\tvalid's multi_logloss: 1.03143\n",
      "[12]\ttrain's multi_logloss: 1.01918\tvalid's multi_logloss: 1.02992\n",
      "[13]\ttrain's multi_logloss: 1.01504\tvalid's multi_logloss: 1.02854\n",
      "[14]\ttrain's multi_logloss: 1.01109\tvalid's multi_logloss: 1.02578\n",
      "[15]\ttrain's multi_logloss: 1.00723\tvalid's multi_logloss: 1.02445\n",
      "[16]\ttrain's multi_logloss: 1.00308\tvalid's multi_logloss: 1.02301\n",
      "[17]\ttrain's multi_logloss: 0.998978\tvalid's multi_logloss: 1.02082\n",
      "[18]\ttrain's multi_logloss: 0.994621\tvalid's multi_logloss: 1.01662\n",
      "[19]\ttrain's multi_logloss: 0.991628\tvalid's multi_logloss: 1.01514\n",
      "[20]\ttrain's multi_logloss: 0.988213\tvalid's multi_logloss: 1.01474\n",
      "[21]\ttrain's multi_logloss: 0.983597\tvalid's multi_logloss: 1.01214\n",
      "[22]\ttrain's multi_logloss: 0.979265\tvalid's multi_logloss: 1.00906\n",
      "[23]\ttrain's multi_logloss: 0.975583\tvalid's multi_logloss: 1.00693\n",
      "[24]\ttrain's multi_logloss: 0.971409\tvalid's multi_logloss: 1.00395\n",
      "[25]\ttrain's multi_logloss: 0.967355\tvalid's multi_logloss: 1.00337\n",
      "[26]\ttrain's multi_logloss: 0.964013\tvalid's multi_logloss: 1.0041\n",
      "[27]\ttrain's multi_logloss: 0.960996\tvalid's multi_logloss: 1.00487\n",
      "[28]\ttrain's multi_logloss: 0.958557\tvalid's multi_logloss: 1.00534\n",
      "[29]\ttrain's multi_logloss: 0.955595\tvalid's multi_logloss: 1.0038\n",
      "[30]\ttrain's multi_logloss: 0.952302\tvalid's multi_logloss: 1.00323\n",
      "[31]\ttrain's multi_logloss: 0.948963\tvalid's multi_logloss: 1.00197\n",
      "[32]\ttrain's multi_logloss: 0.945821\tvalid's multi_logloss: 1.00041\n",
      "[33]\ttrain's multi_logloss: 0.942543\tvalid's multi_logloss: 0.999012\n",
      "[34]\ttrain's multi_logloss: 0.939633\tvalid's multi_logloss: 0.998956\n",
      "[35]\ttrain's multi_logloss: 0.936527\tvalid's multi_logloss: 0.998513\n",
      "[36]\ttrain's multi_logloss: 0.934454\tvalid's multi_logloss: 0.995385\n",
      "[37]\ttrain's multi_logloss: 0.931559\tvalid's multi_logloss: 0.993605\n",
      "[38]\ttrain's multi_logloss: 0.92903\tvalid's multi_logloss: 0.993018\n",
      "[39]\ttrain's multi_logloss: 0.926117\tvalid's multi_logloss: 0.992179\n",
      "[40]\ttrain's multi_logloss: 0.92308\tvalid's multi_logloss: 0.988406\n",
      "[41]\ttrain's multi_logloss: 0.920382\tvalid's multi_logloss: 0.987102\n",
      "[42]\ttrain's multi_logloss: 0.917751\tvalid's multi_logloss: 0.984449\n",
      "[43]\ttrain's multi_logloss: 0.91538\tvalid's multi_logloss: 0.981602\n",
      "[44]\ttrain's multi_logloss: 0.913094\tvalid's multi_logloss: 0.980349\n",
      "[45]\ttrain's multi_logloss: 0.910641\tvalid's multi_logloss: 0.977224\n",
      "[46]\ttrain's multi_logloss: 0.90835\tvalid's multi_logloss: 0.975122\n",
      "[47]\ttrain's multi_logloss: 0.906056\tvalid's multi_logloss: 0.972502\n",
      "[48]\ttrain's multi_logloss: 0.903568\tvalid's multi_logloss: 0.970092\n",
      "[49]\ttrain's multi_logloss: 0.9016\tvalid's multi_logloss: 0.967911\n",
      "[50]\ttrain's multi_logloss: 0.899419\tvalid's multi_logloss: 0.965395\n",
      "[51]\ttrain's multi_logloss: 0.897106\tvalid's multi_logloss: 0.961877\n",
      "[52]\ttrain's multi_logloss: 0.895378\tvalid's multi_logloss: 0.960752\n",
      "[53]\ttrain's multi_logloss: 0.89281\tvalid's multi_logloss: 0.957778\n",
      "[54]\ttrain's multi_logloss: 0.891222\tvalid's multi_logloss: 0.954708\n",
      "[55]\ttrain's multi_logloss: 0.88854\tvalid's multi_logloss: 0.952641\n",
      "[56]\ttrain's multi_logloss: 0.885554\tvalid's multi_logloss: 0.949665\n",
      "[57]\ttrain's multi_logloss: 0.882758\tvalid's multi_logloss: 0.946503\n",
      "[58]\ttrain's multi_logloss: 0.8803\tvalid's multi_logloss: 0.945226\n",
      "[59]\ttrain's multi_logloss: 0.878007\tvalid's multi_logloss: 0.943186\n",
      "[60]\ttrain's multi_logloss: 0.87583\tvalid's multi_logloss: 0.941093\n",
      "[61]\ttrain's multi_logloss: 0.87237\tvalid's multi_logloss: 0.939739\n",
      "[62]\ttrain's multi_logloss: 0.869196\tvalid's multi_logloss: 0.937481\n",
      "[63]\ttrain's multi_logloss: 0.866017\tvalid's multi_logloss: 0.936446\n",
      "[64]\ttrain's multi_logloss: 0.863179\tvalid's multi_logloss: 0.935822\n",
      "[65]\ttrain's multi_logloss: 0.860811\tvalid's multi_logloss: 0.934029\n",
      "[66]\ttrain's multi_logloss: 0.857881\tvalid's multi_logloss: 0.932538\n",
      "[67]\ttrain's multi_logloss: 0.855349\tvalid's multi_logloss: 0.930641\n",
      "[68]\ttrain's multi_logloss: 0.8531\tvalid's multi_logloss: 0.929392\n",
      "[69]\ttrain's multi_logloss: 0.850894\tvalid's multi_logloss: 0.927116\n",
      "[70]\ttrain's multi_logloss: 0.848521\tvalid's multi_logloss: 0.924989\n",
      "[71]\ttrain's multi_logloss: 0.846159\tvalid's multi_logloss: 0.924343\n",
      "[72]\ttrain's multi_logloss: 0.843582\tvalid's multi_logloss: 0.922267\n",
      "[73]\ttrain's multi_logloss: 0.841402\tvalid's multi_logloss: 0.919753\n",
      "[74]\ttrain's multi_logloss: 0.83901\tvalid's multi_logloss: 0.91855\n",
      "[75]\ttrain's multi_logloss: 0.836418\tvalid's multi_logloss: 0.916826\n",
      "[76]\ttrain's multi_logloss: 0.833779\tvalid's multi_logloss: 0.914866\n",
      "[77]\ttrain's multi_logloss: 0.83143\tvalid's multi_logloss: 0.914541\n",
      "[78]\ttrain's multi_logloss: 0.829692\tvalid's multi_logloss: 0.912999\n",
      "[79]\ttrain's multi_logloss: 0.827705\tvalid's multi_logloss: 0.91378\n",
      "[80]\ttrain's multi_logloss: 0.825592\tvalid's multi_logloss: 0.91199\n",
      "[81]\ttrain's multi_logloss: 0.823028\tvalid's multi_logloss: 0.911867\n",
      "[82]\ttrain's multi_logloss: 0.821565\tvalid's multi_logloss: 0.909973\n",
      "[83]\ttrain's multi_logloss: 0.819354\tvalid's multi_logloss: 0.908563\n",
      "[84]\ttrain's multi_logloss: 0.817499\tvalid's multi_logloss: 0.907926\n",
      "[85]\ttrain's multi_logloss: 0.814994\tvalid's multi_logloss: 0.906789\n",
      "[86]\ttrain's multi_logloss: 0.812945\tvalid's multi_logloss: 0.903877\n",
      "[87]\ttrain's multi_logloss: 0.811413\tvalid's multi_logloss: 0.901873\n",
      "[88]\ttrain's multi_logloss: 0.809802\tvalid's multi_logloss: 0.900401\n",
      "[89]\ttrain's multi_logloss: 0.808198\tvalid's multi_logloss: 0.898786\n",
      "[90]\ttrain's multi_logloss: 0.806414\tvalid's multi_logloss: 0.895884\n",
      "[91]\ttrain's multi_logloss: 0.804026\tvalid's multi_logloss: 0.896066\n",
      "[92]\ttrain's multi_logloss: 0.802151\tvalid's multi_logloss: 0.896407\n",
      "[93]\ttrain's multi_logloss: 0.799626\tvalid's multi_logloss: 0.894688\n",
      "[94]\ttrain's multi_logloss: 0.797306\tvalid's multi_logloss: 0.893467\n",
      "[95]\ttrain's multi_logloss: 0.79475\tvalid's multi_logloss: 0.891451\n",
      "[96]\ttrain's multi_logloss: 0.793373\tvalid's multi_logloss: 0.889957\n",
      "[97]\ttrain's multi_logloss: 0.791912\tvalid's multi_logloss: 0.886679\n",
      "[98]\ttrain's multi_logloss: 0.79008\tvalid's multi_logloss: 0.885334\n",
      "[99]\ttrain's multi_logloss: 0.788024\tvalid's multi_logloss: 0.883249\n",
      "[100]\ttrain's multi_logloss: 0.787016\tvalid's multi_logloss: 0.880495\n",
      "[101]\ttrain's multi_logloss: 0.784292\tvalid's multi_logloss: 0.879784\n",
      "[102]\ttrain's multi_logloss: 0.781464\tvalid's multi_logloss: 0.88025\n",
      "[103]\ttrain's multi_logloss: 0.778846\tvalid's multi_logloss: 0.881658\n",
      "[104]\ttrain's multi_logloss: 0.776514\tvalid's multi_logloss: 0.881081\n",
      "[105]\ttrain's multi_logloss: 0.774087\tvalid's multi_logloss: 0.879198\n",
      "[106]\ttrain's multi_logloss: 0.772601\tvalid's multi_logloss: 0.879208\n",
      "[107]\ttrain's multi_logloss: 0.770882\tvalid's multi_logloss: 0.879522\n",
      "[108]\ttrain's multi_logloss: 0.769611\tvalid's multi_logloss: 0.880698\n",
      "[109]\ttrain's multi_logloss: 0.767875\tvalid's multi_logloss: 0.880049\n",
      "[110]\ttrain's multi_logloss: 0.766173\tvalid's multi_logloss: 0.880027\n",
      "[111]\ttrain's multi_logloss: 0.763767\tvalid's multi_logloss: 0.879424\n",
      "[112]\ttrain's multi_logloss: 0.761605\tvalid's multi_logloss: 0.877795\n",
      "[113]\ttrain's multi_logloss: 0.759635\tvalid's multi_logloss: 0.877212\n",
      "[114]\ttrain's multi_logloss: 0.757302\tvalid's multi_logloss: 0.876065\n",
      "[115]\ttrain's multi_logloss: 0.754976\tvalid's multi_logloss: 0.875683\n",
      "[116]\ttrain's multi_logloss: 0.753134\tvalid's multi_logloss: 0.875929\n",
      "[117]\ttrain's multi_logloss: 0.751113\tvalid's multi_logloss: 0.875732\n",
      "[118]\ttrain's multi_logloss: 0.749558\tvalid's multi_logloss: 0.873911\n",
      "[119]\ttrain's multi_logloss: 0.747577\tvalid's multi_logloss: 0.873486\n",
      "[120]\ttrain's multi_logloss: 0.74636\tvalid's multi_logloss: 0.871438\n",
      "[121]\ttrain's multi_logloss: 0.744303\tvalid's multi_logloss: 0.869717\n",
      "[122]\ttrain's multi_logloss: 0.742659\tvalid's multi_logloss: 0.868906\n",
      "[123]\ttrain's multi_logloss: 0.740996\tvalid's multi_logloss: 0.86721\n",
      "[124]\ttrain's multi_logloss: 0.739183\tvalid's multi_logloss: 0.867606\n",
      "[125]\ttrain's multi_logloss: 0.737304\tvalid's multi_logloss: 0.866933\n",
      "[126]\ttrain's multi_logloss: 0.735505\tvalid's multi_logloss: 0.864263\n",
      "[127]\ttrain's multi_logloss: 0.733794\tvalid's multi_logloss: 0.862016\n",
      "[128]\ttrain's multi_logloss: 0.731896\tvalid's multi_logloss: 0.858929\n",
      "[129]\ttrain's multi_logloss: 0.73046\tvalid's multi_logloss: 0.859436\n",
      "[130]\ttrain's multi_logloss: 0.728795\tvalid's multi_logloss: 0.858875\n",
      "[131]\ttrain's multi_logloss: 0.727347\tvalid's multi_logloss: 0.859376\n",
      "[132]\ttrain's multi_logloss: 0.725082\tvalid's multi_logloss: 0.857007\n",
      "[133]\ttrain's multi_logloss: 0.722821\tvalid's multi_logloss: 0.854133\n",
      "[134]\ttrain's multi_logloss: 0.720755\tvalid's multi_logloss: 0.851387\n",
      "[135]\ttrain's multi_logloss: 0.718715\tvalid's multi_logloss: 0.850383\n",
      "[136]\ttrain's multi_logloss: 0.716616\tvalid's multi_logloss: 0.852448\n",
      "[137]\ttrain's multi_logloss: 0.71482\tvalid's multi_logloss: 0.852381\n",
      "[138]\ttrain's multi_logloss: 0.712958\tvalid's multi_logloss: 0.853657\n",
      "[139]\ttrain's multi_logloss: 0.711039\tvalid's multi_logloss: 0.853422\n",
      "[140]\ttrain's multi_logloss: 0.709683\tvalid's multi_logloss: 0.852092\n",
      "[141]\ttrain's multi_logloss: 0.708683\tvalid's multi_logloss: 0.850292\n",
      "[142]\ttrain's multi_logloss: 0.707197\tvalid's multi_logloss: 0.849534\n",
      "[143]\ttrain's multi_logloss: 0.705979\tvalid's multi_logloss: 0.847393\n",
      "[144]\ttrain's multi_logloss: 0.704423\tvalid's multi_logloss: 0.8466\n",
      "[145]\ttrain's multi_logloss: 0.7033\tvalid's multi_logloss: 0.844539\n",
      "[146]\ttrain's multi_logloss: 0.7017\tvalid's multi_logloss: 0.842727\n",
      "[147]\ttrain's multi_logloss: 0.699615\tvalid's multi_logloss: 0.841692\n",
      "[148]\ttrain's multi_logloss: 0.697811\tvalid's multi_logloss: 0.841506\n",
      "[149]\ttrain's multi_logloss: 0.696733\tvalid's multi_logloss: 0.842545\n",
      "[150]\ttrain's multi_logloss: 0.69546\tvalid's multi_logloss: 0.841617\n",
      "[151]\ttrain's multi_logloss: 0.69374\tvalid's multi_logloss: 0.840366\n",
      "[152]\ttrain's multi_logloss: 0.691885\tvalid's multi_logloss: 0.839631\n",
      "[153]\ttrain's multi_logloss: 0.690052\tvalid's multi_logloss: 0.839444\n",
      "[154]\ttrain's multi_logloss: 0.688569\tvalid's multi_logloss: 0.838535\n",
      "[155]\ttrain's multi_logloss: 0.686616\tvalid's multi_logloss: 0.839019\n",
      "[156]\ttrain's multi_logloss: 0.684881\tvalid's multi_logloss: 0.837549\n",
      "[157]\ttrain's multi_logloss: 0.68271\tvalid's multi_logloss: 0.836324\n",
      "[158]\ttrain's multi_logloss: 0.680773\tvalid's multi_logloss: 0.835045\n",
      "[159]\ttrain's multi_logloss: 0.678919\tvalid's multi_logloss: 0.835806\n",
      "[160]\ttrain's multi_logloss: 0.677237\tvalid's multi_logloss: 0.834805\n",
      "[161]\ttrain's multi_logloss: 0.675899\tvalid's multi_logloss: 0.834981\n",
      "[162]\ttrain's multi_logloss: 0.6747\tvalid's multi_logloss: 0.834391\n",
      "[163]\ttrain's multi_logloss: 0.673928\tvalid's multi_logloss: 0.835153\n",
      "[164]\ttrain's multi_logloss: 0.67309\tvalid's multi_logloss: 0.834646\n",
      "[165]\ttrain's multi_logloss: 0.671933\tvalid's multi_logloss: 0.833966\n",
      "[166]\ttrain's multi_logloss: 0.670584\tvalid's multi_logloss: 0.834439\n",
      "[167]\ttrain's multi_logloss: 0.669289\tvalid's multi_logloss: 0.834005\n",
      "[168]\ttrain's multi_logloss: 0.66805\tvalid's multi_logloss: 0.833831\n",
      "[169]\ttrain's multi_logloss: 0.666759\tvalid's multi_logloss: 0.833175\n",
      "[170]\ttrain's multi_logloss: 0.665409\tvalid's multi_logloss: 0.832396\n",
      "[171]\ttrain's multi_logloss: 0.66421\tvalid's multi_logloss: 0.831114\n",
      "[172]\ttrain's multi_logloss: 0.663172\tvalid's multi_logloss: 0.828824\n",
      "[173]\ttrain's multi_logloss: 0.662163\tvalid's multi_logloss: 0.826576\n",
      "[174]\ttrain's multi_logloss: 0.66096\tvalid's multi_logloss: 0.824476\n",
      "[175]\ttrain's multi_logloss: 0.659994\tvalid's multi_logloss: 0.822292\n",
      "[176]\ttrain's multi_logloss: 0.658193\tvalid's multi_logloss: 0.822843\n",
      "[177]\ttrain's multi_logloss: 0.656569\tvalid's multi_logloss: 0.822153\n",
      "[178]\ttrain's multi_logloss: 0.655265\tvalid's multi_logloss: 0.822236\n",
      "[179]\ttrain's multi_logloss: 0.653843\tvalid's multi_logloss: 0.821257\n",
      "[180]\ttrain's multi_logloss: 0.65221\tvalid's multi_logloss: 0.820623\n",
      "[181]\ttrain's multi_logloss: 0.650861\tvalid's multi_logloss: 0.820095\n",
      "[182]\ttrain's multi_logloss: 0.649225\tvalid's multi_logloss: 0.818212\n",
      "[183]\ttrain's multi_logloss: 0.647942\tvalid's multi_logloss: 0.815505\n",
      "[184]\ttrain's multi_logloss: 0.646512\tvalid's multi_logloss: 0.81576\n",
      "[185]\ttrain's multi_logloss: 0.64499\tvalid's multi_logloss: 0.815673\n",
      "[186]\ttrain's multi_logloss: 0.643212\tvalid's multi_logloss: 0.813569\n",
      "[187]\ttrain's multi_logloss: 0.64164\tvalid's multi_logloss: 0.814692\n",
      "[188]\ttrain's multi_logloss: 0.640057\tvalid's multi_logloss: 0.814738\n",
      "[189]\ttrain's multi_logloss: 0.638464\tvalid's multi_logloss: 0.81534\n",
      "[190]\ttrain's multi_logloss: 0.637129\tvalid's multi_logloss: 0.814531\n",
      "[191]\ttrain's multi_logloss: 0.635696\tvalid's multi_logloss: 0.814153\n",
      "[192]\ttrain's multi_logloss: 0.634354\tvalid's multi_logloss: 0.813218\n",
      "[193]\ttrain's multi_logloss: 0.632583\tvalid's multi_logloss: 0.814562\n",
      "[194]\ttrain's multi_logloss: 0.631109\tvalid's multi_logloss: 0.812683\n",
      "[195]\ttrain's multi_logloss: 0.629788\tvalid's multi_logloss: 0.813564\n",
      "[196]\ttrain's multi_logloss: 0.628037\tvalid's multi_logloss: 0.81113\n",
      "[197]\ttrain's multi_logloss: 0.626346\tvalid's multi_logloss: 0.808081\n",
      "[198]\ttrain's multi_logloss: 0.62465\tvalid's multi_logloss: 0.806035\n",
      "[199]\ttrain's multi_logloss: 0.623284\tvalid's multi_logloss: 0.804901\n",
      "[200]\ttrain's multi_logloss: 0.621766\tvalid's multi_logloss: 0.802741\n",
      "[201]\ttrain's multi_logloss: 0.619706\tvalid's multi_logloss: 0.802655\n",
      "[202]\ttrain's multi_logloss: 0.617944\tvalid's multi_logloss: 0.802999\n",
      "[203]\ttrain's multi_logloss: 0.616006\tvalid's multi_logloss: 0.802663\n",
      "[204]\ttrain's multi_logloss: 0.614245\tvalid's multi_logloss: 0.802885\n",
      "[205]\ttrain's multi_logloss: 0.612342\tvalid's multi_logloss: 0.801882\n",
      "[206]\ttrain's multi_logloss: 0.611219\tvalid's multi_logloss: 0.799883\n",
      "[207]\ttrain's multi_logloss: 0.610077\tvalid's multi_logloss: 0.800704\n",
      "[208]\ttrain's multi_logloss: 0.608958\tvalid's multi_logloss: 0.800896\n",
      "[209]\ttrain's multi_logloss: 0.608093\tvalid's multi_logloss: 0.799695\n",
      "[210]\ttrain's multi_logloss: 0.606848\tvalid's multi_logloss: 0.79848\n",
      "[211]\ttrain's multi_logloss: 0.605932\tvalid's multi_logloss: 0.795567\n",
      "[212]\ttrain's multi_logloss: 0.6049\tvalid's multi_logloss: 0.795418\n",
      "[213]\ttrain's multi_logloss: 0.604076\tvalid's multi_logloss: 0.792799\n",
      "[214]\ttrain's multi_logloss: 0.60317\tvalid's multi_logloss: 0.791689\n",
      "[215]\ttrain's multi_logloss: 0.602286\tvalid's multi_logloss: 0.7905\n",
      "[216]\ttrain's multi_logloss: 0.601108\tvalid's multi_logloss: 0.790469\n",
      "[217]\ttrain's multi_logloss: 0.59996\tvalid's multi_logloss: 0.789648\n",
      "[218]\ttrain's multi_logloss: 0.598729\tvalid's multi_logloss: 0.788848\n",
      "[219]\ttrain's multi_logloss: 0.597512\tvalid's multi_logloss: 0.788079\n",
      "[220]\ttrain's multi_logloss: 0.596601\tvalid's multi_logloss: 0.788125\n",
      "[221]\ttrain's multi_logloss: 0.595014\tvalid's multi_logloss: 0.786532\n",
      "[222]\ttrain's multi_logloss: 0.593393\tvalid's multi_logloss: 0.785573\n",
      "[223]\ttrain's multi_logloss: 0.591661\tvalid's multi_logloss: 0.784749\n",
      "[224]\ttrain's multi_logloss: 0.590254\tvalid's multi_logloss: 0.785206\n",
      "[225]\ttrain's multi_logloss: 0.588951\tvalid's multi_logloss: 0.784774\n",
      "[226]\ttrain's multi_logloss: 0.58758\tvalid's multi_logloss: 0.785021\n",
      "[227]\ttrain's multi_logloss: 0.585982\tvalid's multi_logloss: 0.785713\n",
      "[228]\ttrain's multi_logloss: 0.584647\tvalid's multi_logloss: 0.787834\n",
      "[229]\ttrain's multi_logloss: 0.583433\tvalid's multi_logloss: 0.788895\n",
      "[230]\ttrain's multi_logloss: 0.582106\tvalid's multi_logloss: 0.790123\n",
      "[231]\ttrain's multi_logloss: 0.581047\tvalid's multi_logloss: 0.789643\n",
      "[232]\ttrain's multi_logloss: 0.580131\tvalid's multi_logloss: 0.788577\n",
      "[233]\ttrain's multi_logloss: 0.579418\tvalid's multi_logloss: 0.787796\n",
      "[234]\ttrain's multi_logloss: 0.578626\tvalid's multi_logloss: 0.787309\n",
      "[235]\ttrain's multi_logloss: 0.577699\tvalid's multi_logloss: 0.786666\n",
      "[236]\ttrain's multi_logloss: 0.576745\tvalid's multi_logloss: 0.783496\n",
      "[237]\ttrain's multi_logloss: 0.57594\tvalid's multi_logloss: 0.781724\n",
      "[238]\ttrain's multi_logloss: 0.575158\tvalid's multi_logloss: 0.778305\n",
      "[239]\ttrain's multi_logloss: 0.57428\tvalid's multi_logloss: 0.775092\n",
      "[240]\ttrain's multi_logloss: 0.573448\tvalid's multi_logloss: 0.772103\n",
      "[241]\ttrain's multi_logloss: 0.572017\tvalid's multi_logloss: 0.771167\n",
      "[242]\ttrain's multi_logloss: 0.570764\tvalid's multi_logloss: 0.771722\n",
      "[243]\ttrain's multi_logloss: 0.569665\tvalid's multi_logloss: 0.772205\n",
      "[244]\ttrain's multi_logloss: 0.56841\tvalid's multi_logloss: 0.771835\n",
      "[245]\ttrain's multi_logloss: 0.567245\tvalid's multi_logloss: 0.771062\n",
      "[246]\ttrain's multi_logloss: 0.566027\tvalid's multi_logloss: 0.769086\n",
      "[247]\ttrain's multi_logloss: 0.564588\tvalid's multi_logloss: 0.768854\n",
      "[248]\ttrain's multi_logloss: 0.563234\tvalid's multi_logloss: 0.76832\n",
      "[249]\ttrain's multi_logloss: 0.561863\tvalid's multi_logloss: 0.769257\n",
      "[250]\ttrain's multi_logloss: 0.560452\tvalid's multi_logloss: 0.769558\n",
      "[251]\ttrain's multi_logloss: 0.559265\tvalid's multi_logloss: 0.770083\n",
      "[252]\ttrain's multi_logloss: 0.558216\tvalid's multi_logloss: 0.770805\n",
      "[253]\ttrain's multi_logloss: 0.557102\tvalid's multi_logloss: 0.770648\n",
      "[254]\ttrain's multi_logloss: 0.555847\tvalid's multi_logloss: 0.771569\n",
      "[255]\ttrain's multi_logloss: 0.554884\tvalid's multi_logloss: 0.77131\n",
      "[256]\ttrain's multi_logloss: 0.553854\tvalid's multi_logloss: 0.770392\n",
      "[257]\ttrain's multi_logloss: 0.552765\tvalid's multi_logloss: 0.768628\n",
      "[258]\ttrain's multi_logloss: 0.551647\tvalid's multi_logloss: 0.767162\n",
      "[259]\ttrain's multi_logloss: 0.550822\tvalid's multi_logloss: 0.766217\n",
      "[260]\ttrain's multi_logloss: 0.549621\tvalid's multi_logloss: 0.764225\n",
      "[261]\ttrain's multi_logloss: 0.548592\tvalid's multi_logloss: 0.763475\n",
      "[262]\ttrain's multi_logloss: 0.547544\tvalid's multi_logloss: 0.764987\n",
      "[263]\ttrain's multi_logloss: 0.546626\tvalid's multi_logloss: 0.764878\n",
      "[264]\ttrain's multi_logloss: 0.545747\tvalid's multi_logloss: 0.765438\n",
      "[265]\ttrain's multi_logloss: 0.544669\tvalid's multi_logloss: 0.766931\n",
      "[266]\ttrain's multi_logloss: 0.543635\tvalid's multi_logloss: 0.768092\n",
      "[267]\ttrain's multi_logloss: 0.542427\tvalid's multi_logloss: 0.770219\n",
      "[268]\ttrain's multi_logloss: 0.541004\tvalid's multi_logloss: 0.771811\n",
      "[269]\ttrain's multi_logloss: 0.540009\tvalid's multi_logloss: 0.772676\n",
      "[270]\ttrain's multi_logloss: 0.538821\tvalid's multi_logloss: 0.774116\n",
      "[271]\ttrain's multi_logloss: 0.538088\tvalid's multi_logloss: 0.77455\n",
      "[272]\ttrain's multi_logloss: 0.53715\tvalid's multi_logloss: 0.774727\n",
      "[273]\ttrain's multi_logloss: 0.536246\tvalid's multi_logloss: 0.774247\n",
      "[274]\ttrain's multi_logloss: 0.535245\tvalid's multi_logloss: 0.774563\n",
      "[275]\ttrain's multi_logloss: 0.534312\tvalid's multi_logloss: 0.77417\n",
      "[276]\ttrain's multi_logloss: 0.533203\tvalid's multi_logloss: 0.774014\n",
      "[277]\ttrain's multi_logloss: 0.532137\tvalid's multi_logloss: 0.77394\n",
      "[278]\ttrain's multi_logloss: 0.531338\tvalid's multi_logloss: 0.774398\n",
      "[279]\ttrain's multi_logloss: 0.530323\tvalid's multi_logloss: 0.772795\n",
      "[280]\ttrain's multi_logloss: 0.529554\tvalid's multi_logloss: 0.773312\n",
      "[281]\ttrain's multi_logloss: 0.528664\tvalid's multi_logloss: 0.772665\n",
      "[282]\ttrain's multi_logloss: 0.527326\tvalid's multi_logloss: 0.772139\n",
      "[283]\ttrain's multi_logloss: 0.526101\tvalid's multi_logloss: 0.771803\n",
      "[284]\ttrain's multi_logloss: 0.525343\tvalid's multi_logloss: 0.770989\n",
      "[285]\ttrain's multi_logloss: 0.524786\tvalid's multi_logloss: 0.769927\n",
      "[286]\ttrain's multi_logloss: 0.523722\tvalid's multi_logloss: 0.769885\n",
      "[287]\ttrain's multi_logloss: 0.522389\tvalid's multi_logloss: 0.771125\n",
      "[288]\ttrain's multi_logloss: 0.521351\tvalid's multi_logloss: 0.768836\n",
      "[289]\ttrain's multi_logloss: 0.51989\tvalid's multi_logloss: 0.770388\n",
      "[290]\ttrain's multi_logloss: 0.518753\tvalid's multi_logloss: 0.769754\n",
      "[291]\ttrain's multi_logloss: 0.517577\tvalid's multi_logloss: 0.769171\n",
      "[292]\ttrain's multi_logloss: 0.516508\tvalid's multi_logloss: 0.768341\n",
      "[293]\ttrain's multi_logloss: 0.5153\tvalid's multi_logloss: 0.76775\n",
      "[294]\ttrain's multi_logloss: 0.514362\tvalid's multi_logloss: 0.766776\n",
      "[295]\ttrain's multi_logloss: 0.513216\tvalid's multi_logloss: 0.766136\n",
      "[296]\ttrain's multi_logloss: 0.512698\tvalid's multi_logloss: 0.767399\n",
      "[297]\ttrain's multi_logloss: 0.512248\tvalid's multi_logloss: 0.767328\n",
      "[298]\ttrain's multi_logloss: 0.511574\tvalid's multi_logloss: 0.768305\n",
      "[299]\ttrain's multi_logloss: 0.51122\tvalid's multi_logloss: 0.769367\n",
      "[300]\ttrain's multi_logloss: 0.510828\tvalid's multi_logloss: 0.767874\n",
      "[301]\ttrain's multi_logloss: 0.510136\tvalid's multi_logloss: 0.766379\n",
      "[302]\ttrain's multi_logloss: 0.509037\tvalid's multi_logloss: 0.765961\n",
      "[303]\ttrain's multi_logloss: 0.50821\tvalid's multi_logloss: 0.76524\n",
      "[304]\ttrain's multi_logloss: 0.507665\tvalid's multi_logloss: 0.764596\n",
      "[305]\ttrain's multi_logloss: 0.507043\tvalid's multi_logloss: 0.763701\n",
      "[306]\ttrain's multi_logloss: 0.505838\tvalid's multi_logloss: 0.761526\n",
      "[307]\ttrain's multi_logloss: 0.504523\tvalid's multi_logloss: 0.759558\n",
      "[308]\ttrain's multi_logloss: 0.503086\tvalid's multi_logloss: 0.757754\n",
      "[309]\ttrain's multi_logloss: 0.501749\tvalid's multi_logloss: 0.756204\n",
      "[310]\ttrain's multi_logloss: 0.500921\tvalid's multi_logloss: 0.755372\n",
      "[311]\ttrain's multi_logloss: 0.500179\tvalid's multi_logloss: 0.756425\n",
      "[312]\ttrain's multi_logloss: 0.499256\tvalid's multi_logloss: 0.757029\n",
      "[313]\ttrain's multi_logloss: 0.498442\tvalid's multi_logloss: 0.757637\n",
      "[314]\ttrain's multi_logloss: 0.497593\tvalid's multi_logloss: 0.759429\n",
      "[315]\ttrain's multi_logloss: 0.496807\tvalid's multi_logloss: 0.759374\n",
      "[316]\ttrain's multi_logloss: 0.496144\tvalid's multi_logloss: 0.759991\n",
      "[317]\ttrain's multi_logloss: 0.495669\tvalid's multi_logloss: 0.761344\n",
      "[318]\ttrain's multi_logloss: 0.495224\tvalid's multi_logloss: 0.761503\n",
      "[319]\ttrain's multi_logloss: 0.494636\tvalid's multi_logloss: 0.762687\n",
      "[320]\ttrain's multi_logloss: 0.494213\tvalid's multi_logloss: 0.763238\n",
      "[321]\ttrain's multi_logloss: 0.492914\tvalid's multi_logloss: 0.764432\n",
      "[322]\ttrain's multi_logloss: 0.491805\tvalid's multi_logloss: 0.764642\n",
      "[323]\ttrain's multi_logloss: 0.490675\tvalid's multi_logloss: 0.764627\n",
      "[324]\ttrain's multi_logloss: 0.489356\tvalid's multi_logloss: 0.765649\n",
      "[325]\ttrain's multi_logloss: 0.488027\tvalid's multi_logloss: 0.766539\n",
      "[326]\ttrain's multi_logloss: 0.487259\tvalid's multi_logloss: 0.765321\n",
      "[327]\ttrain's multi_logloss: 0.486413\tvalid's multi_logloss: 0.763444\n",
      "[328]\ttrain's multi_logloss: 0.485239\tvalid's multi_logloss: 0.764338\n",
      "[329]\ttrain's multi_logloss: 0.48418\tvalid's multi_logloss: 0.76404\n",
      "[330]\ttrain's multi_logloss: 0.483317\tvalid's multi_logloss: 0.763982\n",
      "[331]\ttrain's multi_logloss: 0.482556\tvalid's multi_logloss: 0.764363\n",
      "[332]\ttrain's multi_logloss: 0.481606\tvalid's multi_logloss: 0.764815\n",
      "[333]\ttrain's multi_logloss: 0.480642\tvalid's multi_logloss: 0.765502\n",
      "[334]\ttrain's multi_logloss: 0.47975\tvalid's multi_logloss: 0.765414\n",
      "[335]\ttrain's multi_logloss: 0.478896\tvalid's multi_logloss: 0.766089\n",
      "[336]\ttrain's multi_logloss: 0.478285\tvalid's multi_logloss: 0.766538\n",
      "[337]\ttrain's multi_logloss: 0.477537\tvalid's multi_logloss: 0.766933\n",
      "[338]\ttrain's multi_logloss: 0.477052\tvalid's multi_logloss: 0.766634\n",
      "[339]\ttrain's multi_logloss: 0.476192\tvalid's multi_logloss: 0.767825\n",
      "[340]\ttrain's multi_logloss: 0.475353\tvalid's multi_logloss: 0.769021\n",
      "[341]\ttrain's multi_logloss: 0.474801\tvalid's multi_logloss: 0.769203\n",
      "[342]\ttrain's multi_logloss: 0.473965\tvalid's multi_logloss: 0.769014\n",
      "[343]\ttrain's multi_logloss: 0.473404\tvalid's multi_logloss: 0.768264\n",
      "[344]\ttrain's multi_logloss: 0.472683\tvalid's multi_logloss: 0.767774\n",
      "[345]\ttrain's multi_logloss: 0.471897\tvalid's multi_logloss: 0.768724\n",
      "[346]\ttrain's multi_logloss: 0.470769\tvalid's multi_logloss: 0.769561\n",
      "[347]\ttrain's multi_logloss: 0.46993\tvalid's multi_logloss: 0.769199\n",
      "[348]\ttrain's multi_logloss: 0.469053\tvalid's multi_logloss: 0.769082\n",
      "[349]\ttrain's multi_logloss: 0.468052\tvalid's multi_logloss: 0.769417\n",
      "[350]\ttrain's multi_logloss: 0.467211\tvalid's multi_logloss: 0.769325\n",
      "[351]\ttrain's multi_logloss: 0.466301\tvalid's multi_logloss: 0.770135\n",
      "[352]\ttrain's multi_logloss: 0.465338\tvalid's multi_logloss: 0.771284\n",
      "[353]\ttrain's multi_logloss: 0.464411\tvalid's multi_logloss: 0.77222\n",
      "[354]\ttrain's multi_logloss: 0.463512\tvalid's multi_logloss: 0.773116\n",
      "[355]\ttrain's multi_logloss: 0.462848\tvalid's multi_logloss: 0.773772\n",
      "[356]\ttrain's multi_logloss: 0.462218\tvalid's multi_logloss: 0.774296\n",
      "[357]\ttrain's multi_logloss: 0.461231\tvalid's multi_logloss: 0.776633\n",
      "[358]\ttrain's multi_logloss: 0.460351\tvalid's multi_logloss: 0.776677\n",
      "[359]\ttrain's multi_logloss: 0.459494\tvalid's multi_logloss: 0.778887\n",
      "[360]\ttrain's multi_logloss: 0.458707\tvalid's multi_logloss: 0.780127\n",
      "[361]\ttrain's multi_logloss: 0.457485\tvalid's multi_logloss: 0.780843\n",
      "[362]\ttrain's multi_logloss: 0.456414\tvalid's multi_logloss: 0.780132\n",
      "[363]\ttrain's multi_logloss: 0.455197\tvalid's multi_logloss: 0.7804\n",
      "[364]\ttrain's multi_logloss: 0.454062\tvalid's multi_logloss: 0.780603\n",
      "[365]\ttrain's multi_logloss: 0.453127\tvalid's multi_logloss: 0.781444\n",
      "[366]\ttrain's multi_logloss: 0.452204\tvalid's multi_logloss: 0.781306\n",
      "[367]\ttrain's multi_logloss: 0.451295\tvalid's multi_logloss: 0.7816\n",
      "[368]\ttrain's multi_logloss: 0.450457\tvalid's multi_logloss: 0.782007\n",
      "[369]\ttrain's multi_logloss: 0.449614\tvalid's multi_logloss: 0.782682\n",
      "[370]\ttrain's multi_logloss: 0.448706\tvalid's multi_logloss: 0.783463\n",
      "[371]\ttrain's multi_logloss: 0.447759\tvalid's multi_logloss: 0.78074\n",
      "[372]\ttrain's multi_logloss: 0.446861\tvalid's multi_logloss: 0.781308\n",
      "[373]\ttrain's multi_logloss: 0.445949\tvalid's multi_logloss: 0.781583\n",
      "[374]\ttrain's multi_logloss: 0.444916\tvalid's multi_logloss: 0.781345\n",
      "[375]\ttrain's multi_logloss: 0.444143\tvalid's multi_logloss: 0.781622\n",
      "[376]\ttrain's multi_logloss: 0.443545\tvalid's multi_logloss: 0.782398\n",
      "[377]\ttrain's multi_logloss: 0.442813\tvalid's multi_logloss: 0.783125\n",
      "[378]\ttrain's multi_logloss: 0.442202\tvalid's multi_logloss: 0.782603\n",
      "[379]\ttrain's multi_logloss: 0.441597\tvalid's multi_logloss: 0.782675\n",
      "[380]\ttrain's multi_logloss: 0.441108\tvalid's multi_logloss: 0.782345\n",
      "[381]\ttrain's multi_logloss: 0.44031\tvalid's multi_logloss: 0.78381\n",
      "[382]\ttrain's multi_logloss: 0.439526\tvalid's multi_logloss: 0.784697\n",
      "[383]\ttrain's multi_logloss: 0.438859\tvalid's multi_logloss: 0.785818\n",
      "[384]\ttrain's multi_logloss: 0.438153\tvalid's multi_logloss: 0.787315\n",
      "[385]\ttrain's multi_logloss: 0.437308\tvalid's multi_logloss: 0.785923\n",
      "[386]\ttrain's multi_logloss: 0.436663\tvalid's multi_logloss: 0.786811\n",
      "[387]\ttrain's multi_logloss: 0.435834\tvalid's multi_logloss: 0.785457\n",
      "[388]\ttrain's multi_logloss: 0.435149\tvalid's multi_logloss: 0.784285\n",
      "[389]\ttrain's multi_logloss: 0.434497\tvalid's multi_logloss: 0.783385\n",
      "[390]\ttrain's multi_logloss: 0.433973\tvalid's multi_logloss: 0.783101\n",
      "[391]\ttrain's multi_logloss: 0.433492\tvalid's multi_logloss: 0.782612\n",
      "[392]\ttrain's multi_logloss: 0.432885\tvalid's multi_logloss: 0.782884\n",
      "[393]\ttrain's multi_logloss: 0.432292\tvalid's multi_logloss: 0.782596\n",
      "[394]\ttrain's multi_logloss: 0.431526\tvalid's multi_logloss: 0.783453\n",
      "[395]\ttrain's multi_logloss: 0.43102\tvalid's multi_logloss: 0.782706\n",
      "[396]\ttrain's multi_logloss: 0.430226\tvalid's multi_logloss: 0.783392\n",
      "[397]\ttrain's multi_logloss: 0.429514\tvalid's multi_logloss: 0.782643\n",
      "[398]\ttrain's multi_logloss: 0.42883\tvalid's multi_logloss: 0.78149\n",
      "[399]\ttrain's multi_logloss: 0.428186\tvalid's multi_logloss: 0.780252\n",
      "[400]\ttrain's multi_logloss: 0.427373\tvalid's multi_logloss: 0.779604\n",
      "[401]\ttrain's multi_logloss: 0.426822\tvalid's multi_logloss: 0.780453\n",
      "[402]\ttrain's multi_logloss: 0.426272\tvalid's multi_logloss: 0.78121\n",
      "[403]\ttrain's multi_logloss: 0.425716\tvalid's multi_logloss: 0.781587\n",
      "[404]\ttrain's multi_logloss: 0.425172\tvalid's multi_logloss: 0.781992\n",
      "[405]\ttrain's multi_logloss: 0.424639\tvalid's multi_logloss: 0.782405\n",
      "[406]\ttrain's multi_logloss: 0.423663\tvalid's multi_logloss: 0.783362\n",
      "[407]\ttrain's multi_logloss: 0.422695\tvalid's multi_logloss: 0.784609\n",
      "[408]\ttrain's multi_logloss: 0.421874\tvalid's multi_logloss: 0.785965\n",
      "[409]\ttrain's multi_logloss: 0.421241\tvalid's multi_logloss: 0.78489\n",
      "[410]\ttrain's multi_logloss: 0.420517\tvalid's multi_logloss: 0.785735\n",
      "Early stopping, best iteration is:\n",
      "[310]\ttrain's multi_logloss: 0.500921\tvalid's multi_logloss: 0.755372\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09281\tvalid's SFC_loss: 1.09397\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08663\tvalid's SFC_loss: 1.09205\n",
      "[3]\ttrain's SFC_loss: 1.08089\tvalid's SFC_loss: 1.08787\n",
      "[4]\ttrain's SFC_loss: 1.07505\tvalid's SFC_loss: 1.08453\n",
      "[5]\ttrain's SFC_loss: 1.06904\tvalid's SFC_loss: 1.08039\n",
      "[6]\ttrain's SFC_loss: 1.06493\tvalid's SFC_loss: 1.07685\n",
      "[7]\ttrain's SFC_loss: 1.05843\tvalid's SFC_loss: 1.07104\n",
      "[8]\ttrain's SFC_loss: 1.05243\tvalid's SFC_loss: 1.06464\n",
      "[9]\ttrain's SFC_loss: 1.04578\tvalid's SFC_loss: 1.05985\n",
      "[10]\ttrain's SFC_loss: 1.0395\tvalid's SFC_loss: 1.05302\n",
      "[11]\ttrain's SFC_loss: 1.03388\tvalid's SFC_loss: 1.05134\n",
      "[12]\ttrain's SFC_loss: 1.02882\tvalid's SFC_loss: 1.04707\n",
      "[13]\ttrain's SFC_loss: 1.02268\tvalid's SFC_loss: 1.04431\n",
      "[14]\ttrain's SFC_loss: 1.01672\tvalid's SFC_loss: 1.04163\n",
      "[15]\ttrain's SFC_loss: 1.01134\tvalid's SFC_loss: 1.04018\n",
      "[16]\ttrain's SFC_loss: 1.0058\tvalid's SFC_loss: 1.03703\n",
      "[17]\ttrain's SFC_loss: 1.00079\tvalid's SFC_loss: 1.03449\n",
      "[18]\ttrain's SFC_loss: 0.995742\tvalid's SFC_loss: 1.03106\n",
      "[19]\ttrain's SFC_loss: 0.990522\tvalid's SFC_loss: 1.02684\n",
      "[20]\ttrain's SFC_loss: 0.98588\tvalid's SFC_loss: 1.02399\n",
      "[21]\ttrain's SFC_loss: 0.980815\tvalid's SFC_loss: 1.01797\n",
      "[22]\ttrain's SFC_loss: 0.97522\tvalid's SFC_loss: 1.01475\n",
      "[23]\ttrain's SFC_loss: 0.969629\tvalid's SFC_loss: 1.0118\n",
      "[24]\ttrain's SFC_loss: 0.964799\tvalid's SFC_loss: 1.00893\n",
      "[25]\ttrain's SFC_loss: 0.959265\tvalid's SFC_loss: 1.00612\n",
      "[26]\ttrain's SFC_loss: 0.954753\tvalid's SFC_loss: 1.00573\n",
      "[27]\ttrain's SFC_loss: 0.951136\tvalid's SFC_loss: 1.00323\n",
      "[28]\ttrain's SFC_loss: 0.947071\tvalid's SFC_loss: 1.00167\n",
      "[29]\ttrain's SFC_loss: 0.942752\tvalid's SFC_loss: 1.00185\n",
      "[30]\ttrain's SFC_loss: 0.938322\tvalid's SFC_loss: 0.999958\n",
      "[31]\ttrain's SFC_loss: 0.9344\tvalid's SFC_loss: 0.998474\n",
      "[32]\ttrain's SFC_loss: 0.930201\tvalid's SFC_loss: 0.994895\n",
      "[33]\ttrain's SFC_loss: 0.926051\tvalid's SFC_loss: 0.994284\n",
      "[34]\ttrain's SFC_loss: 0.922338\tvalid's SFC_loss: 0.989653\n",
      "[35]\ttrain's SFC_loss: 0.918468\tvalid's SFC_loss: 0.988235\n",
      "[36]\ttrain's SFC_loss: 0.91515\tvalid's SFC_loss: 0.982149\n",
      "[37]\ttrain's SFC_loss: 0.911584\tvalid's SFC_loss: 0.978979\n",
      "[38]\ttrain's SFC_loss: 0.907602\tvalid's SFC_loss: 0.973795\n",
      "[39]\ttrain's SFC_loss: 0.904093\tvalid's SFC_loss: 0.971323\n",
      "[40]\ttrain's SFC_loss: 0.900507\tvalid's SFC_loss: 0.968915\n",
      "[41]\ttrain's SFC_loss: 0.897807\tvalid's SFC_loss: 0.965095\n",
      "[42]\ttrain's SFC_loss: 0.894826\tvalid's SFC_loss: 0.964916\n",
      "[43]\ttrain's SFC_loss: 0.891763\tvalid's SFC_loss: 0.963817\n",
      "[44]\ttrain's SFC_loss: 0.889229\tvalid's SFC_loss: 0.961671\n",
      "[45]\ttrain's SFC_loss: 0.886215\tvalid's SFC_loss: 0.95969\n",
      "[46]\ttrain's SFC_loss: 0.883594\tvalid's SFC_loss: 0.957242\n",
      "[47]\ttrain's SFC_loss: 0.880846\tvalid's SFC_loss: 0.953976\n",
      "[48]\ttrain's SFC_loss: 0.878199\tvalid's SFC_loss: 0.951348\n",
      "[49]\ttrain's SFC_loss: 0.875669\tvalid's SFC_loss: 0.947937\n",
      "[50]\ttrain's SFC_loss: 0.873329\tvalid's SFC_loss: 0.944608\n",
      "[51]\ttrain's SFC_loss: 0.870542\tvalid's SFC_loss: 0.94285\n",
      "[52]\ttrain's SFC_loss: 0.86721\tvalid's SFC_loss: 0.940685\n",
      "[53]\ttrain's SFC_loss: 0.863759\tvalid's SFC_loss: 0.940104\n",
      "[54]\ttrain's SFC_loss: 0.860442\tvalid's SFC_loss: 0.94008\n",
      "[55]\ttrain's SFC_loss: 0.857765\tvalid's SFC_loss: 0.937916\n",
      "[56]\ttrain's SFC_loss: 0.854668\tvalid's SFC_loss: 0.935561\n",
      "[57]\ttrain's SFC_loss: 0.852036\tvalid's SFC_loss: 0.932684\n",
      "[58]\ttrain's SFC_loss: 0.849332\tvalid's SFC_loss: 0.929425\n",
      "[59]\ttrain's SFC_loss: 0.846707\tvalid's SFC_loss: 0.926566\n",
      "[60]\ttrain's SFC_loss: 0.843826\tvalid's SFC_loss: 0.923917\n",
      "[61]\ttrain's SFC_loss: 0.839897\tvalid's SFC_loss: 0.922445\n",
      "[62]\ttrain's SFC_loss: 0.837147\tvalid's SFC_loss: 0.922701\n",
      "[63]\ttrain's SFC_loss: 0.83339\tvalid's SFC_loss: 0.921327\n",
      "[64]\ttrain's SFC_loss: 0.829707\tvalid's SFC_loss: 0.919805\n",
      "[65]\ttrain's SFC_loss: 0.826141\tvalid's SFC_loss: 0.918552\n",
      "[66]\ttrain's SFC_loss: 0.823625\tvalid's SFC_loss: 0.914095\n",
      "[67]\ttrain's SFC_loss: 0.820269\tvalid's SFC_loss: 0.912267\n",
      "[68]\ttrain's SFC_loss: 0.817552\tvalid's SFC_loss: 0.910974\n",
      "[69]\ttrain's SFC_loss: 0.814518\tvalid's SFC_loss: 0.908508\n",
      "[70]\ttrain's SFC_loss: 0.811403\tvalid's SFC_loss: 0.906955\n",
      "[71]\ttrain's SFC_loss: 0.808882\tvalid's SFC_loss: 0.907643\n",
      "[72]\ttrain's SFC_loss: 0.806838\tvalid's SFC_loss: 0.90544\n",
      "[73]\ttrain's SFC_loss: 0.804304\tvalid's SFC_loss: 0.906565\n",
      "[74]\ttrain's SFC_loss: 0.801963\tvalid's SFC_loss: 0.906369\n",
      "[75]\ttrain's SFC_loss: 0.799356\tvalid's SFC_loss: 0.906385\n",
      "[76]\ttrain's SFC_loss: 0.797522\tvalid's SFC_loss: 0.907835\n",
      "[77]\ttrain's SFC_loss: 0.795132\tvalid's SFC_loss: 0.905544\n",
      "[78]\ttrain's SFC_loss: 0.793178\tvalid's SFC_loss: 0.906916\n",
      "[79]\ttrain's SFC_loss: 0.791328\tvalid's SFC_loss: 0.906438\n",
      "[80]\ttrain's SFC_loss: 0.789483\tvalid's SFC_loss: 0.907011\n",
      "[81]\ttrain's SFC_loss: 0.787022\tvalid's SFC_loss: 0.906288\n",
      "[82]\ttrain's SFC_loss: 0.784147\tvalid's SFC_loss: 0.90504\n",
      "[83]\ttrain's SFC_loss: 0.781084\tvalid's SFC_loss: 0.906283\n",
      "[84]\ttrain's SFC_loss: 0.77898\tvalid's SFC_loss: 0.905155\n",
      "[85]\ttrain's SFC_loss: 0.776262\tvalid's SFC_loss: 0.903791\n",
      "[86]\ttrain's SFC_loss: 0.774135\tvalid's SFC_loss: 0.901142\n",
      "[87]\ttrain's SFC_loss: 0.772679\tvalid's SFC_loss: 0.898375\n",
      "[88]\ttrain's SFC_loss: 0.771006\tvalid's SFC_loss: 0.896656\n",
      "[89]\ttrain's SFC_loss: 0.769262\tvalid's SFC_loss: 0.894115\n",
      "[90]\ttrain's SFC_loss: 0.767912\tvalid's SFC_loss: 0.892157\n",
      "[91]\ttrain's SFC_loss: 0.765174\tvalid's SFC_loss: 0.890017\n",
      "[92]\ttrain's SFC_loss: 0.7623\tvalid's SFC_loss: 0.887204\n",
      "[93]\ttrain's SFC_loss: 0.759609\tvalid's SFC_loss: 0.886169\n",
      "[94]\ttrain's SFC_loss: 0.756821\tvalid's SFC_loss: 0.885984\n",
      "[95]\ttrain's SFC_loss: 0.754054\tvalid's SFC_loss: 0.88424\n",
      "[96]\ttrain's SFC_loss: 0.75231\tvalid's SFC_loss: 0.88047\n",
      "[97]\ttrain's SFC_loss: 0.75073\tvalid's SFC_loss: 0.876941\n",
      "[98]\ttrain's SFC_loss: 0.749092\tvalid's SFC_loss: 0.874465\n",
      "[99]\ttrain's SFC_loss: 0.747188\tvalid's SFC_loss: 0.873747\n",
      "[100]\ttrain's SFC_loss: 0.745185\tvalid's SFC_loss: 0.872483\n",
      "[101]\ttrain's SFC_loss: 0.743069\tvalid's SFC_loss: 0.872323\n",
      "[102]\ttrain's SFC_loss: 0.739992\tvalid's SFC_loss: 0.874324\n",
      "[103]\ttrain's SFC_loss: 0.737136\tvalid's SFC_loss: 0.874309\n",
      "[104]\ttrain's SFC_loss: 0.735109\tvalid's SFC_loss: 0.876313\n",
      "[105]\ttrain's SFC_loss: 0.73343\tvalid's SFC_loss: 0.875964\n",
      "[106]\ttrain's SFC_loss: 0.731436\tvalid's SFC_loss: 0.877835\n",
      "[107]\ttrain's SFC_loss: 0.729791\tvalid's SFC_loss: 0.877397\n",
      "[108]\ttrain's SFC_loss: 0.728087\tvalid's SFC_loss: 0.879377\n",
      "[109]\ttrain's SFC_loss: 0.726188\tvalid's SFC_loss: 0.880361\n",
      "[110]\ttrain's SFC_loss: 0.724532\tvalid's SFC_loss: 0.881986\n",
      "[111]\ttrain's SFC_loss: 0.7229\tvalid's SFC_loss: 0.880402\n",
      "[112]\ttrain's SFC_loss: 0.720636\tvalid's SFC_loss: 0.881551\n",
      "[113]\ttrain's SFC_loss: 0.719191\tvalid's SFC_loss: 0.881324\n",
      "[114]\ttrain's SFC_loss: 0.71708\tvalid's SFC_loss: 0.879839\n",
      "[115]\ttrain's SFC_loss: 0.715596\tvalid's SFC_loss: 0.879942\n",
      "[116]\ttrain's SFC_loss: 0.713485\tvalid's SFC_loss: 0.879561\n",
      "[117]\ttrain's SFC_loss: 0.711431\tvalid's SFC_loss: 0.879105\n",
      "[118]\ttrain's SFC_loss: 0.709366\tvalid's SFC_loss: 0.878938\n",
      "[119]\ttrain's SFC_loss: 0.707644\tvalid's SFC_loss: 0.879217\n",
      "[120]\ttrain's SFC_loss: 0.706359\tvalid's SFC_loss: 0.877318\n",
      "[121]\ttrain's SFC_loss: 0.704066\tvalid's SFC_loss: 0.876616\n",
      "[122]\ttrain's SFC_loss: 0.701768\tvalid's SFC_loss: 0.877582\n",
      "[123]\ttrain's SFC_loss: 0.699442\tvalid's SFC_loss: 0.878836\n",
      "[124]\ttrain's SFC_loss: 0.697482\tvalid's SFC_loss: 0.876721\n",
      "[125]\ttrain's SFC_loss: 0.695705\tvalid's SFC_loss: 0.874952\n",
      "[126]\ttrain's SFC_loss: 0.694189\tvalid's SFC_loss: 0.87413\n",
      "[127]\ttrain's SFC_loss: 0.692509\tvalid's SFC_loss: 0.872864\n",
      "[128]\ttrain's SFC_loss: 0.690646\tvalid's SFC_loss: 0.871401\n",
      "[129]\ttrain's SFC_loss: 0.689228\tvalid's SFC_loss: 0.872061\n",
      "[130]\ttrain's SFC_loss: 0.687741\tvalid's SFC_loss: 0.872563\n",
      "[131]\ttrain's SFC_loss: 0.685603\tvalid's SFC_loss: 0.871787\n",
      "[132]\ttrain's SFC_loss: 0.682987\tvalid's SFC_loss: 0.870019\n",
      "[133]\ttrain's SFC_loss: 0.680843\tvalid's SFC_loss: 0.868132\n",
      "[134]\ttrain's SFC_loss: 0.678821\tvalid's SFC_loss: 0.867408\n",
      "[135]\ttrain's SFC_loss: 0.676347\tvalid's SFC_loss: 0.867215\n",
      "[136]\ttrain's SFC_loss: 0.674978\tvalid's SFC_loss: 0.866543\n",
      "[137]\ttrain's SFC_loss: 0.673462\tvalid's SFC_loss: 0.867539\n",
      "[138]\ttrain's SFC_loss: 0.671355\tvalid's SFC_loss: 0.866745\n",
      "[139]\ttrain's SFC_loss: 0.669374\tvalid's SFC_loss: 0.86598\n",
      "[140]\ttrain's SFC_loss: 0.667629\tvalid's SFC_loss: 0.865182\n",
      "[141]\ttrain's SFC_loss: 0.666146\tvalid's SFC_loss: 0.864686\n",
      "[142]\ttrain's SFC_loss: 0.665292\tvalid's SFC_loss: 0.864535\n",
      "[143]\ttrain's SFC_loss: 0.663943\tvalid's SFC_loss: 0.864169\n",
      "[144]\ttrain's SFC_loss: 0.662572\tvalid's SFC_loss: 0.863743\n",
      "[145]\ttrain's SFC_loss: 0.661251\tvalid's SFC_loss: 0.863424\n",
      "[146]\ttrain's SFC_loss: 0.659571\tvalid's SFC_loss: 0.863332\n",
      "[147]\ttrain's SFC_loss: 0.657772\tvalid's SFC_loss: 0.862759\n",
      "[148]\ttrain's SFC_loss: 0.656469\tvalid's SFC_loss: 0.861996\n",
      "[149]\ttrain's SFC_loss: 0.655483\tvalid's SFC_loss: 0.861522\n",
      "[150]\ttrain's SFC_loss: 0.654173\tvalid's SFC_loss: 0.861018\n",
      "[151]\ttrain's SFC_loss: 0.652425\tvalid's SFC_loss: 0.859341\n",
      "[152]\ttrain's SFC_loss: 0.650828\tvalid's SFC_loss: 0.859826\n",
      "[153]\ttrain's SFC_loss: 0.649075\tvalid's SFC_loss: 0.861723\n",
      "[154]\ttrain's SFC_loss: 0.64737\tvalid's SFC_loss: 0.863632\n",
      "[155]\ttrain's SFC_loss: 0.646\tvalid's SFC_loss: 0.865018\n",
      "[156]\ttrain's SFC_loss: 0.644171\tvalid's SFC_loss: 0.864223\n",
      "[157]\ttrain's SFC_loss: 0.642369\tvalid's SFC_loss: 0.865079\n",
      "[158]\ttrain's SFC_loss: 0.640462\tvalid's SFC_loss: 0.865341\n",
      "[159]\ttrain's SFC_loss: 0.63856\tvalid's SFC_loss: 0.864036\n",
      "[160]\ttrain's SFC_loss: 0.637022\tvalid's SFC_loss: 0.862515\n",
      "[161]\ttrain's SFC_loss: 0.636123\tvalid's SFC_loss: 0.861739\n",
      "[162]\ttrain's SFC_loss: 0.635389\tvalid's SFC_loss: 0.861174\n",
      "[163]\ttrain's SFC_loss: 0.634753\tvalid's SFC_loss: 0.859857\n",
      "[164]\ttrain's SFC_loss: 0.633819\tvalid's SFC_loss: 0.859976\n",
      "[165]\ttrain's SFC_loss: 0.633403\tvalid's SFC_loss: 0.860103\n",
      "[166]\ttrain's SFC_loss: 0.632344\tvalid's SFC_loss: 0.860456\n",
      "[167]\ttrain's SFC_loss: 0.631281\tvalid's SFC_loss: 0.859772\n",
      "[168]\ttrain's SFC_loss: 0.630097\tvalid's SFC_loss: 0.861102\n",
      "[169]\ttrain's SFC_loss: 0.62841\tvalid's SFC_loss: 0.859948\n",
      "[170]\ttrain's SFC_loss: 0.626964\tvalid's SFC_loss: 0.861676\n",
      "[171]\ttrain's SFC_loss: 0.625377\tvalid's SFC_loss: 0.860082\n",
      "[172]\ttrain's SFC_loss: 0.624376\tvalid's SFC_loss: 0.858112\n",
      "[173]\ttrain's SFC_loss: 0.623234\tvalid's SFC_loss: 0.855892\n",
      "[174]\ttrain's SFC_loss: 0.622231\tvalid's SFC_loss: 0.854452\n",
      "[175]\ttrain's SFC_loss: 0.620979\tvalid's SFC_loss: 0.853912\n",
      "[176]\ttrain's SFC_loss: 0.619475\tvalid's SFC_loss: 0.855655\n",
      "[177]\ttrain's SFC_loss: 0.617888\tvalid's SFC_loss: 0.855257\n",
      "[178]\ttrain's SFC_loss: 0.616301\tvalid's SFC_loss: 0.856887\n",
      "[179]\ttrain's SFC_loss: 0.614728\tvalid's SFC_loss: 0.855315\n",
      "[180]\ttrain's SFC_loss: 0.613566\tvalid's SFC_loss: 0.854841\n",
      "[181]\ttrain's SFC_loss: 0.612095\tvalid's SFC_loss: 0.853707\n",
      "[182]\ttrain's SFC_loss: 0.610602\tvalid's SFC_loss: 0.853502\n",
      "[183]\ttrain's SFC_loss: 0.608962\tvalid's SFC_loss: 0.853102\n",
      "[184]\ttrain's SFC_loss: 0.607596\tvalid's SFC_loss: 0.852757\n",
      "[185]\ttrain's SFC_loss: 0.606449\tvalid's SFC_loss: 0.85194\n",
      "[186]\ttrain's SFC_loss: 0.604634\tvalid's SFC_loss: 0.851889\n",
      "[187]\ttrain's SFC_loss: 0.602969\tvalid's SFC_loss: 0.851282\n",
      "[188]\ttrain's SFC_loss: 0.601087\tvalid's SFC_loss: 0.854362\n",
      "[189]\ttrain's SFC_loss: 0.59962\tvalid's SFC_loss: 0.855344\n",
      "[190]\ttrain's SFC_loss: 0.598006\tvalid's SFC_loss: 0.854768\n",
      "[191]\ttrain's SFC_loss: 0.596731\tvalid's SFC_loss: 0.854674\n",
      "[192]\ttrain's SFC_loss: 0.595697\tvalid's SFC_loss: 0.854724\n",
      "[193]\ttrain's SFC_loss: 0.594804\tvalid's SFC_loss: 0.851917\n",
      "[194]\ttrain's SFC_loss: 0.594158\tvalid's SFC_loss: 0.851277\n",
      "[195]\ttrain's SFC_loss: 0.59313\tvalid's SFC_loss: 0.850633\n",
      "[196]\ttrain's SFC_loss: 0.59182\tvalid's SFC_loss: 0.851857\n",
      "[197]\ttrain's SFC_loss: 0.590294\tvalid's SFC_loss: 0.848791\n",
      "[198]\ttrain's SFC_loss: 0.588937\tvalid's SFC_loss: 0.849429\n",
      "[199]\ttrain's SFC_loss: 0.587334\tvalid's SFC_loss: 0.84727\n",
      "[200]\ttrain's SFC_loss: 0.586083\tvalid's SFC_loss: 0.847006\n",
      "[201]\ttrain's SFC_loss: 0.584354\tvalid's SFC_loss: 0.846941\n",
      "[202]\ttrain's SFC_loss: 0.582705\tvalid's SFC_loss: 0.846279\n",
      "[203]\ttrain's SFC_loss: 0.580891\tvalid's SFC_loss: 0.846466\n",
      "[204]\ttrain's SFC_loss: 0.578878\tvalid's SFC_loss: 0.849041\n",
      "[205]\ttrain's SFC_loss: 0.577108\tvalid's SFC_loss: 0.849682\n",
      "[206]\ttrain's SFC_loss: 0.576294\tvalid's SFC_loss: 0.847908\n",
      "[207]\ttrain's SFC_loss: 0.575488\tvalid's SFC_loss: 0.847268\n",
      "[208]\ttrain's SFC_loss: 0.574891\tvalid's SFC_loss: 0.846047\n",
      "[209]\ttrain's SFC_loss: 0.574257\tvalid's SFC_loss: 0.845416\n",
      "[210]\ttrain's SFC_loss: 0.573349\tvalid's SFC_loss: 0.844654\n",
      "[211]\ttrain's SFC_loss: 0.572556\tvalid's SFC_loss: 0.842003\n",
      "[212]\ttrain's SFC_loss: 0.571683\tvalid's SFC_loss: 0.839601\n",
      "[213]\ttrain's SFC_loss: 0.570879\tvalid's SFC_loss: 0.836234\n",
      "[214]\ttrain's SFC_loss: 0.570033\tvalid's SFC_loss: 0.835354\n",
      "[215]\ttrain's SFC_loss: 0.569149\tvalid's SFC_loss: 0.833416\n",
      "[216]\ttrain's SFC_loss: 0.567636\tvalid's SFC_loss: 0.83301\n",
      "[217]\ttrain's SFC_loss: 0.566591\tvalid's SFC_loss: 0.833685\n",
      "[218]\ttrain's SFC_loss: 0.565248\tvalid's SFC_loss: 0.834424\n",
      "[219]\ttrain's SFC_loss: 0.563778\tvalid's SFC_loss: 0.832545\n",
      "[220]\ttrain's SFC_loss: 0.562491\tvalid's SFC_loss: 0.829544\n",
      "[221]\ttrain's SFC_loss: 0.56096\tvalid's SFC_loss: 0.830324\n",
      "[222]\ttrain's SFC_loss: 0.559603\tvalid's SFC_loss: 0.830585\n",
      "[223]\ttrain's SFC_loss: 0.558142\tvalid's SFC_loss: 0.829164\n",
      "[224]\ttrain's SFC_loss: 0.556687\tvalid's SFC_loss: 0.829129\n",
      "[225]\ttrain's SFC_loss: 0.555072\tvalid's SFC_loss: 0.828725\n",
      "[226]\ttrain's SFC_loss: 0.554024\tvalid's SFC_loss: 0.830617\n",
      "[227]\ttrain's SFC_loss: 0.552694\tvalid's SFC_loss: 0.83087\n",
      "[228]\ttrain's SFC_loss: 0.551587\tvalid's SFC_loss: 0.83222\n",
      "[229]\ttrain's SFC_loss: 0.550592\tvalid's SFC_loss: 0.832234\n",
      "[230]\ttrain's SFC_loss: 0.5492\tvalid's SFC_loss: 0.832224\n",
      "[231]\ttrain's SFC_loss: 0.548387\tvalid's SFC_loss: 0.831701\n",
      "[232]\ttrain's SFC_loss: 0.547615\tvalid's SFC_loss: 0.831205\n",
      "[233]\ttrain's SFC_loss: 0.547066\tvalid's SFC_loss: 0.830455\n",
      "[234]\ttrain's SFC_loss: 0.546634\tvalid's SFC_loss: 0.83003\n",
      "[235]\ttrain's SFC_loss: 0.545992\tvalid's SFC_loss: 0.82928\n",
      "[236]\ttrain's SFC_loss: 0.545111\tvalid's SFC_loss: 0.826698\n",
      "[237]\ttrain's SFC_loss: 0.54469\tvalid's SFC_loss: 0.825557\n",
      "[238]\ttrain's SFC_loss: 0.543671\tvalid's SFC_loss: 0.822092\n",
      "[239]\ttrain's SFC_loss: 0.542623\tvalid's SFC_loss: 0.817826\n",
      "[240]\ttrain's SFC_loss: 0.541668\tvalid's SFC_loss: 0.815859\n",
      "[241]\ttrain's SFC_loss: 0.54074\tvalid's SFC_loss: 0.817085\n",
      "[242]\ttrain's SFC_loss: 0.539782\tvalid's SFC_loss: 0.819772\n",
      "[243]\ttrain's SFC_loss: 0.538905\tvalid's SFC_loss: 0.820512\n",
      "[244]\ttrain's SFC_loss: 0.53795\tvalid's SFC_loss: 0.822925\n",
      "[245]\ttrain's SFC_loss: 0.536914\tvalid's SFC_loss: 0.824284\n",
      "[246]\ttrain's SFC_loss: 0.535864\tvalid's SFC_loss: 0.826694\n",
      "[247]\ttrain's SFC_loss: 0.534845\tvalid's SFC_loss: 0.826643\n",
      "[248]\ttrain's SFC_loss: 0.533558\tvalid's SFC_loss: 0.828131\n",
      "[249]\ttrain's SFC_loss: 0.532443\tvalid's SFC_loss: 0.829493\n",
      "[250]\ttrain's SFC_loss: 0.531407\tvalid's SFC_loss: 0.830653\n",
      "[251]\ttrain's SFC_loss: 0.530586\tvalid's SFC_loss: 0.831903\n",
      "[252]\ttrain's SFC_loss: 0.529882\tvalid's SFC_loss: 0.833254\n",
      "[253]\ttrain's SFC_loss: 0.529096\tvalid's SFC_loss: 0.833952\n",
      "[254]\ttrain's SFC_loss: 0.528191\tvalid's SFC_loss: 0.83521\n",
      "[255]\ttrain's SFC_loss: 0.527227\tvalid's SFC_loss: 0.835529\n",
      "[256]\ttrain's SFC_loss: 0.526164\tvalid's SFC_loss: 0.834167\n",
      "[257]\ttrain's SFC_loss: 0.525394\tvalid's SFC_loss: 0.832036\n",
      "[258]\ttrain's SFC_loss: 0.524402\tvalid's SFC_loss: 0.830486\n",
      "[259]\ttrain's SFC_loss: 0.523653\tvalid's SFC_loss: 0.828393\n",
      "[260]\ttrain's SFC_loss: 0.522542\tvalid's SFC_loss: 0.827002\n",
      "[261]\ttrain's SFC_loss: 0.521383\tvalid's SFC_loss: 0.829248\n",
      "[262]\ttrain's SFC_loss: 0.520559\tvalid's SFC_loss: 0.83151\n",
      "[263]\ttrain's SFC_loss: 0.519662\tvalid's SFC_loss: 0.831577\n",
      "[264]\ttrain's SFC_loss: 0.518829\tvalid's SFC_loss: 0.834578\n",
      "[265]\ttrain's SFC_loss: 0.517966\tvalid's SFC_loss: 0.837205\n",
      "[266]\ttrain's SFC_loss: 0.516662\tvalid's SFC_loss: 0.838841\n",
      "[267]\ttrain's SFC_loss: 0.515603\tvalid's SFC_loss: 0.840083\n",
      "[268]\ttrain's SFC_loss: 0.51466\tvalid's SFC_loss: 0.841452\n",
      "[269]\ttrain's SFC_loss: 0.51362\tvalid's SFC_loss: 0.841477\n",
      "[270]\ttrain's SFC_loss: 0.512765\tvalid's SFC_loss: 0.840654\n",
      "[271]\ttrain's SFC_loss: 0.511655\tvalid's SFC_loss: 0.84087\n",
      "[272]\ttrain's SFC_loss: 0.51061\tvalid's SFC_loss: 0.841517\n",
      "[273]\ttrain's SFC_loss: 0.509715\tvalid's SFC_loss: 0.841302\n",
      "[274]\ttrain's SFC_loss: 0.508546\tvalid's SFC_loss: 0.84272\n",
      "[275]\ttrain's SFC_loss: 0.507455\tvalid's SFC_loss: 0.843434\n",
      "[276]\ttrain's SFC_loss: 0.506318\tvalid's SFC_loss: 0.842661\n",
      "[277]\ttrain's SFC_loss: 0.505201\tvalid's SFC_loss: 0.843218\n",
      "[278]\ttrain's SFC_loss: 0.504339\tvalid's SFC_loss: 0.844874\n",
      "[279]\ttrain's SFC_loss: 0.503487\tvalid's SFC_loss: 0.846875\n",
      "[280]\ttrain's SFC_loss: 0.502703\tvalid's SFC_loss: 0.848262\n",
      "[281]\ttrain's SFC_loss: 0.501582\tvalid's SFC_loss: 0.84769\n",
      "[282]\ttrain's SFC_loss: 0.500695\tvalid's SFC_loss: 0.847298\n",
      "[283]\ttrain's SFC_loss: 0.499782\tvalid's SFC_loss: 0.846944\n",
      "[284]\ttrain's SFC_loss: 0.498842\tvalid's SFC_loss: 0.846012\n",
      "[285]\ttrain's SFC_loss: 0.497948\tvalid's SFC_loss: 0.845958\n",
      "[286]\ttrain's SFC_loss: 0.496939\tvalid's SFC_loss: 0.845684\n",
      "[287]\ttrain's SFC_loss: 0.495916\tvalid's SFC_loss: 0.845168\n",
      "[288]\ttrain's SFC_loss: 0.49487\tvalid's SFC_loss: 0.844192\n",
      "[289]\ttrain's SFC_loss: 0.493978\tvalid's SFC_loss: 0.841895\n",
      "[290]\ttrain's SFC_loss: 0.492986\tvalid's SFC_loss: 0.838862\n",
      "[291]\ttrain's SFC_loss: 0.492333\tvalid's SFC_loss: 0.837769\n",
      "[292]\ttrain's SFC_loss: 0.491459\tvalid's SFC_loss: 0.837491\n",
      "[293]\ttrain's SFC_loss: 0.490333\tvalid's SFC_loss: 0.836733\n",
      "[294]\ttrain's SFC_loss: 0.489292\tvalid's SFC_loss: 0.836543\n",
      "[295]\ttrain's SFC_loss: 0.488303\tvalid's SFC_loss: 0.835971\n",
      "[296]\ttrain's SFC_loss: 0.488019\tvalid's SFC_loss: 0.836367\n",
      "[297]\ttrain's SFC_loss: 0.487549\tvalid's SFC_loss: 0.837153\n",
      "[298]\ttrain's SFC_loss: 0.487339\tvalid's SFC_loss: 0.838475\n",
      "[299]\ttrain's SFC_loss: 0.486842\tvalid's SFC_loss: 0.838824\n",
      "[300]\ttrain's SFC_loss: 0.486682\tvalid's SFC_loss: 0.838925\n",
      "[301]\ttrain's SFC_loss: 0.485813\tvalid's SFC_loss: 0.837137\n",
      "[302]\ttrain's SFC_loss: 0.485204\tvalid's SFC_loss: 0.835894\n",
      "[303]\ttrain's SFC_loss: 0.484362\tvalid's SFC_loss: 0.835161\n",
      "[304]\ttrain's SFC_loss: 0.483832\tvalid's SFC_loss: 0.8339\n",
      "[305]\ttrain's SFC_loss: 0.483198\tvalid's SFC_loss: 0.836481\n",
      "[306]\ttrain's SFC_loss: 0.481836\tvalid's SFC_loss: 0.834764\n",
      "[307]\ttrain's SFC_loss: 0.480672\tvalid's SFC_loss: 0.834959\n",
      "[308]\ttrain's SFC_loss: 0.479532\tvalid's SFC_loss: 0.834627\n",
      "[309]\ttrain's SFC_loss: 0.478253\tvalid's SFC_loss: 0.83434\n",
      "[310]\ttrain's SFC_loss: 0.47697\tvalid's SFC_loss: 0.833616\n",
      "[311]\ttrain's SFC_loss: 0.476102\tvalid's SFC_loss: 0.833899\n",
      "[312]\ttrain's SFC_loss: 0.475523\tvalid's SFC_loss: 0.833784\n",
      "[313]\ttrain's SFC_loss: 0.474579\tvalid's SFC_loss: 0.835543\n",
      "[314]\ttrain's SFC_loss: 0.473866\tvalid's SFC_loss: 0.836322\n",
      "[315]\ttrain's SFC_loss: 0.47313\tvalid's SFC_loss: 0.836544\n",
      "[316]\ttrain's SFC_loss: 0.472622\tvalid's SFC_loss: 0.837477\n",
      "[317]\ttrain's SFC_loss: 0.47212\tvalid's SFC_loss: 0.837553\n",
      "[318]\ttrain's SFC_loss: 0.47166\tvalid's SFC_loss: 0.839208\n",
      "[319]\ttrain's SFC_loss: 0.471127\tvalid's SFC_loss: 0.841136\n",
      "[320]\ttrain's SFC_loss: 0.470792\tvalid's SFC_loss: 0.84279\n",
      "[321]\ttrain's SFC_loss: 0.469717\tvalid's SFC_loss: 0.843863\n",
      "[322]\ttrain's SFC_loss: 0.468969\tvalid's SFC_loss: 0.844832\n",
      "[323]\ttrain's SFC_loss: 0.467873\tvalid's SFC_loss: 0.84699\n",
      "[324]\ttrain's SFC_loss: 0.466878\tvalid's SFC_loss: 0.848494\n",
      "[325]\ttrain's SFC_loss: 0.465977\tvalid's SFC_loss: 0.848871\n",
      "[326]\ttrain's SFC_loss: 0.464993\tvalid's SFC_loss: 0.849949\n",
      "[327]\ttrain's SFC_loss: 0.464042\tvalid's SFC_loss: 0.851031\n",
      "[328]\ttrain's SFC_loss: 0.463349\tvalid's SFC_loss: 0.85025\n",
      "[329]\ttrain's SFC_loss: 0.462537\tvalid's SFC_loss: 0.848799\n",
      "[330]\ttrain's SFC_loss: 0.461878\tvalid's SFC_loss: 0.846943\n",
      "[331]\ttrain's SFC_loss: 0.460987\tvalid's SFC_loss: 0.847393\n",
      "[332]\ttrain's SFC_loss: 0.459989\tvalid's SFC_loss: 0.847177\n",
      "[333]\ttrain's SFC_loss: 0.458842\tvalid's SFC_loss: 0.847131\n",
      "[334]\ttrain's SFC_loss: 0.457926\tvalid's SFC_loss: 0.849012\n",
      "[335]\ttrain's SFC_loss: 0.456899\tvalid's SFC_loss: 0.849119\n",
      "[336]\ttrain's SFC_loss: 0.456165\tvalid's SFC_loss: 0.850141\n",
      "[337]\ttrain's SFC_loss: 0.455538\tvalid's SFC_loss: 0.849394\n",
      "[338]\ttrain's SFC_loss: 0.455179\tvalid's SFC_loss: 0.849345\n",
      "[339]\ttrain's SFC_loss: 0.454837\tvalid's SFC_loss: 0.849316\n",
      "[340]\ttrain's SFC_loss: 0.454512\tvalid's SFC_loss: 0.849307\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttrain's SFC_loss: 0.541668\tvalid's SFC_loss: 0.815859\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1    0.011792\n",
      "1    feature2   -0.002934\n",
      "2    feature3   -0.120012\n",
      "3    feature4   -0.017505\n",
      "4    feature5    0.035932\n",
      "5    feature6    0.067427\n",
      "6    feature7   -0.057404\n",
      "7    feature8    0.046650\n",
      "8    feature9    0.061452\n",
      "9   feature10   -0.047754\n",
      "10  feature11    0.005626\n",
      "11  feature12   -0.029604\n",
      "12  feature13   -0.009558\n",
      "13  feature14   -0.091729\n",
      "14  feature15   -0.002610\n",
      "15  feature16    0.047324\n",
      "16  feature17    0.025561\n",
      "17  feature18    0.097659\n",
      "18  feature19    0.100577\n",
      "19  feature20   -0.120890\n",
      "-------------------- 1 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05752\tvalid's multi_logloss: 1.06822\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05287\tvalid's multi_logloss: 1.06819\n",
      "[3]\ttrain's multi_logloss: 1.04903\tvalid's multi_logloss: 1.07082\n",
      "[4]\ttrain's multi_logloss: 1.04616\tvalid's multi_logloss: 1.07013\n",
      "[5]\ttrain's multi_logloss: 1.04316\tvalid's multi_logloss: 1.07106\n",
      "[6]\ttrain's multi_logloss: 1.0392\tvalid's multi_logloss: 1.07126\n",
      "[7]\ttrain's multi_logloss: 1.03484\tvalid's multi_logloss: 1.06883\n",
      "[8]\ttrain's multi_logloss: 1.03115\tvalid's multi_logloss: 1.06742\n",
      "[9]\ttrain's multi_logloss: 1.02726\tvalid's multi_logloss: 1.06741\n",
      "[10]\ttrain's multi_logloss: 1.02368\tvalid's multi_logloss: 1.06605\n",
      "[11]\ttrain's multi_logloss: 1.01949\tvalid's multi_logloss: 1.06932\n",
      "[12]\ttrain's multi_logloss: 1.01521\tvalid's multi_logloss: 1.07176\n",
      "[13]\ttrain's multi_logloss: 1.01102\tvalid's multi_logloss: 1.07287\n",
      "[14]\ttrain's multi_logloss: 1.00759\tvalid's multi_logloss: 1.0743\n",
      "[15]\ttrain's multi_logloss: 1.00376\tvalid's multi_logloss: 1.0764\n",
      "[16]\ttrain's multi_logloss: 0.999494\tvalid's multi_logloss: 1.07693\n",
      "[17]\ttrain's multi_logloss: 0.995748\tvalid's multi_logloss: 1.07575\n",
      "[18]\ttrain's multi_logloss: 0.992977\tvalid's multi_logloss: 1.07865\n",
      "[19]\ttrain's multi_logloss: 0.989451\tvalid's multi_logloss: 1.07992\n",
      "[20]\ttrain's multi_logloss: 0.986557\tvalid's multi_logloss: 1.08067\n",
      "[21]\ttrain's multi_logloss: 0.98322\tvalid's multi_logloss: 1.07837\n",
      "[22]\ttrain's multi_logloss: 0.979104\tvalid's multi_logloss: 1.07886\n",
      "[23]\ttrain's multi_logloss: 0.97503\tvalid's multi_logloss: 1.07932\n",
      "[24]\ttrain's multi_logloss: 0.971167\tvalid's multi_logloss: 1.08004\n",
      "[25]\ttrain's multi_logloss: 0.967401\tvalid's multi_logloss: 1.07982\n",
      "[26]\ttrain's multi_logloss: 0.964099\tvalid's multi_logloss: 1.0766\n",
      "[27]\ttrain's multi_logloss: 0.960165\tvalid's multi_logloss: 1.07823\n",
      "[28]\ttrain's multi_logloss: 0.956576\tvalid's multi_logloss: 1.07916\n",
      "[29]\ttrain's multi_logloss: 0.953212\tvalid's multi_logloss: 1.07591\n",
      "[30]\ttrain's multi_logloss: 0.949151\tvalid's multi_logloss: 1.07552\n",
      "[31]\ttrain's multi_logloss: 0.946259\tvalid's multi_logloss: 1.07616\n",
      "[32]\ttrain's multi_logloss: 0.942633\tvalid's multi_logloss: 1.07799\n",
      "[33]\ttrain's multi_logloss: 0.939507\tvalid's multi_logloss: 1.07817\n",
      "[34]\ttrain's multi_logloss: 0.936302\tvalid's multi_logloss: 1.07975\n",
      "[35]\ttrain's multi_logloss: 0.933116\tvalid's multi_logloss: 1.07992\n",
      "[36]\ttrain's multi_logloss: 0.92936\tvalid's multi_logloss: 1.08007\n",
      "[37]\ttrain's multi_logloss: 0.92574\tvalid's multi_logloss: 1.08135\n",
      "[38]\ttrain's multi_logloss: 0.921431\tvalid's multi_logloss: 1.08226\n",
      "[39]\ttrain's multi_logloss: 0.917243\tvalid's multi_logloss: 1.08323\n",
      "[40]\ttrain's multi_logloss: 0.913222\tvalid's multi_logloss: 1.08534\n",
      "[41]\ttrain's multi_logloss: 0.910331\tvalid's multi_logloss: 1.08459\n",
      "[42]\ttrain's multi_logloss: 0.908176\tvalid's multi_logloss: 1.08365\n",
      "[43]\ttrain's multi_logloss: 0.905869\tvalid's multi_logloss: 1.08347\n",
      "[44]\ttrain's multi_logloss: 0.903422\tvalid's multi_logloss: 1.08212\n",
      "[45]\ttrain's multi_logloss: 0.90018\tvalid's multi_logloss: 1.08395\n",
      "[46]\ttrain's multi_logloss: 0.897705\tvalid's multi_logloss: 1.08374\n",
      "[47]\ttrain's multi_logloss: 0.895481\tvalid's multi_logloss: 1.08393\n",
      "[48]\ttrain's multi_logloss: 0.893254\tvalid's multi_logloss: 1.08517\n",
      "[49]\ttrain's multi_logloss: 0.891468\tvalid's multi_logloss: 1.08574\n",
      "[50]\ttrain's multi_logloss: 0.889359\tvalid's multi_logloss: 1.08596\n",
      "[51]\ttrain's multi_logloss: 0.886357\tvalid's multi_logloss: 1.08797\n",
      "[52]\ttrain's multi_logloss: 0.883816\tvalid's multi_logloss: 1.08865\n",
      "[53]\ttrain's multi_logloss: 0.88121\tvalid's multi_logloss: 1.08918\n",
      "[54]\ttrain's multi_logloss: 0.879446\tvalid's multi_logloss: 1.09015\n",
      "[55]\ttrain's multi_logloss: 0.876605\tvalid's multi_logloss: 1.09223\n",
      "[56]\ttrain's multi_logloss: 0.87401\tvalid's multi_logloss: 1.09236\n",
      "[57]\ttrain's multi_logloss: 0.87175\tvalid's multi_logloss: 1.09298\n",
      "[58]\ttrain's multi_logloss: 0.869379\tvalid's multi_logloss: 1.09181\n",
      "[59]\ttrain's multi_logloss: 0.866569\tvalid's multi_logloss: 1.0924\n",
      "[60]\ttrain's multi_logloss: 0.863941\tvalid's multi_logloss: 1.09346\n",
      "[61]\ttrain's multi_logloss: 0.860957\tvalid's multi_logloss: 1.09482\n",
      "[62]\ttrain's multi_logloss: 0.857935\tvalid's multi_logloss: 1.09025\n",
      "[63]\ttrain's multi_logloss: 0.855278\tvalid's multi_logloss: 1.0891\n",
      "[64]\ttrain's multi_logloss: 0.852745\tvalid's multi_logloss: 1.08859\n",
      "[65]\ttrain's multi_logloss: 0.850256\tvalid's multi_logloss: 1.08839\n",
      "[66]\ttrain's multi_logloss: 0.847666\tvalid's multi_logloss: 1.08991\n",
      "[67]\ttrain's multi_logloss: 0.844613\tvalid's multi_logloss: 1.09299\n",
      "[68]\ttrain's multi_logloss: 0.84184\tvalid's multi_logloss: 1.09627\n",
      "[69]\ttrain's multi_logloss: 0.839058\tvalid's multi_logloss: 1.09925\n",
      "[70]\ttrain's multi_logloss: 0.836327\tvalid's multi_logloss: 1.10227\n",
      "[71]\ttrain's multi_logloss: 0.834503\tvalid's multi_logloss: 1.1036\n",
      "[72]\ttrain's multi_logloss: 0.832718\tvalid's multi_logloss: 1.10541\n",
      "[73]\ttrain's multi_logloss: 0.830708\tvalid's multi_logloss: 1.10692\n",
      "[74]\ttrain's multi_logloss: 0.828924\tvalid's multi_logloss: 1.10785\n",
      "[75]\ttrain's multi_logloss: 0.827572\tvalid's multi_logloss: 1.10726\n",
      "[76]\ttrain's multi_logloss: 0.82461\tvalid's multi_logloss: 1.10882\n",
      "[77]\ttrain's multi_logloss: 0.82186\tvalid's multi_logloss: 1.10994\n",
      "[78]\ttrain's multi_logloss: 0.819277\tvalid's multi_logloss: 1.10958\n",
      "[79]\ttrain's multi_logloss: 0.816624\tvalid's multi_logloss: 1.10767\n",
      "[80]\ttrain's multi_logloss: 0.813904\tvalid's multi_logloss: 1.10772\n",
      "[81]\ttrain's multi_logloss: 0.81192\tvalid's multi_logloss: 1.10455\n",
      "[82]\ttrain's multi_logloss: 0.81001\tvalid's multi_logloss: 1.1031\n",
      "[83]\ttrain's multi_logloss: 0.807815\tvalid's multi_logloss: 1.10271\n",
      "[84]\ttrain's multi_logloss: 0.805705\tvalid's multi_logloss: 1.10277\n",
      "[85]\ttrain's multi_logloss: 0.803289\tvalid's multi_logloss: 1.10261\n",
      "[86]\ttrain's multi_logloss: 0.800513\tvalid's multi_logloss: 1.10247\n",
      "[87]\ttrain's multi_logloss: 0.798326\tvalid's multi_logloss: 1.1026\n",
      "[88]\ttrain's multi_logloss: 0.795909\tvalid's multi_logloss: 1.10186\n",
      "[89]\ttrain's multi_logloss: 0.793261\tvalid's multi_logloss: 1.10183\n",
      "[90]\ttrain's multi_logloss: 0.791081\tvalid's multi_logloss: 1.10238\n",
      "[91]\ttrain's multi_logloss: 0.788564\tvalid's multi_logloss: 1.10031\n",
      "[92]\ttrain's multi_logloss: 0.786784\tvalid's multi_logloss: 1.09961\n",
      "[93]\ttrain's multi_logloss: 0.783887\tvalid's multi_logloss: 1.0993\n",
      "[94]\ttrain's multi_logloss: 0.781478\tvalid's multi_logloss: 1.09807\n",
      "[95]\ttrain's multi_logloss: 0.778946\tvalid's multi_logloss: 1.09646\n",
      "[96]\ttrain's multi_logloss: 0.776473\tvalid's multi_logloss: 1.09783\n",
      "[97]\ttrain's multi_logloss: 0.774023\tvalid's multi_logloss: 1.10046\n",
      "[98]\ttrain's multi_logloss: 0.771841\tvalid's multi_logloss: 1.10302\n",
      "[99]\ttrain's multi_logloss: 0.769463\tvalid's multi_logloss: 1.10567\n",
      "[100]\ttrain's multi_logloss: 0.767103\tvalid's multi_logloss: 1.10761\n",
      "[101]\ttrain's multi_logloss: 0.765173\tvalid's multi_logloss: 1.10813\n",
      "[102]\ttrain's multi_logloss: 0.762747\tvalid's multi_logloss: 1.11008\n",
      "[103]\ttrain's multi_logloss: 0.760848\tvalid's multi_logloss: 1.11459\n",
      "[104]\ttrain's multi_logloss: 0.758658\tvalid's multi_logloss: 1.11317\n",
      "[105]\ttrain's multi_logloss: 0.756919\tvalid's multi_logloss: 1.11311\n",
      "[106]\ttrain's multi_logloss: 0.755543\tvalid's multi_logloss: 1.11438\n",
      "[107]\ttrain's multi_logloss: 0.753673\tvalid's multi_logloss: 1.11367\n",
      "[108]\ttrain's multi_logloss: 0.75186\tvalid's multi_logloss: 1.1128\n",
      "[109]\ttrain's multi_logloss: 0.750164\tvalid's multi_logloss: 1.11283\n",
      "[110]\ttrain's multi_logloss: 0.748357\tvalid's multi_logloss: 1.11138\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttrain's multi_logloss: 1.02368\tvalid's multi_logloss: 1.06605\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.45454545454545453\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's SFC_loss: 1.09296\tvalid's SFC_loss: 1.09802\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's SFC_loss: 1.08697\tvalid's SFC_loss: 1.0989\n",
      "[3]\ttrain's SFC_loss: 1.08027\tvalid's SFC_loss: 1.09694\n",
      "[4]\ttrain's SFC_loss: 1.07524\tvalid's SFC_loss: 1.09674\n",
      "[5]\ttrain's SFC_loss: 1.07099\tvalid's SFC_loss: 1.0964\n",
      "[6]\ttrain's SFC_loss: 1.06587\tvalid's SFC_loss: 1.0983\n",
      "[7]\ttrain's SFC_loss: 1.05965\tvalid's SFC_loss: 1.09354\n",
      "[8]\ttrain's SFC_loss: 1.05372\tvalid's SFC_loss: 1.09488\n",
      "[9]\ttrain's SFC_loss: 1.04824\tvalid's SFC_loss: 1.09225\n",
      "[10]\ttrain's SFC_loss: 1.04238\tvalid's SFC_loss: 1.0929\n",
      "[11]\ttrain's SFC_loss: 1.03715\tvalid's SFC_loss: 1.09401\n",
      "[12]\ttrain's SFC_loss: 1.03112\tvalid's SFC_loss: 1.09826\n",
      "[13]\ttrain's SFC_loss: 1.02513\tvalid's SFC_loss: 1.10229\n",
      "[14]\ttrain's SFC_loss: 1.01991\tvalid's SFC_loss: 1.10492\n",
      "[15]\ttrain's SFC_loss: 1.0144\tvalid's SFC_loss: 1.10722\n",
      "[16]\ttrain's SFC_loss: 1.0096\tvalid's SFC_loss: 1.10793\n",
      "[17]\ttrain's SFC_loss: 1.00482\tvalid's SFC_loss: 1.109\n",
      "[18]\ttrain's SFC_loss: 1.00076\tvalid's SFC_loss: 1.11147\n",
      "[19]\ttrain's SFC_loss: 0.997045\tvalid's SFC_loss: 1.11349\n",
      "[20]\ttrain's SFC_loss: 0.99385\tvalid's SFC_loss: 1.11658\n",
      "[21]\ttrain's SFC_loss: 0.989362\tvalid's SFC_loss: 1.1125\n",
      "[22]\ttrain's SFC_loss: 0.985461\tvalid's SFC_loss: 1.11058\n",
      "[23]\ttrain's SFC_loss: 0.980809\tvalid's SFC_loss: 1.10756\n",
      "[24]\ttrain's SFC_loss: 0.976135\tvalid's SFC_loss: 1.1056\n",
      "[25]\ttrain's SFC_loss: 0.971338\tvalid's SFC_loss: 1.10658\n",
      "[26]\ttrain's SFC_loss: 0.966258\tvalid's SFC_loss: 1.10724\n",
      "[27]\ttrain's SFC_loss: 0.961032\tvalid's SFC_loss: 1.10879\n",
      "[28]\ttrain's SFC_loss: 0.956493\tvalid's SFC_loss: 1.10912\n",
      "[29]\ttrain's SFC_loss: 0.95173\tvalid's SFC_loss: 1.10911\n",
      "[30]\ttrain's SFC_loss: 0.946326\tvalid's SFC_loss: 1.11497\n",
      "[31]\ttrain's SFC_loss: 0.941994\tvalid's SFC_loss: 1.11581\n",
      "[32]\ttrain's SFC_loss: 0.937666\tvalid's SFC_loss: 1.11572\n",
      "[33]\ttrain's SFC_loss: 0.933385\tvalid's SFC_loss: 1.11642\n",
      "[34]\ttrain's SFC_loss: 0.92892\tvalid's SFC_loss: 1.11844\n",
      "[35]\ttrain's SFC_loss: 0.924873\tvalid's SFC_loss: 1.11508\n",
      "[36]\ttrain's SFC_loss: 0.920605\tvalid's SFC_loss: 1.11167\n",
      "[37]\ttrain's SFC_loss: 0.916313\tvalid's SFC_loss: 1.11494\n",
      "[38]\ttrain's SFC_loss: 0.91123\tvalid's SFC_loss: 1.11721\n",
      "[39]\ttrain's SFC_loss: 0.906367\tvalid's SFC_loss: 1.11977\n",
      "[40]\ttrain's SFC_loss: 0.901963\tvalid's SFC_loss: 1.12204\n",
      "[41]\ttrain's SFC_loss: 0.899122\tvalid's SFC_loss: 1.11962\n",
      "[42]\ttrain's SFC_loss: 0.895993\tvalid's SFC_loss: 1.11846\n",
      "[43]\ttrain's SFC_loss: 0.892409\tvalid's SFC_loss: 1.11722\n",
      "[44]\ttrain's SFC_loss: 0.8901\tvalid's SFC_loss: 1.1159\n",
      "[45]\ttrain's SFC_loss: 0.887132\tvalid's SFC_loss: 1.11596\n",
      "[46]\ttrain's SFC_loss: 0.884409\tvalid's SFC_loss: 1.11552\n",
      "[47]\ttrain's SFC_loss: 0.880968\tvalid's SFC_loss: 1.11097\n",
      "[48]\ttrain's SFC_loss: 0.878472\tvalid's SFC_loss: 1.1108\n",
      "[49]\ttrain's SFC_loss: 0.876253\tvalid's SFC_loss: 1.11195\n",
      "[50]\ttrain's SFC_loss: 0.873204\tvalid's SFC_loss: 1.11011\n",
      "[51]\ttrain's SFC_loss: 0.870254\tvalid's SFC_loss: 1.10946\n",
      "[52]\ttrain's SFC_loss: 0.866735\tvalid's SFC_loss: 1.11217\n",
      "[53]\ttrain's SFC_loss: 0.864007\tvalid's SFC_loss: 1.11525\n",
      "[54]\ttrain's SFC_loss: 0.860821\tvalid's SFC_loss: 1.1186\n",
      "[55]\ttrain's SFC_loss: 0.858522\tvalid's SFC_loss: 1.12154\n",
      "[56]\ttrain's SFC_loss: 0.855593\tvalid's SFC_loss: 1.12011\n",
      "[57]\ttrain's SFC_loss: 0.852582\tvalid's SFC_loss: 1.12024\n",
      "[58]\ttrain's SFC_loss: 0.849475\tvalid's SFC_loss: 1.12125\n",
      "[59]\ttrain's SFC_loss: 0.846405\tvalid's SFC_loss: 1.12177\n",
      "[60]\ttrain's SFC_loss: 0.84341\tvalid's SFC_loss: 1.12233\n",
      "[61]\ttrain's SFC_loss: 0.840266\tvalid's SFC_loss: 1.12001\n",
      "[62]\ttrain's SFC_loss: 0.837306\tvalid's SFC_loss: 1.12179\n",
      "[63]\ttrain's SFC_loss: 0.834058\tvalid's SFC_loss: 1.12125\n",
      "[64]\ttrain's SFC_loss: 0.830443\tvalid's SFC_loss: 1.11682\n",
      "[65]\ttrain's SFC_loss: 0.827331\tvalid's SFC_loss: 1.11668\n",
      "[66]\ttrain's SFC_loss: 0.824926\tvalid's SFC_loss: 1.11555\n",
      "[67]\ttrain's SFC_loss: 0.821293\tvalid's SFC_loss: 1.11842\n",
      "[68]\ttrain's SFC_loss: 0.818284\tvalid's SFC_loss: 1.12175\n",
      "[69]\ttrain's SFC_loss: 0.815223\tvalid's SFC_loss: 1.12588\n",
      "[70]\ttrain's SFC_loss: 0.811973\tvalid's SFC_loss: 1.13055\n",
      "[71]\ttrain's SFC_loss: 0.808911\tvalid's SFC_loss: 1.13152\n",
      "[72]\ttrain's SFC_loss: 0.806701\tvalid's SFC_loss: 1.13234\n",
      "[73]\ttrain's SFC_loss: 0.80463\tvalid's SFC_loss: 1.13309\n",
      "[74]\ttrain's SFC_loss: 0.802072\tvalid's SFC_loss: 1.13266\n",
      "[75]\ttrain's SFC_loss: 0.799804\tvalid's SFC_loss: 1.13184\n",
      "[76]\ttrain's SFC_loss: 0.796381\tvalid's SFC_loss: 1.13399\n",
      "[77]\ttrain's SFC_loss: 0.793597\tvalid's SFC_loss: 1.1328\n",
      "[78]\ttrain's SFC_loss: 0.790275\tvalid's SFC_loss: 1.13244\n",
      "[79]\ttrain's SFC_loss: 0.787741\tvalid's SFC_loss: 1.13248\n",
      "[80]\ttrain's SFC_loss: 0.78476\tvalid's SFC_loss: 1.13494\n",
      "[81]\ttrain's SFC_loss: 0.782334\tvalid's SFC_loss: 1.13466\n",
      "[82]\ttrain's SFC_loss: 0.779741\tvalid's SFC_loss: 1.13268\n",
      "[83]\ttrain's SFC_loss: 0.777223\tvalid's SFC_loss: 1.13023\n",
      "[84]\ttrain's SFC_loss: 0.774929\tvalid's SFC_loss: 1.13194\n",
      "[85]\ttrain's SFC_loss: 0.772412\tvalid's SFC_loss: 1.13205\n",
      "[86]\ttrain's SFC_loss: 0.769801\tvalid's SFC_loss: 1.13162\n",
      "[87]\ttrain's SFC_loss: 0.768013\tvalid's SFC_loss: 1.13161\n",
      "[88]\ttrain's SFC_loss: 0.765713\tvalid's SFC_loss: 1.12917\n",
      "[89]\ttrain's SFC_loss: 0.763408\tvalid's SFC_loss: 1.12926\n",
      "[90]\ttrain's SFC_loss: 0.760767\tvalid's SFC_loss: 1.12934\n",
      "[91]\ttrain's SFC_loss: 0.757752\tvalid's SFC_loss: 1.125\n",
      "[92]\ttrain's SFC_loss: 0.754794\tvalid's SFC_loss: 1.12895\n",
      "[93]\ttrain's SFC_loss: 0.752216\tvalid's SFC_loss: 1.12864\n",
      "[94]\ttrain's SFC_loss: 0.749514\tvalid's SFC_loss: 1.13491\n",
      "[95]\ttrain's SFC_loss: 0.746653\tvalid's SFC_loss: 1.13494\n",
      "[96]\ttrain's SFC_loss: 0.744183\tvalid's SFC_loss: 1.13679\n",
      "[97]\ttrain's SFC_loss: 0.742078\tvalid's SFC_loss: 1.13989\n",
      "[98]\ttrain's SFC_loss: 0.739483\tvalid's SFC_loss: 1.13951\n",
      "[99]\ttrain's SFC_loss: 0.73719\tvalid's SFC_loss: 1.14144\n",
      "[100]\ttrain's SFC_loss: 0.73486\tvalid's SFC_loss: 1.1445\n",
      "[101]\ttrain's SFC_loss: 0.732561\tvalid's SFC_loss: 1.14632\n",
      "[102]\ttrain's SFC_loss: 0.729948\tvalid's SFC_loss: 1.14666\n",
      "[103]\ttrain's SFC_loss: 0.727486\tvalid's SFC_loss: 1.14433\n",
      "[104]\ttrain's SFC_loss: 0.725807\tvalid's SFC_loss: 1.14351\n",
      "[105]\ttrain's SFC_loss: 0.723711\tvalid's SFC_loss: 1.14332\n",
      "[106]\ttrain's SFC_loss: 0.721396\tvalid's SFC_loss: 1.14141\n",
      "[107]\ttrain's SFC_loss: 0.719185\tvalid's SFC_loss: 1.13952\n",
      "[108]\ttrain's SFC_loss: 0.716759\tvalid's SFC_loss: 1.13749\n",
      "[109]\ttrain's SFC_loss: 0.714611\tvalid's SFC_loss: 1.13524\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttrain's SFC_loss: 1.04824\tvalid's SFC_loss: 1.09225\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.36363636363636365\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "      feature  importance\n",
      "0    feature1    0.007199\n",
      "1    feature2    0.046315\n",
      "2    feature3   -0.161034\n",
      "3    feature4    0.011309\n",
      "4    feature5    0.000000\n",
      "5    feature6   -0.048367\n",
      "6    feature7    0.000000\n",
      "7    feature8    0.000000\n",
      "8    feature9   -0.013306\n",
      "9   feature10    0.036864\n",
      "10  feature11    0.014966\n",
      "11  feature12   -0.146428\n",
      "12  feature13    0.000000\n",
      "13  feature14   -0.032985\n",
      "14  feature15    0.294963\n",
      "15  feature16    0.003535\n",
      "16  feature17    0.050753\n",
      "17  feature18   -0.065139\n",
      "18  feature19    0.034096\n",
      "19  feature20   -0.032741\n",
      "-------------------- 2 --------------------\n",
      "(97, 20) (97,)\n",
      "(11, 20) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05821\tvalid's multi_logloss: 1.06583\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05435\tvalid's multi_logloss: 1.06476\n",
      "[3]\ttrain's multi_logloss: 1.05074\tvalid's multi_logloss: 1.0632\n",
      "[4]\ttrain's multi_logloss: 1.0475\tvalid's multi_logloss: 1.06317\n",
      "[5]\ttrain's multi_logloss: 1.04375\tvalid's multi_logloss: 1.06206\n",
      "[6]\ttrain's multi_logloss: 1.04022\tvalid's multi_logloss: 1.05857\n",
      "[7]\ttrain's multi_logloss: 1.03656\tvalid's multi_logloss: 1.05698\n",
      "[8]\ttrain's multi_logloss: 1.03304\tvalid's multi_logloss: 1.05394\n",
      "[9]\ttrain's multi_logloss: 1.03008\tvalid's multi_logloss: 1.05374\n",
      "[10]\ttrain's multi_logloss: 1.02663\tvalid's multi_logloss: 1.05076\n",
      "[11]\ttrain's multi_logloss: 1.02235\tvalid's multi_logloss: 1.04816\n",
      "[12]\ttrain's multi_logloss: 1.01839\tvalid's multi_logloss: 1.04543\n",
      "[13]\ttrain's multi_logloss: 1.01433\tvalid's multi_logloss: 1.04263\n",
      "[14]\ttrain's multi_logloss: 1.01141\tvalid's multi_logloss: 1.0397\n",
      "[15]\ttrain's multi_logloss: 1.00789\tvalid's multi_logloss: 1.03857\n",
      "[16]\ttrain's multi_logloss: 1.00411\tvalid's multi_logloss: 1.0349\n",
      "[17]\ttrain's multi_logloss: 1.00063\tvalid's multi_logloss: 1.03277\n",
      "[18]\ttrain's multi_logloss: 0.997194\tvalid's multi_logloss: 1.03086\n",
      "[19]\ttrain's multi_logloss: 0.993408\tvalid's multi_logloss: 1.02865\n",
      "[20]\ttrain's multi_logloss: 0.989867\tvalid's multi_logloss: 1.02872\n",
      "[21]\ttrain's multi_logloss: 0.986148\tvalid's multi_logloss: 1.02593\n",
      "[22]\ttrain's multi_logloss: 0.982238\tvalid's multi_logloss: 1.0259\n",
      "[23]\ttrain's multi_logloss: 0.978404\tvalid's multi_logloss: 1.0241\n",
      "[24]\ttrain's multi_logloss: 0.97498\tvalid's multi_logloss: 1.02375\n",
      "[25]\ttrain's multi_logloss: 0.971406\tvalid's multi_logloss: 1.02352\n",
      "[26]\ttrain's multi_logloss: 0.967916\tvalid's multi_logloss: 1.02213\n",
      "[27]\ttrain's multi_logloss: 0.964105\tvalid's multi_logloss: 1.02235\n",
      "[28]\ttrain's multi_logloss: 0.960534\tvalid's multi_logloss: 1.02068\n",
      "[29]\ttrain's multi_logloss: 0.957904\tvalid's multi_logloss: 1.01989\n",
      "[30]\ttrain's multi_logloss: 0.954286\tvalid's multi_logloss: 1.01751\n",
      "[31]\ttrain's multi_logloss: 0.950897\tvalid's multi_logloss: 1.01457\n",
      "[32]\ttrain's multi_logloss: 0.947803\tvalid's multi_logloss: 1.01189\n",
      "[33]\ttrain's multi_logloss: 0.944358\tvalid's multi_logloss: 1.00971\n",
      "[34]\ttrain's multi_logloss: 0.941741\tvalid's multi_logloss: 1.00688\n",
      "[35]\ttrain's multi_logloss: 0.939019\tvalid's multi_logloss: 1.00302\n",
      "[36]\ttrain's multi_logloss: 0.936302\tvalid's multi_logloss: 1.00101\n",
      "[37]\ttrain's multi_logloss: 0.933589\tvalid's multi_logloss: 0.999894\n",
      "[38]\ttrain's multi_logloss: 0.930789\tvalid's multi_logloss: 0.999418\n",
      "[39]\ttrain's multi_logloss: 0.928312\tvalid's multi_logloss: 0.998241\n",
      "[40]\ttrain's multi_logloss: 0.925649\tvalid's multi_logloss: 0.99582\n",
      "[41]\ttrain's multi_logloss: 0.922004\tvalid's multi_logloss: 0.996214\n",
      "[42]\ttrain's multi_logloss: 0.918788\tvalid's multi_logloss: 0.998225\n",
      "[43]\ttrain's multi_logloss: 0.915238\tvalid's multi_logloss: 0.998868\n",
      "[44]\ttrain's multi_logloss: 0.911726\tvalid's multi_logloss: 0.99692\n",
      "[45]\ttrain's multi_logloss: 0.90836\tvalid's multi_logloss: 0.99863\n",
      "[46]\ttrain's multi_logloss: 0.906014\tvalid's multi_logloss: 0.9958\n",
      "[47]\ttrain's multi_logloss: 0.903186\tvalid's multi_logloss: 0.994427\n",
      "[48]\ttrain's multi_logloss: 0.900544\tvalid's multi_logloss: 0.991956\n",
      "[49]\ttrain's multi_logloss: 0.898143\tvalid's multi_logloss: 0.989876\n",
      "[50]\ttrain's multi_logloss: 0.895921\tvalid's multi_logloss: 0.987253\n",
      "[51]\ttrain's multi_logloss: 0.893434\tvalid's multi_logloss: 0.987081\n",
      "[52]\ttrain's multi_logloss: 0.89161\tvalid's multi_logloss: 0.987716\n",
      "[53]\ttrain's multi_logloss: 0.889153\tvalid's multi_logloss: 0.987574\n",
      "[54]\ttrain's multi_logloss: 0.887069\tvalid's multi_logloss: 0.987163\n",
      "[55]\ttrain's multi_logloss: 0.884883\tvalid's multi_logloss: 0.987453\n",
      "[56]\ttrain's multi_logloss: 0.882031\tvalid's multi_logloss: 0.986452\n",
      "[57]\ttrain's multi_logloss: 0.879231\tvalid's multi_logloss: 0.985483\n",
      "[58]\ttrain's multi_logloss: 0.876658\tvalid's multi_logloss: 0.985955\n",
      "[59]\ttrain's multi_logloss: 0.874284\tvalid's multi_logloss: 0.985645\n",
      "[60]\ttrain's multi_logloss: 0.87209\tvalid's multi_logloss: 0.985589\n",
      "[61]\ttrain's multi_logloss: 0.868483\tvalid's multi_logloss: 0.983312\n",
      "[62]\ttrain's multi_logloss: 0.866141\tvalid's multi_logloss: 0.984043\n",
      "[63]\ttrain's multi_logloss: 0.863276\tvalid's multi_logloss: 0.982931\n",
      "[64]\ttrain's multi_logloss: 0.860942\tvalid's multi_logloss: 0.98249\n",
      "[65]\ttrain's multi_logloss: 0.858726\tvalid's multi_logloss: 0.984453\n",
      "[66]\ttrain's multi_logloss: 0.855913\tvalid's multi_logloss: 0.984725\n",
      "[67]\ttrain's multi_logloss: 0.853179\tvalid's multi_logloss: 0.983154\n",
      "[68]\ttrain's multi_logloss: 0.850685\tvalid's multi_logloss: 0.983009\n",
      "[69]\ttrain's multi_logloss: 0.848098\tvalid's multi_logloss: 0.98259\n",
      "[70]\ttrain's multi_logloss: 0.846023\tvalid's multi_logloss: 0.981156\n",
      "[71]\ttrain's multi_logloss: 0.84354\tvalid's multi_logloss: 0.979629\n",
      "[72]\ttrain's multi_logloss: 0.840993\tvalid's multi_logloss: 0.977823\n",
      "[73]\ttrain's multi_logloss: 0.838588\tvalid's multi_logloss: 0.978158\n",
      "[74]\ttrain's multi_logloss: 0.836313\tvalid's multi_logloss: 0.977273\n",
      "[75]\ttrain's multi_logloss: 0.833518\tvalid's multi_logloss: 0.974885\n",
      "[76]\ttrain's multi_logloss: 0.831096\tvalid's multi_logloss: 0.975308\n",
      "[77]\ttrain's multi_logloss: 0.828831\tvalid's multi_logloss: 0.975263\n",
      "[78]\ttrain's multi_logloss: 0.826226\tvalid's multi_logloss: 0.97344\n",
      "[79]\ttrain's multi_logloss: 0.824287\tvalid's multi_logloss: 0.972654\n",
      "[80]\ttrain's multi_logloss: 0.822455\tvalid's multi_logloss: 0.971862\n",
      "[81]\ttrain's multi_logloss: 0.820982\tvalid's multi_logloss: 0.969121\n",
      "[82]\ttrain's multi_logloss: 0.819382\tvalid's multi_logloss: 0.966572\n",
      "[83]\ttrain's multi_logloss: 0.817251\tvalid's multi_logloss: 0.963115\n",
      "[84]\ttrain's multi_logloss: 0.815166\tvalid's multi_logloss: 0.959706\n",
      "[85]\ttrain's multi_logloss: 0.813205\tvalid's multi_logloss: 0.954233\n",
      "[86]\ttrain's multi_logloss: 0.810949\tvalid's multi_logloss: 0.950807\n",
      "[87]\ttrain's multi_logloss: 0.809046\tvalid's multi_logloss: 0.950258\n",
      "[88]\ttrain's multi_logloss: 0.807246\tvalid's multi_logloss: 0.947709\n",
      "[89]\ttrain's multi_logloss: 0.805482\tvalid's multi_logloss: 0.945339\n",
      "[90]\ttrain's multi_logloss: 0.803517\tvalid's multi_logloss: 0.94277\n",
      "[91]\ttrain's multi_logloss: 0.801307\tvalid's multi_logloss: 0.941946\n",
      "[92]\ttrain's multi_logloss: 0.799196\tvalid's multi_logloss: 0.941552\n",
      "[93]\ttrain's multi_logloss: 0.79724\tvalid's multi_logloss: 0.94138\n",
      "[94]\ttrain's multi_logloss: 0.795177\tvalid's multi_logloss: 0.941794\n",
      "[95]\ttrain's multi_logloss: 0.793553\tvalid's multi_logloss: 0.940713\n",
      "[96]\ttrain's multi_logloss: 0.791988\tvalid's multi_logloss: 0.941588\n",
      "[97]\ttrain's multi_logloss: 0.790159\tvalid's multi_logloss: 0.941938\n",
      "[98]\ttrain's multi_logloss: 0.788863\tvalid's multi_logloss: 0.942432\n",
      "[99]\ttrain's multi_logloss: 0.787411\tvalid's multi_logloss: 0.942022\n",
      "[100]\ttrain's multi_logloss: 0.785785\tvalid's multi_logloss: 0.942205\n",
      "[101]\ttrain's multi_logloss: 0.783303\tvalid's multi_logloss: 0.940883\n",
      "[102]\ttrain's multi_logloss: 0.780735\tvalid's multi_logloss: 0.940174\n",
      "[103]\ttrain's multi_logloss: 0.778597\tvalid's multi_logloss: 0.938953\n",
      "[104]\ttrain's multi_logloss: 0.776232\tvalid's multi_logloss: 0.936772\n",
      "[105]\ttrain's multi_logloss: 0.77367\tvalid's multi_logloss: 0.936529\n",
      "[106]\ttrain's multi_logloss: 0.772367\tvalid's multi_logloss: 0.937778\n",
      "[107]\ttrain's multi_logloss: 0.770891\tvalid's multi_logloss: 0.938066\n",
      "[108]\ttrain's multi_logloss: 0.769441\tvalid's multi_logloss: 0.936414\n",
      "[109]\ttrain's multi_logloss: 0.767727\tvalid's multi_logloss: 0.936077\n",
      "[110]\ttrain's multi_logloss: 0.766023\tvalid's multi_logloss: 0.935798\n",
      "[111]\ttrain's multi_logloss: 0.764636\tvalid's multi_logloss: 0.936162\n",
      "[112]\ttrain's multi_logloss: 0.763345\tvalid's multi_logloss: 0.9356\n",
      "[113]\ttrain's multi_logloss: 0.762029\tvalid's multi_logloss: 0.936012\n",
      "[114]\ttrain's multi_logloss: 0.760297\tvalid's multi_logloss: 0.935763\n",
      "[115]\ttrain's multi_logloss: 0.758794\tvalid's multi_logloss: 0.935623\n",
      "[116]\ttrain's multi_logloss: 0.757002\tvalid's multi_logloss: 0.933493\n",
      "[117]\ttrain's multi_logloss: 0.755\tvalid's multi_logloss: 0.933162\n",
      "[118]\ttrain's multi_logloss: 0.753192\tvalid's multi_logloss: 0.933922\n",
      "[119]\ttrain's multi_logloss: 0.751408\tvalid's multi_logloss: 0.932502\n",
      "[120]\ttrain's multi_logloss: 0.750189\tvalid's multi_logloss: 0.932549\n",
      "[121]\ttrain's multi_logloss: 0.748247\tvalid's multi_logloss: 0.932093\n",
      "[122]\ttrain's multi_logloss: 0.746764\tvalid's multi_logloss: 0.931042\n",
      "[123]\ttrain's multi_logloss: 0.744699\tvalid's multi_logloss: 0.931369\n",
      "[124]\ttrain's multi_logloss: 0.742835\tvalid's multi_logloss: 0.93097\n",
      "[125]\ttrain's multi_logloss: 0.741013\tvalid's multi_logloss: 0.930604\n",
      "[126]\ttrain's multi_logloss: 0.739961\tvalid's multi_logloss: 0.930012\n",
      "[127]\ttrain's multi_logloss: 0.738407\tvalid's multi_logloss: 0.929416\n",
      "[128]\ttrain's multi_logloss: 0.737067\tvalid's multi_logloss: 0.928518\n",
      "[129]\ttrain's multi_logloss: 0.735709\tvalid's multi_logloss: 0.928419\n",
      "[130]\ttrain's multi_logloss: 0.734167\tvalid's multi_logloss: 0.926886\n",
      "[131]\ttrain's multi_logloss: 0.732698\tvalid's multi_logloss: 0.925862\n",
      "[132]\ttrain's multi_logloss: 0.730139\tvalid's multi_logloss: 0.925527\n",
      "[133]\ttrain's multi_logloss: 0.727624\tvalid's multi_logloss: 0.925219\n",
      "[134]\ttrain's multi_logloss: 0.725152\tvalid's multi_logloss: 0.924937\n",
      "[135]\ttrain's multi_logloss: 0.723036\tvalid's multi_logloss: 0.922804\n",
      "[136]\ttrain's multi_logloss: 0.721448\tvalid's multi_logloss: 0.922459\n",
      "[137]\ttrain's multi_logloss: 0.719317\tvalid's multi_logloss: 0.921535\n",
      "[138]\ttrain's multi_logloss: 0.717793\tvalid's multi_logloss: 0.920081\n",
      "[139]\ttrain's multi_logloss: 0.716074\tvalid's multi_logloss: 0.91919\n",
      "[140]\ttrain's multi_logloss: 0.714479\tvalid's multi_logloss: 0.918418\n",
      "[141]\ttrain's multi_logloss: 0.712447\tvalid's multi_logloss: 0.917218\n",
      "[142]\ttrain's multi_logloss: 0.710713\tvalid's multi_logloss: 0.915762\n",
      "[143]\ttrain's multi_logloss: 0.709195\tvalid's multi_logloss: 0.915356\n",
      "[144]\ttrain's multi_logloss: 0.707565\tvalid's multi_logloss: 0.914368\n",
      "[145]\ttrain's multi_logloss: 0.706142\tvalid's multi_logloss: 0.914778\n",
      "[146]\ttrain's multi_logloss: 0.704638\tvalid's multi_logloss: 0.914778\n",
      "[147]\ttrain's multi_logloss: 0.703091\tvalid's multi_logloss: 0.914821\n",
      "[148]\ttrain's multi_logloss: 0.701631\tvalid's multi_logloss: 0.913855\n",
      "[149]\ttrain's multi_logloss: 0.700451\tvalid's multi_logloss: 0.915003\n",
      "[150]\ttrain's multi_logloss: 0.699028\tvalid's multi_logloss: 0.914867\n",
      "[151]\ttrain's multi_logloss: 0.697465\tvalid's multi_logloss: 0.912863\n",
      "[152]\ttrain's multi_logloss: 0.696057\tvalid's multi_logloss: 0.912338\n",
      "[153]\ttrain's multi_logloss: 0.69453\tvalid's multi_logloss: 0.910152\n",
      "[154]\ttrain's multi_logloss: 0.693017\tvalid's multi_logloss: 0.908652\n",
      "[155]\ttrain's multi_logloss: 0.691293\tvalid's multi_logloss: 0.906184\n",
      "[156]\ttrain's multi_logloss: 0.69019\tvalid's multi_logloss: 0.904443\n",
      "[157]\ttrain's multi_logloss: 0.688647\tvalid's multi_logloss: 0.904233\n",
      "[158]\ttrain's multi_logloss: 0.687051\tvalid's multi_logloss: 0.903401\n",
      "[159]\ttrain's multi_logloss: 0.685349\tvalid's multi_logloss: 0.903737\n",
      "[160]\ttrain's multi_logloss: 0.683747\tvalid's multi_logloss: 0.903157\n",
      "[161]\ttrain's multi_logloss: 0.682457\tvalid's multi_logloss: 0.903967\n",
      "[162]\ttrain's multi_logloss: 0.681194\tvalid's multi_logloss: 0.904795\n",
      "[163]\ttrain's multi_logloss: 0.679855\tvalid's multi_logloss: 0.905252\n",
      "[164]\ttrain's multi_logloss: 0.678413\tvalid's multi_logloss: 0.90673\n",
      "[165]\ttrain's multi_logloss: 0.677212\tvalid's multi_logloss: 0.908062\n",
      "[166]\ttrain's multi_logloss: 0.675497\tvalid's multi_logloss: 0.905021\n",
      "[167]\ttrain's multi_logloss: 0.673767\tvalid's multi_logloss: 0.902867\n",
      "[168]\ttrain's multi_logloss: 0.672306\tvalid's multi_logloss: 0.900657\n",
      "[169]\ttrain's multi_logloss: 0.670812\tvalid's multi_logloss: 0.897791\n",
      "[170]\ttrain's multi_logloss: 0.669443\tvalid's multi_logloss: 0.895618\n",
      "[171]\ttrain's multi_logloss: 0.667745\tvalid's multi_logloss: 0.894348\n",
      "[172]\ttrain's multi_logloss: 0.666312\tvalid's multi_logloss: 0.894096\n",
      "[173]\ttrain's multi_logloss: 0.664593\tvalid's multi_logloss: 0.892268\n",
      "[174]\ttrain's multi_logloss: 0.66293\tvalid's multi_logloss: 0.890427\n",
      "[175]\ttrain's multi_logloss: 0.661299\tvalid's multi_logloss: 0.888619\n",
      "[176]\ttrain's multi_logloss: 0.660015\tvalid's multi_logloss: 0.887752\n",
      "[177]\ttrain's multi_logloss: 0.658253\tvalid's multi_logloss: 0.887518\n",
      "[178]\ttrain's multi_logloss: 0.657061\tvalid's multi_logloss: 0.888393\n",
      "[179]\ttrain's multi_logloss: 0.655438\tvalid's multi_logloss: 0.886626\n",
      "[180]\ttrain's multi_logloss: 0.654112\tvalid's multi_logloss: 0.888195\n",
      "[181]\ttrain's multi_logloss: 0.652743\tvalid's multi_logloss: 0.887937\n",
      "[182]\ttrain's multi_logloss: 0.651426\tvalid's multi_logloss: 0.887192\n",
      "[183]\ttrain's multi_logloss: 0.649861\tvalid's multi_logloss: 0.88656\n",
      "[184]\ttrain's multi_logloss: 0.648511\tvalid's multi_logloss: 0.886543\n",
      "[185]\ttrain's multi_logloss: 0.647002\tvalid's multi_logloss: 0.886122\n",
      "[186]\ttrain's multi_logloss: 0.6452\tvalid's multi_logloss: 0.886403\n",
      "[187]\ttrain's multi_logloss: 0.643849\tvalid's multi_logloss: 0.886529\n",
      "[188]\ttrain's multi_logloss: 0.642531\tvalid's multi_logloss: 0.887236\n",
      "[189]\ttrain's multi_logloss: 0.640821\tvalid's multi_logloss: 0.88653\n",
      "[190]\ttrain's multi_logloss: 0.63917\tvalid's multi_logloss: 0.886334\n",
      "[191]\ttrain's multi_logloss: 0.638168\tvalid's multi_logloss: 0.886066\n",
      "[192]\ttrain's multi_logloss: 0.637223\tvalid's multi_logloss: 0.88582\n",
      "[193]\ttrain's multi_logloss: 0.636177\tvalid's multi_logloss: 0.884889\n",
      "[194]\ttrain's multi_logloss: 0.634771\tvalid's multi_logloss: 0.886037\n",
      "[195]\ttrain's multi_logloss: 0.633834\tvalid's multi_logloss: 0.885826\n",
      "[196]\ttrain's multi_logloss: 0.632424\tvalid's multi_logloss: 0.885928\n",
      "[197]\ttrain's multi_logloss: 0.630993\tvalid's multi_logloss: 0.884589\n",
      "[198]\ttrain's multi_logloss: 0.629625\tvalid's multi_logloss: 0.884519\n",
      "[199]\ttrain's multi_logloss: 0.62835\tvalid's multi_logloss: 0.883504\n",
      "[200]\ttrain's multi_logloss: 0.626872\tvalid's multi_logloss: 0.88376\n",
      "[201]\ttrain's multi_logloss: 0.625167\tvalid's multi_logloss: 0.883364\n",
      "[202]\ttrain's multi_logloss: 0.623308\tvalid's multi_logloss: 0.883856\n",
      "[203]\ttrain's multi_logloss: 0.621254\tvalid's multi_logloss: 0.882939\n",
      "[204]\ttrain's multi_logloss: 0.619878\tvalid's multi_logloss: 0.882019\n",
      "[205]\ttrain's multi_logloss: 0.617888\tvalid's multi_logloss: 0.881078\n",
      "[206]\ttrain's multi_logloss: 0.617033\tvalid's multi_logloss: 0.881377\n",
      "[207]\ttrain's multi_logloss: 0.616344\tvalid's multi_logloss: 0.88212\n",
      "[208]\ttrain's multi_logloss: 0.615437\tvalid's multi_logloss: 0.882619\n",
      "[209]\ttrain's multi_logloss: 0.614538\tvalid's multi_logloss: 0.883121\n",
      "[210]\ttrain's multi_logloss: 0.613885\tvalid's multi_logloss: 0.883884\n",
      "[211]\ttrain's multi_logloss: 0.612642\tvalid's multi_logloss: 0.882071\n",
      "[212]\ttrain's multi_logloss: 0.61137\tvalid's multi_logloss: 0.88112\n",
      "[213]\ttrain's multi_logloss: 0.61022\tvalid's multi_logloss: 0.882001\n",
      "[214]\ttrain's multi_logloss: 0.609142\tvalid's multi_logloss: 0.882052\n",
      "[215]\ttrain's multi_logloss: 0.607916\tvalid's multi_logloss: 0.882603\n",
      "[216]\ttrain's multi_logloss: 0.606649\tvalid's multi_logloss: 0.881342\n",
      "[217]\ttrain's multi_logloss: 0.605419\tvalid's multi_logloss: 0.881294\n",
      "[218]\ttrain's multi_logloss: 0.604215\tvalid's multi_logloss: 0.880691\n",
      "[219]\ttrain's multi_logloss: 0.603092\tvalid's multi_logloss: 0.879615\n",
      "[220]\ttrain's multi_logloss: 0.601818\tvalid's multi_logloss: 0.879752\n",
      "[221]\ttrain's multi_logloss: 0.60026\tvalid's multi_logloss: 0.877955\n",
      "[222]\ttrain's multi_logloss: 0.59853\tvalid's multi_logloss: 0.877275\n",
      "[223]\ttrain's multi_logloss: 0.596932\tvalid's multi_logloss: 0.876719\n",
      "[224]\ttrain's multi_logloss: 0.595288\tvalid's multi_logloss: 0.875592\n",
      "[225]\ttrain's multi_logloss: 0.593781\tvalid's multi_logloss: 0.874605\n",
      "[226]\ttrain's multi_logloss: 0.592509\tvalid's multi_logloss: 0.873558\n",
      "[227]\ttrain's multi_logloss: 0.590965\tvalid's multi_logloss: 0.873116\n",
      "[228]\ttrain's multi_logloss: 0.58967\tvalid's multi_logloss: 0.87109\n",
      "[229]\ttrain's multi_logloss: 0.588807\tvalid's multi_logloss: 0.869582\n",
      "[230]\ttrain's multi_logloss: 0.587661\tvalid's multi_logloss: 0.867812\n",
      "[231]\ttrain's multi_logloss: 0.58648\tvalid's multi_logloss: 0.867495\n",
      "[232]\ttrain's multi_logloss: 0.585178\tvalid's multi_logloss: 0.866603\n",
      "[233]\ttrain's multi_logloss: 0.584235\tvalid's multi_logloss: 0.865654\n",
      "[234]\ttrain's multi_logloss: 0.583223\tvalid's multi_logloss: 0.865246\n",
      "[235]\ttrain's multi_logloss: 0.58213\tvalid's multi_logloss: 0.864194\n",
      "[236]\ttrain's multi_logloss: 0.580915\tvalid's multi_logloss: 0.863199\n",
      "[237]\ttrain's multi_logloss: 0.58004\tvalid's multi_logloss: 0.862559\n",
      "[238]\ttrain's multi_logloss: 0.57911\tvalid's multi_logloss: 0.86198\n",
      "[239]\ttrain's multi_logloss: 0.577988\tvalid's multi_logloss: 0.862075\n",
      "[240]\ttrain's multi_logloss: 0.57696\tvalid's multi_logloss: 0.862607\n",
      "[241]\ttrain's multi_logloss: 0.576238\tvalid's multi_logloss: 0.863056\n",
      "[242]\ttrain's multi_logloss: 0.575482\tvalid's multi_logloss: 0.862733\n",
      "[243]\ttrain's multi_logloss: 0.574264\tvalid's multi_logloss: 0.862715\n",
      "[244]\ttrain's multi_logloss: 0.573182\tvalid's multi_logloss: 0.862265\n",
      "[245]\ttrain's multi_logloss: 0.571825\tvalid's multi_logloss: 0.861556\n",
      "[246]\ttrain's multi_logloss: 0.570681\tvalid's multi_logloss: 0.861766\n",
      "[247]\ttrain's multi_logloss: 0.569609\tvalid's multi_logloss: 0.859355\n",
      "[248]\ttrain's multi_logloss: 0.568483\tvalid's multi_logloss: 0.859697\n",
      "[249]\ttrain's multi_logloss: 0.567492\tvalid's multi_logloss: 0.857951\n",
      "[250]\ttrain's multi_logloss: 0.566441\tvalid's multi_logloss: 0.85617\n",
      "[251]\ttrain's multi_logloss: 0.565115\tvalid's multi_logloss: 0.856103\n",
      "[252]\ttrain's multi_logloss: 0.563717\tvalid's multi_logloss: 0.855722\n",
      "[253]\ttrain's multi_logloss: 0.562616\tvalid's multi_logloss: 0.855325\n",
      "[254]\ttrain's multi_logloss: 0.561479\tvalid's multi_logloss: 0.854224\n",
      "[255]\ttrain's multi_logloss: 0.560134\tvalid's multi_logloss: 0.854243\n",
      "[256]\ttrain's multi_logloss: 0.559222\tvalid's multi_logloss: 0.854442\n",
      "[257]\ttrain's multi_logloss: 0.558255\tvalid's multi_logloss: 0.853809\n",
      "[258]\ttrain's multi_logloss: 0.55738\tvalid's multi_logloss: 0.854027\n",
      "[259]\ttrain's multi_logloss: 0.556698\tvalid's multi_logloss: 0.852877\n",
      "[260]\ttrain's multi_logloss: 0.555943\tvalid's multi_logloss: 0.852642\n",
      "[261]\ttrain's multi_logloss: 0.554995\tvalid's multi_logloss: 0.852229\n",
      "[262]\ttrain's multi_logloss: 0.554045\tvalid's multi_logloss: 0.851529\n",
      "[263]\ttrain's multi_logloss: 0.5534\tvalid's multi_logloss: 0.851709\n",
      "[264]\ttrain's multi_logloss: 0.552253\tvalid's multi_logloss: 0.851773\n",
      "[265]\ttrain's multi_logloss: 0.5512\tvalid's multi_logloss: 0.851649\n",
      "[266]\ttrain's multi_logloss: 0.549994\tvalid's multi_logloss: 0.85014\n",
      "[267]\ttrain's multi_logloss: 0.549046\tvalid's multi_logloss: 0.850245\n",
      "[268]\ttrain's multi_logloss: 0.547663\tvalid's multi_logloss: 0.849641\n",
      "[269]\ttrain's multi_logloss: 0.546202\tvalid's multi_logloss: 0.84902\n",
      "[270]\ttrain's multi_logloss: 0.545008\tvalid's multi_logloss: 0.847906\n"
     ]
    }
   ],
   "source": [
    "# cross validation \n",
    "best_params_GC = trial.params\n",
    "best_params_GC.update({\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 3,\n",
    "    'num_iteration': 3000,\n",
    "    'verbosity': -1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'importance_type': 'gain',\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "# params in SFC\n",
    "best_params_SFC = trial.params.copy()\n",
    "best_params_SFC.update({\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'none',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 3,\n",
    "    'num_iteration': 3000,\n",
    "    'verbosity': -1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# divide dataset\n",
    "n_splits = 10\n",
    "cv = list(StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = 2022).split(x_gbm, y_gbm))\n",
    "feature_names = [f\"feature{i+1}\" for i in range(x_gbm.shape[1])]\n",
    "CV_names = [f\"CV{i}\" for i in range(n_splits)]\n",
    "overlap_w_list = [0.01, 0.025, 0.05, 0.075, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.5]\n",
    "df_importance_Diff_rank_ol_sensitivity = pd.DataFrame(columns = overlap_w_list, index = feature_names)\n",
    "\n",
    "for ol_id, overlap_w in enumerate(overlap_w_list):\n",
    "    df_acc = pd.DataFrame(columns=CV_names, index=['GC','SFC'])\n",
    "    df_importance_GC = pd.DataFrame(columns=CV_names, index=feature_names)\n",
    "    df_importance_SFC = pd.DataFrame(columns=CV_names, index=feature_names)\n",
    "    df_importance_Diff = pd.DataFrame(columns=CV_names, index=feature_names)\n",
    "    df_importance_Diff_rank = pd.DataFrame(index = df_importance_Diff.index)\n",
    "\n",
    "    # labeling\n",
    "    grand_truth_SFC = np.array([[1.-overlap_w,overlap_w,0],[overlap_w,1.-2*overlap_w,overlap_w],[0,overlap_w,1.-overlap_w]])\n",
    "    print( grand_truth_SFC)\n",
    "\n",
    "    # CV loop\n",
    "    for nfold, (train_index, valid_index) in enumerate(cv):\n",
    "        print(\"-\"*20, nfold, \"-\"*20)\n",
    "        x_tr, y_tr = x_gbm[train_index], y_gbm[train_index]\n",
    "        x_va, y_va = x_gbm[valid_index], y_gbm[valid_index]\n",
    "        print(x_tr.shape, y_tr.shape)\n",
    "        print(x_va.shape, y_va.shape)\n",
    "\n",
    "        # define lgb dataset\n",
    "        lgb_train = lgb.Dataset(x_tr, label = y_tr)\n",
    "        lgb_eval = lgb.Dataset(x_va, label = y_va, reference = lgb_train)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*20, \"GC model learning\", \"-\"*20, \"\\n\")\n",
    "\n",
    "\n",
    "        # model training\n",
    "        evaluation_results={}\n",
    "        model_GC = lgb.train(best_params_GC,\n",
    "                          train_set=lgb_train,\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          valid_sets=[lgb_train, lgb_eval],\n",
    "                          evals_result=evaluation_results,\n",
    "                          early_stopping_rounds = 100\n",
    "                         )\n",
    "\n",
    "        # calculate accuracy of test data\n",
    "        y_pred_prob = model_GC.predict(x_va, num_iteration = model_GC.best_iteration)\n",
    "        y_pred_GC = np.argmax(y_pred_prob,axis=1)\n",
    "        acc_GC = accuracy_score(y_va, y_pred_GC)\n",
    "        print(\"-\"*20, \"Test data accuracy\", \"-\"*20, \"\\n\")\n",
    "        print(acc_GC)\n",
    "\n",
    "\n",
    "        # model training\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*20, \"SFC model learning\", \"-\"*20, \"\\n\")\n",
    "        evaluation_results2={}\n",
    "\n",
    "        SFC_loss = MultiLoglossForLGBM(n_class = 3, grand_truth=grand_truth_SFC, use_softmax = True)\n",
    "        model_SFC = lgb.train(best_params_SFC,\n",
    "                                    train_set=lgb_train,\n",
    "                                    valid_names=['train', 'valid'],\n",
    "                                    valid_sets=[lgb_train, lgb_eval],\n",
    "                                    evals_result=evaluation_results2,\n",
    "                                    fobj=SFC_loss.return_grad_and_hess,\n",
    "                                    feval=lambda preds, data: SFC_loss.return_loss(preds, data),\n",
    "                                    early_stopping_rounds = 100\n",
    "                                   )\n",
    "\n",
    "        # calculate accuracy of test data\n",
    "        y_pred_prob_SFC = model_SFC.predict(x_va, num_iteration = model_SFC.best_iteration)\n",
    "        y_pred_SFC = np.argmax(y_pred_prob_SFC,axis=1)\n",
    "        acc_SFC = accuracy_score(y_va, y_pred_SFC)\n",
    "        print(\"-\"*20, \"Test data accuracy\", \"-\"*20, \"\\n\")\n",
    "        print(acc_SFC)\n",
    "\n",
    "        # print(\"-\"*20, \"gain importance in GC\", \"-\"*20, \"\\n\")\n",
    "        # print(df_importance)\n",
    "\n",
    "        # gain features\n",
    "        cols = list(df_20dims.drop('patient number', axis = 1).columns)\n",
    "\n",
    "        # calculate differences between GC and SFC\n",
    "        f_importance_GC = np.array(model_GC.feature_importance(importance_type='gain'))\n",
    "        f_importance_GC = f_importance_GC / np.sum(f_importance_GC)\n",
    "        f_importance_SFC = np.array(model_SFC.feature_importance(importance_type='gain'))\n",
    "        f_importance_SFC = f_importance_SFC / np.sum(f_importance_SFC)\n",
    "        # after - before gain\n",
    "        df_importance_subtracted = f_importance_SFC - f_importance_GC\n",
    "        df_importance_subtracted = df_importance_subtracted / np.sum(np.abs(df_importance_subtracted))\n",
    "        df_importance_subtracted = pd.DataFrame({'feature':feature_names , 'importance':df_importance_subtracted})\n",
    "        # df_importance_subtracted = df_importance_subtracted.sort_values('importance', ascending=False)\n",
    "        print(\"-\"*20, \"Difference of importance\", \"-\"*20, \"\\n\")\n",
    "\n",
    "        print(df_importance_subtracted)\n",
    "        df_acc.at['GC',CV_names[nfold]] = acc_GC\n",
    "        df_acc.at['SFC',CV_names[nfold]] = acc_SFC\n",
    "        df_importance_GC[CV_names[nfold]] = f_importance_SFC\n",
    "        df_importance_SFC[CV_names[nfold]] = f_importance_SFC\n",
    "        df_importance_Diff[CV_names[nfold]] = f_importance_SFC - f_importance_GC\n",
    "\n",
    "    acc_th = 2./3.\n",
    "    df_acc.loc[:,(df_acc>=acc_th).sum(axis=0)==2]\n",
    "    for col in df_importance_Diff.columns:\n",
    "        rank = df_importance_Diff[col].rank(method=\"min\" , ascending=False)\n",
    "        df_importance_Diff_rank[col] = rank\n",
    "    df_importance_Diff_rank['rank_sum']=df_importance_Diff_rank.loc[:,(df_acc>=acc_th).sum(axis=0)==2].sum(axis=1)\n",
    "    df_importance_Diff_rank_ol_sensitivity[overlap_w]=df_importance_Diff_rank['rank_sum'].copy()\n",
    "\n",
    "    file_name='revise_250603_calc_log_overlap_id_'+str(ol_id)+'.pkl'\n",
    "    with open(file_name,'wb') as f:\n",
    "        pickle.dump([model_GC,model_SFC, df_acc,df_importance_GC,df_importance_SFC,df_importance_Diff,df_importance_Diff_rank], f)\n",
    "    del df_acc,df_importance_GC,df_importance_SFC,df_importance_Diff,df_importance_Diff_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d2f7907b-6a9a-4d11-8a5e-1e97c14a0647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.010</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.050</th>\n",
       "      <th>0.075</th>\n",
       "      <th>0.100</th>\n",
       "      <th>0.150</th>\n",
       "      <th>0.200</th>\n",
       "      <th>0.250</th>\n",
       "      <th>0.300</th>\n",
       "      <th>0.350</th>\n",
       "      <th>0.400</th>\n",
       "      <th>0.450</th>\n",
       "      <th>0.500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>64.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>67.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>61.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>55.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>59.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>48.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>51.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>61.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature11</th>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature12</th>\n",
       "      <td>76.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature13</th>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature14</th>\n",
       "      <td>55.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature15</th>\n",
       "      <td>67.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature16</th>\n",
       "      <td>38.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature17</th>\n",
       "      <td>51.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature18</th>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature19</th>\n",
       "      <td>21.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature20</th>\n",
       "      <td>73.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0.010  0.025  0.050  0.075  0.100  0.150  0.200  0.250  0.300  \\\n",
       "feature1    50.0   53.0   51.0   61.0   62.0   46.0   43.0   49.0   33.0   \n",
       "feature2    64.0   55.0   53.0   45.0   34.0   20.0   21.0   23.0   15.0   \n",
       "feature3    67.0   78.0   62.0   81.0   86.0   59.0   59.0   58.0   40.0   \n",
       "feature4    61.0   52.0   59.0   54.0   54.0   31.0   30.0   32.0   23.0   \n",
       "feature5    38.0   38.0   52.0   33.0   32.0   20.0   28.0   26.0   17.0   \n",
       "feature6    55.0   59.0   70.0   68.0   70.0   35.0   34.0   35.0   24.0   \n",
       "feature7    59.0   58.0   69.0   69.0   73.0   46.0   48.0   46.0   30.0   \n",
       "feature8    48.0   60.0   64.0   62.0   73.0   44.0   44.0   42.0   23.0   \n",
       "feature9    51.0   58.0   24.0   30.0   25.0    9.0    7.0    7.0    6.0   \n",
       "feature10   61.0   63.0   68.0   64.0   68.0   39.0   36.0   32.0   20.0   \n",
       "feature11   34.0   29.0   39.0   32.0   36.0   22.0   23.0   26.0   18.0   \n",
       "feature12   76.0   72.0   61.0   65.0   67.0   49.0   44.0   44.0   30.0   \n",
       "feature13   45.0   40.0   50.0   40.0   36.0   24.0   30.0   28.0   17.0   \n",
       "feature14   55.0   39.0   61.0   52.0   48.0   23.0   29.0   29.0   22.0   \n",
       "feature15   67.0   70.0   69.0   72.0   81.0   48.0   45.0   44.0   30.0   \n",
       "feature16   38.0   34.0   26.0   29.0   25.0    5.0    5.0    6.0    4.0   \n",
       "feature17   51.0   41.0   40.0   32.0   28.0   21.0   20.0   19.0   14.0   \n",
       "feature18   36.0   36.0   27.0   30.0   21.0    7.0    7.0    7.0    4.0   \n",
       "feature19   21.0   35.0   28.0   41.0   38.0   24.0   19.0   18.0   12.0   \n",
       "feature20   73.0   80.0   77.0   90.0   93.0   58.0   58.0   59.0   38.0   \n",
       "\n",
       "           0.350  0.400  0.450  0.500  \n",
       "feature1     0.0    0.0    0.0    0.0  \n",
       "feature2     0.0    0.0    0.0    0.0  \n",
       "feature3     0.0    0.0    0.0    0.0  \n",
       "feature4     0.0    0.0    0.0    0.0  \n",
       "feature5     0.0    0.0    0.0    0.0  \n",
       "feature6     0.0    0.0    0.0    0.0  \n",
       "feature7     0.0    0.0    0.0    0.0  \n",
       "feature8     0.0    0.0    0.0    0.0  \n",
       "feature9     0.0    0.0    0.0    0.0  \n",
       "feature10    0.0    0.0    0.0    0.0  \n",
       "feature11    0.0    0.0    0.0    0.0  \n",
       "feature12    0.0    0.0    0.0    0.0  \n",
       "feature13    0.0    0.0    0.0    0.0  \n",
       "feature14    0.0    0.0    0.0    0.0  \n",
       "feature15    0.0    0.0    0.0    0.0  \n",
       "feature16    0.0    0.0    0.0    0.0  \n",
       "feature17    0.0    0.0    0.0    0.0  \n",
       "feature18    0.0    0.0    0.0    0.0  \n",
       "feature19    0.0    0.0    0.0    0.0  \n",
       "feature20    0.0    0.0    0.0    0.0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance_Diff_rank_ol_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0bafb50c-a037-4aa5-8413-5bf68ed8037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.010</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.050</th>\n",
       "      <th>0.075</th>\n",
       "      <th>0.100</th>\n",
       "      <th>0.150</th>\n",
       "      <th>0.200</th>\n",
       "      <th>0.250</th>\n",
       "      <th>0.300</th>\n",
       "      <th>0.350</th>\n",
       "      <th>0.400</th>\n",
       "      <th>0.450</th>\n",
       "      <th>0.500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.050476</td>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.059048</td>\n",
       "      <td>0.073016</td>\n",
       "      <td>0.068254</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.078571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>0.060952</td>\n",
       "      <td>0.052381</td>\n",
       "      <td>0.050476</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.032381</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>0.063810</td>\n",
       "      <td>0.074286</td>\n",
       "      <td>0.059048</td>\n",
       "      <td>0.077143</td>\n",
       "      <td>0.081905</td>\n",
       "      <td>0.093651</td>\n",
       "      <td>0.093651</td>\n",
       "      <td>0.092063</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.056190</td>\n",
       "      <td>0.051429</td>\n",
       "      <td>0.051429</td>\n",
       "      <td>0.049206</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.050794</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.031429</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.041270</td>\n",
       "      <td>0.040476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>0.052381</td>\n",
       "      <td>0.056190</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.064762</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.053968</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>0.056190</td>\n",
       "      <td>0.055238</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>0.069524</td>\n",
       "      <td>0.073016</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>0.073016</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>0.045714</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.060952</td>\n",
       "      <td>0.059048</td>\n",
       "      <td>0.069524</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.054762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.055238</td>\n",
       "      <td>0.022857</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.064762</td>\n",
       "      <td>0.060952</td>\n",
       "      <td>0.064762</td>\n",
       "      <td>0.061905</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.050794</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature11</th>\n",
       "      <td>0.032381</td>\n",
       "      <td>0.027619</td>\n",
       "      <td>0.037143</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.034921</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.041270</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature12</th>\n",
       "      <td>0.072381</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.061905</td>\n",
       "      <td>0.063810</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature13</th>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.040476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature14</th>\n",
       "      <td>0.052381</td>\n",
       "      <td>0.037143</td>\n",
       "      <td>0.058095</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>0.036508</td>\n",
       "      <td>0.046032</td>\n",
       "      <td>0.046032</td>\n",
       "      <td>0.052381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature15</th>\n",
       "      <td>0.063810</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.065714</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.077143</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.069841</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature16</th>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.032381</td>\n",
       "      <td>0.024762</td>\n",
       "      <td>0.027619</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature17</th>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.039048</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.030476</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.030159</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature18</th>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.034286</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature19</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.039048</td>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.038095</td>\n",
       "      <td>0.030159</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature20</th>\n",
       "      <td>0.069524</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>0.073333</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>0.092063</td>\n",
       "      <td>0.092063</td>\n",
       "      <td>0.093651</td>\n",
       "      <td>0.090476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0.010     0.025     0.050     0.075     0.100     0.150  \\\n",
       "feature1   0.047619  0.050476  0.048571  0.058095  0.059048  0.073016   \n",
       "feature2   0.060952  0.052381  0.050476  0.042857  0.032381  0.031746   \n",
       "feature3   0.063810  0.074286  0.059048  0.077143  0.081905  0.093651   \n",
       "feature4   0.058095  0.049524  0.056190  0.051429  0.051429  0.049206   \n",
       "feature5   0.036190  0.036190  0.049524  0.031429  0.030476  0.031746   \n",
       "feature6   0.052381  0.056190  0.066667  0.064762  0.066667  0.055556   \n",
       "feature7   0.056190  0.055238  0.065714  0.065714  0.069524  0.073016   \n",
       "feature8   0.045714  0.057143  0.060952  0.059048  0.069524  0.069841   \n",
       "feature9   0.048571  0.055238  0.022857  0.028571  0.023810  0.014286   \n",
       "feature10  0.058095  0.060000  0.064762  0.060952  0.064762  0.061905   \n",
       "feature11  0.032381  0.027619  0.037143  0.030476  0.034286  0.034921   \n",
       "feature12  0.072381  0.068571  0.058095  0.061905  0.063810  0.077778   \n",
       "feature13  0.042857  0.038095  0.047619  0.038095  0.034286  0.038095   \n",
       "feature14  0.052381  0.037143  0.058095  0.049524  0.045714  0.036508   \n",
       "feature15  0.063810  0.066667  0.065714  0.068571  0.077143  0.076190   \n",
       "feature16  0.036190  0.032381  0.024762  0.027619  0.023810  0.007937   \n",
       "feature17  0.048571  0.039048  0.038095  0.030476  0.026667  0.033333   \n",
       "feature18  0.034286  0.034286  0.025714  0.028571  0.020000  0.011111   \n",
       "feature19  0.020000  0.033333  0.026667  0.039048  0.036190  0.038095   \n",
       "feature20  0.069524  0.076190  0.073333  0.085714  0.088571  0.092063   \n",
       "\n",
       "              0.200     0.250     0.300  0.350  0.400  0.450  0.500  \n",
       "feature1   0.068254  0.077778  0.078571    NaN    NaN    NaN    NaN  \n",
       "feature2   0.033333  0.036508  0.035714    NaN    NaN    NaN    NaN  \n",
       "feature3   0.093651  0.092063  0.095238    NaN    NaN    NaN    NaN  \n",
       "feature4   0.047619  0.050794  0.054762    NaN    NaN    NaN    NaN  \n",
       "feature5   0.044444  0.041270  0.040476    NaN    NaN    NaN    NaN  \n",
       "feature6   0.053968  0.055556  0.057143    NaN    NaN    NaN    NaN  \n",
       "feature7   0.076190  0.073016  0.071429    NaN    NaN    NaN    NaN  \n",
       "feature8   0.069841  0.066667  0.054762    NaN    NaN    NaN    NaN  \n",
       "feature9   0.011111  0.011111  0.014286    NaN    NaN    NaN    NaN  \n",
       "feature10  0.057143  0.050794  0.047619    NaN    NaN    NaN    NaN  \n",
       "feature11  0.036508  0.041270  0.042857    NaN    NaN    NaN    NaN  \n",
       "feature12  0.069841  0.069841  0.071429    NaN    NaN    NaN    NaN  \n",
       "feature13  0.047619  0.044444  0.040476    NaN    NaN    NaN    NaN  \n",
       "feature14  0.046032  0.046032  0.052381    NaN    NaN    NaN    NaN  \n",
       "feature15  0.071429  0.069841  0.071429    NaN    NaN    NaN    NaN  \n",
       "feature16  0.007937  0.009524  0.009524    NaN    NaN    NaN    NaN  \n",
       "feature17  0.031746  0.030159  0.033333    NaN    NaN    NaN    NaN  \n",
       "feature18  0.011111  0.011111  0.009524    NaN    NaN    NaN    NaN  \n",
       "feature19  0.030159  0.028571  0.028571    NaN    NaN    NaN    NaN  \n",
       "feature20  0.092063  0.093651  0.090476    NaN    NaN    NaN    NaN  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance_Diff_rank_ol_sensitivity/df_importance_Diff_rank_ol_sensitivity.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ef03ec2f-aa4d-4211-aa82-cba7ef259470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_Diff_percentile_ol_sensitivity=((df_importance_Diff_rank_ol_sensitivity-df_importance_Diff_rank_ol_sensitivity.min(axis=0))/\n",
    "                                              (df_importance_Diff_rank_ol_sensitivity.max(axis=0)-\n",
    "                                              df_importance_Diff_rank_ol_sensitivity.min(axis=0))).iloc[:,:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "36818ee5-cd81-4347-95c9-e2b07b7bba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_Diff_percentile_ol_sensitivity2=df_importance_Diff_percentile_ol_sensitivity.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "eabcafa5-e96d-433a-8f13-a899a45f2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importance_Diff_percentile_ol_sensitivity2['mean']=df_importance_Diff_percentile_ol_sensitivity.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "41e5baad-19ba-4619-a8f3-c32a0bcadc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.075</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.15</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.3</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature16</th>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature18</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.152396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>0.236111</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature11</th>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.243477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature17</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.049180</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.258464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.528302</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.377358</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.297154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature13</th>\n",
       "      <td>0.436364</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.415094</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.346922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>0.781818</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.320755</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.386892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature14</th>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.441796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.660377</td>\n",
       "      <td>0.409836</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.518843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.574074</td>\n",
       "      <td>0.490566</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.621043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.547170</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.621062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.631241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.640905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.724339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature12</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.749616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature15</th>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.778204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.716981</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981132</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature20</th>\n",
       "      <td>0.945455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.983651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0.01     0.025      0.05     0.075       0.1      0.15  \\\n",
       "feature16  0.309091  0.098039  0.037736  0.000000  0.055556  0.000000   \n",
       "feature18  0.272727  0.137255  0.056604  0.016393  0.000000  0.037037   \n",
       "feature9   0.545455  0.568627  0.000000  0.016393  0.055556  0.074074   \n",
       "feature19  0.000000  0.117647  0.075472  0.196721  0.236111  0.351852   \n",
       "feature11  0.236364  0.000000  0.283019  0.049180  0.208333  0.314815   \n",
       "feature17  0.545455  0.235294  0.301887  0.049180  0.097222  0.296296   \n",
       "feature5   0.309091  0.176471  0.528302  0.065574  0.152778  0.277778   \n",
       "feature13  0.436364  0.215686  0.490566  0.180328  0.208333  0.351852   \n",
       "feature2   0.781818  0.509804  0.547170  0.262295  0.180556  0.277778   \n",
       "feature14  0.618182  0.196078  0.698113  0.377049  0.375000  0.333333   \n",
       "feature4   0.727273  0.450980  0.660377  0.409836  0.458333  0.481481   \n",
       "feature10  0.727273  0.666667  0.830189  0.573770  0.652778  0.629630   \n",
       "feature6   0.618182  0.588235  0.867925  0.639344  0.680556  0.555556   \n",
       "feature1   0.527273  0.470588  0.509434  0.524590  0.569444  0.759259   \n",
       "feature8   0.490909  0.607843  0.754717  0.540984  0.722222  0.722222   \n",
       "feature7   0.690909  0.568627  0.849057  0.655738  0.722222  0.759259   \n",
       "feature12  1.000000  0.843137  0.698113  0.590164  0.638889  0.814815   \n",
       "feature15  0.836364  0.803922  0.849057  0.704918  0.833333  0.796296   \n",
       "feature3   0.836364  0.960784  0.716981  0.852459  0.902778  1.000000   \n",
       "feature20  0.945455  1.000000  1.000000  1.000000  1.000000  0.981481   \n",
       "\n",
       "                0.2      0.25       0.3      mean  \n",
       "feature16  0.000000  0.000000  0.000000  0.055602  \n",
       "feature18  0.037037  0.018868  0.000000  0.063991  \n",
       "feature9   0.037037  0.018868  0.055556  0.152396  \n",
       "feature19  0.259259  0.226415  0.222222  0.187300  \n",
       "feature11  0.333333  0.377358  0.388889  0.243477  \n",
       "feature17  0.277778  0.245283  0.277778  0.258464  \n",
       "feature5   0.425926  0.377358  0.361111  0.297154  \n",
       "feature13  0.462963  0.415094  0.361111  0.346922  \n",
       "feature2   0.296296  0.320755  0.305556  0.386892  \n",
       "feature14  0.444444  0.433962  0.500000  0.441796  \n",
       "feature4   0.462963  0.490566  0.527778  0.518843  \n",
       "feature10  0.574074  0.490566  0.444444  0.621043  \n",
       "feature6   0.537037  0.547170  0.555556  0.621062  \n",
       "feature1   0.703704  0.811321  0.805556  0.631241  \n",
       "feature8   0.722222  0.679245  0.527778  0.640905  \n",
       "feature7   0.796296  0.754717  0.722222  0.724339  \n",
       "feature12  0.722222  0.716981  0.722222  0.749616  \n",
       "feature15  0.740741  0.716981  0.722222  0.778204  \n",
       "feature3   1.000000  0.981132  1.000000  0.916722  \n",
       "feature20  0.981481  1.000000  0.944444  0.983651  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance_Diff_percentile_ol_sensitivity2.sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cf61208f-3a98-4920-9d4a-0faa157133f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cf01340e-1235-464d-a61e-dca0b903f8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'feature')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(df_importance_Diff_percentile_ol_sensitivity2.sort_values('mean').iloc[:,:-1], cmap='bwr_r')\n",
    "plt.xlabel('overlap proportion (hyper parameter)')\n",
    "plt.ylabel('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1645239a-6aad-4847-b791-d30686d68758",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='revise_250603_overlap_sensitivity.pkl'\n",
    "with open(file_name,'wb') as f:\n",
    "    pickle.dump([df_importance_Diff_percentile_ol_sensitivity,df_importance_Diff_percentile_ol_sensitivity2], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6e4909e8-a580-4c98-8b4f-e9e7cdcb34c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Revise_PD_HD_20dim_classification_CV10_optimized_ol_sensitivity.ipynb to html\n",
      "[NbConvertApp] Writing 1973879 bytes to Revise_PD_HD_20dim_classification_CV10_optimized_ol_sensitivity.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html Revise_PD_HD_20dim_classification_CV10_optimized_ol_sensitivity.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbe7a2-2d04-468c-b02c-17c64d09946c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
