{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24299a06-cc90-4986-ad2f-ec21c911f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39868932-6135-4362-a04b-3f5586697a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ENSG</th>\n",
       "      <th>hgnc_symbol</th>\n",
       "      <th>gene_length</th>\n",
       "      <th>C_0002</th>\n",
       "      <th>C_0003</th>\n",
       "      <th>C_0004</th>\n",
       "      <th>C_0005</th>\n",
       "      <th>C_0006</th>\n",
       "      <th>C_0008</th>\n",
       "      <th>...</th>\n",
       "      <th>H_0740</th>\n",
       "      <th>H_0750</th>\n",
       "      <th>H_0513</th>\n",
       "      <th>H_0601</th>\n",
       "      <th>H_0656</th>\n",
       "      <th>H_0689</th>\n",
       "      <th>H_0709</th>\n",
       "      <th>H_0723</th>\n",
       "      <th>H_1104</th>\n",
       "      <th>H_1105</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ENSG00000000003</td>\n",
       "      <td>TSPAN6</td>\n",
       "      <td>12883</td>\n",
       "      <td>2.703162</td>\n",
       "      <td>4.679815</td>\n",
       "      <td>3.827170</td>\n",
       "      <td>3.940771</td>\n",
       "      <td>3.956139</td>\n",
       "      <td>4.749555</td>\n",
       "      <td>...</td>\n",
       "      <td>4.799810</td>\n",
       "      <td>8.733498</td>\n",
       "      <td>3.666497</td>\n",
       "      <td>4.563176</td>\n",
       "      <td>2.492904</td>\n",
       "      <td>2.949728</td>\n",
       "      <td>3.314066</td>\n",
       "      <td>6.985665</td>\n",
       "      <td>3.947979</td>\n",
       "      <td>3.784485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ENSG00000000005</td>\n",
       "      <td>TNMD</td>\n",
       "      <td>14949</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>0.053272</td>\n",
       "      <td>0.108531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024476</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.083015</td>\n",
       "      <td>0.078651</td>\n",
       "      <td>0.050749</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>0.138079</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>0.064583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ENSG00000000419</td>\n",
       "      <td>DPM1</td>\n",
       "      <td>24273</td>\n",
       "      <td>2.625862</td>\n",
       "      <td>4.387499</td>\n",
       "      <td>4.450224</td>\n",
       "      <td>5.390720</td>\n",
       "      <td>3.707350</td>\n",
       "      <td>5.356800</td>\n",
       "      <td>...</td>\n",
       "      <td>4.100151</td>\n",
       "      <td>6.470481</td>\n",
       "      <td>5.783705</td>\n",
       "      <td>6.878268</td>\n",
       "      <td>3.167153</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>6.380522</td>\n",
       "      <td>8.912018</td>\n",
       "      <td>6.420788</td>\n",
       "      <td>9.267549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ENSG00000000457</td>\n",
       "      <td>SCYL3</td>\n",
       "      <td>44636</td>\n",
       "      <td>0.896318</td>\n",
       "      <td>1.025353</td>\n",
       "      <td>1.568379</td>\n",
       "      <td>1.315815</td>\n",
       "      <td>1.617599</td>\n",
       "      <td>1.350063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610696</td>\n",
       "      <td>0.892573</td>\n",
       "      <td>1.098204</td>\n",
       "      <td>1.589228</td>\n",
       "      <td>0.708180</td>\n",
       "      <td>0.416006</td>\n",
       "      <td>0.628301</td>\n",
       "      <td>1.156094</td>\n",
       "      <td>1.317197</td>\n",
       "      <td>2.011547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ENSG00000000460</td>\n",
       "      <td>C1orf112</td>\n",
       "      <td>192073</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>0.074862</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>0.114637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030003</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.036344</td>\n",
       "      <td>0.091820</td>\n",
       "      <td>0.025015</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>0.070371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21858</th>\n",
       "      <td>26560</td>\n",
       "      <td>ENSG00000273372</td>\n",
       "      <td>SFTPD-AS1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.680299</td>\n",
       "      <td>0.616103</td>\n",
       "      <td>0.715117</td>\n",
       "      <td>1.061834</td>\n",
       "      <td>1.061811</td>\n",
       "      <td>1.437003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311009</td>\n",
       "      <td>0.202893</td>\n",
       "      <td>0.201662</td>\n",
       "      <td>1.018982</td>\n",
       "      <td>0.050577</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247696</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.096546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21859</th>\n",
       "      <td>26565</td>\n",
       "      <td>ENSG00000273396</td>\n",
       "      <td>LINC01396</td>\n",
       "      <td>6639</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.119952</td>\n",
       "      <td>0.139644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013778</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21860</th>\n",
       "      <td>26570</td>\n",
       "      <td>ENSG00000273409</td>\n",
       "      <td>LINC02712</td>\n",
       "      <td>66004</td>\n",
       "      <td>0.019632</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005544</td>\n",
       "      <td>0.027945</td>\n",
       "      <td>0.054055</td>\n",
       "      <td>0.065315</td>\n",
       "      <td>0.015325</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.058509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21861</th>\n",
       "      <td>26571</td>\n",
       "      <td>ENSG00000273415</td>\n",
       "      <td>LINC02725</td>\n",
       "      <td>87913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21862</th>\n",
       "      <td>26591</td>\n",
       "      <td>ENSG00000273492</td>\n",
       "      <td>APP-DT</td>\n",
       "      <td>46510</td>\n",
       "      <td>0.417913</td>\n",
       "      <td>0.416324</td>\n",
       "      <td>0.716177</td>\n",
       "      <td>0.516058</td>\n",
       "      <td>0.388106</td>\n",
       "      <td>0.294017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709995</td>\n",
       "      <td>0.362869</td>\n",
       "      <td>0.411909</td>\n",
       "      <td>0.564575</td>\n",
       "      <td>0.402350</td>\n",
       "      <td>0.102132</td>\n",
       "      <td>0.890978</td>\n",
       "      <td>0.781096</td>\n",
       "      <td>0.642094</td>\n",
       "      <td>0.332128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21863 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0             ENSG hgnc_symbol  gene_length    C_0002  \\\n",
       "0               1  ENSG00000000003      TSPAN6        12883  2.703162   \n",
       "1               2  ENSG00000000005        TNMD        14949  0.005418   \n",
       "2               3  ENSG00000000419        DPM1        24273  2.625862   \n",
       "3               4  ENSG00000000457       SCYL3        44636  0.896318   \n",
       "4               5  ENSG00000000460    C1orf112       192073  0.033732   \n",
       "...           ...              ...         ...          ...       ...   \n",
       "21858       26560  ENSG00000273372   SFTPD-AS1         5000  0.680299   \n",
       "21859       26565  ENSG00000273396   LINC01396         6639  0.036596   \n",
       "21860       26570  ENSG00000273409   LINC02712        66004  0.019632   \n",
       "21861       26571  ENSG00000273415   LINC02725        87913  0.000000   \n",
       "21862       26591  ENSG00000273492      APP-DT        46510  0.417913   \n",
       "\n",
       "         C_0003    C_0004    C_0005    C_0006    C_0008  ...    H_0740  \\\n",
       "0      4.679815  3.827170  3.940771  3.956139  4.749555  ...  4.799810   \n",
       "1      0.029438  0.088121  0.036995  0.053272  0.108531  ...  0.024476   \n",
       "2      4.387499  4.450224  5.390720  3.707350  5.356800  ...  4.100151   \n",
       "3      1.025353  1.568379  1.315815  1.617599  1.350063  ...  0.610696   \n",
       "4      0.123724  0.065645  0.074862  0.063574  0.114637  ...  0.030003   \n",
       "...         ...       ...       ...       ...       ...  ...       ...   \n",
       "21858  0.616103  0.715117  1.061834  1.061811  1.437003  ...  0.311009   \n",
       "21859  0.066286  0.226767  0.049981  0.119952  0.139644  ...  0.013778   \n",
       "21860  0.040004  0.068428  0.016758  0.036196  0.017558  ...  0.005544   \n",
       "21861  0.000000  0.000000  0.000000  0.000000  0.000000  ...  0.000000   \n",
       "21862  0.416324  0.716177  0.516058  0.388106  0.294017  ...  0.709995   \n",
       "\n",
       "         H_0750    H_0513    H_0601    H_0656    H_0689    H_0709    H_0723  \\\n",
       "0      8.733498  3.666497  4.563176  2.492904  2.949728  3.314066  6.985665   \n",
       "1      0.086370  0.083015  0.078651  0.050749  0.014444  0.028001  0.138079   \n",
       "2      6.470481  5.783705  6.878268  3.167153  1.530000  6.380522  8.912018   \n",
       "3      0.892573  1.098204  1.589228  0.708180  0.416006  0.628301  1.156094   \n",
       "4      0.052817  0.036344  0.091820  0.025015  0.042717  0.034868  0.060181   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21858  0.202893  0.201662  1.018982  0.050577  0.647750  0.000000  0.247696   \n",
       "21859  0.027783  0.000000  0.000000  0.038090  0.000000  0.000000  0.000000   \n",
       "21860  0.027945  0.054055  0.065315  0.015325  0.009814  0.000000  0.012509   \n",
       "21861  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21862  0.362869  0.411909  0.564575  0.402350  0.102132  0.890978  0.781096   \n",
       "\n",
       "         H_1104    H_1105  \n",
       "0      3.947979  3.784485  \n",
       "1      0.031214  0.064583  \n",
       "2      6.420788  9.267549  \n",
       "3      1.317197  2.011547  \n",
       "4      0.072882  0.070371  \n",
       "...         ...       ...  \n",
       "21858  0.559947  0.096546  \n",
       "21859  0.000000  0.000000  \n",
       "21860  0.028278  0.058509  \n",
       "21861  0.000000  0.000000  \n",
       "21862  0.642094  0.332128  \n",
       "\n",
       "[21863 rows x 112 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"tpm_data.csv\", header=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4d0d4a-91a4-47b7-bd12-3e9108421e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.621517048846005e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_pre=data.iloc[:,4:]\n",
    "dataX=dataX_pre.T\n",
    "dataX_min=np.min(dataX.values[dataX.values>0])\n",
    "dataX_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b41363e-3fc9-4f6c-8e7e-7d49dd1c4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert 0 into minimized number which cannot affect analysis\n",
    "convert_num = 8.621517048846005e-06\n",
    "dataX[dataX== 0] = convert_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de048b5-7083-489e-a08a-8f201531cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX_copy=dataX.copy()\n",
    "dataX_copy_log=dataX_copy.apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06aa8e6b-f410-4b88-82a6-7146c9a2f2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>2.703162</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>2.625862</td>\n",
       "      <td>0.896318</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>1.520210</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>2.390171</td>\n",
       "      <td>1.079804</td>\n",
       "      <td>1.846578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024701</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>12.989309</td>\n",
       "      <td>2.596021</td>\n",
       "      <td>0.680299</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.019632</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.417913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>4.679815</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>4.387499</td>\n",
       "      <td>1.025353</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>2.417254</td>\n",
       "      <td>0.541779</td>\n",
       "      <td>6.715102</td>\n",
       "      <td>2.762668</td>\n",
       "      <td>5.757188</td>\n",
       "      <td>...</td>\n",
       "      <td>3.380591</td>\n",
       "      <td>0.400795</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>42.426011</td>\n",
       "      <td>2.992243</td>\n",
       "      <td>0.616103</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.416324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>3.827170</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>4.450224</td>\n",
       "      <td>1.568379</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>1.855760</td>\n",
       "      <td>0.840779</td>\n",
       "      <td>4.118158</td>\n",
       "      <td>1.709962</td>\n",
       "      <td>3.075835</td>\n",
       "      <td>...</td>\n",
       "      <td>1.955870</td>\n",
       "      <td>0.171392</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>24.410100</td>\n",
       "      <td>2.010758</td>\n",
       "      <td>0.715117</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.716177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>3.940771</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>5.390720</td>\n",
       "      <td>1.315815</td>\n",
       "      <td>0.074862</td>\n",
       "      <td>1.803517</td>\n",
       "      <td>1.254086</td>\n",
       "      <td>5.239934</td>\n",
       "      <td>2.649874</td>\n",
       "      <td>4.998753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698587</td>\n",
       "      <td>0.201471</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.525642</td>\n",
       "      <td>3.975217</td>\n",
       "      <td>1.061834</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.516058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>3.956139</td>\n",
       "      <td>0.053272</td>\n",
       "      <td>3.707350</td>\n",
       "      <td>1.617599</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>1.354761</td>\n",
       "      <td>0.569268</td>\n",
       "      <td>4.474453</td>\n",
       "      <td>2.367638</td>\n",
       "      <td>4.753598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719709</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>33.152522</td>\n",
       "      <td>1.418155</td>\n",
       "      <td>1.061811</td>\n",
       "      <td>0.119952</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.388106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>2.949728</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.416006</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>4.108964</td>\n",
       "      <td>1.871441</td>\n",
       "      <td>2.094295</td>\n",
       "      <td>0.747223</td>\n",
       "      <td>2.215735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.196645</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>9.461733</td>\n",
       "      <td>2.411892</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.102132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>3.314066</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>6.380522</td>\n",
       "      <td>0.628301</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>1.158647</td>\n",
       "      <td>0.860249</td>\n",
       "      <td>3.787714</td>\n",
       "      <td>1.060191</td>\n",
       "      <td>1.706804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945729</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>31.182523</td>\n",
       "      <td>3.252682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.890978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>6.985665</td>\n",
       "      <td>0.138079</td>\n",
       "      <td>8.912018</td>\n",
       "      <td>1.156094</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>1.946203</td>\n",
       "      <td>1.799320</td>\n",
       "      <td>4.370479</td>\n",
       "      <td>1.842778</td>\n",
       "      <td>3.689338</td>\n",
       "      <td>...</td>\n",
       "      <td>2.425103</td>\n",
       "      <td>1.503924</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>57.708965</td>\n",
       "      <td>6.215465</td>\n",
       "      <td>0.247696</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.781096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>3.947979</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>6.420788</td>\n",
       "      <td>1.317197</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>2.361264</td>\n",
       "      <td>1.357402</td>\n",
       "      <td>3.256530</td>\n",
       "      <td>1.497821</td>\n",
       "      <td>3.361442</td>\n",
       "      <td>...</td>\n",
       "      <td>2.319405</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.742013</td>\n",
       "      <td>3.172758</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.642094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>3.784485</td>\n",
       "      <td>0.064583</td>\n",
       "      <td>9.267549</td>\n",
       "      <td>2.011547</td>\n",
       "      <td>0.070371</td>\n",
       "      <td>3.758100</td>\n",
       "      <td>0.637426</td>\n",
       "      <td>7.137567</td>\n",
       "      <td>1.807773</td>\n",
       "      <td>3.100193</td>\n",
       "      <td>...</td>\n",
       "      <td>5.671453</td>\n",
       "      <td>0.439643</td>\n",
       "      <td>0.021656</td>\n",
       "      <td>57.538120</td>\n",
       "      <td>5.157850</td>\n",
       "      <td>0.096546</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.058509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.332128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21862 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002  2.703162  0.005418  2.625862  0.896318  0.033732  1.520210  0.668186   \n",
       "C_0003  4.679815  0.029438  4.387499  1.025353  0.123724  2.417254  0.541779   \n",
       "C_0004  3.827170  0.088121  4.450224  1.568379  0.065645  1.855760  0.840779   \n",
       "C_0005  3.940771  0.036995  5.390720  1.315815  0.074862  1.803517  1.254086   \n",
       "C_0006  3.956139  0.053272  3.707350  1.617599  0.063574  1.354761  0.569268   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689  2.949728  0.014444  1.530000  0.416006  0.042717  4.108964  1.871441   \n",
       "H_0709  3.314066  0.028001  6.380522  0.628301  0.034868  1.158647  0.860249   \n",
       "H_0723  6.985665  0.138079  8.912018  1.156094  0.060181  1.946203  1.799320   \n",
       "H_1104  3.947979  0.031214  6.420788  1.317197  0.072882  2.361264  1.357402   \n",
       "H_1105  3.784485  0.064583  9.267549  2.011547  0.070371  3.758100  0.637426   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002  2.390171  1.079804  1.846578  ...  1.024701  0.000009  0.007266   \n",
       "C_0003  6.715102  2.762668  5.757188  ...  3.380591  0.400795  0.000009   \n",
       "C_0004  4.118158  1.709962  3.075835  ...  1.955870  0.171392  0.008442   \n",
       "C_0005  5.239934  2.649874  4.998753  ...  3.698587  0.201471  0.000009   \n",
       "C_0006  4.474453  2.367638  4.753598  ...  0.719709  0.000009  0.000009   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689  2.094295  0.747223  2.215735  ...  0.487837  0.196645  0.009686   \n",
       "H_0709  3.787714  1.060191  1.706804  ...  0.945729  0.000009  0.000009   \n",
       "H_0723  4.370479  1.842778  3.689338  ...  2.425103  1.503924  0.018520   \n",
       "H_1104  3.256530  1.497821  3.361442  ...  2.319405  0.000009  0.000009   \n",
       "H_1105  7.137567  1.807773  3.100193  ...  5.671453  0.439643  0.021656   \n",
       "\n",
       "            21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002  12.989309  2.596021  0.680299  0.036596  0.019632  0.000009  0.417913  \n",
       "C_0003  42.426011  2.992243  0.616103  0.066286  0.040004  0.000009  0.416324  \n",
       "C_0004  24.410100  2.010758  0.715117  0.226767  0.068428  0.000009  0.716177  \n",
       "C_0005  24.525642  3.975217  1.061834  0.049981  0.016758  0.000009  0.516058  \n",
       "C_0006  33.152522  1.418155  1.061811  0.119952  0.036196  0.000009  0.388106  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0689   9.461733  2.411892  0.647750  0.000009  0.009814  0.000009  0.102132  \n",
       "H_0709  31.182523  3.252682  0.000009  0.000009  0.000009  0.000009  0.890978  \n",
       "H_0723  57.708965  6.215465  0.247696  0.000009  0.012509  0.000009  0.781096  \n",
       "H_1104  24.742013  3.172758  0.559947  0.000009  0.028278  0.000009  0.642094  \n",
       "H_1105  57.538120  5.157850  0.096546  0.000009  0.058509  0.000009  0.332128  \n",
       "\n",
       "[108 rows x 21862 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete a column which has a smallest std (deleting LINC02694)\n",
    "dataX_after_del=dataX_copy.drop(dataX_copy.columns[13609],axis=1)\n",
    "dataX_after_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc3a7ab-76e6-4b05-aea9-0d6f7cfb7ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>2.703162</td>\n",
       "      <td>0.005418</td>\n",
       "      <td>2.625862</td>\n",
       "      <td>0.896318</td>\n",
       "      <td>0.033732</td>\n",
       "      <td>1.520210</td>\n",
       "      <td>0.668186</td>\n",
       "      <td>2.390171</td>\n",
       "      <td>1.079804</td>\n",
       "      <td>1.846578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024701</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>12.989309</td>\n",
       "      <td>2.596021</td>\n",
       "      <td>0.680299</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.019632</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.417913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>4.679815</td>\n",
       "      <td>0.029438</td>\n",
       "      <td>4.387499</td>\n",
       "      <td>1.025353</td>\n",
       "      <td>0.123724</td>\n",
       "      <td>2.417254</td>\n",
       "      <td>0.541779</td>\n",
       "      <td>6.715102</td>\n",
       "      <td>2.762668</td>\n",
       "      <td>5.757188</td>\n",
       "      <td>...</td>\n",
       "      <td>3.380591</td>\n",
       "      <td>0.400795</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>42.426011</td>\n",
       "      <td>2.992243</td>\n",
       "      <td>0.616103</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.416324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>3.827170</td>\n",
       "      <td>0.088121</td>\n",
       "      <td>4.450224</td>\n",
       "      <td>1.568379</td>\n",
       "      <td>0.065645</td>\n",
       "      <td>1.855760</td>\n",
       "      <td>0.840779</td>\n",
       "      <td>4.118158</td>\n",
       "      <td>1.709962</td>\n",
       "      <td>3.075835</td>\n",
       "      <td>...</td>\n",
       "      <td>1.955870</td>\n",
       "      <td>0.171392</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>24.410100</td>\n",
       "      <td>2.010758</td>\n",
       "      <td>0.715117</td>\n",
       "      <td>0.226767</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.716177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>3.940771</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>5.390720</td>\n",
       "      <td>1.315815</td>\n",
       "      <td>0.074862</td>\n",
       "      <td>1.803517</td>\n",
       "      <td>1.254086</td>\n",
       "      <td>5.239934</td>\n",
       "      <td>2.649874</td>\n",
       "      <td>4.998753</td>\n",
       "      <td>...</td>\n",
       "      <td>3.698587</td>\n",
       "      <td>0.201471</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.525642</td>\n",
       "      <td>3.975217</td>\n",
       "      <td>1.061834</td>\n",
       "      <td>0.049981</td>\n",
       "      <td>0.016758</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.516058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>3.956139</td>\n",
       "      <td>0.053272</td>\n",
       "      <td>3.707350</td>\n",
       "      <td>1.617599</td>\n",
       "      <td>0.063574</td>\n",
       "      <td>1.354761</td>\n",
       "      <td>0.569268</td>\n",
       "      <td>4.474453</td>\n",
       "      <td>2.367638</td>\n",
       "      <td>4.753598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719709</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>33.152522</td>\n",
       "      <td>1.418155</td>\n",
       "      <td>1.061811</td>\n",
       "      <td>0.119952</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.388106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>2.949728</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.416006</td>\n",
       "      <td>0.042717</td>\n",
       "      <td>4.108964</td>\n",
       "      <td>1.871441</td>\n",
       "      <td>2.094295</td>\n",
       "      <td>0.747223</td>\n",
       "      <td>2.215735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487837</td>\n",
       "      <td>0.196645</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>9.461733</td>\n",
       "      <td>2.411892</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.009814</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.102132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>3.314066</td>\n",
       "      <td>0.028001</td>\n",
       "      <td>6.380522</td>\n",
       "      <td>0.628301</td>\n",
       "      <td>0.034868</td>\n",
       "      <td>1.158647</td>\n",
       "      <td>0.860249</td>\n",
       "      <td>3.787714</td>\n",
       "      <td>1.060191</td>\n",
       "      <td>1.706804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945729</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>31.182523</td>\n",
       "      <td>3.252682</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.890978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>6.985665</td>\n",
       "      <td>0.138079</td>\n",
       "      <td>8.912018</td>\n",
       "      <td>1.156094</td>\n",
       "      <td>0.060181</td>\n",
       "      <td>1.946203</td>\n",
       "      <td>1.799320</td>\n",
       "      <td>4.370479</td>\n",
       "      <td>1.842778</td>\n",
       "      <td>3.689338</td>\n",
       "      <td>...</td>\n",
       "      <td>2.425103</td>\n",
       "      <td>1.503924</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>57.708965</td>\n",
       "      <td>6.215465</td>\n",
       "      <td>0.247696</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.781096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>3.947979</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>6.420788</td>\n",
       "      <td>1.317197</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>2.361264</td>\n",
       "      <td>1.357402</td>\n",
       "      <td>3.256530</td>\n",
       "      <td>1.497821</td>\n",
       "      <td>3.361442</td>\n",
       "      <td>...</td>\n",
       "      <td>2.319405</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>24.742013</td>\n",
       "      <td>3.172758</td>\n",
       "      <td>0.559947</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.028278</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.642094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>3.784485</td>\n",
       "      <td>0.064583</td>\n",
       "      <td>9.267549</td>\n",
       "      <td>2.011547</td>\n",
       "      <td>0.070371</td>\n",
       "      <td>3.758100</td>\n",
       "      <td>0.637426</td>\n",
       "      <td>7.137567</td>\n",
       "      <td>1.807773</td>\n",
       "      <td>3.100193</td>\n",
       "      <td>...</td>\n",
       "      <td>5.671453</td>\n",
       "      <td>0.439643</td>\n",
       "      <td>0.021656</td>\n",
       "      <td>57.538120</td>\n",
       "      <td>5.157850</td>\n",
       "      <td>0.096546</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.058509</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.332128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002  2.703162  0.005418  2.625862  0.896318  0.033732  1.520210  0.668186   \n",
       "C_0003  4.679815  0.029438  4.387499  1.025353  0.123724  2.417254  0.541779   \n",
       "C_0004  3.827170  0.088121  4.450224  1.568379  0.065645  1.855760  0.840779   \n",
       "C_0005  3.940771  0.036995  5.390720  1.315815  0.074862  1.803517  1.254086   \n",
       "C_0006  3.956139  0.053272  3.707350  1.617599  0.063574  1.354761  0.569268   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689  2.949728  0.014444  1.530000  0.416006  0.042717  4.108964  1.871441   \n",
       "H_0709  3.314066  0.028001  6.380522  0.628301  0.034868  1.158647  0.860249   \n",
       "H_0723  6.985665  0.138079  8.912018  1.156094  0.060181  1.946203  1.799320   \n",
       "H_1104  3.947979  0.031214  6.420788  1.317197  0.072882  2.361264  1.357402   \n",
       "H_1105  3.784485  0.064583  9.267549  2.011547  0.070371  3.758100  0.637426   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002  2.390171  1.079804  1.846578  ...  1.024701  0.000009  0.007266   \n",
       "C_0003  6.715102  2.762668  5.757188  ...  3.380591  0.400795  0.000009   \n",
       "C_0004  4.118158  1.709962  3.075835  ...  1.955870  0.171392  0.008442   \n",
       "C_0005  5.239934  2.649874  4.998753  ...  3.698587  0.201471  0.000009   \n",
       "C_0006  4.474453  2.367638  4.753598  ...  0.719709  0.000009  0.000009   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689  2.094295  0.747223  2.215735  ...  0.487837  0.196645  0.009686   \n",
       "H_0709  3.787714  1.060191  1.706804  ...  0.945729  0.000009  0.000009   \n",
       "H_0723  4.370479  1.842778  3.689338  ...  2.425103  1.503924  0.018520   \n",
       "H_1104  3.256530  1.497821  3.361442  ...  2.319405  0.000009  0.000009   \n",
       "H_1105  7.137567  1.807773  3.100193  ...  5.671453  0.439643  0.021656   \n",
       "\n",
       "            21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002  12.989309  2.596021  0.680299  0.036596  0.019632  0.000009  0.417913  \n",
       "C_0003  42.426011  2.992243  0.616103  0.066286  0.040004  0.000009  0.416324  \n",
       "C_0004  24.410100  2.010758  0.715117  0.226767  0.068428  0.000009  0.716177  \n",
       "C_0005  24.525642  3.975217  1.061834  0.049981  0.016758  0.000009  0.516058  \n",
       "C_0006  33.152522  1.418155  1.061811  0.119952  0.036196  0.000009  0.388106  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0689   9.461733  2.411892  0.647750  0.000009  0.009814  0.000009  0.102132  \n",
       "H_0709  31.182523  3.252682  0.000009  0.000009  0.000009  0.000009  0.890978  \n",
       "H_0723  57.708965  6.215465  0.247696  0.000009  0.012509  0.000009  0.781096  \n",
       "H_1104  24.742013  3.172758  0.559947  0.000009  0.028278  0.000009  0.642094  \n",
       "H_1105  57.538120  5.157850  0.096546  0.000009  0.058509  0.000009  0.332128  \n",
       "\n",
       "[108 rows x 21861 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delete a column which has a smallest std\n",
    "dataX_after_del2=dataX_after_del.drop(dataX_copy.columns[19081],axis=1)\n",
    "dataX_after_del2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfd76653-736d-438d-a369-92836102a120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>0.994422</td>\n",
       "      <td>-5.218099</td>\n",
       "      <td>0.965409</td>\n",
       "      <td>-0.109460</td>\n",
       "      <td>-3.389303</td>\n",
       "      <td>0.418849</td>\n",
       "      <td>-0.403189</td>\n",
       "      <td>0.871365</td>\n",
       "      <td>0.076779</td>\n",
       "      <td>0.613334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024401</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-4.924490</td>\n",
       "      <td>2.564127</td>\n",
       "      <td>0.953980</td>\n",
       "      <td>-0.385223</td>\n",
       "      <td>-3.307803</td>\n",
       "      <td>-3.930581</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.872483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>1.543259</td>\n",
       "      <td>-3.525458</td>\n",
       "      <td>1.478759</td>\n",
       "      <td>0.025037</td>\n",
       "      <td>-2.089705</td>\n",
       "      <td>0.882632</td>\n",
       "      <td>-0.612896</td>\n",
       "      <td>1.904359</td>\n",
       "      <td>1.016197</td>\n",
       "      <td>1.750449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.218051</td>\n",
       "      <td>-0.914304</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.747762</td>\n",
       "      <td>1.096023</td>\n",
       "      <td>-0.484341</td>\n",
       "      <td>-2.713775</td>\n",
       "      <td>-3.218770</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.876291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>1.342126</td>\n",
       "      <td>-2.429044</td>\n",
       "      <td>1.492955</td>\n",
       "      <td>0.450043</td>\n",
       "      <td>-2.723493</td>\n",
       "      <td>0.618294</td>\n",
       "      <td>-0.173427</td>\n",
       "      <td>1.415406</td>\n",
       "      <td>0.536471</td>\n",
       "      <td>1.123576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670835</td>\n",
       "      <td>-1.763801</td>\n",
       "      <td>-4.774493</td>\n",
       "      <td>3.194997</td>\n",
       "      <td>0.698512</td>\n",
       "      <td>-0.335309</td>\n",
       "      <td>-1.483830</td>\n",
       "      <td>-2.681972</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.333828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>1.371376</td>\n",
       "      <td>-3.296972</td>\n",
       "      <td>1.684679</td>\n",
       "      <td>0.274456</td>\n",
       "      <td>-2.592106</td>\n",
       "      <td>0.589739</td>\n",
       "      <td>0.226407</td>\n",
       "      <td>1.656309</td>\n",
       "      <td>0.974512</td>\n",
       "      <td>1.609188</td>\n",
       "      <td>...</td>\n",
       "      <td>1.307951</td>\n",
       "      <td>-1.602108</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.199719</td>\n",
       "      <td>1.380079</td>\n",
       "      <td>0.059998</td>\n",
       "      <td>-2.996114</td>\n",
       "      <td>-4.088895</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.661535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>1.375269</td>\n",
       "      <td>-2.932350</td>\n",
       "      <td>1.310317</td>\n",
       "      <td>0.480943</td>\n",
       "      <td>-2.755552</td>\n",
       "      <td>0.303625</td>\n",
       "      <td>-0.563404</td>\n",
       "      <td>1.498384</td>\n",
       "      <td>0.861893</td>\n",
       "      <td>1.558902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328908</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.501119</td>\n",
       "      <td>0.349357</td>\n",
       "      <td>0.059976</td>\n",
       "      <td>-2.120667</td>\n",
       "      <td>-3.318809</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.946478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>1.081713</td>\n",
       "      <td>-4.237507</td>\n",
       "      <td>0.425267</td>\n",
       "      <td>-0.877056</td>\n",
       "      <td>-3.153152</td>\n",
       "      <td>1.413171</td>\n",
       "      <td>0.626709</td>\n",
       "      <td>0.739217</td>\n",
       "      <td>-0.291392</td>\n",
       "      <td>0.795584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717774</td>\n",
       "      <td>-1.626353</td>\n",
       "      <td>-4.637045</td>\n",
       "      <td>2.247256</td>\n",
       "      <td>0.880411</td>\n",
       "      <td>-0.434250</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-4.623965</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-2.281487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>1.198176</td>\n",
       "      <td>-3.575533</td>\n",
       "      <td>1.853250</td>\n",
       "      <td>-0.464737</td>\n",
       "      <td>-3.356175</td>\n",
       "      <td>0.147253</td>\n",
       "      <td>-0.150534</td>\n",
       "      <td>1.331763</td>\n",
       "      <td>0.058449</td>\n",
       "      <td>0.534623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055800</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.439858</td>\n",
       "      <td>1.179480</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.115436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>1.943860</td>\n",
       "      <td>-1.979933</td>\n",
       "      <td>2.187401</td>\n",
       "      <td>0.145047</td>\n",
       "      <td>-2.810397</td>\n",
       "      <td>0.665880</td>\n",
       "      <td>0.587409</td>\n",
       "      <td>1.474873</td>\n",
       "      <td>0.611274</td>\n",
       "      <td>1.305447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885874</td>\n",
       "      <td>0.408078</td>\n",
       "      <td>-3.988909</td>\n",
       "      <td>4.055413</td>\n",
       "      <td>1.827040</td>\n",
       "      <td>-1.395552</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-4.381294</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.247057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>1.373204</td>\n",
       "      <td>-3.466880</td>\n",
       "      <td>1.859541</td>\n",
       "      <td>0.275506</td>\n",
       "      <td>-2.618914</td>\n",
       "      <td>0.859197</td>\n",
       "      <td>0.305573</td>\n",
       "      <td>1.180662</td>\n",
       "      <td>0.404012</td>\n",
       "      <td>1.212370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841311</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>3.208503</td>\n",
       "      <td>1.154601</td>\n",
       "      <td>-0.579914</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-3.565657</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-0.443020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>1.330910</td>\n",
       "      <td>-2.739799</td>\n",
       "      <td>2.226519</td>\n",
       "      <td>0.698904</td>\n",
       "      <td>-2.653973</td>\n",
       "      <td>1.323913</td>\n",
       "      <td>-0.450317</td>\n",
       "      <td>1.965372</td>\n",
       "      <td>0.592096</td>\n",
       "      <td>1.131464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.735445</td>\n",
       "      <td>-0.821792</td>\n",
       "      <td>-3.832485</td>\n",
       "      <td>4.052448</td>\n",
       "      <td>1.640520</td>\n",
       "      <td>-2.337740</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-2.838576</td>\n",
       "      <td>-11.661249</td>\n",
       "      <td>-1.102234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002  0.994422 -5.218099  0.965409 -0.109460 -3.389303  0.418849 -0.403189   \n",
       "C_0003  1.543259 -3.525458  1.478759  0.025037 -2.089705  0.882632 -0.612896   \n",
       "C_0004  1.342126 -2.429044  1.492955  0.450043 -2.723493  0.618294 -0.173427   \n",
       "C_0005  1.371376 -3.296972  1.684679  0.274456 -2.592106  0.589739  0.226407   \n",
       "C_0006  1.375269 -2.932350  1.310317  0.480943 -2.755552  0.303625 -0.563404   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689  1.081713 -4.237507  0.425267 -0.877056 -3.153152  1.413171  0.626709   \n",
       "H_0709  1.198176 -3.575533  1.853250 -0.464737 -3.356175  0.147253 -0.150534   \n",
       "H_0723  1.943860 -1.979933  2.187401  0.145047 -2.810397  0.665880  0.587409   \n",
       "H_1104  1.373204 -3.466880  1.859541  0.275506 -2.618914  0.859197  0.305573   \n",
       "H_1105  1.330910 -2.739799  2.226519  0.698904 -2.653973  1.323913 -0.450317   \n",
       "\n",
       "           7         8         9      ...     21853      21854      21855  \\\n",
       "C_0002  0.871365  0.076779  0.613334  ...  0.024401 -11.661249  -4.924490   \n",
       "C_0003  1.904359  1.016197  1.750449  ...  1.218051  -0.914304 -11.661249   \n",
       "C_0004  1.415406  0.536471  1.123576  ...  0.670835  -1.763801  -4.774493   \n",
       "C_0005  1.656309  0.974512  1.609188  ...  1.307951  -1.602108 -11.661249   \n",
       "C_0006  1.498384  0.861893  1.558902  ... -0.328908 -11.661249 -11.661249   \n",
       "...          ...       ...       ...  ...       ...        ...        ...   \n",
       "H_0689  0.739217 -0.291392  0.795584  ... -0.717774  -1.626353  -4.637045   \n",
       "H_0709  1.331763  0.058449  0.534623  ... -0.055800 -11.661249 -11.661249   \n",
       "H_0723  1.474873  0.611274  1.305447  ...  0.885874   0.408078  -3.988909   \n",
       "H_1104  1.180662  0.404012  1.212370  ...  0.841311 -11.661249 -11.661249   \n",
       "H_1105  1.965372  0.592096  1.131464  ...  1.735445  -0.821792  -3.832485   \n",
       "\n",
       "           21856     21857      21858      21859      21860      21861  \\\n",
       "C_0002  2.564127  0.953980  -0.385223  -3.307803  -3.930581 -11.661249   \n",
       "C_0003  3.747762  1.096023  -0.484341  -2.713775  -3.218770 -11.661249   \n",
       "C_0004  3.194997  0.698512  -0.335309  -1.483830  -2.681972 -11.661249   \n",
       "C_0005  3.199719  1.380079   0.059998  -2.996114  -4.088895 -11.661249   \n",
       "C_0006  3.501119  0.349357   0.059976  -2.120667  -3.318809 -11.661249   \n",
       "...          ...       ...        ...        ...        ...        ...   \n",
       "H_0689  2.247256  0.880411  -0.434250 -11.661249  -4.623965 -11.661249   \n",
       "H_0709  3.439858  1.179480 -11.661249 -11.661249 -11.661249 -11.661249   \n",
       "H_0723  4.055413  1.827040  -1.395552 -11.661249  -4.381294 -11.661249   \n",
       "H_1104  3.208503  1.154601  -0.579914 -11.661249  -3.565657 -11.661249   \n",
       "H_1105  4.052448  1.640520  -2.337740 -11.661249  -2.838576 -11.661249   \n",
       "\n",
       "           21862  \n",
       "C_0002 -0.872483  \n",
       "C_0003 -0.876291  \n",
       "C_0004 -0.333828  \n",
       "C_0005 -0.661535  \n",
       "C_0006 -0.946478  \n",
       "...          ...  \n",
       "H_0689 -2.281487  \n",
       "H_0709 -0.115436  \n",
       "H_0723 -0.247057  \n",
       "H_1104 -0.443020  \n",
       "H_1105 -1.102234  \n",
       "\n",
       "[108 rows x 21861 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#log conversion of Dataframe\n",
    "dataX_log_del2=dataX_after_del2.apply(np.log)\n",
    "dataX_log_del2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b86feb26-3de8-46da-b22f-a7f3db9e1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresToScale=dataX_log_del2.columns\n",
    "sX=pp.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "dataX_log_del2.loc[:, featuresToScale]=sX.fit_transform(dataX_log_del2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d441e13-f7bc-44a8-a428-195cc4360c7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-0.489755</td>\n",
       "      <td>-1.404819</td>\n",
       "      <td>-0.661768</td>\n",
       "      <td>-1.214104</td>\n",
       "      <td>-0.438326</td>\n",
       "      <td>-0.542205</td>\n",
       "      <td>-1.549951</td>\n",
       "      <td>-1.144974</td>\n",
       "      <td>-1.361823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.737597</td>\n",
       "      <td>-1.819190</td>\n",
       "      <td>-0.178338</td>\n",
       "      <td>0.264969</td>\n",
       "      <td>0.721004</td>\n",
       "      <td>-0.094753</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.142859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>-0.094326</td>\n",
       "      <td>0.128294</td>\n",
       "      <td>-0.400562</td>\n",
       "      <td>-0.392906</td>\n",
       "      <td>0.908731</td>\n",
       "      <td>0.454217</td>\n",
       "      <td>-0.792399</td>\n",
       "      <td>0.733360</td>\n",
       "      <td>0.611638</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767006</td>\n",
       "      <td>0.814752</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.719449</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.220818</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.298774</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.149023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>-0.513695</td>\n",
       "      <td>0.528638</td>\n",
       "      <td>-0.372793</td>\n",
       "      <td>0.456696</td>\n",
       "      <td>-0.126533</td>\n",
       "      <td>-0.054497</td>\n",
       "      <td>-0.268084</td>\n",
       "      <td>-0.347413</td>\n",
       "      <td>-0.285398</td>\n",
       "      <td>-0.322790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051075</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.776952</td>\n",
       "      <td>-0.466111</td>\n",
       "      <td>-0.596202</td>\n",
       "      <td>0.287203</td>\n",
       "      <td>1.144874</td>\n",
       "      <td>0.595544</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.728854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>-0.452707</td>\n",
       "      <td>0.211724</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.088081</td>\n",
       "      <td>-0.109452</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.533692</td>\n",
       "      <td>0.666086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884624</td>\n",
       "      <td>0.681302</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.455983</td>\n",
       "      <td>0.518626</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.793437</td>\n",
       "      <td>-0.182278</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.198520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>-0.444591</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>-0.730082</td>\n",
       "      <td>0.518467</td>\n",
       "      <td>-0.178900</td>\n",
       "      <td>-0.660072</td>\n",
       "      <td>-0.733352</td>\n",
       "      <td>-0.164000</td>\n",
       "      <td>0.323106</td>\n",
       "      <td>0.563685</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.256904</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.190454</td>\n",
       "      <td>-1.167309</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.996881</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.262606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0008</th>\n",
       "      <td>-0.063484</td>\n",
       "      <td>0.604704</td>\n",
       "      <td>-0.010074</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.784126</td>\n",
       "      <td>0.523687</td>\n",
       "      <td>0.705062</td>\n",
       "      <td>0.107073</td>\n",
       "      <td>0.650657</td>\n",
       "      <td>0.591910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>0.690349</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>0.357423</td>\n",
       "      <td>0.598058</td>\n",
       "      <td>1.032206</td>\n",
       "      <td>-0.156499</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.711918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0009</th>\n",
       "      <td>-1.014420</td>\n",
       "      <td>0.027920</td>\n",
       "      <td>0.485191</td>\n",
       "      <td>1.511839</td>\n",
       "      <td>2.342554</td>\n",
       "      <td>-0.059718</td>\n",
       "      <td>-0.901242</td>\n",
       "      <td>2.952803</td>\n",
       "      <td>3.037081</td>\n",
       "      <td>1.972919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056384</td>\n",
       "      <td>1.073685</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>2.372708</td>\n",
       "      <td>1.781837</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>1.391432</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-3.818267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0010</th>\n",
       "      <td>-1.071431</td>\n",
       "      <td>0.484398</td>\n",
       "      <td>-0.933368</td>\n",
       "      <td>-0.658817</td>\n",
       "      <td>-0.998986</td>\n",
       "      <td>-0.943378</td>\n",
       "      <td>-1.362364</td>\n",
       "      <td>-0.970939</td>\n",
       "      <td>-0.853908</td>\n",
       "      <td>-0.605820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.555258</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.785608</td>\n",
       "      <td>-0.303165</td>\n",
       "      <td>0.070641</td>\n",
       "      <td>0.277815</td>\n",
       "      <td>0.669304</td>\n",
       "      <td>0.071529</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.462936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0011</th>\n",
       "      <td>-0.921800</td>\n",
       "      <td>0.182561</td>\n",
       "      <td>-2.304816</td>\n",
       "      <td>-1.383902</td>\n",
       "      <td>-1.199928</td>\n",
       "      <td>-0.621190</td>\n",
       "      <td>-1.538195</td>\n",
       "      <td>-1.155816</td>\n",
       "      <td>-0.558486</td>\n",
       "      <td>-0.865617</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.144206</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.399549</td>\n",
       "      <td>-1.752028</td>\n",
       "      <td>-0.121130</td>\n",
       "      <td>0.774877</td>\n",
       "      <td>0.128418</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.916628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0012</th>\n",
       "      <td>0.370489</td>\n",
       "      <td>0.568469</td>\n",
       "      <td>0.573826</td>\n",
       "      <td>1.847973</td>\n",
       "      <td>2.247641</td>\n",
       "      <td>-1.047992</td>\n",
       "      <td>-1.840582</td>\n",
       "      <td>2.485153</td>\n",
       "      <td>2.363211</td>\n",
       "      <td>2.430353</td>\n",
       "      <td>...</td>\n",
       "      <td>1.381163</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.987434</td>\n",
       "      <td>1.425060</td>\n",
       "      <td>0.034088</td>\n",
       "      <td>0.766789</td>\n",
       "      <td>0.848066</td>\n",
       "      <td>0.465206</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.968155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0013</th>\n",
       "      <td>0.237404</td>\n",
       "      <td>0.468118</td>\n",
       "      <td>-0.328469</td>\n",
       "      <td>0.401016</td>\n",
       "      <td>0.664377</td>\n",
       "      <td>-0.110870</td>\n",
       "      <td>1.175788</td>\n",
       "      <td>0.527173</td>\n",
       "      <td>0.230430</td>\n",
       "      <td>0.598487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245687</td>\n",
       "      <td>0.782167</td>\n",
       "      <td>0.773910</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.823008</td>\n",
       "      <td>0.794247</td>\n",
       "      <td>0.658943</td>\n",
       "      <td>0.046881</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.089385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0014</th>\n",
       "      <td>-0.043427</td>\n",
       "      <td>0.164883</td>\n",
       "      <td>-0.241251</td>\n",
       "      <td>-0.231408</td>\n",
       "      <td>-0.153178</td>\n",
       "      <td>0.290115</td>\n",
       "      <td>-0.123583</td>\n",
       "      <td>-0.380659</td>\n",
       "      <td>-0.117571</td>\n",
       "      <td>-0.196329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203878</td>\n",
       "      <td>0.699708</td>\n",
       "      <td>1.132509</td>\n",
       "      <td>-0.447637</td>\n",
       "      <td>-0.504793</td>\n",
       "      <td>0.158031</td>\n",
       "      <td>0.882336</td>\n",
       "      <td>0.306069</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.752933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0015</th>\n",
       "      <td>-0.159249</td>\n",
       "      <td>0.653677</td>\n",
       "      <td>0.067769</td>\n",
       "      <td>0.653778</td>\n",
       "      <td>-0.451082</td>\n",
       "      <td>-1.435228</td>\n",
       "      <td>-1.653990</td>\n",
       "      <td>-0.421961</td>\n",
       "      <td>-0.142727</td>\n",
       "      <td>0.361802</td>\n",
       "      <td>...</td>\n",
       "      <td>1.742628</td>\n",
       "      <td>0.581885</td>\n",
       "      <td>0.973181</td>\n",
       "      <td>-0.168088</td>\n",
       "      <td>-0.960749</td>\n",
       "      <td>0.403107</td>\n",
       "      <td>1.090746</td>\n",
       "      <td>-0.082349</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.371974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0016</th>\n",
       "      <td>-0.313939</td>\n",
       "      <td>0.505943</td>\n",
       "      <td>-0.474605</td>\n",
       "      <td>0.395045</td>\n",
       "      <td>-0.454325</td>\n",
       "      <td>0.768875</td>\n",
       "      <td>1.061524</td>\n",
       "      <td>-0.027705</td>\n",
       "      <td>-0.504156</td>\n",
       "      <td>0.133814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959270</td>\n",
       "      <td>0.802266</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.934674</td>\n",
       "      <td>-0.521819</td>\n",
       "      <td>0.304097</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.469263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0017</th>\n",
       "      <td>0.617328</td>\n",
       "      <td>-2.842400</td>\n",
       "      <td>-0.042863</td>\n",
       "      <td>0.876221</td>\n",
       "      <td>0.948050</td>\n",
       "      <td>-0.556515</td>\n",
       "      <td>-0.586851</td>\n",
       "      <td>1.194970</td>\n",
       "      <td>1.516010</td>\n",
       "      <td>1.081449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927239</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.039801</td>\n",
       "      <td>0.731721</td>\n",
       "      <td>0.243670</td>\n",
       "      <td>0.670608</td>\n",
       "      <td>0.733369</td>\n",
       "      <td>0.427240</td>\n",
       "      <td>2.775695</td>\n",
       "      <td>-0.883431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0018</th>\n",
       "      <td>-0.841823</td>\n",
       "      <td>-2.842400</td>\n",
       "      <td>0.611775</td>\n",
       "      <td>2.342445</td>\n",
       "      <td>2.457968</td>\n",
       "      <td>-1.436767</td>\n",
       "      <td>-1.273134</td>\n",
       "      <td>1.043109</td>\n",
       "      <td>0.310334</td>\n",
       "      <td>2.145601</td>\n",
       "      <td>...</td>\n",
       "      <td>2.039750</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.883744</td>\n",
       "      <td>2.903385</td>\n",
       "      <td>-0.352164</td>\n",
       "      <td>0.753175</td>\n",
       "      <td>0.917304</td>\n",
       "      <td>-0.170010</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.633091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0019</th>\n",
       "      <td>-0.536535</td>\n",
       "      <td>0.282387</td>\n",
       "      <td>-0.669082</td>\n",
       "      <td>-0.261382</td>\n",
       "      <td>-0.587647</td>\n",
       "      <td>-0.113586</td>\n",
       "      <td>0.768471</td>\n",
       "      <td>-0.019680</td>\n",
       "      <td>-0.749318</td>\n",
       "      <td>0.085651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332958</td>\n",
       "      <td>0.683476</td>\n",
       "      <td>0.928697</td>\n",
       "      <td>-1.103343</td>\n",
       "      <td>-0.071915</td>\n",
       "      <td>0.273526</td>\n",
       "      <td>0.796041</td>\n",
       "      <td>-0.234333</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.359326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0020</th>\n",
       "      <td>-1.286744</td>\n",
       "      <td>0.683392</td>\n",
       "      <td>-0.230312</td>\n",
       "      <td>0.520323</td>\n",
       "      <td>-0.197100</td>\n",
       "      <td>-1.564600</td>\n",
       "      <td>-1.281168</td>\n",
       "      <td>-0.488723</td>\n",
       "      <td>-0.260558</td>\n",
       "      <td>0.360519</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246719</td>\n",
       "      <td>0.644466</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.094803</td>\n",
       "      <td>-0.970501</td>\n",
       "      <td>0.396902</td>\n",
       "      <td>0.910396</td>\n",
       "      <td>0.320131</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.042961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0021</th>\n",
       "      <td>0.526197</td>\n",
       "      <td>-2.842400</td>\n",
       "      <td>-0.390967</td>\n",
       "      <td>0.445794</td>\n",
       "      <td>0.184534</td>\n",
       "      <td>-1.203421</td>\n",
       "      <td>-1.145355</td>\n",
       "      <td>0.687426</td>\n",
       "      <td>0.684889</td>\n",
       "      <td>0.804522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987853</td>\n",
       "      <td>0.836414</td>\n",
       "      <td>1.029127</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>-1.018371</td>\n",
       "      <td>0.330028</td>\n",
       "      <td>0.723915</td>\n",
       "      <td>0.062511</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.227033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0022</th>\n",
       "      <td>-1.362488</td>\n",
       "      <td>0.307889</td>\n",
       "      <td>-0.984365</td>\n",
       "      <td>-1.156695</td>\n",
       "      <td>-1.288870</td>\n",
       "      <td>-0.259354</td>\n",
       "      <td>-0.329153</td>\n",
       "      <td>-1.661139</td>\n",
       "      <td>-1.751085</td>\n",
       "      <td>-1.280175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299670</td>\n",
       "      <td>0.732401</td>\n",
       "      <td>0.706613</td>\n",
       "      <td>-0.743866</td>\n",
       "      <td>-2.689368</td>\n",
       "      <td>0.031758</td>\n",
       "      <td>0.760415</td>\n",
       "      <td>-0.233864</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.712032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0023</th>\n",
       "      <td>-1.226360</td>\n",
       "      <td>-0.380853</td>\n",
       "      <td>-2.068576</td>\n",
       "      <td>-1.459295</td>\n",
       "      <td>-2.173385</td>\n",
       "      <td>-1.454016</td>\n",
       "      <td>-1.745151</td>\n",
       "      <td>-1.424173</td>\n",
       "      <td>-1.924884</td>\n",
       "      <td>-1.365756</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.136616</td>\n",
       "      <td>0.678694</td>\n",
       "      <td>0.997710</td>\n",
       "      <td>-0.693899</td>\n",
       "      <td>-2.348000</td>\n",
       "      <td>-0.091542</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.313074</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.879198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0024</th>\n",
       "      <td>0.218499</td>\n",
       "      <td>0.742546</td>\n",
       "      <td>-0.366158</td>\n",
       "      <td>0.410088</td>\n",
       "      <td>-0.554448</td>\n",
       "      <td>-0.343820</td>\n",
       "      <td>-0.535454</td>\n",
       "      <td>-0.260692</td>\n",
       "      <td>-0.020393</td>\n",
       "      <td>0.194022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.626982</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.048342</td>\n",
       "      <td>-0.461180</td>\n",
       "      <td>0.103370</td>\n",
       "      <td>0.875968</td>\n",
       "      <td>0.238226</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.705871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0025</th>\n",
       "      <td>-0.525737</td>\n",
       "      <td>0.242389</td>\n",
       "      <td>0.106966</td>\n",
       "      <td>0.424007</td>\n",
       "      <td>0.286912</td>\n",
       "      <td>-0.878035</td>\n",
       "      <td>-1.127625</td>\n",
       "      <td>0.521370</td>\n",
       "      <td>0.787054</td>\n",
       "      <td>0.509760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186735</td>\n",
       "      <td>0.740892</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.404844</td>\n",
       "      <td>-0.157597</td>\n",
       "      <td>0.652556</td>\n",
       "      <td>1.144599</td>\n",
       "      <td>0.312478</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.480518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0026</th>\n",
       "      <td>-1.102176</td>\n",
       "      <td>0.412127</td>\n",
       "      <td>-1.320566</td>\n",
       "      <td>-0.620084</td>\n",
       "      <td>-1.046110</td>\n",
       "      <td>-0.585962</td>\n",
       "      <td>-0.764696</td>\n",
       "      <td>-0.931632</td>\n",
       "      <td>-0.883488</td>\n",
       "      <td>-0.745137</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.254419</td>\n",
       "      <td>0.617929</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.567240</td>\n",
       "      <td>-1.194215</td>\n",
       "      <td>0.164191</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.037894</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.236674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0029</th>\n",
       "      <td>-1.437548</td>\n",
       "      <td>-0.297698</td>\n",
       "      <td>-1.784748</td>\n",
       "      <td>-1.028572</td>\n",
       "      <td>-1.752626</td>\n",
       "      <td>0.407274</td>\n",
       "      <td>0.008234</td>\n",
       "      <td>-0.784135</td>\n",
       "      <td>-1.550076</td>\n",
       "      <td>-1.090772</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.328883</td>\n",
       "      <td>0.857367</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.160892</td>\n",
       "      <td>-0.607662</td>\n",
       "      <td>-0.239373</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.291046</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.386633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0060</th>\n",
       "      <td>1.016182</td>\n",
       "      <td>0.544289</td>\n",
       "      <td>-0.468397</td>\n",
       "      <td>-0.756485</td>\n",
       "      <td>-0.454883</td>\n",
       "      <td>-0.032562</td>\n",
       "      <td>0.310191</td>\n",
       "      <td>-1.481712</td>\n",
       "      <td>-0.330176</td>\n",
       "      <td>-1.178254</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717043</td>\n",
       "      <td>0.766825</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.787432</td>\n",
       "      <td>-1.118817</td>\n",
       "      <td>-0.086020</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.220998</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.530488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0061</th>\n",
       "      <td>-0.473287</td>\n",
       "      <td>0.056637</td>\n",
       "      <td>0.117051</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>0.056609</td>\n",
       "      <td>-0.557902</td>\n",
       "      <td>-0.201168</td>\n",
       "      <td>-0.439586</td>\n",
       "      <td>0.132665</td>\n",
       "      <td>-0.769386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.141075</td>\n",
       "      <td>0.875788</td>\n",
       "      <td>1.023826</td>\n",
       "      <td>0.292035</td>\n",
       "      <td>0.637192</td>\n",
       "      <td>0.554100</td>\n",
       "      <td>0.558140</td>\n",
       "      <td>-0.417092</td>\n",
       "      <td>2.371024</td>\n",
       "      <td>-1.178913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0062</th>\n",
       "      <td>0.745442</td>\n",
       "      <td>0.616147</td>\n",
       "      <td>1.085803</td>\n",
       "      <td>0.833815</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>-0.484330</td>\n",
       "      <td>0.229278</td>\n",
       "      <td>0.747268</td>\n",
       "      <td>0.973920</td>\n",
       "      <td>0.660299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880805</td>\n",
       "      <td>0.883678</td>\n",
       "      <td>0.986659</td>\n",
       "      <td>0.537345</td>\n",
       "      <td>0.138341</td>\n",
       "      <td>0.284933</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.463572</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.086073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0065</th>\n",
       "      <td>4.464034</td>\n",
       "      <td>0.193050</td>\n",
       "      <td>1.630309</td>\n",
       "      <td>1.204508</td>\n",
       "      <td>1.248069</td>\n",
       "      <td>-0.647544</td>\n",
       "      <td>0.895345</td>\n",
       "      <td>2.280033</td>\n",
       "      <td>2.542494</td>\n",
       "      <td>1.493681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648056</td>\n",
       "      <td>1.039465</td>\n",
       "      <td>1.362254</td>\n",
       "      <td>2.228318</td>\n",
       "      <td>1.156093</td>\n",
       "      <td>0.712185</td>\n",
       "      <td>0.806037</td>\n",
       "      <td>1.366535</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.422842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0069</th>\n",
       "      <td>0.348975</td>\n",
       "      <td>0.462294</td>\n",
       "      <td>1.085377</td>\n",
       "      <td>0.647411</td>\n",
       "      <td>-1.216716</td>\n",
       "      <td>-1.487006</td>\n",
       "      <td>-0.768889</td>\n",
       "      <td>0.030915</td>\n",
       "      <td>-0.054809</td>\n",
       "      <td>-0.638935</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.164933</td>\n",
       "      <td>0.779073</td>\n",
       "      <td>1.085613</td>\n",
       "      <td>0.088204</td>\n",
       "      <td>0.281165</td>\n",
       "      <td>-0.110367</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.438022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0071</th>\n",
       "      <td>1.644933</td>\n",
       "      <td>0.098840</td>\n",
       "      <td>0.919072</td>\n",
       "      <td>1.045445</td>\n",
       "      <td>1.664938</td>\n",
       "      <td>-0.048870</td>\n",
       "      <td>0.641201</td>\n",
       "      <td>1.740010</td>\n",
       "      <td>1.594438</td>\n",
       "      <td>1.463288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622412</td>\n",
       "      <td>0.664615</td>\n",
       "      <td>0.796809</td>\n",
       "      <td>1.809818</td>\n",
       "      <td>-0.077944</td>\n",
       "      <td>0.215619</td>\n",
       "      <td>0.934529</td>\n",
       "      <td>0.536589</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.677777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0075</th>\n",
       "      <td>-0.655847</td>\n",
       "      <td>0.101901</td>\n",
       "      <td>-0.216873</td>\n",
       "      <td>-1.024741</td>\n",
       "      <td>-0.981342</td>\n",
       "      <td>0.315110</td>\n",
       "      <td>-0.255794</td>\n",
       "      <td>-0.916104</td>\n",
       "      <td>-0.796130</td>\n",
       "      <td>-1.089311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355271</td>\n",
       "      <td>0.800728</td>\n",
       "      <td>1.039417</td>\n",
       "      <td>-0.704558</td>\n",
       "      <td>-1.319216</td>\n",
       "      <td>-0.153142</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-1.114976</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.983068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0076</th>\n",
       "      <td>-0.310756</td>\n",
       "      <td>0.693401</td>\n",
       "      <td>1.007125</td>\n",
       "      <td>0.964734</td>\n",
       "      <td>-0.202587</td>\n",
       "      <td>-1.844818</td>\n",
       "      <td>-0.896235</td>\n",
       "      <td>-0.697784</td>\n",
       "      <td>0.181026</td>\n",
       "      <td>0.569351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868714</td>\n",
       "      <td>0.862941</td>\n",
       "      <td>0.958617</td>\n",
       "      <td>-0.126775</td>\n",
       "      <td>-1.118319</td>\n",
       "      <td>0.262785</td>\n",
       "      <td>0.916767</td>\n",
       "      <td>0.052875</td>\n",
       "      <td>2.609635</td>\n",
       "      <td>0.057696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0081</th>\n",
       "      <td>0.291229</td>\n",
       "      <td>0.286913</td>\n",
       "      <td>0.724079</td>\n",
       "      <td>0.436671</td>\n",
       "      <td>0.206326</td>\n",
       "      <td>-0.243124</td>\n",
       "      <td>0.625176</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.202261</td>\n",
       "      <td>0.767037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673424</td>\n",
       "      <td>0.630064</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.061468</td>\n",
       "      <td>-1.003477</td>\n",
       "      <td>0.538467</td>\n",
       "      <td>1.011856</td>\n",
       "      <td>0.731500</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.429805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0082</th>\n",
       "      <td>-0.370661</td>\n",
       "      <td>0.739036</td>\n",
       "      <td>1.271797</td>\n",
       "      <td>0.726100</td>\n",
       "      <td>0.766660</td>\n",
       "      <td>-1.076300</td>\n",
       "      <td>-0.679681</td>\n",
       "      <td>1.173660</td>\n",
       "      <td>0.952265</td>\n",
       "      <td>0.059237</td>\n",
       "      <td>...</td>\n",
       "      <td>1.384673</td>\n",
       "      <td>0.901840</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.644238</td>\n",
       "      <td>-0.047370</td>\n",
       "      <td>-0.184204</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>1.046076</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.261577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0083</th>\n",
       "      <td>0.388913</td>\n",
       "      <td>0.232071</td>\n",
       "      <td>1.041605</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.923106</td>\n",
       "      <td>-1.479170</td>\n",
       "      <td>-0.725972</td>\n",
       "      <td>0.561737</td>\n",
       "      <td>1.005341</td>\n",
       "      <td>0.631400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753887</td>\n",
       "      <td>0.656740</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.344238</td>\n",
       "      <td>-0.614887</td>\n",
       "      <td>0.347417</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.130942</td>\n",
       "      <td>3.000866</td>\n",
       "      <td>0.672039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0031</th>\n",
       "      <td>-0.173575</td>\n",
       "      <td>0.559403</td>\n",
       "      <td>1.404774</td>\n",
       "      <td>0.862172</td>\n",
       "      <td>-0.182821</td>\n",
       "      <td>-0.912828</td>\n",
       "      <td>-1.302345</td>\n",
       "      <td>1.019318</td>\n",
       "      <td>0.738750</td>\n",
       "      <td>-0.484735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.864565</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>1.135026</td>\n",
       "      <td>-0.832697</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.263231</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.020141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0032</th>\n",
       "      <td>1.363014</td>\n",
       "      <td>0.852724</td>\n",
       "      <td>1.620287</td>\n",
       "      <td>1.712404</td>\n",
       "      <td>1.276279</td>\n",
       "      <td>0.174185</td>\n",
       "      <td>0.437325</td>\n",
       "      <td>1.204032</td>\n",
       "      <td>1.578419</td>\n",
       "      <td>1.610603</td>\n",
       "      <td>...</td>\n",
       "      <td>1.564273</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.916243</td>\n",
       "      <td>1.594664</td>\n",
       "      <td>-1.017494</td>\n",
       "      <td>0.866281</td>\n",
       "      <td>1.040314</td>\n",
       "      <td>0.346796</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.727455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0033</th>\n",
       "      <td>0.004265</td>\n",
       "      <td>0.846777</td>\n",
       "      <td>1.335345</td>\n",
       "      <td>0.400130</td>\n",
       "      <td>0.469740</td>\n",
       "      <td>-0.250658</td>\n",
       "      <td>-0.528948</td>\n",
       "      <td>0.849011</td>\n",
       "      <td>0.816911</td>\n",
       "      <td>0.121975</td>\n",
       "      <td>...</td>\n",
       "      <td>1.370601</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.024995</td>\n",
       "      <td>0.610686</td>\n",
       "      <td>0.834647</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.510835</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.217325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0034</th>\n",
       "      <td>1.169427</td>\n",
       "      <td>0.556844</td>\n",
       "      <td>0.808415</td>\n",
       "      <td>0.667631</td>\n",
       "      <td>0.581447</td>\n",
       "      <td>0.060103</td>\n",
       "      <td>0.540228</td>\n",
       "      <td>0.516714</td>\n",
       "      <td>0.714247</td>\n",
       "      <td>0.972676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120977</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.938123</td>\n",
       "      <td>-0.844161</td>\n",
       "      <td>0.578631</td>\n",
       "      <td>0.970715</td>\n",
       "      <td>0.564428</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.950002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0035</th>\n",
       "      <td>-0.360567</td>\n",
       "      <td>0.846949</td>\n",
       "      <td>0.979596</td>\n",
       "      <td>0.268736</td>\n",
       "      <td>0.448585</td>\n",
       "      <td>-0.310850</td>\n",
       "      <td>-0.422807</td>\n",
       "      <td>-0.655380</td>\n",
       "      <td>0.036763</td>\n",
       "      <td>-0.277446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262686</td>\n",
       "      <td>0.833449</td>\n",
       "      <td>1.131501</td>\n",
       "      <td>0.188816</td>\n",
       "      <td>-1.644843</td>\n",
       "      <td>0.195078</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.131956</td>\n",
       "      <td>3.609404</td>\n",
       "      <td>1.375492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0036</th>\n",
       "      <td>-0.064681</td>\n",
       "      <td>0.223731</td>\n",
       "      <td>1.326424</td>\n",
       "      <td>1.318422</td>\n",
       "      <td>0.348091</td>\n",
       "      <td>-0.331799</td>\n",
       "      <td>-1.112680</td>\n",
       "      <td>0.753750</td>\n",
       "      <td>0.474937</td>\n",
       "      <td>0.662958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854944</td>\n",
       "      <td>0.999951</td>\n",
       "      <td>1.463038</td>\n",
       "      <td>1.332991</td>\n",
       "      <td>-0.539399</td>\n",
       "      <td>0.473270</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.443274</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.807493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0037</th>\n",
       "      <td>-0.031287</td>\n",
       "      <td>-0.020087</td>\n",
       "      <td>1.275895</td>\n",
       "      <td>0.788813</td>\n",
       "      <td>0.389590</td>\n",
       "      <td>-2.367784</td>\n",
       "      <td>-1.431053</td>\n",
       "      <td>0.742751</td>\n",
       "      <td>0.413208</td>\n",
       "      <td>0.224323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156028</td>\n",
       "      <td>0.870393</td>\n",
       "      <td>0.893215</td>\n",
       "      <td>0.189604</td>\n",
       "      <td>-0.610693</td>\n",
       "      <td>-0.209467</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.945151</td>\n",
       "      <td>2.847851</td>\n",
       "      <td>-0.032311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0038</th>\n",
       "      <td>0.127102</td>\n",
       "      <td>0.735821</td>\n",
       "      <td>0.962326</td>\n",
       "      <td>1.252127</td>\n",
       "      <td>1.027538</td>\n",
       "      <td>-1.468612</td>\n",
       "      <td>-0.540858</td>\n",
       "      <td>1.084620</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>1.476899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902514</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>1.325798</td>\n",
       "      <td>0.474221</td>\n",
       "      <td>0.154646</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.909238</td>\n",
       "      <td>3.778292</td>\n",
       "      <td>1.619436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0039</th>\n",
       "      <td>0.076393</td>\n",
       "      <td>0.369525</td>\n",
       "      <td>0.262198</td>\n",
       "      <td>-0.434926</td>\n",
       "      <td>-0.641064</td>\n",
       "      <td>-0.367527</td>\n",
       "      <td>-0.258453</td>\n",
       "      <td>-0.416765</td>\n",
       "      <td>-0.246040</td>\n",
       "      <td>-0.748481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543181</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.039146</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>-1.086837</td>\n",
       "      <td>-0.351680</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.517124</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.052990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0050</th>\n",
       "      <td>1.048922</td>\n",
       "      <td>0.577184</td>\n",
       "      <td>1.692505</td>\n",
       "      <td>1.645615</td>\n",
       "      <td>0.750238</td>\n",
       "      <td>-0.066897</td>\n",
       "      <td>-0.130413</td>\n",
       "      <td>1.106432</td>\n",
       "      <td>1.015931</td>\n",
       "      <td>1.764417</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333553</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.216004</td>\n",
       "      <td>0.990117</td>\n",
       "      <td>-0.753045</td>\n",
       "      <td>0.618545</td>\n",
       "      <td>1.050514</td>\n",
       "      <td>0.563625</td>\n",
       "      <td>2.764119</td>\n",
       "      <td>1.325121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0053</th>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>1.198284</td>\n",
       "      <td>0.221591</td>\n",
       "      <td>0.145348</td>\n",
       "      <td>-0.039695</td>\n",
       "      <td>0.745301</td>\n",
       "      <td>0.947575</td>\n",
       "      <td>0.749415</td>\n",
       "      <td>-0.464770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274710</td>\n",
       "      <td>0.532431</td>\n",
       "      <td>1.088167</td>\n",
       "      <td>0.843921</td>\n",
       "      <td>1.440228</td>\n",
       "      <td>0.082751</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>1.027156</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.287373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0070</th>\n",
       "      <td>-1.309004</td>\n",
       "      <td>0.213414</td>\n",
       "      <td>-0.418802</td>\n",
       "      <td>-1.097679</td>\n",
       "      <td>-1.892068</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>-1.458774</td>\n",
       "      <td>-2.380129</td>\n",
       "      <td>-1.965949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788725</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-2.301812</td>\n",
       "      <td>-1.832232</td>\n",
       "      <td>-0.425255</td>\n",
       "      <td>0.830336</td>\n",
       "      <td>-1.228546</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.297272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0077</th>\n",
       "      <td>0.308685</td>\n",
       "      <td>0.423766</td>\n",
       "      <td>1.251951</td>\n",
       "      <td>1.631973</td>\n",
       "      <td>1.143059</td>\n",
       "      <td>-1.097219</td>\n",
       "      <td>0.168213</td>\n",
       "      <td>0.915839</td>\n",
       "      <td>1.162923</td>\n",
       "      <td>1.594137</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098347</td>\n",
       "      <td>0.702783</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.351349</td>\n",
       "      <td>0.462246</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>1.047099</td>\n",
       "      <td>0.768713</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.443454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0087</th>\n",
       "      <td>0.262581</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.801761</td>\n",
       "      <td>0.796105</td>\n",
       "      <td>0.392620</td>\n",
       "      <td>-0.952609</td>\n",
       "      <td>0.115493</td>\n",
       "      <td>0.295912</td>\n",
       "      <td>0.798726</td>\n",
       "      <td>0.749829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412925</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.797906</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>-1.457010</td>\n",
       "      <td>0.273234</td>\n",
       "      <td>0.680197</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.079904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002 -1.238669 -0.489755 -1.404819 -0.661768 -1.214104 -0.438326 -0.542205   \n",
       "C_0003 -0.094326  0.128294 -0.400562 -0.392906  0.908731  0.454217 -0.792399   \n",
       "C_0004 -0.513695  0.528638 -0.372793  0.456696 -0.126533 -0.054497 -0.268084   \n",
       "C_0005 -0.452707  0.211724  0.002274  0.105692  0.088081 -0.109452  0.208944   \n",
       "C_0006 -0.444591  0.344861 -0.730082  0.518467 -0.178900 -0.660072 -0.733352   \n",
       "C_0008 -0.063484  0.604704 -0.010074  0.157058  0.784126  0.523687  0.705062   \n",
       "C_0009 -1.014420  0.027920  0.485191  1.511839  2.342554 -0.059718 -0.901242   \n",
       "C_0010 -1.071431  0.484398 -0.933368 -0.658817 -0.998986 -0.943378 -1.362364   \n",
       "C_0011 -0.921800  0.182561 -2.304816 -1.383902 -1.199928 -0.621190 -1.538195   \n",
       "C_0012  0.370489  0.568469  0.573826  1.847973  2.247641 -1.047992 -1.840582   \n",
       "C_0013  0.237404  0.468118 -0.328469  0.401016  0.664377 -0.110870  1.175788   \n",
       "C_0014 -0.043427  0.164883 -0.241251 -0.231408 -0.153178  0.290115 -0.123583   \n",
       "C_0015 -0.159249  0.653677  0.067769  0.653778 -0.451082 -1.435228 -1.653990   \n",
       "C_0016 -0.313939  0.505943 -0.474605  0.395045 -0.454325  0.768875  1.061524   \n",
       "C_0017  0.617328 -2.842400 -0.042863  0.876221  0.948050 -0.556515 -0.586851   \n",
       "C_0018 -0.841823 -2.842400  0.611775  2.342445  2.457968 -1.436767 -1.273134   \n",
       "C_0019 -0.536535  0.282387 -0.669082 -0.261382 -0.587647 -0.113586  0.768471   \n",
       "C_0020 -1.286744  0.683392 -0.230312  0.520323 -0.197100 -1.564600 -1.281168   \n",
       "C_0021  0.526197 -2.842400 -0.390967  0.445794  0.184534 -1.203421 -1.145355   \n",
       "C_0022 -1.362488  0.307889 -0.984365 -1.156695 -1.288870 -0.259354 -0.329153   \n",
       "C_0023 -1.226360 -0.380853 -2.068576 -1.459295 -2.173385 -1.454016 -1.745151   \n",
       "C_0024  0.218499  0.742546 -0.366158  0.410088 -0.554448 -0.343820 -0.535454   \n",
       "C_0025 -0.525737  0.242389  0.106966  0.424007  0.286912 -0.878035 -1.127625   \n",
       "C_0026 -1.102176  0.412127 -1.320566 -0.620084 -1.046110 -0.585962 -0.764696   \n",
       "C_0029 -1.437548 -0.297698 -1.784748 -1.028572 -1.752626  0.407274  0.008234   \n",
       "C_0060  1.016182  0.544289 -0.468397 -0.756485 -0.454883 -0.032562  0.310191   \n",
       "C_0061 -0.473287  0.056637  0.117051 -0.359745  0.056609 -0.557902 -0.201168   \n",
       "C_0062  0.745442  0.616147  1.085803  0.833815  0.022261 -0.484330  0.229278   \n",
       "C_0065  4.464034  0.193050  1.630309  1.204508  1.248069 -0.647544  0.895345   \n",
       "C_0069  0.348975  0.462294  1.085377  0.647411 -1.216716 -1.487006 -0.768889   \n",
       "C_0071  1.644933  0.098840  0.919072  1.045445  1.664938 -0.048870  0.641201   \n",
       "C_0075 -0.655847  0.101901 -0.216873 -1.024741 -0.981342  0.315110 -0.255794   \n",
       "C_0076 -0.310756  0.693401  1.007125  0.964734 -0.202587 -1.844818 -0.896235   \n",
       "C_0081  0.291229  0.286913  0.724079  0.436671  0.206326 -0.243124  0.625176   \n",
       "C_0082 -0.370661  0.739036  1.271797  0.726100  0.766660 -1.076300 -0.679681   \n",
       "C_0083  0.388913  0.232071  1.041605  0.902932  0.923106 -1.479170 -0.725972   \n",
       "C_0031 -0.173575  0.559403  1.404774  0.862172 -0.182821 -0.912828 -1.302345   \n",
       "C_0032  1.363014  0.852724  1.620287  1.712404  1.276279  0.174185  0.437325   \n",
       "C_0033  0.004265  0.846777  1.335345  0.400130  0.469740 -0.250658 -0.528948   \n",
       "C_0034  1.169427  0.556844  0.808415  0.667631  0.581447  0.060103  0.540228   \n",
       "C_0035 -0.360567  0.846949  0.979596  0.268736  0.448585 -0.310850 -0.422807   \n",
       "C_0036 -0.064681  0.223731  1.326424  1.318422  0.348091 -0.331799 -1.112680   \n",
       "C_0037 -0.031287 -0.020087  1.275895  0.788813  0.389590 -2.367784 -1.431053   \n",
       "C_0038  0.127102  0.735821  0.962326  1.252127  1.027538 -1.468612 -0.540858   \n",
       "C_0039  0.076393  0.369525  0.262198 -0.434926 -0.641064 -0.367527 -0.258453   \n",
       "C_0050  1.048922  0.577184  1.692505  1.645615  0.750238 -0.066897 -0.130413   \n",
       "C_0053  0.433121  0.560606  1.198284  0.221591  0.145348 -0.039695  0.745301   \n",
       "C_0070 -1.309004  0.213414 -0.418802 -1.097679 -1.892068  0.063291  0.524590   \n",
       "C_0077  0.308685  0.423766  1.251951  1.631973  1.143059 -1.097219  0.168213   \n",
       "C_0087  0.262581  0.557800  0.801761  0.796105  0.392620 -0.952609  0.115493   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002 -1.549951 -1.144974 -1.361823  ... -0.794664 -1.270403  0.737597   \n",
       "C_0003  0.733360  0.611638  0.953743  ...  0.767006  0.814752 -1.029937   \n",
       "C_0004 -0.347413 -0.285398 -0.322790  ...  0.051075  0.649930  0.776952   \n",
       "C_0005  0.185074  0.533692  0.666086  ...  0.884624  0.681302 -1.029937   \n",
       "C_0006 -0.164000  0.323106  0.563685  ... -1.256904 -1.270403 -1.029937   \n",
       "C_0008  0.107073  0.650657  0.591910  ...  0.626894  0.690349 -1.029937   \n",
       "C_0009  2.952803  3.037081  1.972919  ...  0.056384  1.073685 -1.029937   \n",
       "C_0010 -0.970939 -0.853908 -0.605820  ... -0.555258 -1.270403  0.785608   \n",
       "C_0011 -1.155816 -0.558486 -0.865617  ... -1.144206 -1.270403 -1.029937   \n",
       "C_0012  2.485153  2.363211  2.430353  ...  1.381163  0.671095  0.987434   \n",
       "C_0013  0.527173  0.230430  0.598487  ...  0.245687  0.782167  0.773910   \n",
       "C_0014 -0.380659 -0.117571 -0.196329  ...  0.203878  0.699708  1.132509   \n",
       "C_0015 -0.421961 -0.142727  0.361802  ...  1.742628  0.581885  0.973181   \n",
       "C_0016 -0.027705 -0.504156  0.133814  ...  0.959270  0.802266 -1.029937   \n",
       "C_0017  1.194970  1.516010  1.081449  ...  0.927239 -1.270403  1.039801   \n",
       "C_0018  1.043109  0.310334  2.145601  ...  2.039750 -1.270403  0.883744   \n",
       "C_0019 -0.019680 -0.749318  0.085651  ...  0.332958  0.683476  0.928697   \n",
       "C_0020 -0.488723 -0.260558  0.360519  ...  1.246719  0.644466 -1.029937   \n",
       "C_0021  0.687426  0.684889  0.804522  ...  0.987853  0.836414  1.029127   \n",
       "C_0022 -1.661139 -1.751085 -1.280175  ... -0.299670  0.732401  0.706613   \n",
       "C_0023 -1.424173 -1.924884 -1.365756  ... -1.136616  0.678694  0.997710   \n",
       "C_0024 -0.260692 -0.020393  0.194022  ...  1.626982 -1.270403 -1.029937   \n",
       "C_0025  0.521370  0.787054  0.509760  ... -0.186735  0.740892 -1.029937   \n",
       "C_0026 -0.931632 -0.883488 -0.745137  ... -1.254419  0.617929 -1.029937   \n",
       "C_0029 -0.784135 -1.550076 -1.090772  ... -1.328883  0.857367 -1.029937   \n",
       "C_0060 -1.481712 -0.330176 -1.178254  ... -0.717043  0.766825 -1.029937   \n",
       "C_0061 -0.439586  0.132665 -0.769386  ... -0.141075  0.875788  1.023826   \n",
       "C_0062  0.747268  0.973920  0.660299  ...  0.880805  0.883678  0.986659   \n",
       "C_0065  2.280033  2.542494  1.493681  ...  0.648056  1.039465  1.362254   \n",
       "C_0069  0.030915 -0.054809 -0.638935  ... -1.164933  0.779073  1.085613   \n",
       "C_0071  1.740010  1.594438  1.463288  ...  0.622412  0.664615  0.796809   \n",
       "C_0075 -0.916104 -0.796130 -1.089311  ... -0.355271  0.800728  1.039417   \n",
       "C_0076 -0.697784  0.181026  0.569351  ...  0.868714  0.862941  0.958617   \n",
       "C_0081  0.001589  0.202261  0.767037  ...  0.673424  0.630064 -1.029937   \n",
       "C_0082  1.173660  0.952265  0.059237  ...  1.384673  0.901840 -1.029937   \n",
       "C_0083  0.561737  1.005341  0.631400  ...  0.753887  0.656740 -1.029937   \n",
       "C_0031  1.019318  0.738750 -0.484735  ... -0.864565 -1.270403 -1.029937   \n",
       "C_0032  1.204032  1.578419  1.610603  ...  1.564273 -1.270403  0.916243   \n",
       "C_0033  0.849011  0.816911  0.121975  ...  1.370601 -1.270403  1.024995   \n",
       "C_0034  0.516714  0.714247  0.972676  ... -0.120977 -1.270403 -1.029937   \n",
       "C_0035 -0.655380  0.036763 -0.277446  ...  0.262686  0.833449  1.131501   \n",
       "C_0036  0.753750  0.474937  0.662958  ...  0.854944  0.999951  1.463038   \n",
       "C_0037  0.742751  0.413208  0.224323  ...  0.156028  0.870393  0.893215   \n",
       "C_0038  1.084620  0.898749  1.476899  ...  0.902514 -1.270403 -1.029937   \n",
       "C_0039 -0.416765 -0.246040 -0.748481  ...  0.543181 -1.270403  1.039146   \n",
       "C_0050  1.106432  1.015931  1.764417  ...  1.333553 -1.270403  1.216004   \n",
       "C_0053  0.947575  0.749415 -0.464770  ...  0.274710  0.532431  1.088167   \n",
       "C_0070 -1.458774 -2.380129 -1.965949  ... -0.788725 -1.270403 -1.029937   \n",
       "C_0077  0.915839  1.162923  1.594137  ...  1.098347  0.702783 -1.029937   \n",
       "C_0087  0.295912  0.798726  0.749829  ...  0.412925 -1.270403  0.797906   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002 -1.819190 -0.178338  0.264969  0.721004 -0.094753 -0.349492 -0.142859  \n",
       "C_0003  0.719449  0.054000  0.220818  0.859050  0.298774 -0.349492 -0.149023  \n",
       "C_0004 -0.466111 -0.596202  0.287203  1.144874  0.595544 -0.349492  0.728854  \n",
       "C_0005 -0.455983  0.518626  0.463286  0.793437 -0.182278 -0.349492  0.198520  \n",
       "C_0006  0.190454 -1.167309  0.463277  0.996881  0.243467 -0.349492 -0.262606  \n",
       "C_0008 -0.004865  0.357423  0.598058  1.032206 -0.156499 -0.349492 -0.711918  \n",
       "C_0009  2.372708  1.781837 -4.757775 -1.220236  1.391432 -0.349492 -3.818267  \n",
       "C_0010 -0.303165  0.070641  0.277815  0.669304  0.071529 -0.349492 -0.462936  \n",
       "C_0011 -1.399549 -1.752028 -0.121130  0.774877  0.128418 -0.349492 -0.916628  \n",
       "C_0012  1.425060  0.034088  0.766789  0.848066  0.465206 -0.349492  0.968155  \n",
       "C_0013  0.154829  0.823008  0.794247  0.658943  0.046881 -0.349492 -0.089385  \n",
       "C_0014 -0.447637 -0.504793  0.158031  0.882336  0.306069 -0.349492  0.752933  \n",
       "C_0015 -0.168088 -0.960749  0.403107  1.090746 -0.082349 -0.349492  1.371974  \n",
       "C_0016 -0.934674 -0.521819  0.304097 -1.220236 -0.120013 -0.349492 -0.469263  \n",
       "C_0017  0.731721  0.243670  0.670608  0.733369  0.427240  2.775695 -0.883431  \n",
       "C_0018  2.903385 -0.352164  0.753175  0.917304 -0.170010 -0.349492  1.633091  \n",
       "C_0019 -1.103343 -0.071915  0.273526  0.796041 -0.234333 -0.349492  0.359326  \n",
       "C_0020 -0.094803 -0.970501  0.396902  0.910396  0.320131 -0.349492 -0.042961  \n",
       "C_0021  0.380531 -1.018371  0.330028  0.723915  0.062511 -0.349492 -0.227033  \n",
       "C_0022 -0.743866 -2.689368  0.031758  0.760415 -0.233864 -0.349492 -0.712032  \n",
       "C_0023 -0.693899 -2.348000 -0.091542 -1.220236 -0.313074 -0.349492 -0.879198  \n",
       "C_0024 -0.048342 -0.461180  0.103370  0.875968  0.238226 -0.349492 -0.705871  \n",
       "C_0025  0.404844 -0.157597  0.652556  1.144599  0.312478 -0.349492  0.480518  \n",
       "C_0026 -0.567240 -1.194215  0.164191 -1.220236 -0.037894 -0.349492  0.236674  \n",
       "C_0029 -1.160892 -0.607662 -0.239373 -1.220236  0.291046 -0.349492 -0.386633  \n",
       "C_0060 -0.787432 -1.118817 -0.086020 -1.220236 -0.220998 -0.349492 -0.530488  \n",
       "C_0061  0.292035  0.637192  0.554100  0.558140 -0.417092  2.371024 -1.178913  \n",
       "C_0062  0.537345  0.138341  0.284933 -1.220236  0.463572 -0.349492  0.086073  \n",
       "C_0065  2.228318  1.156093  0.712185  0.806037  1.366535 -0.349492  1.422842  \n",
       "C_0069  0.088204  0.281165 -0.110367 -1.220236  0.255357 -0.349492  1.438022  \n",
       "C_0071  1.809818 -0.077944  0.215619  0.934529  0.536589 -0.349492  0.677777  \n",
       "C_0075 -0.704558 -1.319216 -0.153142 -1.220236 -1.114976 -0.349492 -0.983068  \n",
       "C_0076 -0.126775 -1.118319  0.262785  0.916767  0.052875  2.609635  0.057696  \n",
       "C_0081 -0.061468 -1.003477  0.538467  1.011856  0.731500 -0.349492  0.429805  \n",
       "C_0082  0.644238 -0.047370 -0.184204 -1.220236  1.046076 -0.349492  0.261577  \n",
       "C_0083  0.344238 -0.614887  0.347417 -1.220236  0.130942  3.000866  0.672039  \n",
       "C_0031  1.135026 -0.832697 -4.757775 -1.220236 -0.263231 -0.349492  0.020141  \n",
       "C_0032  1.594664 -1.017494  0.866281  1.040314  0.346796 -0.349492  0.727455  \n",
       "C_0033  0.610686  0.834647 -4.757775 -1.220236  0.510835 -0.349492  1.217325  \n",
       "C_0034  0.938123 -0.844161  0.578631  0.970715  0.564428 -0.349492  0.950002  \n",
       "C_0035  0.188816 -1.644843  0.195078 -1.220236 -0.131956  3.609404  1.375492  \n",
       "C_0036  1.332991 -0.539399  0.473270 -1.220236  0.443274 -0.349492  1.807493  \n",
       "C_0037  0.189604 -0.610693 -0.209467 -1.220236  0.945151  2.847851 -0.032311  \n",
       "C_0038  1.325798  0.474221  0.154646 -1.220236  0.909238  3.778292  1.619436  \n",
       "C_0039  0.081579 -1.086837 -0.351680 -1.220236  0.517124 -0.349492 -1.052990  \n",
       "C_0050  0.990117 -0.753045  0.618545  1.050514  0.563625  2.764119  1.325121  \n",
       "C_0053  0.843921  1.440228  0.082751 -1.220236  1.027156 -0.349492  1.287373  \n",
       "C_0070 -2.301812 -1.832232 -0.425255  0.830336 -1.228546 -0.349492 -0.297272  \n",
       "C_0077  0.351349  0.462246  0.408542  1.047099  0.768713 -0.349492  0.443454  \n",
       "C_0087  0.189569 -1.457010  0.273234  0.680197  0.341713 -0.349492  1.079904  \n",
       "\n",
       "[50 rows x 21861 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_C=dataX_log_del2.filter(like='C', axis=0)\n",
    "dataX_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dc0ae04-8f34-4726-9bf6-0d68be2ee005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2241646/805336276.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataX_C['cluster']=1\n",
      "/tmp/ipykernel_2241646/805336276.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataX_P['cluster']=0\n",
      "/tmp/ipykernel_2241646/805336276.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataX_H['cluster']=2\n"
     ]
    }
   ],
   "source": [
    "dataX_C['cluster']=1\n",
    "dataX_P=dataX_log_del2.filter(like='P', axis=0)\n",
    "dataX_P['cluster']=0\n",
    "dataX_H=dataX_log_del2.filter(like='H',axis=0)\n",
    "dataX_H['cluster']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "931a7f25-37c2-4fbb-9fd3-ff2c5dca5e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-0.489755</td>\n",
       "      <td>-1.404819</td>\n",
       "      <td>-0.661768</td>\n",
       "      <td>-1.214104</td>\n",
       "      <td>-0.438326</td>\n",
       "      <td>-0.542205</td>\n",
       "      <td>-1.549951</td>\n",
       "      <td>-1.144974</td>\n",
       "      <td>-1.361823</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.737597</td>\n",
       "      <td>-1.819190</td>\n",
       "      <td>-0.178338</td>\n",
       "      <td>0.264969</td>\n",
       "      <td>0.721004</td>\n",
       "      <td>-0.094753</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.142859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>-0.094326</td>\n",
       "      <td>0.128294</td>\n",
       "      <td>-0.400562</td>\n",
       "      <td>-0.392906</td>\n",
       "      <td>0.908731</td>\n",
       "      <td>0.454217</td>\n",
       "      <td>-0.792399</td>\n",
       "      <td>0.733360</td>\n",
       "      <td>0.611638</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814752</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.719449</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.220818</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.298774</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.149023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>-0.513695</td>\n",
       "      <td>0.528638</td>\n",
       "      <td>-0.372793</td>\n",
       "      <td>0.456696</td>\n",
       "      <td>-0.126533</td>\n",
       "      <td>-0.054497</td>\n",
       "      <td>-0.268084</td>\n",
       "      <td>-0.347413</td>\n",
       "      <td>-0.285398</td>\n",
       "      <td>-0.322790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.776952</td>\n",
       "      <td>-0.466111</td>\n",
       "      <td>-0.596202</td>\n",
       "      <td>0.287203</td>\n",
       "      <td>1.144874</td>\n",
       "      <td>0.595544</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.728854</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>-0.452707</td>\n",
       "      <td>0.211724</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.088081</td>\n",
       "      <td>-0.109452</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.533692</td>\n",
       "      <td>0.666086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681302</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.455983</td>\n",
       "      <td>0.518626</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.793437</td>\n",
       "      <td>-0.182278</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.198520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>-0.444591</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>-0.730082</td>\n",
       "      <td>0.518467</td>\n",
       "      <td>-0.178900</td>\n",
       "      <td>-0.660072</td>\n",
       "      <td>-0.733352</td>\n",
       "      <td>-0.164000</td>\n",
       "      <td>0.323106</td>\n",
       "      <td>0.563685</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.190454</td>\n",
       "      <td>-1.167309</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.996881</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.262606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>-1.056665</td>\n",
       "      <td>-0.131702</td>\n",
       "      <td>-2.461487</td>\n",
       "      <td>-2.196218</td>\n",
       "      <td>-0.828361</td>\n",
       "      <td>1.475229</td>\n",
       "      <td>0.686531</td>\n",
       "      <td>-1.842048</td>\n",
       "      <td>-1.833416</td>\n",
       "      <td>-0.990698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676598</td>\n",
       "      <td>0.813014</td>\n",
       "      <td>-2.498809</td>\n",
       "      <td>-0.298672</td>\n",
       "      <td>0.243131</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.478093</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-2.423072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>-0.813836</td>\n",
       "      <td>0.110010</td>\n",
       "      <td>0.332046</td>\n",
       "      <td>-1.371978</td>\n",
       "      <td>-1.159991</td>\n",
       "      <td>-0.961007</td>\n",
       "      <td>-0.240771</td>\n",
       "      <td>-0.532297</td>\n",
       "      <td>-1.179250</td>\n",
       "      <td>-1.522107</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.059062</td>\n",
       "      <td>0.190509</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-4.368677</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.082281</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>0.740941</td>\n",
       "      <td>0.692626</td>\n",
       "      <td>0.985739</td>\n",
       "      <td>-0.153001</td>\n",
       "      <td>-0.268487</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>0.639643</td>\n",
       "      <td>-0.215969</td>\n",
       "      <td>-0.145524</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071325</td>\n",
       "      <td>0.983066</td>\n",
       "      <td>1.379293</td>\n",
       "      <td>1.249711</td>\n",
       "      <td>-0.185067</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.343931</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.869277</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>-0.448897</td>\n",
       "      <td>0.149684</td>\n",
       "      <td>0.344353</td>\n",
       "      <td>0.107790</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>0.409116</td>\n",
       "      <td>0.303394</td>\n",
       "      <td>-0.866286</td>\n",
       "      <td>-0.533084</td>\n",
       "      <td>-0.141975</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.437144</td>\n",
       "      <td>0.149815</td>\n",
       "      <td>0.178247</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>-0.537081</td>\n",
       "      <td>0.415169</td>\n",
       "      <td>1.062265</td>\n",
       "      <td>0.954178</td>\n",
       "      <td>-0.012975</td>\n",
       "      <td>1.303455</td>\n",
       "      <td>-0.598432</td>\n",
       "      <td>0.868221</td>\n",
       "      <td>-0.181386</td>\n",
       "      <td>-0.306728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832702</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>1.372934</td>\n",
       "      <td>0.944623</td>\n",
       "      <td>-0.604752</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.508965</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.514668</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21862 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "C_0002 -1.238669 -0.489755 -1.404819 -0.661768 -1.214104 -0.438326 -0.542205   \n",
       "C_0003 -0.094326  0.128294 -0.400562 -0.392906  0.908731  0.454217 -0.792399   \n",
       "C_0004 -0.513695  0.528638 -0.372793  0.456696 -0.126533 -0.054497 -0.268084   \n",
       "C_0005 -0.452707  0.211724  0.002274  0.105692  0.088081 -0.109452  0.208944   \n",
       "C_0006 -0.444591  0.344861 -0.730082  0.518467 -0.178900 -0.660072 -0.733352   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689 -1.056665 -0.131702 -2.461487 -2.196218 -0.828361  1.475229  0.686531   \n",
       "H_0709 -0.813836  0.110010  0.332046 -1.371978 -1.159991 -0.961007 -0.240771   \n",
       "H_0723  0.740941  0.692626  0.985739 -0.153001 -0.268487  0.037081  0.639643   \n",
       "H_1104 -0.448897  0.149684  0.344353  0.107790  0.044293  0.409116  0.303394   \n",
       "H_1105 -0.537081  0.415169  1.062265  0.954178 -0.012975  1.303455 -0.598432   \n",
       "\n",
       "               7         8         9  ...     21854     21855     21856  \\\n",
       "C_0002 -1.549951 -1.144974 -1.361823  ... -1.270403  0.737597 -1.819190   \n",
       "C_0003  0.733360  0.611638  0.953743  ...  0.814752 -1.029937  0.719449   \n",
       "C_0004 -0.347413 -0.285398 -0.322790  ...  0.649930  0.776952 -0.466111   \n",
       "C_0005  0.185074  0.533692  0.666086  ...  0.681302 -1.029937 -0.455983   \n",
       "C_0006 -0.164000  0.323106  0.563685  ... -1.270403 -1.029937  0.190454   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689 -1.842048 -1.833416 -0.990698  ...  0.676598  0.813014 -2.498809   \n",
       "H_0709 -0.532297 -1.179250 -1.522107  ... -1.270403 -1.029937  0.059062   \n",
       "H_0723 -0.215969 -0.145524  0.047562  ...  1.071325  0.983066  1.379293   \n",
       "H_1104 -0.866286 -0.533084 -0.141975  ... -1.270403 -1.029937 -0.437144   \n",
       "H_1105  0.868221 -0.181386 -0.306728  ...  0.832702  1.024108  1.372934   \n",
       "\n",
       "           21857     21858     21859     21860     21861     21862  cluster  \n",
       "C_0002 -0.178338  0.264969  0.721004 -0.094753 -0.349492 -0.142859        1  \n",
       "C_0003  0.054000  0.220818  0.859050  0.298774 -0.349492 -0.149023        1  \n",
       "C_0004 -0.596202  0.287203  1.144874  0.595544 -0.349492  0.728854        1  \n",
       "C_0005  0.518626  0.463286  0.793437 -0.182278 -0.349492  0.198520        1  \n",
       "C_0006 -1.167309  0.463277  0.996881  0.243467 -0.349492 -0.262606        1  \n",
       "...          ...       ...       ...       ...       ...       ...      ...  \n",
       "H_0689 -0.298672  0.243131 -1.220236 -0.478093 -0.349492 -2.423072        2  \n",
       "H_0709  0.190509 -4.757775 -1.220236 -4.368677 -0.349492  1.082281        2  \n",
       "H_0723  1.249711 -0.185067 -1.220236 -0.343931 -0.349492  0.869277        2  \n",
       "H_1104  0.149815  0.178247 -1.220236  0.106997 -0.349492  0.552147        2  \n",
       "H_1105  0.944623 -0.604752 -1.220236  0.508965 -0.349492 -0.514668        2  \n",
       "\n",
       "[108 rows x 21862 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_cluster=pd.concat([dataX_C, dataX_P, dataX_H], ignore_index=False)\n",
    "dataX_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9b74899-a683-4f54-bb64-e68e5d8a490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0002</th>\n",
       "      <td>-1.238669</td>\n",
       "      <td>-0.489755</td>\n",
       "      <td>-1.404819</td>\n",
       "      <td>-0.661768</td>\n",
       "      <td>-1.214104</td>\n",
       "      <td>-0.438326</td>\n",
       "      <td>-0.542205</td>\n",
       "      <td>-1.549951</td>\n",
       "      <td>-1.144974</td>\n",
       "      <td>-1.361823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.737597</td>\n",
       "      <td>-1.819190</td>\n",
       "      <td>-0.178338</td>\n",
       "      <td>0.264969</td>\n",
       "      <td>0.721004</td>\n",
       "      <td>-0.094753</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.142859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0003</th>\n",
       "      <td>-0.094326</td>\n",
       "      <td>0.128294</td>\n",
       "      <td>-0.400562</td>\n",
       "      <td>-0.392906</td>\n",
       "      <td>0.908731</td>\n",
       "      <td>0.454217</td>\n",
       "      <td>-0.792399</td>\n",
       "      <td>0.733360</td>\n",
       "      <td>0.611638</td>\n",
       "      <td>0.953743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767006</td>\n",
       "      <td>0.814752</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.719449</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.220818</td>\n",
       "      <td>0.859050</td>\n",
       "      <td>0.298774</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.149023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0004</th>\n",
       "      <td>-0.513695</td>\n",
       "      <td>0.528638</td>\n",
       "      <td>-0.372793</td>\n",
       "      <td>0.456696</td>\n",
       "      <td>-0.126533</td>\n",
       "      <td>-0.054497</td>\n",
       "      <td>-0.268084</td>\n",
       "      <td>-0.347413</td>\n",
       "      <td>-0.285398</td>\n",
       "      <td>-0.322790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051075</td>\n",
       "      <td>0.649930</td>\n",
       "      <td>0.776952</td>\n",
       "      <td>-0.466111</td>\n",
       "      <td>-0.596202</td>\n",
       "      <td>0.287203</td>\n",
       "      <td>1.144874</td>\n",
       "      <td>0.595544</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.728854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0005</th>\n",
       "      <td>-0.452707</td>\n",
       "      <td>0.211724</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.105692</td>\n",
       "      <td>0.088081</td>\n",
       "      <td>-0.109452</td>\n",
       "      <td>0.208944</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>0.533692</td>\n",
       "      <td>0.666086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884624</td>\n",
       "      <td>0.681302</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.455983</td>\n",
       "      <td>0.518626</td>\n",
       "      <td>0.463286</td>\n",
       "      <td>0.793437</td>\n",
       "      <td>-0.182278</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.198520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0006</th>\n",
       "      <td>-0.444591</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>-0.730082</td>\n",
       "      <td>0.518467</td>\n",
       "      <td>-0.178900</td>\n",
       "      <td>-0.660072</td>\n",
       "      <td>-0.733352</td>\n",
       "      <td>-0.164000</td>\n",
       "      <td>0.323106</td>\n",
       "      <td>0.563685</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.256904</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.190454</td>\n",
       "      <td>-1.167309</td>\n",
       "      <td>0.463277</td>\n",
       "      <td>0.996881</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.262606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0689</th>\n",
       "      <td>-1.056665</td>\n",
       "      <td>-0.131702</td>\n",
       "      <td>-2.461487</td>\n",
       "      <td>-2.196218</td>\n",
       "      <td>-0.828361</td>\n",
       "      <td>1.475229</td>\n",
       "      <td>0.686531</td>\n",
       "      <td>-1.842048</td>\n",
       "      <td>-1.833416</td>\n",
       "      <td>-0.990698</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.765663</td>\n",
       "      <td>0.676598</td>\n",
       "      <td>0.813014</td>\n",
       "      <td>-2.498809</td>\n",
       "      <td>-0.298672</td>\n",
       "      <td>0.243131</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.478093</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-2.423072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0709</th>\n",
       "      <td>-0.813836</td>\n",
       "      <td>0.110010</td>\n",
       "      <td>0.332046</td>\n",
       "      <td>-1.371978</td>\n",
       "      <td>-1.159991</td>\n",
       "      <td>-0.961007</td>\n",
       "      <td>-0.240771</td>\n",
       "      <td>-0.532297</td>\n",
       "      <td>-1.179250</td>\n",
       "      <td>-1.522107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.899592</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.059062</td>\n",
       "      <td>0.190509</td>\n",
       "      <td>-4.757775</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-4.368677</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.082281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0723</th>\n",
       "      <td>0.740941</td>\n",
       "      <td>0.692626</td>\n",
       "      <td>0.985739</td>\n",
       "      <td>-0.153001</td>\n",
       "      <td>-0.268487</td>\n",
       "      <td>0.037081</td>\n",
       "      <td>0.639643</td>\n",
       "      <td>-0.215969</td>\n",
       "      <td>-0.145524</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332414</td>\n",
       "      <td>1.071325</td>\n",
       "      <td>0.983066</td>\n",
       "      <td>1.379293</td>\n",
       "      <td>1.249711</td>\n",
       "      <td>-0.185067</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.343931</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.869277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1104</th>\n",
       "      <td>-0.448897</td>\n",
       "      <td>0.149684</td>\n",
       "      <td>0.344353</td>\n",
       "      <td>0.107790</td>\n",
       "      <td>0.044293</td>\n",
       "      <td>0.409116</td>\n",
       "      <td>0.303394</td>\n",
       "      <td>-0.866286</td>\n",
       "      <td>-0.533084</td>\n",
       "      <td>-0.141975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274111</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.437144</td>\n",
       "      <td>0.149815</td>\n",
       "      <td>0.178247</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.106997</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.552147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_1105</th>\n",
       "      <td>-0.537081</td>\n",
       "      <td>0.415169</td>\n",
       "      <td>1.062265</td>\n",
       "      <td>0.954178</td>\n",
       "      <td>-0.012975</td>\n",
       "      <td>1.303455</td>\n",
       "      <td>-0.598432</td>\n",
       "      <td>0.868221</td>\n",
       "      <td>-0.181386</td>\n",
       "      <td>-0.306728</td>\n",
       "      <td>...</td>\n",
       "      <td>1.443922</td>\n",
       "      <td>0.832702</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>1.372934</td>\n",
       "      <td>0.944623</td>\n",
       "      <td>-0.604752</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.508965</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.514668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0002 -1.238669 -0.489755 -1.404819 -0.661768 -1.214104 -0.438326 -0.542205   \n",
       "C_0003 -0.094326  0.128294 -0.400562 -0.392906  0.908731  0.454217 -0.792399   \n",
       "C_0004 -0.513695  0.528638 -0.372793  0.456696 -0.126533 -0.054497 -0.268084   \n",
       "C_0005 -0.452707  0.211724  0.002274  0.105692  0.088081 -0.109452  0.208944   \n",
       "C_0006 -0.444591  0.344861 -0.730082  0.518467 -0.178900 -0.660072 -0.733352   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0689 -1.056665 -0.131702 -2.461487 -2.196218 -0.828361  1.475229  0.686531   \n",
       "H_0709 -0.813836  0.110010  0.332046 -1.371978 -1.159991 -0.961007 -0.240771   \n",
       "H_0723  0.740941  0.692626  0.985739 -0.153001 -0.268487  0.037081  0.639643   \n",
       "H_1104 -0.448897  0.149684  0.344353  0.107790  0.044293  0.409116  0.303394   \n",
       "H_1105 -0.537081  0.415169  1.062265  0.954178 -0.012975  1.303455 -0.598432   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0002 -1.549951 -1.144974 -1.361823  ... -0.794664 -1.270403  0.737597   \n",
       "C_0003  0.733360  0.611638  0.953743  ...  0.767006  0.814752 -1.029937   \n",
       "C_0004 -0.347413 -0.285398 -0.322790  ...  0.051075  0.649930  0.776952   \n",
       "C_0005  0.185074  0.533692  0.666086  ...  0.884624  0.681302 -1.029937   \n",
       "C_0006 -0.164000  0.323106  0.563685  ... -1.256904 -1.270403 -1.029937   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0689 -1.842048 -1.833416 -0.990698  ... -1.765663  0.676598  0.813014   \n",
       "H_0709 -0.532297 -1.179250 -1.522107  ... -0.899592 -1.270403 -1.029937   \n",
       "H_0723 -0.215969 -0.145524  0.047562  ...  0.332414  1.071325  0.983066   \n",
       "H_1104 -0.866286 -0.533084 -0.141975  ...  0.274111 -1.270403 -1.029937   \n",
       "H_1105  0.868221 -0.181386 -0.306728  ...  1.443922  0.832702  1.024108   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0002 -1.819190 -0.178338  0.264969  0.721004 -0.094753 -0.349492 -0.142859  \n",
       "C_0003  0.719449  0.054000  0.220818  0.859050  0.298774 -0.349492 -0.149023  \n",
       "C_0004 -0.466111 -0.596202  0.287203  1.144874  0.595544 -0.349492  0.728854  \n",
       "C_0005 -0.455983  0.518626  0.463286  0.793437 -0.182278 -0.349492  0.198520  \n",
       "C_0006  0.190454 -1.167309  0.463277  0.996881  0.243467 -0.349492 -0.262606  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0689 -2.498809 -0.298672  0.243131 -1.220236 -0.478093 -0.349492 -2.423072  \n",
       "H_0709  0.059062  0.190509 -4.757775 -1.220236 -4.368677 -0.349492  1.082281  \n",
       "H_0723  1.379293  1.249711 -0.185067 -1.220236 -0.343931 -0.349492  0.869277  \n",
       "H_1104 -0.437144  0.149815  0.178247 -1.220236  0.106997 -0.349492  0.552147  \n",
       "H_1105  1.372934  0.944623 -0.604752 -1.220236  0.508965 -0.349492 -0.514668  \n",
       "\n",
       "[108 rows x 21861 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX_log_del2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d0c30c8-d3b0-4387-a2dc-69dec6674d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_num=dataX_cluster['cluster']\n",
    "patient_num_array=patient_num.values\n",
    "patient_num_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a56815bf-75a8-4a18-a135-e05f2c6697ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_patient_num, test_patient_num=train_test_split(patient_num_array, test_size=0.05, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5833407d-7120-4f34-a5f6-c3392655b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_del2,X_test_del2=train_test_split(dataX_log_del2,test_size=0.05,random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30e5669f-d76a-4f24-afe2-97ce911afd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(dataX_cluster, test_size=0.05,random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceebda60-7a7a-4fe0-bdb2-249e4a367ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corre_plot(X_train, X_train_pred, X_test, X_test_pred):\n",
    "    x=np.linspace(-2,10)\n",
    "    y=x\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('train correlation')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('test correlation')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('train correlation narrow')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title('test correlation narrow')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e41faf6b-7ace-4e40-8b4d-6c5fdfbdecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'], loc='upper right')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'],loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cfaef88-e50f-4cca-9bd0-7a15b3946e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(df): \n",
    "    n_features = len(df)\n",
    "    df_plot = df.sort_values('importance')\n",
    "    f_importance_plot = df_plot['importance'].values\n",
    "    plt.barh(range(n_features), f_importance_plot, align='center')\n",
    "    cols_plot = df_plot['feature'].values             \n",
    "    plt.yticks(np.arange(n_features), cols_plot)      \n",
    "    plt.xlabel('Feature importance')                  \n",
    "    plt.ylabel('Feature') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "221ea45b-1889-40cf-bb5b-96c4dea8175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 19:13:45.329448: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f5a5e6-a197-429b-85b4-9756983f44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') memory growth: True\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU') memory growth: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 19:13:46.514268: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-05-29 19:13:46.515316: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2025-05-29 19:13:46.527121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.527777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-05-29 19:13:46.527855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.528590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:05:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-05-29 19:13:46.528665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.529030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GT 710 computeCapability: 3.5\n",
      "coreClock: 0.954GHz coreCount: 1 deviceMemorySize: 978.25MiB deviceMemoryBandwidth: 11.92GiB/s\n",
      "2025-05-29 19:13:46.529068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-05-29 19:13:46.530745: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2025-05-29 19:13:46.530795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2025-05-29 19:13:46.560674: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-05-29 19:13:46.561259: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-05-29 19:13:46.562859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-05-29 19:13:46.563789: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2025-05-29 19:13:46.567202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2025-05-29 19:13:46.567392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.568128: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.568901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.569353: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.569994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.570710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:46.571083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1847] Ignoring visible gpu device (device: 2, name: NVIDIA GeForce GT 710, pci bus id: 0000:01:00.0, compute capability: 3.5) with core count: 1. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2025-05-29 19:13:46.571102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "        tf.config.set_soft_device_placement(True)\n",
    "        growth = tf.config.experimental.get_memory_growth(device)\n",
    "        print('{} memory growth: {}'.format(device, growth))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00d63c61-8517-4a39-8721-7cec45016136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corre_plot(X_train, X_train_pred, X_test, X_test_pred):\n",
    "    x=np.linspace(-2,10)\n",
    "    y=x\n",
    "    plt.figure(constrained_layout=True)\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.title('train correlation')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.title('test correlation')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.title('train correlation narrow')\n",
    "    plt.scatter(X_train,X_train_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.title('test correlation narrow')\n",
    "    plt.scatter(X_test,X_test_pred,alpha=0.02,s=1,c='black')\n",
    "    plt.xlim(-3,3)\n",
    "    plt.ylim(-3,3)\n",
    "    plt.plot(x,y,color='green')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd29a136-ff84-4e51-aa5a-ad879881f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'], loc='upper right')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','eval'],loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c799a028-042c-4bac-b850-34b6a400e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(df): \n",
    "    n_features = len(df)\n",
    "    df_plot = df.sort_values('importance')\n",
    "    f_importance_plot = df_plot['importance'].values\n",
    "    plt.barh(range(n_features), f_importance_plot, align='center')\n",
    "    cols_plot = df_plot['feature'].values             \n",
    "    plt.yticks(np.arange(n_features), cols_plot)      \n",
    "    plt.xlabel('Feature importance')                  \n",
    "    plt.ylabel('Feature') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "645098ee-d101-4acb-9410-d9f15f26c9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 19:13:56.270673: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-29 19:13:56.271844: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-05-29 19:13:56.375965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:56.376724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:04:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-05-29 19:13:56.376904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:56.377657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:05:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2025-05-29 19:13:56.377775: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-05-29 19:13:56.377832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2025-05-29 19:13:56.377865: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2025-05-29 19:13:56.377895: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-05-29 19:13:56.377926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-05-29 19:13:56.377958: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-05-29 19:13:56.378003: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2025-05-29 19:13:56.378053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2025-05-29 19:13:56.378201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:56.378943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:56.379688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:56.380368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:56.381037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2025-05-29 19:13:56.381103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2025-05-29 19:13:57.040664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-05-29 19:13:57.040705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2025-05-29 19:13:57.040717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2025-05-29 19:13:57.040725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2025-05-29 19:13:57.040998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:57.041681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:57.042411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:57.043059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:57.043677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8545 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:04:00.0, compute capability: 3.7)\n",
      "2025-05-29 19:13:57.044047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:57.044752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-29 19:13:57.045414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10494 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_222\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_908 (Dense)            (None, 100)               2186200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_688 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_909 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_689 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_910 (Dense)            (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "batch_normalization_690 (Bat (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "dense_911 (Dense)            (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "batch_normalization_691 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_912 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_692 (Bat (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_913 (Dense)            (None, 21861)             2207961   \n",
      "=================================================================\n",
      "Total params: 4,418,111\n",
      "Trainable params: 4,417,291\n",
      "Non-trainable params: 820\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model10dims = load_model(\"model10_11_10dim.h5\")\n",
    "model10dims.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f76141bb-ca03-480f-86c5-8db8aa8c96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = X_train.iloc[:, :-1]\n",
    "df_test = X_test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f13fe710-6a14-442b-9b75-dae31f068a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C_0083</th>\n",
       "      <td>0.388913</td>\n",
       "      <td>0.232071</td>\n",
       "      <td>1.041605</td>\n",
       "      <td>0.902932</td>\n",
       "      <td>0.923106</td>\n",
       "      <td>-1.479170</td>\n",
       "      <td>-0.725972</td>\n",
       "      <td>0.561737</td>\n",
       "      <td>1.005341</td>\n",
       "      <td>0.631400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753887</td>\n",
       "      <td>0.656740</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.344238</td>\n",
       "      <td>-0.614887</td>\n",
       "      <td>0.347417</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.130942</td>\n",
       "      <td>3.000866</td>\n",
       "      <td>0.672039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0014</th>\n",
       "      <td>-0.491357</td>\n",
       "      <td>-0.208702</td>\n",
       "      <td>-0.949570</td>\n",
       "      <td>-0.080523</td>\n",
       "      <td>0.650120</td>\n",
       "      <td>-0.477062</td>\n",
       "      <td>0.117402</td>\n",
       "      <td>-0.177543</td>\n",
       "      <td>-0.005107</td>\n",
       "      <td>0.238695</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227848</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.623604</td>\n",
       "      <td>0.379853</td>\n",
       "      <td>0.394208</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>0.454151</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.281494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0006</th>\n",
       "      <td>1.007862</td>\n",
       "      <td>0.051293</td>\n",
       "      <td>0.647164</td>\n",
       "      <td>-0.520091</td>\n",
       "      <td>0.337481</td>\n",
       "      <td>1.187124</td>\n",
       "      <td>0.750467</td>\n",
       "      <td>-0.115304</td>\n",
       "      <td>-0.631415</td>\n",
       "      <td>-0.789085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340969</td>\n",
       "      <td>0.852506</td>\n",
       "      <td>1.126368</td>\n",
       "      <td>0.267152</td>\n",
       "      <td>0.448367</td>\n",
       "      <td>-0.559285</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.912924</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.014469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0008</th>\n",
       "      <td>-0.063484</td>\n",
       "      <td>0.604704</td>\n",
       "      <td>-0.010074</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.784126</td>\n",
       "      <td>0.523687</td>\n",
       "      <td>0.705062</td>\n",
       "      <td>0.107073</td>\n",
       "      <td>0.650657</td>\n",
       "      <td>0.591910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>0.690349</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>0.357423</td>\n",
       "      <td>0.598058</td>\n",
       "      <td>1.032206</td>\n",
       "      <td>-0.156499</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.711918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0132</th>\n",
       "      <td>-0.379078</td>\n",
       "      <td>-0.063351</td>\n",
       "      <td>-0.422360</td>\n",
       "      <td>-1.034857</td>\n",
       "      <td>-0.855160</td>\n",
       "      <td>0.566042</td>\n",
       "      <td>-0.034787</td>\n",
       "      <td>-0.017356</td>\n",
       "      <td>-0.152592</td>\n",
       "      <td>-1.035450</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.987399</td>\n",
       "      <td>0.712918</td>\n",
       "      <td>1.102537</td>\n",
       "      <td>0.679585</td>\n",
       "      <td>0.828777</td>\n",
       "      <td>-0.162848</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.291017</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.054538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0012</th>\n",
       "      <td>-1.521997</td>\n",
       "      <td>0.067175</td>\n",
       "      <td>-0.349465</td>\n",
       "      <td>-1.227161</td>\n",
       "      <td>-1.159223</td>\n",
       "      <td>0.147176</td>\n",
       "      <td>0.480327</td>\n",
       "      <td>-1.637841</td>\n",
       "      <td>-1.043687</td>\n",
       "      <td>-0.477915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128161</td>\n",
       "      <td>0.782275</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.323493</td>\n",
       "      <td>-0.181305</td>\n",
       "      <td>-0.492980</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-4.368677</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.423462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0015</th>\n",
       "      <td>-1.501659</td>\n",
       "      <td>-0.015775</td>\n",
       "      <td>-2.043978</td>\n",
       "      <td>-1.415894</td>\n",
       "      <td>-0.616190</td>\n",
       "      <td>-1.331602</td>\n",
       "      <td>-1.422446</td>\n",
       "      <td>-1.490010</td>\n",
       "      <td>-1.171014</td>\n",
       "      <td>-0.957152</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.225591</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.174205</td>\n",
       "      <td>0.032423</td>\n",
       "      <td>0.012057</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>-0.143522</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.608102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0087</th>\n",
       "      <td>0.262581</td>\n",
       "      <td>0.557800</td>\n",
       "      <td>0.801761</td>\n",
       "      <td>0.796105</td>\n",
       "      <td>0.392620</td>\n",
       "      <td>-0.952609</td>\n",
       "      <td>0.115493</td>\n",
       "      <td>0.295912</td>\n",
       "      <td>0.798726</td>\n",
       "      <td>0.749829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412925</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.797906</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>-1.457010</td>\n",
       "      <td>0.273234</td>\n",
       "      <td>0.680197</td>\n",
       "      <td>0.341713</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.079904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0050</th>\n",
       "      <td>1.048922</td>\n",
       "      <td>0.577184</td>\n",
       "      <td>1.692505</td>\n",
       "      <td>1.645615</td>\n",
       "      <td>0.750238</td>\n",
       "      <td>-0.066897</td>\n",
       "      <td>-0.130413</td>\n",
       "      <td>1.106432</td>\n",
       "      <td>1.015931</td>\n",
       "      <td>1.764417</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333553</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.216004</td>\n",
       "      <td>0.990117</td>\n",
       "      <td>-0.753045</td>\n",
       "      <td>0.618545</td>\n",
       "      <td>1.050514</td>\n",
       "      <td>0.563625</td>\n",
       "      <td>2.764119</td>\n",
       "      <td>1.325121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0657</th>\n",
       "      <td>-0.064258</td>\n",
       "      <td>0.172921</td>\n",
       "      <td>0.519725</td>\n",
       "      <td>-0.133270</td>\n",
       "      <td>-0.175916</td>\n",
       "      <td>1.377655</td>\n",
       "      <td>0.786557</td>\n",
       "      <td>0.458264</td>\n",
       "      <td>-0.163786</td>\n",
       "      <td>-0.114812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179268</td>\n",
       "      <td>0.526197</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.282419</td>\n",
       "      <td>-0.833780</td>\n",
       "      <td>-0.073411</td>\n",
       "      <td>0.835596</td>\n",
       "      <td>0.287229</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.182819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "C_0083  0.388913  0.232071  1.041605  0.902932  0.923106 -1.479170 -0.725972   \n",
       "P_0014 -0.491357 -0.208702 -0.949570 -0.080523  0.650120 -0.477062  0.117402   \n",
       "H_0006  1.007862  0.051293  0.647164 -0.520091  0.337481  1.187124  0.750467   \n",
       "C_0008 -0.063484  0.604704 -0.010074  0.157058  0.784126  0.523687  0.705062   \n",
       "P_0132 -0.379078 -0.063351 -0.422360 -1.034857 -0.855160  0.566042 -0.034787   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "H_0012 -1.521997  0.067175 -0.349465 -1.227161 -1.159223  0.147176  0.480327   \n",
       "P_0015 -1.501659 -0.015775 -2.043978 -1.415894 -0.616190 -1.331602 -1.422446   \n",
       "C_0087  0.262581  0.557800  0.801761  0.796105  0.392620 -0.952609  0.115493   \n",
       "C_0050  1.048922  0.577184  1.692505  1.645615  0.750238 -0.066897 -0.130413   \n",
       "H_0657 -0.064258  0.172921  0.519725 -0.133270 -0.175916  1.377655  0.786557   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "C_0083  0.561737  1.005341  0.631400  ...  0.753887  0.656740 -1.029937   \n",
       "P_0014 -0.177543 -0.005107  0.238695  ... -0.227848 -1.270403 -1.029937   \n",
       "H_0006 -0.115304 -0.631415 -0.789085  ... -0.340969  0.852506  1.126368   \n",
       "C_0008  0.107073  0.650657  0.591910  ...  0.626894  0.690349 -1.029937   \n",
       "P_0132 -0.017356 -0.152592 -1.035450  ... -1.987399  0.712918  1.102537   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "H_0012 -1.637841 -1.043687 -0.477915  ...  0.128161  0.782275 -1.029937   \n",
       "P_0015 -1.490010 -1.171014 -0.957152  ... -1.225591 -1.270403 -1.029937   \n",
       "C_0087  0.295912  0.798726  0.749829  ...  0.412925 -1.270403  0.797906   \n",
       "C_0050  1.106432  1.015931  1.764417  ...  1.333553 -1.270403  1.216004   \n",
       "H_0657  0.458264 -0.163786 -0.114812  ...  0.179268  0.526197 -1.029937   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "C_0083  0.344238 -0.614887  0.347417 -1.220236  0.130942  3.000866  0.672039  \n",
       "P_0014 -0.623604  0.379853  0.394208  0.644572  0.454151 -0.349492 -1.281494  \n",
       "H_0006  0.267152  0.448367 -0.559285 -1.220236  0.912924 -0.349492  1.014469  \n",
       "C_0008 -0.004865  0.357423  0.598058  1.032206 -0.156499 -0.349492 -0.711918  \n",
       "P_0132  0.679585  0.828777 -0.162848 -1.220236  0.291017 -0.349492 -1.054538  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "H_0012 -1.323493 -0.181305 -0.492980 -1.220236 -4.368677 -0.349492 -1.423462  \n",
       "P_0015 -1.174205  0.032423  0.012057 -1.220236 -0.143522 -0.349492 -1.608102  \n",
       "C_0087  0.189569 -1.457010  0.273234  0.680197  0.341713 -0.349492  1.079904  \n",
       "C_0050  0.990117 -0.753045  0.618545  1.050514  0.563625  2.764119  1.325121  \n",
       "H_0657  0.282419 -0.833780 -0.073411  0.835596  0.287229 -0.349492  0.182819  \n",
       "\n",
       "[102 rows x 21861 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12ebc6fb-daec-4fd9-9cb8-4eb1f079a90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21853</th>\n",
       "      <th>21854</th>\n",
       "      <th>21855</th>\n",
       "      <th>21856</th>\n",
       "      <th>21857</th>\n",
       "      <th>21858</th>\n",
       "      <th>21859</th>\n",
       "      <th>21860</th>\n",
       "      <th>21861</th>\n",
       "      <th>21862</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P_0063</th>\n",
       "      <td>-0.354154</td>\n",
       "      <td>-0.443490</td>\n",
       "      <td>-0.519781</td>\n",
       "      <td>-0.938225</td>\n",
       "      <td>-0.692398</td>\n",
       "      <td>-1.225452</td>\n",
       "      <td>-0.567080</td>\n",
       "      <td>-0.929789</td>\n",
       "      <td>-0.391042</td>\n",
       "      <td>-0.918295</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.361050</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>0.770840</td>\n",
       "      <td>-0.650630</td>\n",
       "      <td>0.148355</td>\n",
       "      <td>-0.417226</td>\n",
       "      <td>0.656224</td>\n",
       "      <td>-1.557539</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.420660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_0029</th>\n",
       "      <td>-0.495266</td>\n",
       "      <td>0.082892</td>\n",
       "      <td>-1.553595</td>\n",
       "      <td>-1.080348</td>\n",
       "      <td>-0.552315</td>\n",
       "      <td>1.010324</td>\n",
       "      <td>-0.309374</td>\n",
       "      <td>-1.022790</td>\n",
       "      <td>-0.761766</td>\n",
       "      <td>-0.485853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.872063</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-1.033890</td>\n",
       "      <td>-0.783647</td>\n",
       "      <td>-0.293198</td>\n",
       "      <td>0.669075</td>\n",
       "      <td>-0.153178</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.354232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H_0010</th>\n",
       "      <td>0.947469</td>\n",
       "      <td>0.610187</td>\n",
       "      <td>1.167948</td>\n",
       "      <td>1.184255</td>\n",
       "      <td>0.972135</td>\n",
       "      <td>0.467329</td>\n",
       "      <td>1.196569</td>\n",
       "      <td>0.468982</td>\n",
       "      <td>0.605396</td>\n",
       "      <td>0.755573</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017267</td>\n",
       "      <td>0.936328</td>\n",
       "      <td>1.164238</td>\n",
       "      <td>0.185686</td>\n",
       "      <td>-0.605207</td>\n",
       "      <td>0.286310</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.613593</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>1.488160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0025</th>\n",
       "      <td>-0.525737</td>\n",
       "      <td>0.242389</td>\n",
       "      <td>0.106966</td>\n",
       "      <td>0.424007</td>\n",
       "      <td>0.286912</td>\n",
       "      <td>-0.878035</td>\n",
       "      <td>-1.127625</td>\n",
       "      <td>0.521370</td>\n",
       "      <td>0.787054</td>\n",
       "      <td>0.509760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186735</td>\n",
       "      <td>0.740892</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>0.404844</td>\n",
       "      <td>-0.157597</td>\n",
       "      <td>0.652556</td>\n",
       "      <td>1.144599</td>\n",
       "      <td>0.312478</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>0.480518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0070</th>\n",
       "      <td>-1.309004</td>\n",
       "      <td>0.213414</td>\n",
       "      <td>-0.418802</td>\n",
       "      <td>-1.097679</td>\n",
       "      <td>-1.892068</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>-1.458774</td>\n",
       "      <td>-2.380129</td>\n",
       "      <td>-1.965949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788725</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>-1.029937</td>\n",
       "      <td>-2.301812</td>\n",
       "      <td>-1.832232</td>\n",
       "      <td>-0.425255</td>\n",
       "      <td>0.830336</td>\n",
       "      <td>-1.228546</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-0.297272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C_0039</th>\n",
       "      <td>0.076393</td>\n",
       "      <td>0.369525</td>\n",
       "      <td>0.262198</td>\n",
       "      <td>-0.434926</td>\n",
       "      <td>-0.641064</td>\n",
       "      <td>-0.367527</td>\n",
       "      <td>-0.258453</td>\n",
       "      <td>-0.416765</td>\n",
       "      <td>-0.246040</td>\n",
       "      <td>-0.748481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543181</td>\n",
       "      <td>-1.270403</td>\n",
       "      <td>1.039146</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>-1.086837</td>\n",
       "      <td>-0.351680</td>\n",
       "      <td>-1.220236</td>\n",
       "      <td>0.517124</td>\n",
       "      <td>-0.349492</td>\n",
       "      <td>-1.052990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 21861 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6      \\\n",
       "P_0063 -0.354154 -0.443490 -0.519781 -0.938225 -0.692398 -1.225452 -0.567080   \n",
       "P_0029 -0.495266  0.082892 -1.553595 -1.080348 -0.552315  1.010324 -0.309374   \n",
       "H_0010  0.947469  0.610187  1.167948  1.184255  0.972135  0.467329  1.196569   \n",
       "C_0025 -0.525737  0.242389  0.106966  0.424007  0.286912 -0.878035 -1.127625   \n",
       "C_0070 -1.309004  0.213414 -0.418802 -1.097679 -1.892068  0.063291  0.524590   \n",
       "C_0039  0.076393  0.369525  0.262198 -0.434926 -0.641064 -0.367527 -0.258453   \n",
       "\n",
       "           7         8         9      ...     21853     21854     21855  \\\n",
       "P_0063 -0.929789 -0.391042 -0.918295  ... -1.361050 -1.270403  0.770840   \n",
       "P_0029 -1.022790 -0.761766 -0.485853  ... -0.872063 -1.270403 -1.029937   \n",
       "H_0010  0.468982  0.605396  0.755573  ...  1.017267  0.936328  1.164238   \n",
       "C_0025  0.521370  0.787054  0.509760  ... -0.186735  0.740892 -1.029937   \n",
       "C_0070 -1.458774 -2.380129 -1.965949  ... -0.788725 -1.270403 -1.029937   \n",
       "C_0039 -0.416765 -0.246040 -0.748481  ...  0.543181 -1.270403  1.039146   \n",
       "\n",
       "           21856     21857     21858     21859     21860     21861     21862  \n",
       "P_0063 -0.650630  0.148355 -0.417226  0.656224 -1.557539 -0.349492 -1.420660  \n",
       "P_0029 -1.033890 -0.783647 -0.293198  0.669075 -0.153178 -0.349492  0.354232  \n",
       "H_0010  0.185686 -0.605207  0.286310 -1.220236  0.613593 -0.349492  1.488160  \n",
       "C_0025  0.404844 -0.157597  0.652556  1.144599  0.312478 -0.349492  0.480518  \n",
       "C_0070 -2.301812 -1.832232 -0.425255  0.830336 -1.228546 -0.349492 -0.297272  \n",
       "C_0039  0.081579 -1.086837 -0.351680 -1.220236  0.517124 -0.349492 -1.052990  \n",
       "\n",
       "[6 rows x 21861 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d438280e-9426-4473-a505-c0706ef2d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "X_all_pred=model10dims.predict(dataX_log_del2,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "29d062f7-8594-4954-8322-f5cccf3a7092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson's correlation cefficient: 0.9911003576008612\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2.5,2.5))\n",
    "plt.scatter(dataX_log_del2.to_numpy().reshape(-1),X_all_pred.reshape(-1), s=0.01)\n",
    "plt.plot([-10,10],[-10,10], 'g--',linewidth=1.)\n",
    "plt.title('Autoencoder (10 dimensions)')\n",
    "plt.xlabel('input data')\n",
    "plt.ylabel('output data')\n",
    "print(f\"pearson's correlation cefficient: {np.corrcoef(dataX_log_del2.to_numpy().reshape(-1),X_all_pred.reshape(-1))[0,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "432fa7ae-a400-4945-b074-b58f6040f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_coefficient(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denominator = np.sqrt(np.sum((x - x_mean) ** 2) * np.sum((y - y_mean) ** 2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b52e18e8-9157-443a-a00b-937ce9fa4a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9872970844075253\n"
     ]
    }
   ],
   "source": [
    "corr_coef_test = correlation_coefficient(df_test.values, X_test_pred)\n",
    "print(corr_coef_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "457c3599-9208-49d3-b2ff-a76677583c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9912588547630568\n"
     ]
    }
   ],
   "source": [
    "corr_coef_tr = correlation_coefficient(df_train.values, X_train_pred)\n",
    "print(corr_coef_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2e07b29-8a4c-44d2-8eb3-a75bb7026677",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'dense_910'\n",
    "latent_layer = Model(inputs = model10dims.input, outputs = model10dims.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2eed3c05-da5b-469b-8484-0c8158f9f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "latent_layer_pred_train = latent_layer.predict(df_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebf8331a-9d21-4bbf-a044-d828e675cc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_layer_pred_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84377fd5-e0df-4264-beee-fc5c853b668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "latent_layer_pred_test=latent_layer.predict(df_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ae33d08-53bc-4bc5-8363-d62341af0f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_layer_pred_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d69f775b-1dfd-4d2c-ac08-d35c527ed69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.009770</td>\n",
       "      <td>-0.116596</td>\n",
       "      <td>-3.884397</td>\n",
       "      <td>9.466447</td>\n",
       "      <td>-6.210791</td>\n",
       "      <td>10.363499</td>\n",
       "      <td>-6.480822</td>\n",
       "      <td>-6.431981</td>\n",
       "      <td>8.317493</td>\n",
       "      <td>-10.151834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.041958</td>\n",
       "      <td>-7.031896</td>\n",
       "      <td>-0.035036</td>\n",
       "      <td>-6.026720</td>\n",
       "      <td>9.640221</td>\n",
       "      <td>-2.293951</td>\n",
       "      <td>2.895993</td>\n",
       "      <td>5.159375</td>\n",
       "      <td>8.464264</td>\n",
       "      <td>-9.648880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.264014</td>\n",
       "      <td>1.035362</td>\n",
       "      <td>-0.534110</td>\n",
       "      <td>5.940433</td>\n",
       "      <td>1.532756</td>\n",
       "      <td>-10.751539</td>\n",
       "      <td>10.312591</td>\n",
       "      <td>-0.970045</td>\n",
       "      <td>-12.190402</td>\n",
       "      <td>14.136766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.562168</td>\n",
       "      <td>-7.238606</td>\n",
       "      <td>8.801069</td>\n",
       "      <td>-11.129960</td>\n",
       "      <td>0.842867</td>\n",
       "      <td>-9.831837</td>\n",
       "      <td>3.189499</td>\n",
       "      <td>-5.384264</td>\n",
       "      <td>12.091418</td>\n",
       "      <td>-2.478377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.696049</td>\n",
       "      <td>-6.322094</td>\n",
       "      <td>-5.523495</td>\n",
       "      <td>-0.676701</td>\n",
       "      <td>9.112110</td>\n",
       "      <td>-5.423116</td>\n",
       "      <td>0.220518</td>\n",
       "      <td>11.457376</td>\n",
       "      <td>-11.707994</td>\n",
       "      <td>-0.188018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.426304</td>\n",
       "      <td>-12.922999</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>7.786605</td>\n",
       "      <td>3.437022</td>\n",
       "      <td>13.803056</td>\n",
       "      <td>-1.683597</td>\n",
       "      <td>-2.044187</td>\n",
       "      <td>0.861588</td>\n",
       "      <td>2.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9.342772</td>\n",
       "      <td>-3.227812</td>\n",
       "      <td>1.095684</td>\n",
       "      <td>3.423271</td>\n",
       "      <td>12.876227</td>\n",
       "      <td>2.455095</td>\n",
       "      <td>3.271398</td>\n",
       "      <td>13.069257</td>\n",
       "      <td>4.655768</td>\n",
       "      <td>4.580972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.916699</td>\n",
       "      <td>12.988074</td>\n",
       "      <td>2.033382</td>\n",
       "      <td>1.448987</td>\n",
       "      <td>-2.773812</td>\n",
       "      <td>10.903077</td>\n",
       "      <td>-11.291394</td>\n",
       "      <td>-11.213188</td>\n",
       "      <td>-5.023474</td>\n",
       "      <td>-8.491025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-12.577526</td>\n",
       "      <td>4.521458</td>\n",
       "      <td>9.041041</td>\n",
       "      <td>1.925805</td>\n",
       "      <td>-6.271043</td>\n",
       "      <td>-0.512782</td>\n",
       "      <td>-4.672748</td>\n",
       "      <td>-8.338315</td>\n",
       "      <td>10.902304</td>\n",
       "      <td>7.582399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-1.165278</td>\n",
       "      <td>4.268226</td>\n",
       "      <td>-0.651403</td>\n",
       "      <td>-7.344012</td>\n",
       "      <td>-12.025347</td>\n",
       "      <td>-12.065077</td>\n",
       "      <td>6.186944</td>\n",
       "      <td>-0.848753</td>\n",
       "      <td>-9.785013</td>\n",
       "      <td>-7.701897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2          3          4          5  \\\n",
       "0     6.009770  -0.116596 -3.884397   9.466447  -6.210791  10.363499   \n",
       "1    -6.041958  -7.031896 -0.035036  -6.026720   9.640221  -2.293951   \n",
       "2     0.264014   1.035362 -0.534110   5.940433   1.532756 -10.751539   \n",
       "3    -7.562168  -7.238606  8.801069 -11.129960   0.842867  -9.831837   \n",
       "4     9.696049  -6.322094 -5.523495  -0.676701   9.112110  -5.423116   \n",
       "..         ...        ...       ...        ...        ...        ...   \n",
       "97    0.426304 -12.922999  0.010505   7.786605   3.437022  13.803056   \n",
       "98    9.342772  -3.227812  1.095684   3.423271  12.876227   2.455095   \n",
       "99    1.916699  12.988074  2.033382   1.448987  -2.773812  10.903077   \n",
       "100 -12.577526   4.521458  9.041041   1.925805  -6.271043  -0.512782   \n",
       "101  -1.165278   4.268226 -0.651403  -7.344012 -12.025347 -12.065077   \n",
       "\n",
       "             6          7          8          9  \n",
       "0    -6.480822  -6.431981   8.317493 -10.151834  \n",
       "1     2.895993   5.159375   8.464264  -9.648880  \n",
       "2    10.312591  -0.970045 -12.190402  14.136766  \n",
       "3     3.189499  -5.384264  12.091418  -2.478377  \n",
       "4     0.220518  11.457376 -11.707994  -0.188018  \n",
       "..         ...        ...        ...        ...  \n",
       "97   -1.683597  -2.044187   0.861588   2.002081  \n",
       "98    3.271398  13.069257   4.655768   4.580972  \n",
       "99  -11.291394 -11.213188  -5.023474  -8.491025  \n",
       "100  -4.672748  -8.338315  10.902304   7.582399  \n",
       "101   6.186944  -0.848753  -9.785013  -7.701897  \n",
       "\n",
       "[102 rows x 10 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_10dims=pd.DataFrame(latent_layer_pred_train)\n",
    "X_train_10dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbcce1c4-2df4-4836-bb4f-6067448add23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.182652</td>\n",
       "      <td>-9.016517</td>\n",
       "      <td>5.416154</td>\n",
       "      <td>4.398924</td>\n",
       "      <td>3.030411</td>\n",
       "      <td>9.104897</td>\n",
       "      <td>-2.629755</td>\n",
       "      <td>-9.648589</td>\n",
       "      <td>-7.168473</td>\n",
       "      <td>1.398691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.695566</td>\n",
       "      <td>-4.088038</td>\n",
       "      <td>7.612744</td>\n",
       "      <td>-7.422001</td>\n",
       "      <td>11.953300</td>\n",
       "      <td>-1.205127</td>\n",
       "      <td>4.046550</td>\n",
       "      <td>12.574976</td>\n",
       "      <td>0.551326</td>\n",
       "      <td>-0.709395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.591668</td>\n",
       "      <td>-2.130538</td>\n",
       "      <td>1.383535</td>\n",
       "      <td>12.661238</td>\n",
       "      <td>-14.747499</td>\n",
       "      <td>-0.825547</td>\n",
       "      <td>-0.599415</td>\n",
       "      <td>-2.178931</td>\n",
       "      <td>9.465315</td>\n",
       "      <td>4.564372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.936712</td>\n",
       "      <td>14.305503</td>\n",
       "      <td>-10.923644</td>\n",
       "      <td>4.596941</td>\n",
       "      <td>-2.134693</td>\n",
       "      <td>5.222218</td>\n",
       "      <td>-1.368526</td>\n",
       "      <td>-4.935497</td>\n",
       "      <td>7.297935</td>\n",
       "      <td>-9.517170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.340047</td>\n",
       "      <td>-11.103286</td>\n",
       "      <td>0.380939</td>\n",
       "      <td>-9.910574</td>\n",
       "      <td>5.785901</td>\n",
       "      <td>8.730873</td>\n",
       "      <td>5.895844</td>\n",
       "      <td>0.523430</td>\n",
       "      <td>-10.370501</td>\n",
       "      <td>10.999421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.454951</td>\n",
       "      <td>-5.118403</td>\n",
       "      <td>-11.746635</td>\n",
       "      <td>2.318626</td>\n",
       "      <td>-7.586737</td>\n",
       "      <td>12.236910</td>\n",
       "      <td>7.013754</td>\n",
       "      <td>7.383508</td>\n",
       "      <td>7.247571</td>\n",
       "      <td>-10.797413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5         6  \\\n",
       "0  12.182652  -9.016517   5.416154   4.398924   3.030411   9.104897 -2.629755   \n",
       "1  -5.695566  -4.088038   7.612744  -7.422001  11.953300  -1.205127  4.046550   \n",
       "2  -4.591668  -2.130538   1.383535  12.661238 -14.747499  -0.825547 -0.599415   \n",
       "3   5.936712  14.305503 -10.923644   4.596941  -2.134693   5.222218 -1.368526   \n",
       "4   3.340047 -11.103286   0.380939  -9.910574   5.785901   8.730873  5.895844   \n",
       "5   8.454951  -5.118403 -11.746635   2.318626  -7.586737  12.236910  7.013754   \n",
       "\n",
       "           7          8          9  \n",
       "0  -9.648589  -7.168473   1.398691  \n",
       "1  12.574976   0.551326  -0.709395  \n",
       "2  -2.178931   9.465315   4.564372  \n",
       "3  -4.935497   7.297935  -9.517170  \n",
       "4   0.523430 -10.370501  10.999421  \n",
       "5   7.383508   7.247571 -10.797413  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_10dims=pd.DataFrame(latent_layer_pred_test)\n",
    "X_test_10dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8aae93f1-62b7-40b9-9860-42389058345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_10dims['patient number'] = X_train.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c10b056a-bf94-4933-930e-9d3e5aae1891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>patient number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.009770</td>\n",
       "      <td>-0.116596</td>\n",
       "      <td>-3.884397</td>\n",
       "      <td>9.466447</td>\n",
       "      <td>-6.210791</td>\n",
       "      <td>10.363499</td>\n",
       "      <td>-6.480822</td>\n",
       "      <td>-6.431981</td>\n",
       "      <td>8.317493</td>\n",
       "      <td>-10.151834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.041958</td>\n",
       "      <td>-7.031896</td>\n",
       "      <td>-0.035036</td>\n",
       "      <td>-6.026720</td>\n",
       "      <td>9.640221</td>\n",
       "      <td>-2.293951</td>\n",
       "      <td>2.895993</td>\n",
       "      <td>5.159375</td>\n",
       "      <td>8.464264</td>\n",
       "      <td>-9.648880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.264014</td>\n",
       "      <td>1.035362</td>\n",
       "      <td>-0.534110</td>\n",
       "      <td>5.940433</td>\n",
       "      <td>1.532756</td>\n",
       "      <td>-10.751539</td>\n",
       "      <td>10.312591</td>\n",
       "      <td>-0.970045</td>\n",
       "      <td>-12.190402</td>\n",
       "      <td>14.136766</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.562168</td>\n",
       "      <td>-7.238606</td>\n",
       "      <td>8.801069</td>\n",
       "      <td>-11.129960</td>\n",
       "      <td>0.842867</td>\n",
       "      <td>-9.831837</td>\n",
       "      <td>3.189499</td>\n",
       "      <td>-5.384264</td>\n",
       "      <td>12.091418</td>\n",
       "      <td>-2.478377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.696049</td>\n",
       "      <td>-6.322094</td>\n",
       "      <td>-5.523495</td>\n",
       "      <td>-0.676701</td>\n",
       "      <td>9.112110</td>\n",
       "      <td>-5.423116</td>\n",
       "      <td>0.220518</td>\n",
       "      <td>11.457376</td>\n",
       "      <td>-11.707994</td>\n",
       "      <td>-0.188018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.426304</td>\n",
       "      <td>-12.922999</td>\n",
       "      <td>0.010505</td>\n",
       "      <td>7.786605</td>\n",
       "      <td>3.437022</td>\n",
       "      <td>13.803056</td>\n",
       "      <td>-1.683597</td>\n",
       "      <td>-2.044187</td>\n",
       "      <td>0.861588</td>\n",
       "      <td>2.002081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>9.342772</td>\n",
       "      <td>-3.227812</td>\n",
       "      <td>1.095684</td>\n",
       "      <td>3.423271</td>\n",
       "      <td>12.876227</td>\n",
       "      <td>2.455095</td>\n",
       "      <td>3.271398</td>\n",
       "      <td>13.069257</td>\n",
       "      <td>4.655768</td>\n",
       "      <td>4.580972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.916699</td>\n",
       "      <td>12.988074</td>\n",
       "      <td>2.033382</td>\n",
       "      <td>1.448987</td>\n",
       "      <td>-2.773812</td>\n",
       "      <td>10.903077</td>\n",
       "      <td>-11.291394</td>\n",
       "      <td>-11.213188</td>\n",
       "      <td>-5.023474</td>\n",
       "      <td>-8.491025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-12.577526</td>\n",
       "      <td>4.521458</td>\n",
       "      <td>9.041041</td>\n",
       "      <td>1.925805</td>\n",
       "      <td>-6.271043</td>\n",
       "      <td>-0.512782</td>\n",
       "      <td>-4.672748</td>\n",
       "      <td>-8.338315</td>\n",
       "      <td>10.902304</td>\n",
       "      <td>7.582399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-1.165278</td>\n",
       "      <td>4.268226</td>\n",
       "      <td>-0.651403</td>\n",
       "      <td>-7.344012</td>\n",
       "      <td>-12.025347</td>\n",
       "      <td>-12.065077</td>\n",
       "      <td>6.186944</td>\n",
       "      <td>-0.848753</td>\n",
       "      <td>-9.785013</td>\n",
       "      <td>-7.701897</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1         2          3          4          5  \\\n",
       "0     6.009770  -0.116596 -3.884397   9.466447  -6.210791  10.363499   \n",
       "1    -6.041958  -7.031896 -0.035036  -6.026720   9.640221  -2.293951   \n",
       "2     0.264014   1.035362 -0.534110   5.940433   1.532756 -10.751539   \n",
       "3    -7.562168  -7.238606  8.801069 -11.129960   0.842867  -9.831837   \n",
       "4     9.696049  -6.322094 -5.523495  -0.676701   9.112110  -5.423116   \n",
       "..         ...        ...       ...        ...        ...        ...   \n",
       "97    0.426304 -12.922999  0.010505   7.786605   3.437022  13.803056   \n",
       "98    9.342772  -3.227812  1.095684   3.423271  12.876227   2.455095   \n",
       "99    1.916699  12.988074  2.033382   1.448987  -2.773812  10.903077   \n",
       "100 -12.577526   4.521458  9.041041   1.925805  -6.271043  -0.512782   \n",
       "101  -1.165278   4.268226 -0.651403  -7.344012 -12.025347 -12.065077   \n",
       "\n",
       "             6          7          8          9  patient number  \n",
       "0    -6.480822  -6.431981   8.317493 -10.151834               1  \n",
       "1     2.895993   5.159375   8.464264  -9.648880               0  \n",
       "2    10.312591  -0.970045 -12.190402  14.136766               2  \n",
       "3     3.189499  -5.384264  12.091418  -2.478377               1  \n",
       "4     0.220518  11.457376 -11.707994  -0.188018               0  \n",
       "..         ...        ...        ...        ...             ...  \n",
       "97   -1.683597  -2.044187   0.861588   2.002081               2  \n",
       "98    3.271398  13.069257   4.655768   4.580972               0  \n",
       "99  -11.291394 -11.213188  -5.023474  -8.491025               1  \n",
       "100  -4.672748  -8.338315  10.902304   7.582399               1  \n",
       "101   6.186944  -0.848753  -9.785013  -7.701897               2  \n",
       "\n",
       "[102 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_10dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71af9251-7a47-4df6-8cc0-4f7e1ae1d1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_10dims['patient number'] = X_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36f4c401-c477-43e5-b651-8b924fd4865c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>patient number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.182652</td>\n",
       "      <td>-9.016517</td>\n",
       "      <td>5.416154</td>\n",
       "      <td>4.398924</td>\n",
       "      <td>3.030411</td>\n",
       "      <td>9.104897</td>\n",
       "      <td>-2.629755</td>\n",
       "      <td>-9.648589</td>\n",
       "      <td>-7.168473</td>\n",
       "      <td>1.398691</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.695566</td>\n",
       "      <td>-4.088038</td>\n",
       "      <td>7.612744</td>\n",
       "      <td>-7.422001</td>\n",
       "      <td>11.953300</td>\n",
       "      <td>-1.205127</td>\n",
       "      <td>4.046550</td>\n",
       "      <td>12.574976</td>\n",
       "      <td>0.551326</td>\n",
       "      <td>-0.709395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.591668</td>\n",
       "      <td>-2.130538</td>\n",
       "      <td>1.383535</td>\n",
       "      <td>12.661238</td>\n",
       "      <td>-14.747499</td>\n",
       "      <td>-0.825547</td>\n",
       "      <td>-0.599415</td>\n",
       "      <td>-2.178931</td>\n",
       "      <td>9.465315</td>\n",
       "      <td>4.564372</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.936712</td>\n",
       "      <td>14.305503</td>\n",
       "      <td>-10.923644</td>\n",
       "      <td>4.596941</td>\n",
       "      <td>-2.134693</td>\n",
       "      <td>5.222218</td>\n",
       "      <td>-1.368526</td>\n",
       "      <td>-4.935497</td>\n",
       "      <td>7.297935</td>\n",
       "      <td>-9.517170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.340047</td>\n",
       "      <td>-11.103286</td>\n",
       "      <td>0.380939</td>\n",
       "      <td>-9.910574</td>\n",
       "      <td>5.785901</td>\n",
       "      <td>8.730873</td>\n",
       "      <td>5.895844</td>\n",
       "      <td>0.523430</td>\n",
       "      <td>-10.370501</td>\n",
       "      <td>10.999421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.454951</td>\n",
       "      <td>-5.118403</td>\n",
       "      <td>-11.746635</td>\n",
       "      <td>2.318626</td>\n",
       "      <td>-7.586737</td>\n",
       "      <td>12.236910</td>\n",
       "      <td>7.013754</td>\n",
       "      <td>7.383508</td>\n",
       "      <td>7.247571</td>\n",
       "      <td>-10.797413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5         6  \\\n",
       "0  12.182652  -9.016517   5.416154   4.398924   3.030411   9.104897 -2.629755   \n",
       "1  -5.695566  -4.088038   7.612744  -7.422001  11.953300  -1.205127  4.046550   \n",
       "2  -4.591668  -2.130538   1.383535  12.661238 -14.747499  -0.825547 -0.599415   \n",
       "3   5.936712  14.305503 -10.923644   4.596941  -2.134693   5.222218 -1.368526   \n",
       "4   3.340047 -11.103286   0.380939  -9.910574   5.785901   8.730873  5.895844   \n",
       "5   8.454951  -5.118403 -11.746635   2.318626  -7.586737  12.236910  7.013754   \n",
       "\n",
       "           7          8          9  patient number  \n",
       "0  -9.648589  -7.168473   1.398691               0  \n",
       "1  12.574976   0.551326  -0.709395               0  \n",
       "2  -2.178931   9.465315   4.564372               2  \n",
       "3  -4.935497   7.297935  -9.517170               1  \n",
       "4   0.523430 -10.370501  10.999421               1  \n",
       "5   7.383508   7.247571 -10.797413               1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_10dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d909235-1b7c-492d-b476-e6abdf821535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>patient number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.009770</td>\n",
       "      <td>-0.116596</td>\n",
       "      <td>-3.884397</td>\n",
       "      <td>9.466447</td>\n",
       "      <td>-6.210791</td>\n",
       "      <td>10.363499</td>\n",
       "      <td>-6.480822</td>\n",
       "      <td>-6.431981</td>\n",
       "      <td>8.317493</td>\n",
       "      <td>-10.151834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.041958</td>\n",
       "      <td>-7.031896</td>\n",
       "      <td>-0.035036</td>\n",
       "      <td>-6.026720</td>\n",
       "      <td>9.640221</td>\n",
       "      <td>-2.293951</td>\n",
       "      <td>2.895993</td>\n",
       "      <td>5.159375</td>\n",
       "      <td>8.464264</td>\n",
       "      <td>-9.648880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.264014</td>\n",
       "      <td>1.035362</td>\n",
       "      <td>-0.534110</td>\n",
       "      <td>5.940433</td>\n",
       "      <td>1.532756</td>\n",
       "      <td>-10.751539</td>\n",
       "      <td>10.312591</td>\n",
       "      <td>-0.970045</td>\n",
       "      <td>-12.190402</td>\n",
       "      <td>14.136766</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-7.562168</td>\n",
       "      <td>-7.238606</td>\n",
       "      <td>8.801069</td>\n",
       "      <td>-11.129960</td>\n",
       "      <td>0.842867</td>\n",
       "      <td>-9.831837</td>\n",
       "      <td>3.189499</td>\n",
       "      <td>-5.384264</td>\n",
       "      <td>12.091418</td>\n",
       "      <td>-2.478377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.696049</td>\n",
       "      <td>-6.322094</td>\n",
       "      <td>-5.523495</td>\n",
       "      <td>-0.676701</td>\n",
       "      <td>9.112110</td>\n",
       "      <td>-5.423116</td>\n",
       "      <td>0.220518</td>\n",
       "      <td>11.457376</td>\n",
       "      <td>-11.707994</td>\n",
       "      <td>-0.188018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-5.695566</td>\n",
       "      <td>-4.088038</td>\n",
       "      <td>7.612744</td>\n",
       "      <td>-7.422001</td>\n",
       "      <td>11.953300</td>\n",
       "      <td>-1.205127</td>\n",
       "      <td>4.046550</td>\n",
       "      <td>12.574976</td>\n",
       "      <td>0.551326</td>\n",
       "      <td>-0.709395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-4.591668</td>\n",
       "      <td>-2.130538</td>\n",
       "      <td>1.383535</td>\n",
       "      <td>12.661238</td>\n",
       "      <td>-14.747499</td>\n",
       "      <td>-0.825547</td>\n",
       "      <td>-0.599415</td>\n",
       "      <td>-2.178931</td>\n",
       "      <td>9.465315</td>\n",
       "      <td>4.564372</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>5.936712</td>\n",
       "      <td>14.305503</td>\n",
       "      <td>-10.923644</td>\n",
       "      <td>4.596941</td>\n",
       "      <td>-2.134693</td>\n",
       "      <td>5.222218</td>\n",
       "      <td>-1.368526</td>\n",
       "      <td>-4.935497</td>\n",
       "      <td>7.297935</td>\n",
       "      <td>-9.517170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>3.340047</td>\n",
       "      <td>-11.103286</td>\n",
       "      <td>0.380939</td>\n",
       "      <td>-9.910574</td>\n",
       "      <td>5.785901</td>\n",
       "      <td>8.730873</td>\n",
       "      <td>5.895844</td>\n",
       "      <td>0.523430</td>\n",
       "      <td>-10.370501</td>\n",
       "      <td>10.999421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>8.454951</td>\n",
       "      <td>-5.118403</td>\n",
       "      <td>-11.746635</td>\n",
       "      <td>2.318626</td>\n",
       "      <td>-7.586737</td>\n",
       "      <td>12.236910</td>\n",
       "      <td>7.013754</td>\n",
       "      <td>7.383508</td>\n",
       "      <td>7.247571</td>\n",
       "      <td>-10.797413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2          3          4          5  \\\n",
       "0    6.009770  -0.116596  -3.884397   9.466447  -6.210791  10.363499   \n",
       "1   -6.041958  -7.031896  -0.035036  -6.026720   9.640221  -2.293951   \n",
       "2    0.264014   1.035362  -0.534110   5.940433   1.532756 -10.751539   \n",
       "3   -7.562168  -7.238606   8.801069 -11.129960   0.842867  -9.831837   \n",
       "4    9.696049  -6.322094  -5.523495  -0.676701   9.112110  -5.423116   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "103 -5.695566  -4.088038   7.612744  -7.422001  11.953300  -1.205127   \n",
       "104 -4.591668  -2.130538   1.383535  12.661238 -14.747499  -0.825547   \n",
       "105  5.936712  14.305503 -10.923644   4.596941  -2.134693   5.222218   \n",
       "106  3.340047 -11.103286   0.380939  -9.910574   5.785901   8.730873   \n",
       "107  8.454951  -5.118403 -11.746635   2.318626  -7.586737  12.236910   \n",
       "\n",
       "             6          7          8          9  patient number  \n",
       "0    -6.480822  -6.431981   8.317493 -10.151834               1  \n",
       "1     2.895993   5.159375   8.464264  -9.648880               0  \n",
       "2    10.312591  -0.970045 -12.190402  14.136766               2  \n",
       "3     3.189499  -5.384264  12.091418  -2.478377               1  \n",
       "4     0.220518  11.457376 -11.707994  -0.188018               0  \n",
       "..         ...        ...        ...        ...             ...  \n",
       "103   4.046550  12.574976   0.551326  -0.709395               0  \n",
       "104  -0.599415  -2.178931   9.465315   4.564372               2  \n",
       "105  -1.368526  -4.935497   7.297935  -9.517170               1  \n",
       "106   5.895844   0.523430 -10.370501  10.999421               1  \n",
       "107   7.013754   7.383508   7.247571 -10.797413               1  \n",
       "\n",
       "[108 rows x 11 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10dims=pd.concat([X_train_10dims, X_test_10dims], axis=0, ignore_index=True)\n",
    "df_10dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8da0311e-0e72-4f7a-bcb0-fccd112b0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name columns\n",
    "df_10dims.columns=['column 1','column 2','column 3','column 4','column 5','column 6','column 7','column 8','column 9','column 10', 'patient number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bee17b9b-e1a1-4738-b0e6-55295bc90649",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_10dims.to_csv('re-latent_df_train_10dims_PD_HD.csv', index = False)\n",
    "X_test_10dims.to_csv('re-latent_df_test_10dims_PD_HD.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd24eacd-1a37-4e3b-a678-6fc575693a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10dims.to_csv('re-latent_df_10dims_PD_HD.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f700baab-9556-4e09-98b8-c22b7e1c57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "315c4efc-ecad-419a-9936-5bcd8741e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split and standardalization\n",
    "x_gbm = df_10dims.drop('patient number', axis = 1).values\n",
    "y_gbm = df_10dims['patient number'].values\n",
    "sc = StandardScaler()\n",
    "sc.fit(x_gbm)\n",
    "x_gbm = sc.transform(x_gbm)\n",
    "x_train_gbm, x_test_gbm, y_train_gbm, y_test_gbm = train_test_split(x_gbm, y_gbm, test_size=0.05, shuffle = True, random_state=2022, stratify=y_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "477f0a45-5412-4492-a9a3-e6aca2185a11",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:16:52,452] A new study created in memory with name: no-name-19137843-faf3-4549-899d-8822c69d98d9\n",
      "[I 2025-05-29 19:16:53,086] Trial 0 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.30928470884258874, 'lambda_l2': 1.850069358101808e-08, 'num_leaves': 251, 'feature_fraction': 0.9306150868083642, 'bagging_fraction': 0.6849231962806424, 'bagging_freq': 6, 'min_child_samples': 57, 'learning_rate': 4.276197604181587e-05}. Best is trial 0 with value: 0.4636363636363637.\n",
      "[I 2025-05-29 19:16:53,370] Trial 1 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 2.129013287851423, 'lambda_l2': 1.8103710296725997, 'num_leaves': 196, 'feature_fraction': 0.5834289620103681, 'bagging_fraction': 0.548460311606987, 'bagging_freq': 1, 'min_child_samples': 65, 'learning_rate': 0.08139248006866365}. Best is trial 0 with value: 0.4636363636363637.\n",
      "[I 2025-05-29 19:16:53,771] Trial 2 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 2.6272723761401837e-07, 'lambda_l2': 3.2425711988261123, 'num_leaves': 42, 'feature_fraction': 0.7720035312142761, 'bagging_fraction': 0.6079808721454598, 'bagging_freq': 6, 'min_child_samples': 83, 'learning_rate': 0.012486011799412264}. Best is trial 0 with value: 0.4636363636363637.\n",
      "[I 2025-05-29 19:16:54,062] Trial 3 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.097774623145459e-05, 'lambda_l2': 2.2935677744126088e-07, 'num_leaves': 114, 'feature_fraction': 0.6864882486329218, 'bagging_fraction': 0.7413822226366302, 'bagging_freq': 7, 'min_child_samples': 59, 'learning_rate': 0.035965268411277805}. Best is trial 0 with value: 0.4636363636363637.\n",
      "[I 2025-05-29 19:17:17,486] Trial 4 finished with value: 0.5845454545454545 and parameters: {'lambda_l1': 3.820428058059873e-05, 'lambda_l2': 4.892548484294232e-07, 'num_leaves': 240, 'feature_fraction': 0.586859710260572, 'bagging_fraction': 0.47462573000816727, 'bagging_freq': 7, 'min_child_samples': 7, 'learning_rate': 0.0003956626915002697}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:17:17,860] Trial 5 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.00020019155478617632, 'lambda_l2': 1.3181630522014795e-07, 'num_leaves': 225, 'feature_fraction': 0.9903804391138187, 'bagging_fraction': 0.6495751451087, 'bagging_freq': 7, 'min_child_samples': 49, 'learning_rate': 2.2436509610318242e-05}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:17:18,106] Trial 6 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 3.5405318474307705, 'lambda_l2': 5.2727358388473736e-08, 'num_leaves': 155, 'feature_fraction': 0.6953848735588499, 'bagging_fraction': 0.7820140383061611, 'bagging_freq': 7, 'min_child_samples': 65, 'learning_rate': 0.0378661299943942}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:17:18,333] Trial 7 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 3.049447921083592, 'lambda_l2': 0.0009822465658152278, 'num_leaves': 202, 'feature_fraction': 0.875704533131082, 'bagging_fraction': 0.595863464408013, 'bagging_freq': 3, 'min_child_samples': 87, 'learning_rate': 0.0041676376984223764}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:17:18,607] Trial 8 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 2.870786336521689e-07, 'lambda_l2': 6.983258302883564e-06, 'num_leaves': 71, 'feature_fraction': 0.5112283868425918, 'bagging_fraction': 0.7350761629772635, 'bagging_freq': 7, 'min_child_samples': 47, 'learning_rate': 0.03945708642702766}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:17:19,581] Trial 9 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.5576702216475569e-07, 'lambda_l2': 0.0001600895688991504, 'num_leaves': 139, 'feature_fraction': 0.892801613907968, 'bagging_fraction': 0.875798752151715, 'bagging_freq': 2, 'min_child_samples': 100, 'learning_rate': 0.0017135151584384348}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:17:46,446] Trial 10 finished with value: 0.5290909090909091 and parameters: {'lambda_l1': 0.0063050755126526125, 'lambda_l2': 0.008382369800163915, 'num_leaves': 9, 'feature_fraction': 0.4067662890732818, 'bagging_fraction': 0.41242873284696047, 'bagging_freq': 4, 'min_child_samples': 5, 'learning_rate': 0.00021133068141627014}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:17:59,673] Trial 11 finished with value: 0.5281818181818181 and parameters: {'lambda_l1': 0.009378440018856345, 'lambda_l2': 0.011590945507764685, 'num_leaves': 11, 'feature_fraction': 0.4003713417255223, 'bagging_fraction': 0.41116814463104934, 'bagging_freq': 4, 'min_child_samples': 10, 'learning_rate': 0.0002601645390803029}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:18:22,325] Trial 12 finished with value: 0.5290909090909091 and parameters: {'lambda_l1': 0.00310423140512092, 'lambda_l2': 0.027388239402086095, 'num_leaves': 97, 'feature_fraction': 0.4088402438533489, 'bagging_fraction': 0.4099566145221485, 'bagging_freq': 5, 'min_child_samples': 7, 'learning_rate': 0.00023066747752993936}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:18:30,377] Trial 13 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 2.7860712298422645e-05, 'lambda_l2': 8.117881102556542e-06, 'num_leaves': 6, 'feature_fraction': 0.5417777214581134, 'bagging_fraction': 0.5162609274687027, 'bagging_freq': 4, 'min_child_samples': 26, 'learning_rate': 0.0002609759165547951}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:18:38,808] Trial 14 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.007387463099397026, 'lambda_l2': 9.123000738298273e-06, 'num_leaves': 167, 'feature_fraction': 0.6084396361930012, 'bagging_fraction': 0.4795037609303503, 'bagging_freq': 5, 'min_child_samples': 26, 'learning_rate': 7.974145929861082e-05}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:18:51,028] Trial 15 finished with value: 0.5581818181818181 and parameters: {'lambda_l1': 1.0680761852745522e-08, 'lambda_l2': 0.03348300215185717, 'num_leaves': 76, 'feature_fraction': 0.48292541870496, 'bagging_fraction': 0.9790947127104563, 'bagging_freq': 3, 'min_child_samples': 24, 'learning_rate': 0.000895598485499756}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:19:00,429] Trial 16 finished with value: 0.53 and parameters: {'lambda_l1': 1.4447594069377934e-08, 'lambda_l2': 0.228024335386251, 'num_leaves': 84, 'feature_fraction': 0.5078990304769874, 'bagging_fraction': 0.9912050653950967, 'bagging_freq': 2, 'min_child_samples': 25, 'learning_rate': 0.0010111684608494916}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:19:05,386] Trial 17 finished with value: 0.5754545454545454 and parameters: {'lambda_l1': 2.295218194493272e-06, 'lambda_l2': 0.00023342714412954847, 'num_leaves': 47, 'feature_fraction': 0.627809888661892, 'bagging_fraction': 0.9926309278616925, 'bagging_freq': 3, 'min_child_samples': 16, 'learning_rate': 0.005094147358180269}. Best is trial 4 with value: 0.5845454545454545.\n",
      "[I 2025-05-29 19:19:09,623] Trial 18 finished with value: 0.6209090909090909 and parameters: {'lambda_l1': 5.778625520332219e-06, 'lambda_l2': 5.538110590586628e-05, 'num_leaves': 55, 'feature_fraction': 0.7925356079918745, 'bagging_fraction': 0.8957447719053475, 'bagging_freq': 3, 'min_child_samples': 39, 'learning_rate': 0.004970028032267218}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:14,497] Trial 19 finished with value: 0.6118181818181818 and parameters: {'lambda_l1': 0.00023336569641091247, 'lambda_l2': 9.324838375476207e-07, 'num_leaves': 252, 'feature_fraction': 0.8095358644401421, 'bagging_fraction': 0.8394154433759282, 'bagging_freq': 5, 'min_child_samples': 36, 'learning_rate': 0.003285650212055651}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:17,533] Trial 20 finished with value: 0.6027272727272727 and parameters: {'lambda_l1': 0.0002697313289427787, 'lambda_l2': 4.3549733613795166e-05, 'num_leaves': 116, 'feature_fraction': 0.7941259430710288, 'bagging_fraction': 0.8541390410721605, 'bagging_freq': 5, 'min_child_samples': 36, 'learning_rate': 0.009395078089362796}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:20,323] Trial 21 finished with value: 0.5927272727272728 and parameters: {'lambda_l1': 0.0005537072733990145, 'lambda_l2': 3.729296871526769e-05, 'num_leaves': 113, 'feature_fraction': 0.8024784140849168, 'bagging_fraction': 0.8654387398599749, 'bagging_freq': 5, 'min_child_samples': 40, 'learning_rate': 0.009350115867825436}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:25,653] Trial 22 finished with value: 0.6118181818181818 and parameters: {'lambda_l1': 0.0004989364612166773, 'lambda_l2': 6.740451409451918e-07, 'num_leaves': 48, 'feature_fraction': 0.7887438124988562, 'bagging_fraction': 0.8747382611318382, 'bagging_freq': 5, 'min_child_samples': 35, 'learning_rate': 0.0026503840866630157}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:31,072] Trial 23 finished with value: 0.6027272727272727 and parameters: {'lambda_l1': 1.5668267953151885e-06, 'lambda_l2': 6.145683063142355e-07, 'num_leaves': 42, 'feature_fraction': 0.76709869230266, 'bagging_fraction': 0.9195943079222809, 'bagging_freq': 6, 'min_child_samples': 37, 'learning_rate': 0.0023180652464808212}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:32,742] Trial 24 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.05133564527231834, 'lambda_l2': 2.2912523485766485e-06, 'num_leaves': 57, 'feature_fraction': 0.8468273734760323, 'bagging_fraction': 0.8023912845436905, 'bagging_freq': 3, 'min_child_samples': 43, 'learning_rate': 0.003599781873436688}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:34,903] Trial 25 finished with value: 0.5936363636363636 and parameters: {'lambda_l1': 0.0006317553404191703, 'lambda_l2': 1.300318331533145e-06, 'num_leaves': 26, 'feature_fraction': 0.7270236328215056, 'bagging_fraction': 0.9102358728273775, 'bagging_freq': 4, 'min_child_samples': 31, 'learning_rate': 0.016102376165669074}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:44,124] Trial 26 finished with value: 0.5745454545454545 and parameters: {'lambda_l1': 6.245455442408067e-05, 'lambda_l2': 0.0012721584826463571, 'num_leaves': 93, 'feature_fraction': 0.8131885403359776, 'bagging_fraction': 0.8083090218784267, 'bagging_freq': 2, 'min_child_samples': 18, 'learning_rate': 0.0008010528864312793}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:44,707] Trial 27 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 6.2865484626067175e-06, 'lambda_l2': 2.4105729678406565e-08, 'num_leaves': 177, 'feature_fraction': 0.742819884832952, 'bagging_fraction': 0.9365164522213938, 'bagging_freq': 5, 'min_child_samples': 53, 'learning_rate': 0.0019202073231150809}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:50,161] Trial 28 finished with value: 0.6027272727272727 and parameters: {'lambda_l1': 0.053909376771611464, 'lambda_l2': 3.33425013769513e-05, 'num_leaves': 141, 'feature_fraction': 0.8430083161336677, 'bagging_fraction': 0.84087833009077, 'bagging_freq': 6, 'min_child_samples': 31, 'learning_rate': 0.006554195297578581}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:50,437] Trial 29 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.0010010218148759492, 'lambda_l2': 1.2375704675284156e-08, 'num_leaves': 68, 'feature_fraction': 0.9429013774672506, 'bagging_fraction': 0.7153198329748636, 'bagging_freq': 4, 'min_child_samples': 57, 'learning_rate': 0.01592507065875822}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:50,684] Trial 30 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.04970475010942198, 'lambda_l2': 3.2425372999322413e-06, 'num_leaves': 248, 'feature_fraction': 0.6673350316272093, 'bagging_fraction': 0.7708660611278578, 'bagging_freq': 1, 'min_child_samples': 45, 'learning_rate': 1.0227086080260025e-05}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:19:55,978] Trial 31 finished with value: 0.6118181818181818 and parameters: {'lambda_l1': 0.00021447690566104845, 'lambda_l2': 4.4721041995287647e-05, 'num_leaves': 110, 'feature_fraction': 0.8005346833119509, 'bagging_fraction': 0.8462848344167289, 'bagging_freq': 5, 'min_child_samples': 35, 'learning_rate': 0.002920501799624869}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:20:02,755] Trial 32 finished with value: 0.5845454545454545 and parameters: {'lambda_l1': 6.71542201751385e-05, 'lambda_l2': 0.0009403402248596873, 'num_leaves': 25, 'feature_fraction': 0.9040108941568231, 'bagging_fraction': 0.9389668587347739, 'bagging_freq': 5, 'min_child_samples': 34, 'learning_rate': 0.002800050158390443}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:20:11,479] Trial 33 finished with value: 0.5554545454545454 and parameters: {'lambda_l1': 0.0017341905415944369, 'lambda_l2': 7.974552315167346e-05, 'num_leaves': 206, 'feature_fraction': 0.8376829818924132, 'bagging_fraction': 0.8899294452713007, 'bagging_freq': 6, 'min_child_samples': 19, 'learning_rate': 0.001203061251456211}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:20:20,001] Trial 34 finished with value: 0.5445454545454546 and parameters: {'lambda_l1': 0.00019765766222411296, 'lambda_l2': 1.2548056833335332e-07, 'num_leaves': 57, 'feature_fraction': 0.7472971710896096, 'bagging_fraction': 0.8295421683710816, 'bagging_freq': 6, 'min_child_samples': 42, 'learning_rate': 0.0005231464543563494}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:20:20,550] Trial 35 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.1205796299998807e-05, 'lambda_l2': 1.3760626849233364e-05, 'num_leaves': 103, 'feature_fraction': 0.9283831916048317, 'bagging_fraction': 0.6828830623614096, 'bagging_freq': 4, 'min_child_samples': 52, 'learning_rate': 0.0235687992461862}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:20:20,765] Trial 36 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 1.3569011267645528e-06, 'lambda_l2': 8.88618105858093e-07, 'num_leaves': 182, 'feature_fraction': 0.6602376194973956, 'bagging_fraction': 0.9553315567834911, 'bagging_freq': 6, 'min_child_samples': 66, 'learning_rate': 0.007342017509431969}. Best is trial 18 with value: 0.6209090909090909.\n",
      "[I 2025-05-29 19:20:21,440] Trial 37 finished with value: 0.639090909090909 and parameters: {'lambda_l1': 8.875581864259021e-05, 'lambda_l2': 3.1191640875498736e-07, 'num_leaves': 27, 'feature_fraction': 0.7772991999049069, 'bagging_fraction': 0.7708461267661634, 'bagging_freq': 5, 'min_child_samples': 31, 'learning_rate': 0.063333467696555}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:22,178] Trial 38 finished with value: 0.63 and parameters: {'lambda_l1': 1.3782329945476644e-05, 'lambda_l2': 8.073706076743669e-08, 'num_leaves': 33, 'feature_fraction': 0.7161437339563709, 'bagging_fraction': 0.7572439798249784, 'bagging_freq': 3, 'min_child_samples': 30, 'learning_rate': 0.0843997317776271}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:23,033] Trial 39 finished with value: 0.6118181818181818 and parameters: {'lambda_l1': 5.47423654251388e-06, 'lambda_l2': 6.136495326074416e-08, 'num_leaves': 31, 'feature_fraction': 0.7118326852097201, 'bagging_fraction': 0.7590320307857865, 'bagging_freq': 3, 'min_child_samples': 13, 'learning_rate': 0.09218387690442582}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:23,263] Trial 40 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 4.59593833002494e-07, 'lambda_l2': 2.2011140645184292e-07, 'num_leaves': 230, 'feature_fraction': 0.7608912248354348, 'bagging_fraction': 0.6919569013553177, 'bagging_freq': 2, 'min_child_samples': 70, 'learning_rate': 0.05846120140803186}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:24,180] Trial 41 finished with value: 0.63 and parameters: {'lambda_l1': 2.5033793570523306e-05, 'lambda_l2': 2.7315035563175174e-07, 'num_leaves': 33, 'feature_fraction': 0.703251787502521, 'bagging_fraction': 0.8015798940128824, 'bagging_freq': 3, 'min_child_samples': 29, 'learning_rate': 0.05812252829454588}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:24,911] Trial 42 finished with value: 0.5927272727272727 and parameters: {'lambda_l1': 2.6568341372477392e-05, 'lambda_l2': 3.840428225530164e-08, 'num_leaves': 33, 'feature_fraction': 0.677818610063292, 'bagging_fraction': 0.6553173686280399, 'bagging_freq': 3, 'min_child_samples': 30, 'learning_rate': 0.05278302430115543}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:27,931] Trial 43 finished with value: 0.5645454545454546 and parameters: {'lambda_l1': 8.257860343138764e-05, 'lambda_l2': 2.53434555700225e-07, 'num_leaves': 22, 'feature_fraction': 0.7106533445871525, 'bagging_fraction': 0.8005687436323833, 'bagging_freq': 3, 'min_child_samples': 41, 'learning_rate': 0.02946425354984232}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:29,156] Trial 44 finished with value: 0.6018181818181818 and parameters: {'lambda_l1': 1.799122721528063e-05, 'lambda_l2': 6.195381580787437e-08, 'num_leaves': 17, 'feature_fraction': 0.6310088330184117, 'bagging_fraction': 0.755428828983744, 'bagging_freq': 2, 'min_child_samples': 20, 'learning_rate': 0.09930887642560111}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:29,869] Trial 45 finished with value: 0.6027272727272728 and parameters: {'lambda_l1': 4.745948666009494e-06, 'lambda_l2': 2.7510054194436823e-07, 'num_leaves': 2, 'feature_fraction': 0.8679832586083496, 'bagging_fraction': 0.7398151503117465, 'bagging_freq': 3, 'min_child_samples': 29, 'learning_rate': 0.05189865659603038}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:30,144] Trial 46 finished with value: 0.4636363636363637 and parameters: {'lambda_l1': 0.00010340974416712076, 'lambda_l2': 9.7483748939582, 'num_leaves': 57, 'feature_fraction': 0.7275180944801924, 'bagging_fraction': 0.7842992524448719, 'bagging_freq': 4, 'min_child_samples': 48, 'learning_rate': 0.020681314565916434}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:31,099] Trial 47 finished with value: 0.6118181818181818 and parameters: {'lambda_l1': 8.048162252571945e-08, 'lambda_l2': 3.4715108694705828e-06, 'num_leaves': 39, 'feature_fraction': 0.8314534397227116, 'bagging_fraction': 0.8223128281947326, 'bagging_freq': 1, 'min_child_samples': 38, 'learning_rate': 0.06449528991834871}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:32,131] Trial 48 finished with value: 0.6290909090909091 and parameters: {'lambda_l1': 1.6624900451910408e-05, 'lambda_l2': 9.570632277366328e-08, 'num_leaves': 68, 'feature_fraction': 0.7714457960442254, 'bagging_fraction': 0.5831992941178558, 'bagging_freq': 4, 'min_child_samples': 13, 'learning_rate': 0.03884508603859001}. Best is trial 37 with value: 0.639090909090909.\n",
      "[I 2025-05-29 19:20:33,460] Trial 49 finished with value: 0.6481818181818182 and parameters: {'lambda_l1': 4.3744841712906576e-07, 'lambda_l2': 9.329268467917373e-08, 'num_leaves': 64, 'feature_fraction': 0.6869326906948521, 'bagging_fraction': 0.5884548097825939, 'bagging_freq': 4, 'min_child_samples': 22, 'learning_rate': 0.036756542228355206}. Best is trial 49 with value: 0.6481818181818182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.6481818181818182\n",
      "  Params: \n",
      "    lambda_l1: 4.3744841712906576e-07\n",
      "    lambda_l2: 9.329268467917373e-08\n",
      "    num_leaves: 64\n",
      "    feature_fraction: 0.6869326906948521\n",
      "    bagging_fraction: 0.5884548097825939\n",
      "    bagging_freq: 4\n",
      "    min_child_samples: 22\n",
      "    learning_rate: 0.036756542228355206\n"
     ]
    }
   ],
   "source": [
    "# parameters optimization using optuna\n",
    "def objective(trial):\n",
    "    # Set LightGBM hyperparameters using Optuna\n",
    "    param = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_class': 3,\n",
    "        'num_iteration': 3000,\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    }\n",
    "\n",
    "    # Set 10-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state = 2022)\n",
    "\n",
    "    # List to save cross-validation results\n",
    "    cv_results = []\n",
    "\n",
    "    # Train and evaluate the model for each fold\n",
    "    for train_idx, valid_idx in cv.split(x_gbm, y_gbm):\n",
    "        X_train, X_valid = x_gbm[train_idx], x_gbm[valid_idx]\n",
    "        y_train, y_valid = y_gbm[train_idx], y_gbm[valid_idx]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(X_train, label = y_train)\n",
    "        lgb_valid = lgb.Dataset(X_valid, label = y_valid, reference = lgb_train)\n",
    "        \n",
    "        gbm = lgb.train(param,\n",
    "                        lgb_train,\n",
    "                        valid_sets = [lgb_train, lgb_valid],\n",
    "                        early_stopping_rounds = 100,\n",
    "                        verbose_eval = False)\n",
    "        y_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n",
    "        y_pred_max = np.argmax(y_pred, axis=1)\n",
    "        cv_results.append(accuracy_score(y_valid, y_pred_max))\n",
    "\n",
    "    # Return the minimum accuracy from cross-validation\n",
    "    return np.mean(cv_results)\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print('  Value: {}'.format(trial.value))\n",
    "\n",
    "print('  Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print('    {}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6540cab6-90db-4f76-809f-754566eedb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Tuple\n",
    "\n",
    "# multiclass_log_loss for LGBM \n",
    "\n",
    "class MultiLoglossForLGBM:\n",
    "\n",
    "    \n",
    "    def __init__(self, n_class: int=3, use_softmax: bool=True, epsilon: float=1e-32) -> None:\n",
    "        # initialize        \n",
    "        self.name = \"my_multiclass_logloss\"\n",
    "        self.n_class = n_class\n",
    "        self.prob_func = self._get_prob_value if use_softmax else lambda x: x\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def __call__(self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None) -> float:\n",
    "        #calculate loss function\n",
    "        #get prob value by softmax\n",
    "        prob = self.prob_func(preds)           # <= from logits to probability\n",
    "        #convert labels to 1-hot\n",
    "        labels = self._get_1hot_label(labels)  # <= labels (1D-array) to 1hot\n",
    "        loss_by_sample = np.sum(- np.log(prob) * labels, axis=1)\n",
    "        loss = np.average(loss_by_sample, weight)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _calc_grad_and_hess(\n",
    "        self, preds: np.ndarray, labels: np.ndarray, weight: Optional[np.ndarray]=None\n",
    "    ) -> Tuple[np.ndarray]:\n",
    "        \"\"\"Calc Grad and Hess\"\"\"\n",
    "        # # get prob value by softmax\n",
    "        prob = self.prob_func(preds)           # <= margin を確率値に直す\n",
    "        # # convert labels to 1-hot\n",
    "        labels = self._get_1hot_label(labels)  # <= labels (1D-array) to 1hot label\n",
    "\n",
    "        grad = prob - labels\n",
    "        hess = prob * (1 - prob)        \n",
    "        if weight is not None:\n",
    "            grad = grad * weight[:, None]\n",
    "            hess = hess * weight[:, None]\n",
    "        return grad, hess\n",
    "    \n",
    "    def return_loss(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[str, float, bool]:\n",
    "        \"\"\"Return Loss for lightgbm\"\"\"\n",
    "        labels = data.get_label()\n",
    "        weight = data.get_weight()\n",
    "        n_example = len(labels)\n",
    "        \n",
    "        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n",
    "        preds = preds.reshape(self.n_class, n_example).T  # <= preds (1D-array) to 2D-array \n",
    "        # # calc loss\n",
    "        loss = self(preds, labels, weight)\n",
    "        \n",
    "        return self.name, loss, False\n",
    "    \n",
    "    def return_grad_and_hess(self, preds: np.ndarray, data: lgb.Dataset) -> Tuple[np.ndarray]:\n",
    "        \"\"\"Return Grad and Hess for lightgbm\"\"\"\n",
    "        labels = data.get_label()\n",
    "        weight = data.get_weight()\n",
    "        n_example = len(labels)\n",
    "        \n",
    "        # # reshape preds: (n_class * n_example,) => (n_class, n_example) =>  (n_example, n_class)\n",
    "        preds = preds.reshape(self.n_class, n_example).T  # <= preds (1D-array) to 2D-array \n",
    "        # # calc grad and hess.\n",
    "        grad, hess =  self._calc_grad_and_hess(preds, labels, weight)\n",
    "\n",
    "        # # reshape grad, hess: (n_example, n_class) => (n_class, n_example) => (n_class * n_example,) \n",
    "        grad = grad.T.reshape(n_example * self.n_class)   # <= return 1D-array \n",
    "        hess = hess.T.reshape(n_example * self.n_class)   # <= return 1D-array \n",
    "        \n",
    "        return grad, hess\n",
    "    \n",
    "    def softmax(x):\n",
    "        return np.exp(x)/np.sum(np.exp(x))\n",
    "    \n",
    "    def _get_prob_value(self, preds: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert Margin(Logit) to Prob by Softmax.\"\"\"\n",
    "        upper = np.exp(preds)\n",
    "        prob = upper / np.sum(upper, axis=1, keepdims=True)\n",
    "        prob = np.clip(prob, self.epsilon, 1 - self.epsilon)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _get_1hot_label(self, labels: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert labels to 1hot array.\"\"\"\n",
    "        n_example = len(labels)\n",
    "        #make a matrix here\n",
    "        onehot = np.zeros((n_example, self.n_class))\n",
    "        #setting overlap\n",
    "        original_array=np.array([[0.95, 0.05, 0],\n",
    "                                 [0.05, 0.90, 0.05],\n",
    "                                 [0, 0.05, 0.95]])\n",
    "        for index, j in enumerate(labels):\n",
    "            if j==0:\n",
    "                onehot[index]=original_array[0]\n",
    "            elif j==1:\n",
    "                onehot[index]=original_array[1]\n",
    "            else:\n",
    "                onehot[index]=original_array[2]\n",
    "        \n",
    "        return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58f99dd2-4a1d-4933-bd7a-b88ef1bb7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(df): \n",
    "    n_features = len(df)\n",
    "    df_plot = df.sort_values('importance')\n",
    "    f_importance_plot = df_plot['importance'].values\n",
    "    plt.barh(range(n_features), f_importance_plot, align='center')\n",
    "    cols_plot = df_plot['feature'].values             \n",
    "    plt.yticks(np.arange(n_features), cols_plot)      \n",
    "    plt.xlabel('Feature importance')                  \n",
    "    plt.ylabel('Feature') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed5ef12c-6633-4cd1-b69b-e27b17849381",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- 0 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05659\tvalid's multi_logloss: 1.06434\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.04919\tvalid's multi_logloss: 1.06679\n",
      "[3]\ttrain's multi_logloss: 1.04185\tvalid's multi_logloss: 1.06375\n",
      "[4]\ttrain's multi_logloss: 1.03494\tvalid's multi_logloss: 1.06101\n",
      "[5]\ttrain's multi_logloss: 1.02799\tvalid's multi_logloss: 1.06067\n",
      "[6]\ttrain's multi_logloss: 1.02404\tvalid's multi_logloss: 1.0615\n",
      "[7]\ttrain's multi_logloss: 1.01549\tvalid's multi_logloss: 1.05409\n",
      "[8]\ttrain's multi_logloss: 1.00725\tvalid's multi_logloss: 1.04604\n",
      "[9]\ttrain's multi_logloss: 0.999622\tvalid's multi_logloss: 1.03797\n",
      "[10]\ttrain's multi_logloss: 0.994275\tvalid's multi_logloss: 1.03411\n",
      "[11]\ttrain's multi_logloss: 0.987357\tvalid's multi_logloss: 1.02665\n",
      "[12]\ttrain's multi_logloss: 0.980886\tvalid's multi_logloss: 1.01957\n",
      "[13]\ttrain's multi_logloss: 0.976218\tvalid's multi_logloss: 1.01642\n",
      "[14]\ttrain's multi_logloss: 0.97282\tvalid's multi_logloss: 1.01585\n",
      "[15]\ttrain's multi_logloss: 0.969778\tvalid's multi_logloss: 1.01554\n",
      "[16]\ttrain's multi_logloss: 0.965538\tvalid's multi_logloss: 1.0106\n",
      "[17]\ttrain's multi_logloss: 0.958215\tvalid's multi_logloss: 1.00064\n",
      "[18]\ttrain's multi_logloss: 0.951697\tvalid's multi_logloss: 0.989695\n",
      "[19]\ttrain's multi_logloss: 0.945979\tvalid's multi_logloss: 0.980892\n",
      "[20]\ttrain's multi_logloss: 0.939991\tvalid's multi_logloss: 0.972423\n",
      "[21]\ttrain's multi_logloss: 0.934765\tvalid's multi_logloss: 0.96687\n",
      "[22]\ttrain's multi_logloss: 0.930213\tvalid's multi_logloss: 0.963463\n",
      "[23]\ttrain's multi_logloss: 0.926328\tvalid's multi_logloss: 0.966288\n",
      "[24]\ttrain's multi_logloss: 0.922122\tvalid's multi_logloss: 0.963183\n",
      "[25]\ttrain's multi_logloss: 0.918252\tvalid's multi_logloss: 0.96027\n",
      "[26]\ttrain's multi_logloss: 0.914085\tvalid's multi_logloss: 0.95749\n",
      "[27]\ttrain's multi_logloss: 0.910198\tvalid's multi_logloss: 0.954972\n",
      "[28]\ttrain's multi_logloss: 0.907026\tvalid's multi_logloss: 0.95787\n",
      "[29]\ttrain's multi_logloss: 0.903098\tvalid's multi_logloss: 0.95247\n",
      "[30]\ttrain's multi_logloss: 0.899484\tvalid's multi_logloss: 0.948147\n",
      "[31]\ttrain's multi_logloss: 0.8945\tvalid's multi_logloss: 0.943165\n",
      "[32]\ttrain's multi_logloss: 0.889976\tvalid's multi_logloss: 0.938618\n",
      "[33]\ttrain's multi_logloss: 0.884086\tvalid's multi_logloss: 0.930462\n",
      "[34]\ttrain's multi_logloss: 0.881455\tvalid's multi_logloss: 0.925759\n",
      "[35]\ttrain's multi_logloss: 0.876257\tvalid's multi_logloss: 0.918366\n",
      "[36]\ttrain's multi_logloss: 0.872972\tvalid's multi_logloss: 0.910435\n",
      "[37]\ttrain's multi_logloss: 0.86857\tvalid's multi_logloss: 0.909833\n",
      "[38]\ttrain's multi_logloss: 0.864872\tvalid's multi_logloss: 0.908764\n",
      "[39]\ttrain's multi_logloss: 0.861493\tvalid's multi_logloss: 0.907914\n",
      "[40]\ttrain's multi_logloss: 0.858783\tvalid's multi_logloss: 0.908867\n",
      "[41]\ttrain's multi_logloss: 0.855137\tvalid's multi_logloss: 0.90776\n",
      "[42]\ttrain's multi_logloss: 0.852032\tvalid's multi_logloss: 0.90332\n",
      "[43]\ttrain's multi_logloss: 0.849996\tvalid's multi_logloss: 0.903888\n",
      "[44]\ttrain's multi_logloss: 0.847183\tvalid's multi_logloss: 0.903612\n",
      "[45]\ttrain's multi_logloss: 0.843867\tvalid's multi_logloss: 0.897396\n",
      "[46]\ttrain's multi_logloss: 0.84052\tvalid's multi_logloss: 0.893286\n",
      "[47]\ttrain's multi_logloss: 0.837186\tvalid's multi_logloss: 0.887789\n",
      "[48]\ttrain's multi_logloss: 0.833648\tvalid's multi_logloss: 0.881662\n",
      "[49]\ttrain's multi_logloss: 0.829989\tvalid's multi_logloss: 0.879754\n",
      "[50]\ttrain's multi_logloss: 0.826242\tvalid's multi_logloss: 0.883792\n",
      "[51]\ttrain's multi_logloss: 0.82345\tvalid's multi_logloss: 0.885646\n",
      "[52]\ttrain's multi_logloss: 0.820873\tvalid's multi_logloss: 0.888431\n",
      "[53]\ttrain's multi_logloss: 0.817218\tvalid's multi_logloss: 0.883944\n",
      "[54]\ttrain's multi_logloss: 0.813921\tvalid's multi_logloss: 0.879867\n",
      "[55]\ttrain's multi_logloss: 0.811055\tvalid's multi_logloss: 0.879175\n",
      "[56]\ttrain's multi_logloss: 0.808815\tvalid's multi_logloss: 0.877872\n",
      "[57]\ttrain's multi_logloss: 0.806031\tvalid's multi_logloss: 0.8827\n",
      "[58]\ttrain's multi_logloss: 0.803141\tvalid's multi_logloss: 0.884429\n",
      "[59]\ttrain's multi_logloss: 0.800125\tvalid's multi_logloss: 0.886429\n",
      "[60]\ttrain's multi_logloss: 0.797284\tvalid's multi_logloss: 0.888471\n",
      "[61]\ttrain's multi_logloss: 0.793797\tvalid's multi_logloss: 0.889374\n",
      "[62]\ttrain's multi_logloss: 0.791774\tvalid's multi_logloss: 0.892303\n",
      "[63]\ttrain's multi_logloss: 0.790055\tvalid's multi_logloss: 0.895508\n",
      "[64]\ttrain's multi_logloss: 0.78736\tvalid's multi_logloss: 0.898871\n",
      "[65]\ttrain's multi_logloss: 0.784825\tvalid's multi_logloss: 0.901211\n",
      "[66]\ttrain's multi_logloss: 0.78247\tvalid's multi_logloss: 0.898631\n",
      "[67]\ttrain's multi_logloss: 0.780759\tvalid's multi_logloss: 0.898226\n",
      "[68]\ttrain's multi_logloss: 0.779362\tvalid's multi_logloss: 0.894528\n",
      "[69]\ttrain's multi_logloss: 0.77652\tvalid's multi_logloss: 0.896055\n",
      "[70]\ttrain's multi_logloss: 0.774017\tvalid's multi_logloss: 0.899782\n",
      "[71]\ttrain's multi_logloss: 0.772263\tvalid's multi_logloss: 0.899939\n",
      "[72]\ttrain's multi_logloss: 0.770486\tvalid's multi_logloss: 0.904156\n",
      "[73]\ttrain's multi_logloss: 0.76751\tvalid's multi_logloss: 0.90088\n",
      "[74]\ttrain's multi_logloss: 0.764601\tvalid's multi_logloss: 0.902132\n",
      "[75]\ttrain's multi_logloss: 0.762584\tvalid's multi_logloss: 0.898938\n",
      "[76]\ttrain's multi_logloss: 0.760332\tvalid's multi_logloss: 0.897276\n",
      "[77]\ttrain's multi_logloss: 0.758724\tvalid's multi_logloss: 0.896148\n",
      "[78]\ttrain's multi_logloss: 0.757311\tvalid's multi_logloss: 0.895256\n",
      "[79]\ttrain's multi_logloss: 0.75573\tvalid's multi_logloss: 0.891742\n",
      "[80]\ttrain's multi_logloss: 0.755088\tvalid's multi_logloss: 0.891079\n",
      "[81]\ttrain's multi_logloss: 0.753116\tvalid's multi_logloss: 0.896679\n",
      "[82]\ttrain's multi_logloss: 0.751357\tvalid's multi_logloss: 0.902294\n",
      "[83]\ttrain's multi_logloss: 0.749854\tvalid's multi_logloss: 0.904493\n",
      "[84]\ttrain's multi_logloss: 0.747653\tvalid's multi_logloss: 0.905062\n",
      "[85]\ttrain's multi_logloss: 0.745234\tvalid's multi_logloss: 0.908954\n",
      "[86]\ttrain's multi_logloss: 0.743964\tvalid's multi_logloss: 0.914508\n",
      "[87]\ttrain's multi_logloss: 0.742082\tvalid's multi_logloss: 0.920338\n",
      "[88]\ttrain's multi_logloss: 0.74066\tvalid's multi_logloss: 0.925983\n",
      "[89]\ttrain's multi_logloss: 0.737492\tvalid's multi_logloss: 0.922118\n",
      "[90]\ttrain's multi_logloss: 0.735134\tvalid's multi_logloss: 0.919878\n",
      "[91]\ttrain's multi_logloss: 0.732979\tvalid's multi_logloss: 0.917958\n",
      "[92]\ttrain's multi_logloss: 0.731262\tvalid's multi_logloss: 0.916954\n",
      "[93]\ttrain's multi_logloss: 0.728842\tvalid's multi_logloss: 0.919747\n",
      "[94]\ttrain's multi_logloss: 0.72683\tvalid's multi_logloss: 0.921482\n",
      "[95]\ttrain's multi_logloss: 0.724957\tvalid's multi_logloss: 0.919788\n",
      "[96]\ttrain's multi_logloss: 0.722683\tvalid's multi_logloss: 0.929118\n",
      "[97]\ttrain's multi_logloss: 0.721295\tvalid's multi_logloss: 0.928809\n",
      "[98]\ttrain's multi_logloss: 0.71999\tvalid's multi_logloss: 0.930465\n",
      "[99]\ttrain's multi_logloss: 0.718391\tvalid's multi_logloss: 0.935456\n",
      "[100]\ttrain's multi_logloss: 0.716891\tvalid's multi_logloss: 0.939678\n",
      "[101]\ttrain's multi_logloss: 0.715772\tvalid's multi_logloss: 0.941079\n",
      "[102]\ttrain's multi_logloss: 0.714163\tvalid's multi_logloss: 0.939379\n",
      "[103]\ttrain's multi_logloss: 0.712723\tvalid's multi_logloss: 0.937867\n",
      "[104]\ttrain's multi_logloss: 0.711457\tvalid's multi_logloss: 0.938197\n",
      "[105]\ttrain's multi_logloss: 0.709242\tvalid's multi_logloss: 0.937857\n",
      "[106]\ttrain's multi_logloss: 0.707627\tvalid's multi_logloss: 0.940024\n",
      "[107]\ttrain's multi_logloss: 0.706038\tvalid's multi_logloss: 0.940648\n",
      "[108]\ttrain's multi_logloss: 0.705559\tvalid's multi_logloss: 0.941151\n",
      "[109]\ttrain's multi_logloss: 0.703881\tvalid's multi_logloss: 0.940281\n",
      "[110]\ttrain's multi_logloss: 0.702341\tvalid's multi_logloss: 0.939552\n",
      "[111]\ttrain's multi_logloss: 0.700934\tvalid's multi_logloss: 0.938953\n",
      "[112]\ttrain's multi_logloss: 0.699649\tvalid's multi_logloss: 0.938478\n",
      "[113]\ttrain's multi_logloss: 0.698565\tvalid's multi_logloss: 0.936518\n",
      "[114]\ttrain's multi_logloss: 0.697382\tvalid's multi_logloss: 0.932396\n",
      "[115]\ttrain's multi_logloss: 0.696276\tvalid's multi_logloss: 0.929535\n",
      "[116]\ttrain's multi_logloss: 0.695509\tvalid's multi_logloss: 0.928008\n",
      "[117]\ttrain's multi_logloss: 0.693898\tvalid's multi_logloss: 0.927345\n",
      "[118]\ttrain's multi_logloss: 0.692468\tvalid's multi_logloss: 0.926815\n",
      "[119]\ttrain's multi_logloss: 0.691185\tvalid's multi_logloss: 0.92895\n",
      "[120]\ttrain's multi_logloss: 0.689684\tvalid's multi_logloss: 0.93011\n",
      "[121]\ttrain's multi_logloss: 0.686904\tvalid's multi_logloss: 0.933947\n",
      "[122]\ttrain's multi_logloss: 0.683574\tvalid's multi_logloss: 0.940534\n",
      "[123]\ttrain's multi_logloss: 0.680573\tvalid's multi_logloss: 0.945612\n",
      "[124]\ttrain's multi_logloss: 0.677704\tvalid's multi_logloss: 0.952319\n",
      "[125]\ttrain's multi_logloss: 0.675576\tvalid's multi_logloss: 0.952058\n",
      "[126]\ttrain's multi_logloss: 0.673716\tvalid's multi_logloss: 0.954041\n",
      "[127]\ttrain's multi_logloss: 0.672408\tvalid's multi_logloss: 0.960093\n",
      "[128]\ttrain's multi_logloss: 0.671367\tvalid's multi_logloss: 0.966136\n",
      "[129]\ttrain's multi_logloss: 0.670484\tvalid's multi_logloss: 0.967583\n",
      "[130]\ttrain's multi_logloss: 0.669366\tvalid's multi_logloss: 0.966584\n",
      "[131]\ttrain's multi_logloss: 0.668362\tvalid's multi_logloss: 0.966242\n",
      "[132]\ttrain's multi_logloss: 0.667431\tvalid's multi_logloss: 0.965077\n",
      "[133]\ttrain's multi_logloss: 0.666708\tvalid's multi_logloss: 0.967431\n",
      "[134]\ttrain's multi_logloss: 0.665863\tvalid's multi_logloss: 0.967611\n",
      "[135]\ttrain's multi_logloss: 0.664808\tvalid's multi_logloss: 0.971868\n",
      "[136]\ttrain's multi_logloss: 0.663894\tvalid's multi_logloss: 0.97441\n",
      "[137]\ttrain's multi_logloss: 0.662547\tvalid's multi_logloss: 0.976337\n",
      "[138]\ttrain's multi_logloss: 0.661298\tvalid's multi_logloss: 0.974385\n",
      "[139]\ttrain's multi_logloss: 0.659987\tvalid's multi_logloss: 0.97698\n",
      "[140]\ttrain's multi_logloss: 0.65888\tvalid's multi_logloss: 0.978723\n",
      "[141]\ttrain's multi_logloss: 0.657813\tvalid's multi_logloss: 0.975438\n",
      "[142]\ttrain's multi_logloss: 0.657063\tvalid's multi_logloss: 0.972499\n",
      "[143]\ttrain's multi_logloss: 0.656273\tvalid's multi_logloss: 0.969566\n",
      "[144]\ttrain's multi_logloss: 0.655566\tvalid's multi_logloss: 0.969544\n",
      "[145]\ttrain's multi_logloss: 0.654411\tvalid's multi_logloss: 0.970089\n",
      "[146]\ttrain's multi_logloss: 0.653484\tvalid's multi_logloss: 0.971078\n",
      "[147]\ttrain's multi_logloss: 0.652627\tvalid's multi_logloss: 0.973426\n",
      "[148]\ttrain's multi_logloss: 0.651928\tvalid's multi_logloss: 0.976778\n",
      "[149]\ttrain's multi_logloss: 0.650113\tvalid's multi_logloss: 0.977257\n",
      "[150]\ttrain's multi_logloss: 0.648121\tvalid's multi_logloss: 0.983371\n",
      "[151]\ttrain's multi_logloss: 0.646484\tvalid's multi_logloss: 0.983919\n",
      "[152]\ttrain's multi_logloss: 0.644954\tvalid's multi_logloss: 0.984506\n",
      "[153]\ttrain's multi_logloss: 0.643998\tvalid's multi_logloss: 0.984461\n",
      "[154]\ttrain's multi_logloss: 0.643157\tvalid's multi_logloss: 0.983885\n",
      "[155]\ttrain's multi_logloss: 0.641937\tvalid's multi_logloss: 0.986543\n",
      "[156]\ttrain's multi_logloss: 0.641151\tvalid's multi_logloss: 0.987384\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttrain's multi_logloss: 0.808815\tvalid's multi_logloss: 0.877872\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.5454545454545454\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.103502\n",
      "1   feature2    0.156318\n",
      "2   feature3    0.023790\n",
      "3   feature4    0.047034\n",
      "4   feature5    0.143839\n",
      "5   feature6    0.056386\n",
      "6   feature7    0.046740\n",
      "7   feature8    0.171052\n",
      "8   feature9    0.179549\n",
      "9  feature10    0.071788\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.0864\tvalid's my_multiclass_logloss: 1.08938\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07664\tvalid's my_multiclass_logloss: 1.08849\n",
      "[3]\ttrain's my_multiclass_logloss: 1.06499\tvalid's my_multiclass_logloss: 1.0875\n",
      "[4]\ttrain's my_multiclass_logloss: 1.05731\tvalid's my_multiclass_logloss: 1.08838\n",
      "[5]\ttrain's my_multiclass_logloss: 1.04714\tvalid's my_multiclass_logloss: 1.07953\n",
      "[6]\ttrain's my_multiclass_logloss: 1.03536\tvalid's my_multiclass_logloss: 1.06893\n",
      "[7]\ttrain's my_multiclass_logloss: 1.02927\tvalid's my_multiclass_logloss: 1.06948\n",
      "[8]\ttrain's my_multiclass_logloss: 1.02032\tvalid's my_multiclass_logloss: 1.06064\n",
      "[9]\ttrain's my_multiclass_logloss: 1.01301\tvalid's my_multiclass_logloss: 1.05696\n",
      "[10]\ttrain's my_multiclass_logloss: 1.00532\tvalid's my_multiclass_logloss: 1.049\n",
      "[11]\ttrain's my_multiclass_logloss: 0.998552\tvalid's my_multiclass_logloss: 1.0424\n",
      "[12]\ttrain's my_multiclass_logloss: 0.992611\tvalid's my_multiclass_logloss: 1.03781\n",
      "[13]\ttrain's my_multiclass_logloss: 0.986466\tvalid's my_multiclass_logloss: 1.03768\n",
      "[14]\ttrain's my_multiclass_logloss: 0.97989\tvalid's my_multiclass_logloss: 1.02927\n",
      "[15]\ttrain's my_multiclass_logloss: 0.973958\tvalid's my_multiclass_logloss: 1.02156\n",
      "[16]\ttrain's my_multiclass_logloss: 0.96865\tvalid's my_multiclass_logloss: 1.01338\n",
      "[17]\ttrain's my_multiclass_logloss: 0.961385\tvalid's my_multiclass_logloss: 1.00411\n",
      "[18]\ttrain's my_multiclass_logloss: 0.955237\tvalid's my_multiclass_logloss: 0.992875\n",
      "[19]\ttrain's my_multiclass_logloss: 0.94925\tvalid's my_multiclass_logloss: 0.984614\n",
      "[20]\ttrain's my_multiclass_logloss: 0.94395\tvalid's my_multiclass_logloss: 0.975456\n",
      "[21]\ttrain's my_multiclass_logloss: 0.938762\tvalid's my_multiclass_logloss: 0.972117\n",
      "[22]\ttrain's my_multiclass_logloss: 0.934493\tvalid's my_multiclass_logloss: 0.97292\n",
      "[23]\ttrain's my_multiclass_logloss: 0.930688\tvalid's my_multiclass_logloss: 0.968264\n",
      "[24]\ttrain's my_multiclass_logloss: 0.92637\tvalid's my_multiclass_logloss: 0.965014\n",
      "[25]\ttrain's my_multiclass_logloss: 0.921668\tvalid's my_multiclass_logloss: 0.958897\n",
      "[26]\ttrain's my_multiclass_logloss: 0.918449\tvalid's my_multiclass_logloss: 0.964546\n",
      "[27]\ttrain's my_multiclass_logloss: 0.915181\tvalid's my_multiclass_logloss: 0.96399\n",
      "[28]\ttrain's my_multiclass_logloss: 0.911376\tvalid's my_multiclass_logloss: 0.961756\n",
      "[29]\ttrain's my_multiclass_logloss: 0.90785\tvalid's my_multiclass_logloss: 0.960349\n",
      "[30]\ttrain's my_multiclass_logloss: 0.903847\tvalid's my_multiclass_logloss: 0.956921\n",
      "[31]\ttrain's my_multiclass_logloss: 0.899745\tvalid's my_multiclass_logloss: 0.95266\n",
      "[32]\ttrain's my_multiclass_logloss: 0.896686\tvalid's my_multiclass_logloss: 0.949925\n",
      "[33]\ttrain's my_multiclass_logloss: 0.891705\tvalid's my_multiclass_logloss: 0.943995\n",
      "[34]\ttrain's my_multiclass_logloss: 0.889127\tvalid's my_multiclass_logloss: 0.939685\n",
      "[35]\ttrain's my_multiclass_logloss: 0.886281\tvalid's my_multiclass_logloss: 0.933007\n",
      "[36]\ttrain's my_multiclass_logloss: 0.882715\tvalid's my_multiclass_logloss: 0.931384\n",
      "[37]\ttrain's my_multiclass_logloss: 0.879993\tvalid's my_multiclass_logloss: 0.932003\n",
      "[38]\ttrain's my_multiclass_logloss: 0.876968\tvalid's my_multiclass_logloss: 0.931334\n",
      "[39]\ttrain's my_multiclass_logloss: 0.874695\tvalid's my_multiclass_logloss: 0.936294\n",
      "[40]\ttrain's my_multiclass_logloss: 0.872959\tvalid's my_multiclass_logloss: 0.938411\n",
      "[41]\ttrain's my_multiclass_logloss: 0.870704\tvalid's my_multiclass_logloss: 0.939671\n",
      "[42]\ttrain's my_multiclass_logloss: 0.868827\tvalid's my_multiclass_logloss: 0.941224\n",
      "[43]\ttrain's my_multiclass_logloss: 0.867704\tvalid's my_multiclass_logloss: 0.944581\n",
      "[44]\ttrain's my_multiclass_logloss: 0.864999\tvalid's my_multiclass_logloss: 0.94614\n",
      "[45]\ttrain's my_multiclass_logloss: 0.861513\tvalid's my_multiclass_logloss: 0.941164\n",
      "[46]\ttrain's my_multiclass_logloss: 0.859121\tvalid's my_multiclass_logloss: 0.935801\n",
      "[47]\ttrain's my_multiclass_logloss: 0.856088\tvalid's my_multiclass_logloss: 0.930003\n",
      "[48]\ttrain's my_multiclass_logloss: 0.853854\tvalid's my_multiclass_logloss: 0.925361\n",
      "[49]\ttrain's my_multiclass_logloss: 0.851935\tvalid's my_multiclass_logloss: 0.924584\n",
      "[50]\ttrain's my_multiclass_logloss: 0.848369\tvalid's my_multiclass_logloss: 0.930897\n",
      "[51]\ttrain's my_multiclass_logloss: 0.845165\tvalid's my_multiclass_logloss: 0.937163\n",
      "[52]\ttrain's my_multiclass_logloss: 0.84151\tvalid's my_multiclass_logloss: 0.942231\n",
      "[53]\ttrain's my_multiclass_logloss: 0.837694\tvalid's my_multiclass_logloss: 0.938932\n",
      "[54]\ttrain's my_multiclass_logloss: 0.834724\tvalid's my_multiclass_logloss: 0.941507\n",
      "[55]\ttrain's my_multiclass_logloss: 0.831803\tvalid's my_multiclass_logloss: 0.939056\n",
      "[56]\ttrain's my_multiclass_logloss: 0.830092\tvalid's my_multiclass_logloss: 0.941707\n",
      "[57]\ttrain's my_multiclass_logloss: 0.827419\tvalid's my_multiclass_logloss: 0.943362\n",
      "[58]\ttrain's my_multiclass_logloss: 0.824578\tvalid's my_multiclass_logloss: 0.942954\n",
      "[59]\ttrain's my_multiclass_logloss: 0.822152\tvalid's my_multiclass_logloss: 0.945398\n",
      "[60]\ttrain's my_multiclass_logloss: 0.82012\tvalid's my_multiclass_logloss: 0.943468\n",
      "[61]\ttrain's my_multiclass_logloss: 0.817335\tvalid's my_multiclass_logloss: 0.943399\n",
      "[62]\ttrain's my_multiclass_logloss: 0.814806\tvalid's my_multiclass_logloss: 0.947623\n",
      "[63]\ttrain's my_multiclass_logloss: 0.812362\tvalid's my_multiclass_logloss: 0.953064\n",
      "[64]\ttrain's my_multiclass_logloss: 0.809857\tvalid's my_multiclass_logloss: 0.955125\n",
      "[65]\ttrain's my_multiclass_logloss: 0.807751\tvalid's my_multiclass_logloss: 0.952918\n",
      "[66]\ttrain's my_multiclass_logloss: 0.806536\tvalid's my_multiclass_logloss: 0.949452\n",
      "[67]\ttrain's my_multiclass_logloss: 0.805442\tvalid's my_multiclass_logloss: 0.94635\n",
      "[68]\ttrain's my_multiclass_logloss: 0.803651\tvalid's my_multiclass_logloss: 0.944824\n",
      "[69]\ttrain's my_multiclass_logloss: 0.801808\tvalid's my_multiclass_logloss: 0.949649\n",
      "[70]\ttrain's my_multiclass_logloss: 0.799143\tvalid's my_multiclass_logloss: 0.954164\n",
      "[71]\ttrain's my_multiclass_logloss: 0.79772\tvalid's my_multiclass_logloss: 0.958957\n",
      "[72]\ttrain's my_multiclass_logloss: 0.795447\tvalid's my_multiclass_logloss: 0.966183\n",
      "[73]\ttrain's my_multiclass_logloss: 0.793119\tvalid's my_multiclass_logloss: 0.967198\n",
      "[74]\ttrain's my_multiclass_logloss: 0.790885\tvalid's my_multiclass_logloss: 0.968567\n",
      "[75]\ttrain's my_multiclass_logloss: 0.788358\tvalid's my_multiclass_logloss: 0.971295\n",
      "[76]\ttrain's my_multiclass_logloss: 0.786188\tvalid's my_multiclass_logloss: 0.9689\n",
      "[77]\ttrain's my_multiclass_logloss: 0.784431\tvalid's my_multiclass_logloss: 0.971154\n",
      "[78]\ttrain's my_multiclass_logloss: 0.783187\tvalid's my_multiclass_logloss: 0.969855\n",
      "[79]\ttrain's my_multiclass_logloss: 0.782053\tvalid's my_multiclass_logloss: 0.967407\n",
      "[80]\ttrain's my_multiclass_logloss: 0.781042\tvalid's my_multiclass_logloss: 0.96892\n",
      "[81]\ttrain's my_multiclass_logloss: 0.778414\tvalid's my_multiclass_logloss: 0.971499\n",
      "[82]\ttrain's my_multiclass_logloss: 0.776675\tvalid's my_multiclass_logloss: 0.975277\n",
      "[83]\ttrain's my_multiclass_logloss: 0.775524\tvalid's my_multiclass_logloss: 0.979652\n",
      "[84]\ttrain's my_multiclass_logloss: 0.77362\tvalid's my_multiclass_logloss: 0.981024\n",
      "[85]\ttrain's my_multiclass_logloss: 0.77201\tvalid's my_multiclass_logloss: 0.986848\n",
      "[86]\ttrain's my_multiclass_logloss: 0.771045\tvalid's my_multiclass_logloss: 0.995572\n",
      "[87]\ttrain's my_multiclass_logloss: 0.769769\tvalid's my_multiclass_logloss: 1.00372\n",
      "[88]\ttrain's my_multiclass_logloss: 0.767866\tvalid's my_multiclass_logloss: 1.00883\n",
      "[89]\ttrain's my_multiclass_logloss: 0.765222\tvalid's my_multiclass_logloss: 1.00628\n",
      "[90]\ttrain's my_multiclass_logloss: 0.763037\tvalid's my_multiclass_logloss: 1.00408\n",
      "[91]\ttrain's my_multiclass_logloss: 0.761459\tvalid's my_multiclass_logloss: 1.00254\n",
      "[92]\ttrain's my_multiclass_logloss: 0.760093\tvalid's my_multiclass_logloss: 1.00357\n",
      "[93]\ttrain's my_multiclass_logloss: 0.757757\tvalid's my_multiclass_logloss: 1.00858\n",
      "[94]\ttrain's my_multiclass_logloss: 0.755468\tvalid's my_multiclass_logloss: 1.00949\n",
      "[95]\ttrain's my_multiclass_logloss: 0.753235\tvalid's my_multiclass_logloss: 1.01527\n",
      "[96]\ttrain's my_multiclass_logloss: 0.751301\tvalid's my_multiclass_logloss: 1.01454\n",
      "[97]\ttrain's my_multiclass_logloss: 0.750061\tvalid's my_multiclass_logloss: 1.01559\n",
      "[98]\ttrain's my_multiclass_logloss: 0.748655\tvalid's my_multiclass_logloss: 1.01639\n",
      "[99]\ttrain's my_multiclass_logloss: 0.747786\tvalid's my_multiclass_logloss: 1.01656\n",
      "[100]\ttrain's my_multiclass_logloss: 0.746769\tvalid's my_multiclass_logloss: 1.01667\n",
      "[101]\ttrain's my_multiclass_logloss: 0.745925\tvalid's my_multiclass_logloss: 1.01985\n",
      "[102]\ttrain's my_multiclass_logloss: 0.745392\tvalid's my_multiclass_logloss: 1.02128\n",
      "[103]\ttrain's my_multiclass_logloss: 0.74528\tvalid's my_multiclass_logloss: 1.02349\n",
      "[104]\ttrain's my_multiclass_logloss: 0.744962\tvalid's my_multiclass_logloss: 1.02513\n",
      "[105]\ttrain's my_multiclass_logloss: 0.743215\tvalid's my_multiclass_logloss: 1.02556\n",
      "[106]\ttrain's my_multiclass_logloss: 0.741329\tvalid's my_multiclass_logloss: 1.0239\n",
      "[107]\ttrain's my_multiclass_logloss: 0.740159\tvalid's my_multiclass_logloss: 1.02544\n",
      "[108]\ttrain's my_multiclass_logloss: 0.739265\tvalid's my_multiclass_logloss: 1.02521\n",
      "[109]\ttrain's my_multiclass_logloss: 0.738205\tvalid's my_multiclass_logloss: 1.02145\n",
      "[110]\ttrain's my_multiclass_logloss: 0.736962\tvalid's my_multiclass_logloss: 1.02114\n",
      "[111]\ttrain's my_multiclass_logloss: 0.736292\tvalid's my_multiclass_logloss: 1.0227\n",
      "[112]\ttrain's my_multiclass_logloss: 0.735757\tvalid's my_multiclass_logloss: 1.02432\n",
      "[113]\ttrain's my_multiclass_logloss: 0.73444\tvalid's my_multiclass_logloss: 1.02092\n",
      "[114]\ttrain's my_multiclass_logloss: 0.733613\tvalid's my_multiclass_logloss: 1.01899\n",
      "[115]\ttrain's my_multiclass_logloss: 0.732681\tvalid's my_multiclass_logloss: 1.0161\n",
      "[116]\ttrain's my_multiclass_logloss: 0.731783\tvalid's my_multiclass_logloss: 1.0125\n",
      "[117]\ttrain's my_multiclass_logloss: 0.730404\tvalid's my_multiclass_logloss: 1.01238\n",
      "[118]\ttrain's my_multiclass_logloss: 0.729523\tvalid's my_multiclass_logloss: 1.01201\n",
      "[119]\ttrain's my_multiclass_logloss: 0.728409\tvalid's my_multiclass_logloss: 1.01611\n",
      "[120]\ttrain's my_multiclass_logloss: 0.727862\tvalid's my_multiclass_logloss: 1.01651\n",
      "[121]\ttrain's my_multiclass_logloss: 0.724688\tvalid's my_multiclass_logloss: 1.02231\n",
      "[122]\ttrain's my_multiclass_logloss: 0.721789\tvalid's my_multiclass_logloss: 1.02824\n",
      "[123]\ttrain's my_multiclass_logloss: 0.719751\tvalid's my_multiclass_logloss: 1.03303\n",
      "[124]\ttrain's my_multiclass_logloss: 0.718134\tvalid's my_multiclass_logloss: 1.03809\n",
      "[125]\ttrain's my_multiclass_logloss: 0.715813\tvalid's my_multiclass_logloss: 1.04152\n",
      "[126]\ttrain's my_multiclass_logloss: 0.714389\tvalid's my_multiclass_logloss: 1.0418\n",
      "[127]\ttrain's my_multiclass_logloss: 0.713395\tvalid's my_multiclass_logloss: 1.0423\n",
      "[128]\ttrain's my_multiclass_logloss: 0.712989\tvalid's my_multiclass_logloss: 1.04777\n",
      "[129]\ttrain's my_multiclass_logloss: 0.712989\tvalid's my_multiclass_logloss: 1.04868\n",
      "[130]\ttrain's my_multiclass_logloss: 0.712801\tvalid's my_multiclass_logloss: 1.04955\n",
      "[131]\ttrain's my_multiclass_logloss: 0.712679\tvalid's my_multiclass_logloss: 1.05138\n",
      "[132]\ttrain's my_multiclass_logloss: 0.712495\tvalid's my_multiclass_logloss: 1.05102\n",
      "[133]\ttrain's my_multiclass_logloss: 0.711543\tvalid's my_multiclass_logloss: 1.05063\n",
      "[134]\ttrain's my_multiclass_logloss: 0.710754\tvalid's my_multiclass_logloss: 1.05108\n",
      "[135]\ttrain's my_multiclass_logloss: 0.710491\tvalid's my_multiclass_logloss: 1.05584\n",
      "[136]\ttrain's my_multiclass_logloss: 0.709948\tvalid's my_multiclass_logloss: 1.05902\n",
      "[137]\ttrain's my_multiclass_logloss: 0.708641\tvalid's my_multiclass_logloss: 1.05959\n",
      "[138]\ttrain's my_multiclass_logloss: 0.707774\tvalid's my_multiclass_logloss: 1.06092\n",
      "[139]\ttrain's my_multiclass_logloss: 0.706964\tvalid's my_multiclass_logloss: 1.05948\n",
      "[140]\ttrain's my_multiclass_logloss: 0.705832\tvalid's my_multiclass_logloss: 1.05891\n",
      "[141]\ttrain's my_multiclass_logloss: 0.705022\tvalid's my_multiclass_logloss: 1.05562\n",
      "[142]\ttrain's my_multiclass_logloss: 0.703788\tvalid's my_multiclass_logloss: 1.05286\n",
      "[143]\ttrain's my_multiclass_logloss: 0.70247\tvalid's my_multiclass_logloss: 1.04999\n",
      "[144]\ttrain's my_multiclass_logloss: 0.701993\tvalid's my_multiclass_logloss: 1.04699\n",
      "[145]\ttrain's my_multiclass_logloss: 0.701421\tvalid's my_multiclass_logloss: 1.05108\n",
      "[146]\ttrain's my_multiclass_logloss: 0.700919\tvalid's my_multiclass_logloss: 1.05325\n",
      "[147]\ttrain's my_multiclass_logloss: 0.700448\tvalid's my_multiclass_logloss: 1.05716\n",
      "[148]\ttrain's my_multiclass_logloss: 0.700077\tvalid's my_multiclass_logloss: 1.05901\n",
      "[149]\ttrain's my_multiclass_logloss: 0.69872\tvalid's my_multiclass_logloss: 1.06592\n",
      "Early stopping, best iteration is:\n",
      "[49]\ttrain's my_multiclass_logloss: 0.851935\tvalid's my_multiclass_logloss: 0.924584\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.5454545454545454\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.103502\n",
      "1   feature2    0.156318\n",
      "2   feature3    0.023790\n",
      "3   feature4    0.047034\n",
      "4   feature5    0.143839\n",
      "5   feature6    0.056386\n",
      "6   feature7    0.046740\n",
      "7   feature8    0.171052\n",
      "8   feature9    0.179549\n",
      "9  feature10    0.071788\n",
      "     feature  importance\n",
      "0   feature1    0.073208\n",
      "1   feature2    0.164954\n",
      "2   feature3    0.019898\n",
      "3   feature4    0.047600\n",
      "4   feature5    0.160513\n",
      "5   feature6    0.068789\n",
      "6   feature7    0.043016\n",
      "7   feature8    0.172895\n",
      "8   feature9    0.158043\n",
      "9  feature10    0.091085\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1   -0.254931\n",
      "1   feature2    0.072666\n",
      "2   feature3   -0.032748\n",
      "3   feature4    0.004759\n",
      "4   feature5    0.140311\n",
      "5   feature6    0.104368\n",
      "6   feature7   -0.031342\n",
      "7   feature8    0.015506\n",
      "8   feature9   -0.180980\n",
      "9  feature10    0.162391\n",
      "-------------------- 1 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05284\tvalid's multi_logloss: 1.05307\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.04575\tvalid's multi_logloss: 1.03855\n",
      "[3]\ttrain's multi_logloss: 1.03763\tvalid's multi_logloss: 1.02575\n",
      "[4]\ttrain's multi_logloss: 1.03\tvalid's multi_logloss: 1.02232\n",
      "[5]\ttrain's multi_logloss: 1.0227\tvalid's multi_logloss: 1.02453\n",
      "[6]\ttrain's multi_logloss: 1.01799\tvalid's multi_logloss: 1.03003\n",
      "[7]\ttrain's multi_logloss: 1.01121\tvalid's multi_logloss: 1.02683\n",
      "[8]\ttrain's multi_logloss: 1.00502\tvalid's multi_logloss: 1.02628\n",
      "[9]\ttrain's multi_logloss: 0.997578\tvalid's multi_logloss: 1.02629\n",
      "[10]\ttrain's multi_logloss: 0.990172\tvalid's multi_logloss: 1.02631\n",
      "[11]\ttrain's multi_logloss: 0.983523\tvalid's multi_logloss: 1.02498\n",
      "[12]\ttrain's multi_logloss: 0.977337\tvalid's multi_logloss: 1.02073\n",
      "[13]\ttrain's multi_logloss: 0.970534\tvalid's multi_logloss: 1.01645\n",
      "[14]\ttrain's multi_logloss: 0.963974\tvalid's multi_logloss: 1.00678\n",
      "[15]\ttrain's multi_logloss: 0.959029\tvalid's multi_logloss: 1.01399\n",
      "[16]\ttrain's multi_logloss: 0.95323\tvalid's multi_logloss: 1.01047\n",
      "[17]\ttrain's multi_logloss: 0.946498\tvalid's multi_logloss: 1.00844\n",
      "[18]\ttrain's multi_logloss: 0.940165\tvalid's multi_logloss: 1.00668\n",
      "[19]\ttrain's multi_logloss: 0.934301\tvalid's multi_logloss: 1.00331\n",
      "[20]\ttrain's multi_logloss: 0.929181\tvalid's multi_logloss: 1.0066\n",
      "[21]\ttrain's multi_logloss: 0.923653\tvalid's multi_logloss: 1.00241\n",
      "[22]\ttrain's multi_logloss: 0.919465\tvalid's multi_logloss: 1.00033\n",
      "[23]\ttrain's multi_logloss: 0.914213\tvalid's multi_logloss: 0.995627\n",
      "[24]\ttrain's multi_logloss: 0.909248\tvalid's multi_logloss: 0.997144\n",
      "[25]\ttrain's multi_logloss: 0.905226\tvalid's multi_logloss: 0.98837\n",
      "[26]\ttrain's multi_logloss: 0.899895\tvalid's multi_logloss: 0.982806\n",
      "[27]\ttrain's multi_logloss: 0.895211\tvalid's multi_logloss: 0.982934\n",
      "[28]\ttrain's multi_logloss: 0.892589\tvalid's multi_logloss: 0.986796\n",
      "[29]\ttrain's multi_logloss: 0.888823\tvalid's multi_logloss: 0.988201\n",
      "[30]\ttrain's multi_logloss: 0.885022\tvalid's multi_logloss: 0.985972\n",
      "[31]\ttrain's multi_logloss: 0.882052\tvalid's multi_logloss: 0.986018\n",
      "[32]\ttrain's multi_logloss: 0.878334\tvalid's multi_logloss: 0.979941\n",
      "[33]\ttrain's multi_logloss: 0.873609\tvalid's multi_logloss: 0.972992\n",
      "[34]\ttrain's multi_logloss: 0.868906\tvalid's multi_logloss: 0.971427\n",
      "[35]\ttrain's multi_logloss: 0.864778\tvalid's multi_logloss: 0.971282\n",
      "[36]\ttrain's multi_logloss: 0.860482\tvalid's multi_logloss: 0.969974\n",
      "[37]\ttrain's multi_logloss: 0.85636\tvalid's multi_logloss: 0.969407\n",
      "[38]\ttrain's multi_logloss: 0.852445\tvalid's multi_logloss: 0.968979\n",
      "[39]\ttrain's multi_logloss: 0.850225\tvalid's multi_logloss: 0.96872\n",
      "[40]\ttrain's multi_logloss: 0.84741\tvalid's multi_logloss: 0.971744\n",
      "[41]\ttrain's multi_logloss: 0.843233\tvalid's multi_logloss: 0.967567\n",
      "[42]\ttrain's multi_logloss: 0.839348\tvalid's multi_logloss: 0.963692\n",
      "[43]\ttrain's multi_logloss: 0.836069\tvalid's multi_logloss: 0.958221\n",
      "[44]\ttrain's multi_logloss: 0.832728\tvalid's multi_logloss: 0.954694\n",
      "[45]\ttrain's multi_logloss: 0.829633\tvalid's multi_logloss: 0.953198\n",
      "[46]\ttrain's multi_logloss: 0.826568\tvalid's multi_logloss: 0.954769\n",
      "[47]\ttrain's multi_logloss: 0.82235\tvalid's multi_logloss: 0.953281\n",
      "[48]\ttrain's multi_logloss: 0.819384\tvalid's multi_logloss: 0.95084\n",
      "[49]\ttrain's multi_logloss: 0.816744\tvalid's multi_logloss: 0.951385\n",
      "[50]\ttrain's multi_logloss: 0.813582\tvalid's multi_logloss: 0.947078\n",
      "[51]\ttrain's multi_logloss: 0.811006\tvalid's multi_logloss: 0.943336\n",
      "[52]\ttrain's multi_logloss: 0.80801\tvalid's multi_logloss: 0.941816\n",
      "[53]\ttrain's multi_logloss: 0.804862\tvalid's multi_logloss: 0.945253\n",
      "[54]\ttrain's multi_logloss: 0.801944\tvalid's multi_logloss: 0.950861\n",
      "[55]\ttrain's multi_logloss: 0.799359\tvalid's multi_logloss: 0.950809\n",
      "[56]\ttrain's multi_logloss: 0.79699\tvalid's multi_logloss: 0.950928\n",
      "[57]\ttrain's multi_logloss: 0.795125\tvalid's multi_logloss: 0.947447\n",
      "[58]\ttrain's multi_logloss: 0.793405\tvalid's multi_logloss: 0.942103\n",
      "[59]\ttrain's multi_logloss: 0.791663\tvalid's multi_logloss: 0.935769\n",
      "[60]\ttrain's multi_logloss: 0.789978\tvalid's multi_logloss: 0.929228\n",
      "[61]\ttrain's multi_logloss: 0.785455\tvalid's multi_logloss: 0.923803\n",
      "[62]\ttrain's multi_logloss: 0.7809\tvalid's multi_logloss: 0.918131\n",
      "[63]\ttrain's multi_logloss: 0.776651\tvalid's multi_logloss: 0.912761\n",
      "[64]\ttrain's multi_logloss: 0.773323\tvalid's multi_logloss: 0.912788\n",
      "[65]\ttrain's multi_logloss: 0.771507\tvalid's multi_logloss: 0.912206\n",
      "[66]\ttrain's multi_logloss: 0.769662\tvalid's multi_logloss: 0.910308\n",
      "[67]\ttrain's multi_logloss: 0.768034\tvalid's multi_logloss: 0.90635\n",
      "[68]\ttrain's multi_logloss: 0.766554\tvalid's multi_logloss: 0.902625\n",
      "[69]\ttrain's multi_logloss: 0.765323\tvalid's multi_logloss: 0.909373\n",
      "[70]\ttrain's multi_logloss: 0.763945\tvalid's multi_logloss: 0.910978\n",
      "[71]\ttrain's multi_logloss: 0.763025\tvalid's multi_logloss: 0.917465\n",
      "[72]\ttrain's multi_logloss: 0.762301\tvalid's multi_logloss: 0.923908\n",
      "[73]\ttrain's multi_logloss: 0.759321\tvalid's multi_logloss: 0.92983\n",
      "[74]\ttrain's multi_logloss: 0.756685\tvalid's multi_logloss: 0.931902\n",
      "[75]\ttrain's multi_logloss: 0.754269\tvalid's multi_logloss: 0.934017\n",
      "[76]\ttrain's multi_logloss: 0.752557\tvalid's multi_logloss: 0.937227\n",
      "[77]\ttrain's multi_logloss: 0.750566\tvalid's multi_logloss: 0.941841\n",
      "[78]\ttrain's multi_logloss: 0.74875\tvalid's multi_logloss: 0.946898\n",
      "[79]\ttrain's multi_logloss: 0.747487\tvalid's multi_logloss: 0.945688\n",
      "[80]\ttrain's multi_logloss: 0.745793\tvalid's multi_logloss: 0.948604\n",
      "[81]\ttrain's multi_logloss: 0.743081\tvalid's multi_logloss: 0.948225\n",
      "[82]\ttrain's multi_logloss: 0.740916\tvalid's multi_logloss: 0.948341\n",
      "[83]\ttrain's multi_logloss: 0.739015\tvalid's multi_logloss: 0.948685\n",
      "[84]\ttrain's multi_logloss: 0.73636\tvalid's multi_logloss: 0.94838\n",
      "[85]\ttrain's multi_logloss: 0.734172\tvalid's multi_logloss: 0.950552\n",
      "[86]\ttrain's multi_logloss: 0.733603\tvalid's multi_logloss: 0.951028\n",
      "[87]\ttrain's multi_logloss: 0.732183\tvalid's multi_logloss: 0.949495\n",
      "[88]\ttrain's multi_logloss: 0.730875\tvalid's multi_logloss: 0.942082\n",
      "[89]\ttrain's multi_logloss: 0.729543\tvalid's multi_logloss: 0.942205\n",
      "[90]\ttrain's multi_logloss: 0.728872\tvalid's multi_logloss: 0.938585\n",
      "[91]\ttrain's multi_logloss: 0.727854\tvalid's multi_logloss: 0.932875\n",
      "[92]\ttrain's multi_logloss: 0.72674\tvalid's multi_logloss: 0.930852\n",
      "[93]\ttrain's multi_logloss: 0.724918\tvalid's multi_logloss: 0.928444\n",
      "[94]\ttrain's multi_logloss: 0.723136\tvalid's multi_logloss: 0.92753\n",
      "[95]\ttrain's multi_logloss: 0.721697\tvalid's multi_logloss: 0.925489\n",
      "[96]\ttrain's multi_logloss: 0.720466\tvalid's multi_logloss: 0.927389\n",
      "[97]\ttrain's multi_logloss: 0.718989\tvalid's multi_logloss: 0.923749\n",
      "[98]\ttrain's multi_logloss: 0.717777\tvalid's multi_logloss: 0.920577\n",
      "[99]\ttrain's multi_logloss: 0.716422\tvalid's multi_logloss: 0.920008\n",
      "[100]\ttrain's multi_logloss: 0.715537\tvalid's multi_logloss: 0.915392\n",
      "[101]\ttrain's multi_logloss: 0.71405\tvalid's multi_logloss: 0.915403\n",
      "[102]\ttrain's multi_logloss: 0.712882\tvalid's multi_logloss: 0.91577\n",
      "[103]\ttrain's multi_logloss: 0.712\tvalid's multi_logloss: 0.916454\n",
      "[104]\ttrain's multi_logloss: 0.711378\tvalid's multi_logloss: 0.91742\n",
      "[105]\ttrain's multi_logloss: 0.709401\tvalid's multi_logloss: 0.916251\n",
      "[106]\ttrain's multi_logloss: 0.707592\tvalid's multi_logloss: 0.911829\n",
      "[107]\ttrain's multi_logloss: 0.706059\tvalid's multi_logloss: 0.907648\n",
      "[108]\ttrain's multi_logloss: 0.705373\tvalid's multi_logloss: 0.904946\n",
      "[109]\ttrain's multi_logloss: 0.703271\tvalid's multi_logloss: 0.901488\n",
      "[110]\ttrain's multi_logloss: 0.701553\tvalid's multi_logloss: 0.901044\n",
      "[111]\ttrain's multi_logloss: 0.699803\tvalid's multi_logloss: 0.898153\n",
      "[112]\ttrain's multi_logloss: 0.698317\tvalid's multi_logloss: 0.899011\n",
      "[113]\ttrain's multi_logloss: 0.696209\tvalid's multi_logloss: 0.899757\n",
      "[114]\ttrain's multi_logloss: 0.694279\tvalid's multi_logloss: 0.90059\n",
      "[115]\ttrain's multi_logloss: 0.692784\tvalid's multi_logloss: 0.903122\n",
      "[116]\ttrain's multi_logloss: 0.69088\tvalid's multi_logloss: 0.906084\n",
      "[117]\ttrain's multi_logloss: 0.6892\tvalid's multi_logloss: 0.90316\n",
      "[118]\ttrain's multi_logloss: 0.687715\tvalid's multi_logloss: 0.901236\n",
      "[119]\ttrain's multi_logloss: 0.686692\tvalid's multi_logloss: 0.899207\n",
      "[120]\ttrain's multi_logloss: 0.685501\tvalid's multi_logloss: 0.899155\n",
      "[121]\ttrain's multi_logloss: 0.683842\tvalid's multi_logloss: 0.900089\n",
      "[122]\ttrain's multi_logloss: 0.682161\tvalid's multi_logloss: 0.90716\n",
      "[123]\ttrain's multi_logloss: 0.680627\tvalid's multi_logloss: 0.911299\n",
      "[124]\ttrain's multi_logloss: 0.67894\tvalid's multi_logloss: 0.913736\n",
      "[125]\ttrain's multi_logloss: 0.676799\tvalid's multi_logloss: 0.914179\n",
      "[126]\ttrain's multi_logloss: 0.675131\tvalid's multi_logloss: 0.917418\n",
      "[127]\ttrain's multi_logloss: 0.673322\tvalid's multi_logloss: 0.91801\n",
      "[128]\ttrain's multi_logloss: 0.672281\tvalid's multi_logloss: 0.916147\n",
      "[129]\ttrain's multi_logloss: 0.671261\tvalid's multi_logloss: 0.912674\n",
      "[130]\ttrain's multi_logloss: 0.670502\tvalid's multi_logloss: 0.909324\n",
      "[131]\ttrain's multi_logloss: 0.669697\tvalid's multi_logloss: 0.906152\n",
      "[132]\ttrain's multi_logloss: 0.669665\tvalid's multi_logloss: 0.903691\n",
      "[133]\ttrain's multi_logloss: 0.667535\tvalid's multi_logloss: 0.905406\n",
      "[134]\ttrain's multi_logloss: 0.665661\tvalid's multi_logloss: 0.906\n",
      "[135]\ttrain's multi_logloss: 0.664292\tvalid's multi_logloss: 0.90861\n",
      "[136]\ttrain's multi_logloss: 0.662305\tvalid's multi_logloss: 0.914848\n",
      "[137]\ttrain's multi_logloss: 0.661519\tvalid's multi_logloss: 0.915363\n",
      "[138]\ttrain's multi_logloss: 0.661102\tvalid's multi_logloss: 0.917911\n",
      "[139]\ttrain's multi_logloss: 0.660677\tvalid's multi_logloss: 0.918788\n",
      "[140]\ttrain's multi_logloss: 0.660263\tvalid's multi_logloss: 0.922407\n",
      "[141]\ttrain's multi_logloss: 0.659082\tvalid's multi_logloss: 0.92096\n",
      "[142]\ttrain's multi_logloss: 0.657969\tvalid's multi_logloss: 0.920829\n",
      "[143]\ttrain's multi_logloss: 0.656971\tvalid's multi_logloss: 0.920744\n",
      "[144]\ttrain's multi_logloss: 0.655993\tvalid's multi_logloss: 0.920064\n",
      "[145]\ttrain's multi_logloss: 0.655077\tvalid's multi_logloss: 0.923001\n",
      "[146]\ttrain's multi_logloss: 0.654381\tvalid's multi_logloss: 0.923174\n",
      "[147]\ttrain's multi_logloss: 0.653892\tvalid's multi_logloss: 0.92459\n",
      "[148]\ttrain's multi_logloss: 0.653194\tvalid's multi_logloss: 0.927586\n",
      "[149]\ttrain's multi_logloss: 0.651934\tvalid's multi_logloss: 0.934391\n",
      "[150]\ttrain's multi_logloss: 0.650946\tvalid's multi_logloss: 0.936076\n",
      "[151]\ttrain's multi_logloss: 0.649875\tvalid's multi_logloss: 0.942698\n",
      "[152]\ttrain's multi_logloss: 0.648833\tvalid's multi_logloss: 0.942104\n",
      "[153]\ttrain's multi_logloss: 0.647738\tvalid's multi_logloss: 0.941531\n",
      "[154]\ttrain's multi_logloss: 0.647141\tvalid's multi_logloss: 0.943099\n",
      "[155]\ttrain's multi_logloss: 0.646329\tvalid's multi_logloss: 0.939147\n",
      "[156]\ttrain's multi_logloss: 0.645142\tvalid's multi_logloss: 0.935298\n",
      "[157]\ttrain's multi_logloss: 0.643579\tvalid's multi_logloss: 0.937404\n",
      "[158]\ttrain's multi_logloss: 0.641623\tvalid's multi_logloss: 0.94554\n",
      "[159]\ttrain's multi_logloss: 0.640586\tvalid's multi_logloss: 0.949926\n",
      "[160]\ttrain's multi_logloss: 0.63909\tvalid's multi_logloss: 0.959498\n",
      "[161]\ttrain's multi_logloss: 0.636825\tvalid's multi_logloss: 0.961299\n",
      "[162]\ttrain's multi_logloss: 0.634781\tvalid's multi_logloss: 0.961922\n",
      "[163]\ttrain's multi_logloss: 0.632854\tvalid's multi_logloss: 0.956693\n",
      "[164]\ttrain's multi_logloss: 0.63167\tvalid's multi_logloss: 0.958413\n",
      "[165]\ttrain's multi_logloss: 0.630947\tvalid's multi_logloss: 0.956772\n",
      "[166]\ttrain's multi_logloss: 0.630226\tvalid's multi_logloss: 0.953586\n",
      "[167]\ttrain's multi_logloss: 0.629581\tvalid's multi_logloss: 0.953309\n",
      "[168]\ttrain's multi_logloss: 0.628903\tvalid's multi_logloss: 0.953739\n",
      "[169]\ttrain's multi_logloss: 0.62764\tvalid's multi_logloss: 0.952511\n",
      "[170]\ttrain's multi_logloss: 0.626872\tvalid's multi_logloss: 0.952895\n",
      "[171]\ttrain's multi_logloss: 0.626126\tvalid's multi_logloss: 0.953556\n",
      "[172]\ttrain's multi_logloss: 0.624892\tvalid's multi_logloss: 0.958671\n",
      "[173]\ttrain's multi_logloss: 0.623617\tvalid's multi_logloss: 0.968189\n",
      "[174]\ttrain's multi_logloss: 0.622526\tvalid's multi_logloss: 0.974728\n",
      "[175]\ttrain's multi_logloss: 0.62173\tvalid's multi_logloss: 0.97848\n",
      "[176]\ttrain's multi_logloss: 0.621162\tvalid's multi_logloss: 0.976517\n",
      "[177]\ttrain's multi_logloss: 0.61993\tvalid's multi_logloss: 0.977226\n",
      "[178]\ttrain's multi_logloss: 0.618763\tvalid's multi_logloss: 0.983124\n",
      "[179]\ttrain's multi_logloss: 0.61742\tvalid's multi_logloss: 0.989354\n",
      "[180]\ttrain's multi_logloss: 0.616223\tvalid's multi_logloss: 0.995477\n",
      "[181]\ttrain's multi_logloss: 0.615647\tvalid's multi_logloss: 0.99303\n",
      "[182]\ttrain's multi_logloss: 0.615134\tvalid's multi_logloss: 0.991089\n",
      "[183]\ttrain's multi_logloss: 0.614248\tvalid's multi_logloss: 0.986026\n",
      "[184]\ttrain's multi_logloss: 0.613258\tvalid's multi_logloss: 0.981426\n",
      "[185]\ttrain's multi_logloss: 0.61218\tvalid's multi_logloss: 0.980437\n",
      "[186]\ttrain's multi_logloss: 0.611013\tvalid's multi_logloss: 0.980216\n",
      "[187]\ttrain's multi_logloss: 0.610004\tvalid's multi_logloss: 0.980177\n",
      "[188]\ttrain's multi_logloss: 0.609311\tvalid's multi_logloss: 0.974655\n",
      "[189]\ttrain's multi_logloss: 0.608636\tvalid's multi_logloss: 0.973355\n",
      "[190]\ttrain's multi_logloss: 0.607837\tvalid's multi_logloss: 0.973869\n",
      "[191]\ttrain's multi_logloss: 0.606715\tvalid's multi_logloss: 0.978306\n",
      "[192]\ttrain's multi_logloss: 0.605678\tvalid's multi_logloss: 0.982682\n",
      "[193]\ttrain's multi_logloss: 0.605394\tvalid's multi_logloss: 0.983767\n",
      "[194]\ttrain's multi_logloss: 0.605098\tvalid's multi_logloss: 0.984438\n",
      "[195]\ttrain's multi_logloss: 0.604689\tvalid's multi_logloss: 0.985115\n",
      "[196]\ttrain's multi_logloss: 0.604365\tvalid's multi_logloss: 0.985823\n",
      "[197]\ttrain's multi_logloss: 0.604024\tvalid's multi_logloss: 0.984835\n",
      "[198]\ttrain's multi_logloss: 0.603496\tvalid's multi_logloss: 0.989798\n",
      "[199]\ttrain's multi_logloss: 0.603301\tvalid's multi_logloss: 0.988947\n",
      "[200]\ttrain's multi_logloss: 0.602995\tvalid's multi_logloss: 0.997327\n",
      "[201]\ttrain's multi_logloss: 0.602748\tvalid's multi_logloss: 0.999729\n",
      "[202]\ttrain's multi_logloss: 0.60217\tvalid's multi_logloss: 1.00616\n",
      "[203]\ttrain's multi_logloss: 0.601507\tvalid's multi_logloss: 1.00806\n",
      "[204]\ttrain's multi_logloss: 0.601222\tvalid's multi_logloss: 1.01449\n",
      "[205]\ttrain's multi_logloss: 0.600559\tvalid's multi_logloss: 1.01937\n",
      "[206]\ttrain's multi_logloss: 0.600075\tvalid's multi_logloss: 1.02258\n",
      "[207]\ttrain's multi_logloss: 0.599599\tvalid's multi_logloss: 1.02722\n",
      "[208]\ttrain's multi_logloss: 0.59927\tvalid's multi_logloss: 1.02869\n",
      "[209]\ttrain's multi_logloss: 0.597447\tvalid's multi_logloss: 1.0311\n",
      "[210]\ttrain's multi_logloss: 0.596845\tvalid's multi_logloss: 1.03126\n",
      "[211]\ttrain's multi_logloss: 0.595358\tvalid's multi_logloss: 1.03387\n",
      "Early stopping, best iteration is:\n",
      "[111]\ttrain's multi_logloss: 0.699803\tvalid's multi_logloss: 0.898153\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.150930\n",
      "1   feature2    0.119383\n",
      "2   feature3    0.022097\n",
      "3   feature4    0.045294\n",
      "4   feature5    0.111164\n",
      "5   feature6    0.053319\n",
      "6   feature7    0.052822\n",
      "7   feature8    0.167775\n",
      "8   feature9    0.167385\n",
      "9  feature10    0.109832\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08435\tvalid's my_multiclass_logloss: 1.08691\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07397\tvalid's my_multiclass_logloss: 1.07578\n",
      "[3]\ttrain's my_multiclass_logloss: 1.06178\tvalid's my_multiclass_logloss: 1.05634\n",
      "[4]\ttrain's my_multiclass_logloss: 1.05093\tvalid's my_multiclass_logloss: 1.03872\n",
      "[5]\ttrain's my_multiclass_logloss: 1.04374\tvalid's my_multiclass_logloss: 1.03722\n",
      "[6]\ttrain's my_multiclass_logloss: 1.03565\tvalid's my_multiclass_logloss: 1.02812\n",
      "[7]\ttrain's my_multiclass_logloss: 1.02935\tvalid's my_multiclass_logloss: 1.02756\n",
      "[8]\ttrain's my_multiclass_logloss: 1.02139\tvalid's my_multiclass_logloss: 1.02671\n",
      "[9]\ttrain's my_multiclass_logloss: 1.01278\tvalid's my_multiclass_logloss: 1.02492\n",
      "[10]\ttrain's my_multiclass_logloss: 1.00519\tvalid's my_multiclass_logloss: 1.02668\n",
      "[11]\ttrain's my_multiclass_logloss: 0.998422\tvalid's my_multiclass_logloss: 1.02182\n",
      "[12]\ttrain's my_multiclass_logloss: 0.991768\tvalid's my_multiclass_logloss: 1.01954\n",
      "[13]\ttrain's my_multiclass_logloss: 0.983988\tvalid's my_multiclass_logloss: 1.00769\n",
      "[14]\ttrain's my_multiclass_logloss: 0.976662\tvalid's my_multiclass_logloss: 1.00284\n",
      "[15]\ttrain's my_multiclass_logloss: 0.970359\tvalid's my_multiclass_logloss: 1.00124\n",
      "[16]\ttrain's my_multiclass_logloss: 0.964189\tvalid's my_multiclass_logloss: 0.99571\n",
      "[17]\ttrain's my_multiclass_logloss: 0.957574\tvalid's my_multiclass_logloss: 0.993679\n",
      "[18]\ttrain's my_multiclass_logloss: 0.950892\tvalid's my_multiclass_logloss: 0.991774\n",
      "[19]\ttrain's my_multiclass_logloss: 0.944886\tvalid's my_multiclass_logloss: 0.998354\n",
      "[20]\ttrain's my_multiclass_logloss: 0.938713\tvalid's my_multiclass_logloss: 1.00472\n",
      "[21]\ttrain's my_multiclass_logloss: 0.932661\tvalid's my_multiclass_logloss: 1.0038\n",
      "[22]\ttrain's my_multiclass_logloss: 0.926406\tvalid's my_multiclass_logloss: 0.997831\n",
      "[23]\ttrain's my_multiclass_logloss: 0.921156\tvalid's my_multiclass_logloss: 0.999039\n",
      "[24]\ttrain's my_multiclass_logloss: 0.915796\tvalid's my_multiclass_logloss: 0.996419\n",
      "[25]\ttrain's my_multiclass_logloss: 0.910976\tvalid's my_multiclass_logloss: 1.00082\n",
      "[26]\ttrain's my_multiclass_logloss: 0.907824\tvalid's my_multiclass_logloss: 0.992553\n",
      "[27]\ttrain's my_multiclass_logloss: 0.904834\tvalid's my_multiclass_logloss: 0.988383\n",
      "[28]\ttrain's my_multiclass_logloss: 0.900037\tvalid's my_multiclass_logloss: 0.989358\n",
      "[29]\ttrain's my_multiclass_logloss: 0.897284\tvalid's my_multiclass_logloss: 0.988615\n",
      "[30]\ttrain's my_multiclass_logloss: 0.894641\tvalid's my_multiclass_logloss: 0.98608\n",
      "[31]\ttrain's my_multiclass_logloss: 0.892027\tvalid's my_multiclass_logloss: 0.98719\n",
      "[32]\ttrain's my_multiclass_logloss: 0.888849\tvalid's my_multiclass_logloss: 0.985047\n",
      "[33]\ttrain's my_multiclass_logloss: 0.883758\tvalid's my_multiclass_logloss: 0.978168\n",
      "[34]\ttrain's my_multiclass_logloss: 0.879943\tvalid's my_multiclass_logloss: 0.979199\n",
      "[35]\ttrain's my_multiclass_logloss: 0.87635\tvalid's my_multiclass_logloss: 0.971654\n",
      "[36]\ttrain's my_multiclass_logloss: 0.872202\tvalid's my_multiclass_logloss: 0.965894\n",
      "[37]\ttrain's my_multiclass_logloss: 0.869878\tvalid's my_multiclass_logloss: 0.969487\n",
      "[38]\ttrain's my_multiclass_logloss: 0.86777\tvalid's my_multiclass_logloss: 0.966708\n",
      "[39]\ttrain's my_multiclass_logloss: 0.865953\tvalid's my_multiclass_logloss: 0.970453\n",
      "[40]\ttrain's my_multiclass_logloss: 0.864384\tvalid's my_multiclass_logloss: 0.974218\n",
      "[41]\ttrain's my_multiclass_logloss: 0.86062\tvalid's my_multiclass_logloss: 0.975307\n",
      "[42]\ttrain's my_multiclass_logloss: 0.857311\tvalid's my_multiclass_logloss: 0.974061\n",
      "[43]\ttrain's my_multiclass_logloss: 0.85485\tvalid's my_multiclass_logloss: 0.973489\n",
      "[44]\ttrain's my_multiclass_logloss: 0.852167\tvalid's my_multiclass_logloss: 0.970233\n",
      "[45]\ttrain's my_multiclass_logloss: 0.848856\tvalid's my_multiclass_logloss: 0.969833\n",
      "[46]\ttrain's my_multiclass_logloss: 0.845957\tvalid's my_multiclass_logloss: 0.969695\n",
      "[47]\ttrain's my_multiclass_logloss: 0.843669\tvalid's my_multiclass_logloss: 0.968181\n",
      "[48]\ttrain's my_multiclass_logloss: 0.841852\tvalid's my_multiclass_logloss: 0.971208\n",
      "[49]\ttrain's my_multiclass_logloss: 0.839064\tvalid's my_multiclass_logloss: 0.974301\n",
      "[50]\ttrain's my_multiclass_logloss: 0.836968\tvalid's my_multiclass_logloss: 0.975071\n",
      "[51]\ttrain's my_multiclass_logloss: 0.834518\tvalid's my_multiclass_logloss: 0.978134\n",
      "[52]\ttrain's my_multiclass_logloss: 0.832115\tvalid's my_multiclass_logloss: 0.971975\n",
      "[53]\ttrain's my_multiclass_logloss: 0.82923\tvalid's my_multiclass_logloss: 0.978691\n",
      "[54]\ttrain's my_multiclass_logloss: 0.827209\tvalid's my_multiclass_logloss: 0.978305\n",
      "[55]\ttrain's my_multiclass_logloss: 0.824952\tvalid's my_multiclass_logloss: 0.985194\n",
      "[56]\ttrain's my_multiclass_logloss: 0.823059\tvalid's my_multiclass_logloss: 0.98475\n",
      "[57]\ttrain's my_multiclass_logloss: 0.821317\tvalid's my_multiclass_logloss: 0.983245\n",
      "[58]\ttrain's my_multiclass_logloss: 0.820395\tvalid's my_multiclass_logloss: 0.981505\n",
      "[59]\ttrain's my_multiclass_logloss: 0.819075\tvalid's my_multiclass_logloss: 0.974088\n",
      "[60]\ttrain's my_multiclass_logloss: 0.818068\tvalid's my_multiclass_logloss: 0.967137\n",
      "[61]\ttrain's my_multiclass_logloss: 0.814392\tvalid's my_multiclass_logloss: 0.96173\n",
      "[62]\ttrain's my_multiclass_logloss: 0.81204\tvalid's my_multiclass_logloss: 0.963183\n",
      "[63]\ttrain's my_multiclass_logloss: 0.808591\tvalid's my_multiclass_logloss: 0.960573\n",
      "[64]\ttrain's my_multiclass_logloss: 0.806242\tvalid's my_multiclass_logloss: 0.95887\n",
      "[65]\ttrain's my_multiclass_logloss: 0.804903\tvalid's my_multiclass_logloss: 0.954168\n",
      "[66]\ttrain's my_multiclass_logloss: 0.803592\tvalid's my_multiclass_logloss: 0.948304\n",
      "[67]\ttrain's my_multiclass_logloss: 0.802493\tvalid's my_multiclass_logloss: 0.94288\n",
      "[68]\ttrain's my_multiclass_logloss: 0.801335\tvalid's my_multiclass_logloss: 0.940018\n",
      "[69]\ttrain's my_multiclass_logloss: 0.800354\tvalid's my_multiclass_logloss: 0.946429\n",
      "[70]\ttrain's my_multiclass_logloss: 0.799561\tvalid's my_multiclass_logloss: 0.952609\n",
      "[71]\ttrain's my_multiclass_logloss: 0.799121\tvalid's my_multiclass_logloss: 0.958828\n",
      "[72]\ttrain's my_multiclass_logloss: 0.798944\tvalid's my_multiclass_logloss: 0.964967\n",
      "[73]\ttrain's my_multiclass_logloss: 0.796578\tvalid's my_multiclass_logloss: 0.970951\n",
      "[74]\ttrain's my_multiclass_logloss: 0.795064\tvalid's my_multiclass_logloss: 0.969192\n",
      "[75]\ttrain's my_multiclass_logloss: 0.793099\tvalid's my_multiclass_logloss: 0.971672\n",
      "[76]\ttrain's my_multiclass_logloss: 0.790994\tvalid's my_multiclass_logloss: 0.979881\n",
      "[77]\ttrain's my_multiclass_logloss: 0.789511\tvalid's my_multiclass_logloss: 0.986719\n",
      "[78]\ttrain's my_multiclass_logloss: 0.788358\tvalid's my_multiclass_logloss: 0.985927\n",
      "[79]\ttrain's my_multiclass_logloss: 0.787323\tvalid's my_multiclass_logloss: 0.984848\n",
      "[80]\ttrain's my_multiclass_logloss: 0.785942\tvalid's my_multiclass_logloss: 0.987626\n",
      "[81]\ttrain's my_multiclass_logloss: 0.783786\tvalid's my_multiclass_logloss: 0.987126\n",
      "[82]\ttrain's my_multiclass_logloss: 0.781622\tvalid's my_multiclass_logloss: 0.99028\n",
      "[83]\ttrain's my_multiclass_logloss: 0.779758\tvalid's my_multiclass_logloss: 0.993236\n",
      "[84]\ttrain's my_multiclass_logloss: 0.778747\tvalid's my_multiclass_logloss: 0.993858\n",
      "[85]\ttrain's my_multiclass_logloss: 0.777425\tvalid's my_multiclass_logloss: 0.987711\n",
      "[86]\ttrain's my_multiclass_logloss: 0.777027\tvalid's my_multiclass_logloss: 0.986028\n",
      "[87]\ttrain's my_multiclass_logloss: 0.775986\tvalid's my_multiclass_logloss: 0.980593\n",
      "[88]\ttrain's my_multiclass_logloss: 0.77548\tvalid's my_multiclass_logloss: 0.976264\n",
      "[89]\ttrain's my_multiclass_logloss: 0.774644\tvalid's my_multiclass_logloss: 0.972005\n",
      "[90]\ttrain's my_multiclass_logloss: 0.773953\tvalid's my_multiclass_logloss: 0.966606\n",
      "[91]\ttrain's my_multiclass_logloss: 0.773705\tvalid's my_multiclass_logloss: 0.962205\n",
      "[92]\ttrain's my_multiclass_logloss: 0.77329\tvalid's my_multiclass_logloss: 0.962833\n",
      "[93]\ttrain's my_multiclass_logloss: 0.772328\tvalid's my_multiclass_logloss: 0.961557\n",
      "[94]\ttrain's my_multiclass_logloss: 0.771042\tvalid's my_multiclass_logloss: 0.963854\n",
      "[95]\ttrain's my_multiclass_logloss: 0.770132\tvalid's my_multiclass_logloss: 0.968282\n",
      "[96]\ttrain's my_multiclass_logloss: 0.768726\tvalid's my_multiclass_logloss: 0.968427\n",
      "[97]\ttrain's my_multiclass_logloss: 0.767401\tvalid's my_multiclass_logloss: 0.970184\n",
      "[98]\ttrain's my_multiclass_logloss: 0.766071\tvalid's my_multiclass_logloss: 0.967395\n",
      "[99]\ttrain's my_multiclass_logloss: 0.76525\tvalid's my_multiclass_logloss: 0.966093\n",
      "[100]\ttrain's my_multiclass_logloss: 0.764401\tvalid's my_multiclass_logloss: 0.96499\n",
      "[101]\ttrain's my_multiclass_logloss: 0.763394\tvalid's my_multiclass_logloss: 0.969497\n",
      "[102]\ttrain's my_multiclass_logloss: 0.762788\tvalid's my_multiclass_logloss: 0.974136\n",
      "[103]\ttrain's my_multiclass_logloss: 0.762922\tvalid's my_multiclass_logloss: 0.971597\n",
      "[104]\ttrain's my_multiclass_logloss: 0.763182\tvalid's my_multiclass_logloss: 0.969429\n",
      "[105]\ttrain's my_multiclass_logloss: 0.762082\tvalid's my_multiclass_logloss: 0.964858\n",
      "[106]\ttrain's my_multiclass_logloss: 0.761329\tvalid's my_multiclass_logloss: 0.960655\n",
      "[107]\ttrain's my_multiclass_logloss: 0.760879\tvalid's my_multiclass_logloss: 0.956787\n",
      "[108]\ttrain's my_multiclass_logloss: 0.761253\tvalid's my_multiclass_logloss: 0.95473\n",
      "[109]\ttrain's my_multiclass_logloss: 0.76025\tvalid's my_multiclass_logloss: 0.954844\n",
      "[110]\ttrain's my_multiclass_logloss: 0.758532\tvalid's my_multiclass_logloss: 0.951979\n",
      "[111]\ttrain's my_multiclass_logloss: 0.757083\tvalid's my_multiclass_logloss: 0.949565\n",
      "[112]\ttrain's my_multiclass_logloss: 0.755928\tvalid's my_multiclass_logloss: 0.950046\n",
      "[113]\ttrain's my_multiclass_logloss: 0.754143\tvalid's my_multiclass_logloss: 0.94988\n",
      "[114]\ttrain's my_multiclass_logloss: 0.75263\tvalid's my_multiclass_logloss: 0.949905\n",
      "[115]\ttrain's my_multiclass_logloss: 0.750664\tvalid's my_multiclass_logloss: 0.94836\n",
      "[116]\ttrain's my_multiclass_logloss: 0.74934\tvalid's my_multiclass_logloss: 0.946006\n",
      "[117]\ttrain's my_multiclass_logloss: 0.748173\tvalid's my_multiclass_logloss: 0.945603\n",
      "[118]\ttrain's my_multiclass_logloss: 0.746728\tvalid's my_multiclass_logloss: 0.950866\n",
      "[119]\ttrain's my_multiclass_logloss: 0.74573\tvalid's my_multiclass_logloss: 0.957088\n",
      "[120]\ttrain's my_multiclass_logloss: 0.744084\tvalid's my_multiclass_logloss: 0.958452\n",
      "[121]\ttrain's my_multiclass_logloss: 0.742632\tvalid's my_multiclass_logloss: 0.960744\n",
      "[122]\ttrain's my_multiclass_logloss: 0.741682\tvalid's my_multiclass_logloss: 0.965505\n",
      "[123]\ttrain's my_multiclass_logloss: 0.740577\tvalid's my_multiclass_logloss: 0.972176\n",
      "[124]\ttrain's my_multiclass_logloss: 0.739558\tvalid's my_multiclass_logloss: 0.979066\n",
      "[125]\ttrain's my_multiclass_logloss: 0.737752\tvalid's my_multiclass_logloss: 0.980024\n",
      "[126]\ttrain's my_multiclass_logloss: 0.736384\tvalid's my_multiclass_logloss: 0.983404\n",
      "[127]\ttrain's my_multiclass_logloss: 0.73552\tvalid's my_multiclass_logloss: 0.979605\n",
      "[128]\ttrain's my_multiclass_logloss: 0.734741\tvalid's my_multiclass_logloss: 0.982815\n",
      "[129]\ttrain's my_multiclass_logloss: 0.734206\tvalid's my_multiclass_logloss: 0.979132\n",
      "[130]\ttrain's my_multiclass_logloss: 0.733629\tvalid's my_multiclass_logloss: 0.974911\n",
      "[131]\ttrain's my_multiclass_logloss: 0.732699\tvalid's my_multiclass_logloss: 0.971948\n",
      "[132]\ttrain's my_multiclass_logloss: 0.732466\tvalid's my_multiclass_logloss: 0.967597\n",
      "[133]\ttrain's my_multiclass_logloss: 0.730557\tvalid's my_multiclass_logloss: 0.977327\n",
      "[134]\ttrain's my_multiclass_logloss: 0.7288\tvalid's my_multiclass_logloss: 0.984302\n",
      "[135]\ttrain's my_multiclass_logloss: 0.727329\tvalid's my_multiclass_logloss: 0.986346\n",
      "[136]\ttrain's my_multiclass_logloss: 0.725888\tvalid's my_multiclass_logloss: 0.988737\n",
      "[137]\ttrain's my_multiclass_logloss: 0.725323\tvalid's my_multiclass_logloss: 0.986438\n",
      "[138]\ttrain's my_multiclass_logloss: 0.724641\tvalid's my_multiclass_logloss: 0.990166\n",
      "[139]\ttrain's my_multiclass_logloss: 0.724472\tvalid's my_multiclass_logloss: 0.991438\n",
      "[140]\ttrain's my_multiclass_logloss: 0.724137\tvalid's my_multiclass_logloss: 0.990863\n",
      "[141]\ttrain's my_multiclass_logloss: 0.723503\tvalid's my_multiclass_logloss: 0.990423\n",
      "[142]\ttrain's my_multiclass_logloss: 0.722887\tvalid's my_multiclass_logloss: 0.988289\n",
      "[143]\ttrain's my_multiclass_logloss: 0.722438\tvalid's my_multiclass_logloss: 0.985887\n",
      "[144]\ttrain's my_multiclass_logloss: 0.721842\tvalid's my_multiclass_logloss: 0.984946\n",
      "[145]\ttrain's my_multiclass_logloss: 0.720476\tvalid's my_multiclass_logloss: 0.991871\n",
      "[146]\ttrain's my_multiclass_logloss: 0.719142\tvalid's my_multiclass_logloss: 1.00317\n",
      "[147]\ttrain's my_multiclass_logloss: 0.718176\tvalid's my_multiclass_logloss: 1.00983\n",
      "[148]\ttrain's my_multiclass_logloss: 0.717384\tvalid's my_multiclass_logloss: 1.01665\n",
      "[149]\ttrain's my_multiclass_logloss: 0.716459\tvalid's my_multiclass_logloss: 1.01905\n",
      "[150]\ttrain's my_multiclass_logloss: 0.715689\tvalid's my_multiclass_logloss: 1.0214\n",
      "[151]\ttrain's my_multiclass_logloss: 0.714657\tvalid's my_multiclass_logloss: 1.02813\n",
      "[152]\ttrain's my_multiclass_logloss: 0.713498\tvalid's my_multiclass_logloss: 1.01904\n",
      "[153]\ttrain's my_multiclass_logloss: 0.712799\tvalid's my_multiclass_logloss: 1.01476\n",
      "[154]\ttrain's my_multiclass_logloss: 0.712256\tvalid's my_multiclass_logloss: 1.01079\n",
      "[155]\ttrain's my_multiclass_logloss: 0.712044\tvalid's my_multiclass_logloss: 1.01067\n",
      "[156]\ttrain's my_multiclass_logloss: 0.712035\tvalid's my_multiclass_logloss: 1.01287\n",
      "[157]\ttrain's my_multiclass_logloss: 0.711714\tvalid's my_multiclass_logloss: 1.01738\n",
      "[158]\ttrain's my_multiclass_logloss: 0.710739\tvalid's my_multiclass_logloss: 1.02308\n",
      "[159]\ttrain's my_multiclass_logloss: 0.70973\tvalid's my_multiclass_logloss: 1.02603\n",
      "[160]\ttrain's my_multiclass_logloss: 0.709081\tvalid's my_multiclass_logloss: 1.03155\n",
      "[161]\ttrain's my_multiclass_logloss: 0.706522\tvalid's my_multiclass_logloss: 1.03244\n",
      "[162]\ttrain's my_multiclass_logloss: 0.705162\tvalid's my_multiclass_logloss: 1.03503\n",
      "[163]\ttrain's my_multiclass_logloss: 0.702395\tvalid's my_multiclass_logloss: 1.03596\n",
      "[164]\ttrain's my_multiclass_logloss: 0.699705\tvalid's my_multiclass_logloss: 1.04038\n",
      "[165]\ttrain's my_multiclass_logloss: 0.6994\tvalid's my_multiclass_logloss: 1.03683\n",
      "[166]\ttrain's my_multiclass_logloss: 0.699177\tvalid's my_multiclass_logloss: 1.0353\n",
      "[167]\ttrain's my_multiclass_logloss: 0.699031\tvalid's my_multiclass_logloss: 1.03389\n",
      "[168]\ttrain's my_multiclass_logloss: 0.698466\tvalid's my_multiclass_logloss: 1.03065\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttrain's my_multiclass_logloss: 0.801335\tvalid's my_multiclass_logloss: 0.940018\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.150930\n",
      "1   feature2    0.119383\n",
      "2   feature3    0.022097\n",
      "3   feature4    0.045294\n",
      "4   feature5    0.111164\n",
      "5   feature6    0.053319\n",
      "6   feature7    0.052822\n",
      "7   feature8    0.167775\n",
      "8   feature9    0.167385\n",
      "9  feature10    0.109832\n",
      "     feature  importance\n",
      "0   feature1    0.127216\n",
      "1   feature2    0.113301\n",
      "2   feature3    0.010585\n",
      "3   feature4    0.001884\n",
      "4   feature5    0.126203\n",
      "5   feature6    0.055058\n",
      "6   feature7    0.061367\n",
      "7   feature8    0.212804\n",
      "8   feature9    0.178057\n",
      "9  feature10    0.113527\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1   -0.139961\n",
      "1   feature2   -0.035896\n",
      "2   feature3   -0.067944\n",
      "3   feature4   -0.256200\n",
      "4   feature5    0.088759\n",
      "5   feature6    0.010260\n",
      "6   feature7    0.050432\n",
      "7   feature8    0.265754\n",
      "8   feature9    0.062983\n",
      "9  feature10    0.021812\n",
      "-------------------- 2 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05514\tvalid's multi_logloss: 1.06764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.04726\tvalid's multi_logloss: 1.06977\n",
      "[3]\ttrain's multi_logloss: 1.03963\tvalid's multi_logloss: 1.05907\n",
      "[4]\ttrain's multi_logloss: 1.03365\tvalid's multi_logloss: 1.05141\n",
      "[5]\ttrain's multi_logloss: 1.02516\tvalid's multi_logloss: 1.05224\n",
      "[6]\ttrain's multi_logloss: 1.01888\tvalid's multi_logloss: 1.05467\n",
      "[7]\ttrain's multi_logloss: 1.01169\tvalid's multi_logloss: 1.05187\n",
      "[8]\ttrain's multi_logloss: 1.00496\tvalid's multi_logloss: 1.04934\n",
      "[9]\ttrain's multi_logloss: 0.998049\tvalid's multi_logloss: 1.04365\n",
      "[10]\ttrain's multi_logloss: 0.994568\tvalid's multi_logloss: 1.0406\n",
      "[11]\ttrain's multi_logloss: 0.988294\tvalid's multi_logloss: 1.0355\n",
      "[12]\ttrain's multi_logloss: 0.983262\tvalid's multi_logloss: 1.02948\n",
      "[13]\ttrain's multi_logloss: 0.976561\tvalid's multi_logloss: 1.02377\n",
      "[14]\ttrain's multi_logloss: 0.970254\tvalid's multi_logloss: 1.02102\n",
      "[15]\ttrain's multi_logloss: 0.965907\tvalid's multi_logloss: 1.01539\n",
      "[16]\ttrain's multi_logloss: 0.961999\tvalid's multi_logloss: 1.00646\n",
      "[17]\ttrain's multi_logloss: 0.955036\tvalid's multi_logloss: 0.997041\n",
      "[18]\ttrain's multi_logloss: 0.94876\tvalid's multi_logloss: 0.988407\n",
      "[19]\ttrain's multi_logloss: 0.94374\tvalid's multi_logloss: 0.983511\n",
      "[20]\ttrain's multi_logloss: 0.939039\tvalid's multi_logloss: 0.974007\n",
      "[21]\ttrain's multi_logloss: 0.933713\tvalid's multi_logloss: 0.979192\n",
      "[22]\ttrain's multi_logloss: 0.927688\tvalid's multi_logloss: 0.981399\n",
      "[23]\ttrain's multi_logloss: 0.924353\tvalid's multi_logloss: 0.984318\n",
      "[24]\ttrain's multi_logloss: 0.918797\tvalid's multi_logloss: 0.986747\n",
      "[25]\ttrain's multi_logloss: 0.913843\tvalid's multi_logloss: 0.991217\n",
      "[26]\ttrain's multi_logloss: 0.907313\tvalid's multi_logloss: 0.990792\n",
      "[27]\ttrain's multi_logloss: 0.901346\tvalid's multi_logloss: 0.993807\n",
      "[28]\ttrain's multi_logloss: 0.898512\tvalid's multi_logloss: 0.99422\n",
      "[29]\ttrain's multi_logloss: 0.894057\tvalid's multi_logloss: 0.993095\n",
      "[30]\ttrain's multi_logloss: 0.890892\tvalid's multi_logloss: 0.995663\n",
      "[31]\ttrain's multi_logloss: 0.886771\tvalid's multi_logloss: 0.994954\n",
      "[32]\ttrain's multi_logloss: 0.883691\tvalid's multi_logloss: 0.996155\n",
      "[33]\ttrain's multi_logloss: 0.879038\tvalid's multi_logloss: 0.986247\n",
      "[34]\ttrain's multi_logloss: 0.876291\tvalid's multi_logloss: 0.986449\n",
      "[35]\ttrain's multi_logloss: 0.872316\tvalid's multi_logloss: 0.978124\n",
      "[36]\ttrain's multi_logloss: 0.869044\tvalid's multi_logloss: 0.971318\n",
      "[37]\ttrain's multi_logloss: 0.866608\tvalid's multi_logloss: 0.970721\n",
      "[38]\ttrain's multi_logloss: 0.863418\tvalid's multi_logloss: 0.96959\n",
      "[39]\ttrain's multi_logloss: 0.860482\tvalid's multi_logloss: 0.9733\n",
      "[40]\ttrain's multi_logloss: 0.858938\tvalid's multi_logloss: 0.973908\n",
      "[41]\ttrain's multi_logloss: 0.855285\tvalid's multi_logloss: 0.96808\n",
      "[42]\ttrain's multi_logloss: 0.851526\tvalid's multi_logloss: 0.971745\n",
      "[43]\ttrain's multi_logloss: 0.849202\tvalid's multi_logloss: 0.972\n",
      "[44]\ttrain's multi_logloss: 0.846019\tvalid's multi_logloss: 0.968753\n",
      "[45]\ttrain's multi_logloss: 0.842346\tvalid's multi_logloss: 0.960475\n",
      "[46]\ttrain's multi_logloss: 0.838688\tvalid's multi_logloss: 0.962769\n",
      "[47]\ttrain's multi_logloss: 0.835324\tvalid's multi_logloss: 0.954882\n",
      "[48]\ttrain's multi_logloss: 0.832339\tvalid's multi_logloss: 0.950489\n",
      "[49]\ttrain's multi_logloss: 0.82803\tvalid's multi_logloss: 0.952708\n",
      "[50]\ttrain's multi_logloss: 0.824626\tvalid's multi_logloss: 0.947181\n",
      "[51]\ttrain's multi_logloss: 0.821528\tvalid's multi_logloss: 0.949164\n",
      "[52]\ttrain's multi_logloss: 0.818793\tvalid's multi_logloss: 0.953515\n",
      "[53]\ttrain's multi_logloss: 0.815633\tvalid's multi_logloss: 0.958453\n",
      "[54]\ttrain's multi_logloss: 0.813432\tvalid's multi_logloss: 0.95724\n",
      "[55]\ttrain's multi_logloss: 0.81112\tvalid's multi_logloss: 0.961445\n",
      "[56]\ttrain's multi_logloss: 0.80873\tvalid's multi_logloss: 0.957745\n",
      "[57]\ttrain's multi_logloss: 0.806186\tvalid's multi_logloss: 0.9554\n",
      "[58]\ttrain's multi_logloss: 0.802542\tvalid's multi_logloss: 0.948764\n",
      "[59]\ttrain's multi_logloss: 0.798477\tvalid's multi_logloss: 0.939829\n",
      "[60]\ttrain's multi_logloss: 0.794369\tvalid's multi_logloss: 0.933496\n",
      "[61]\ttrain's multi_logloss: 0.791587\tvalid's multi_logloss: 0.937357\n",
      "[62]\ttrain's multi_logloss: 0.789126\tvalid's multi_logloss: 0.941079\n",
      "[63]\ttrain's multi_logloss: 0.787557\tvalid's multi_logloss: 0.941917\n",
      "[64]\ttrain's multi_logloss: 0.785141\tvalid's multi_logloss: 0.944873\n",
      "[65]\ttrain's multi_logloss: 0.783059\tvalid's multi_logloss: 0.943514\n",
      "[66]\ttrain's multi_logloss: 0.780958\tvalid's multi_logloss: 0.937427\n",
      "[67]\ttrain's multi_logloss: 0.77924\tvalid's multi_logloss: 0.936958\n",
      "[68]\ttrain's multi_logloss: 0.777513\tvalid's multi_logloss: 0.933174\n",
      "[69]\ttrain's multi_logloss: 0.774211\tvalid's multi_logloss: 0.92676\n",
      "[70]\ttrain's multi_logloss: 0.771913\tvalid's multi_logloss: 0.925433\n",
      "[71]\ttrain's multi_logloss: 0.770978\tvalid's multi_logloss: 0.923728\n",
      "[72]\ttrain's multi_logloss: 0.770167\tvalid's multi_logloss: 0.92249\n",
      "[73]\ttrain's multi_logloss: 0.767331\tvalid's multi_logloss: 0.923385\n",
      "[74]\ttrain's multi_logloss: 0.76501\tvalid's multi_logloss: 0.922958\n",
      "[75]\ttrain's multi_logloss: 0.762564\tvalid's multi_logloss: 0.928093\n",
      "[76]\ttrain's multi_logloss: 0.759856\tvalid's multi_logloss: 0.935032\n",
      "[77]\ttrain's multi_logloss: 0.758037\tvalid's multi_logloss: 0.93642\n",
      "[78]\ttrain's multi_logloss: 0.756319\tvalid's multi_logloss: 0.936725\n",
      "[79]\ttrain's multi_logloss: 0.754341\tvalid's multi_logloss: 0.936737\n",
      "[80]\ttrain's multi_logloss: 0.753261\tvalid's multi_logloss: 0.938231\n",
      "[81]\ttrain's multi_logloss: 0.750664\tvalid's multi_logloss: 0.935152\n",
      "[82]\ttrain's multi_logloss: 0.748186\tvalid's multi_logloss: 0.940007\n",
      "[83]\ttrain's multi_logloss: 0.74581\tvalid's multi_logloss: 0.943729\n",
      "[84]\ttrain's multi_logloss: 0.743774\tvalid's multi_logloss: 0.93664\n",
      "[85]\ttrain's multi_logloss: 0.742537\tvalid's multi_logloss: 0.940967\n",
      "[86]\ttrain's multi_logloss: 0.739916\tvalid's multi_logloss: 0.933084\n",
      "[87]\ttrain's multi_logloss: 0.737717\tvalid's multi_logloss: 0.93299\n",
      "[88]\ttrain's multi_logloss: 0.735715\tvalid's multi_logloss: 0.933394\n",
      "[89]\ttrain's multi_logloss: 0.733524\tvalid's multi_logloss: 0.929081\n",
      "[90]\ttrain's multi_logloss: 0.733171\tvalid's multi_logloss: 0.932191\n",
      "[91]\ttrain's multi_logloss: 0.732291\tvalid's multi_logloss: 0.931436\n",
      "[92]\ttrain's multi_logloss: 0.731073\tvalid's multi_logloss: 0.931326\n",
      "[93]\ttrain's multi_logloss: 0.729146\tvalid's multi_logloss: 0.925515\n",
      "[94]\ttrain's multi_logloss: 0.728338\tvalid's multi_logloss: 0.927358\n",
      "[95]\ttrain's multi_logloss: 0.72711\tvalid's multi_logloss: 0.925006\n",
      "[96]\ttrain's multi_logloss: 0.726548\tvalid's multi_logloss: 0.926933\n",
      "[97]\ttrain's multi_logloss: 0.725437\tvalid's multi_logloss: 0.92765\n",
      "[98]\ttrain's multi_logloss: 0.724025\tvalid's multi_logloss: 0.931235\n",
      "[99]\ttrain's multi_logloss: 0.722167\tvalid's multi_logloss: 0.926766\n",
      "[100]\ttrain's multi_logloss: 0.720458\tvalid's multi_logloss: 0.922525\n",
      "[101]\ttrain's multi_logloss: 0.720018\tvalid's multi_logloss: 0.922147\n",
      "[102]\ttrain's multi_logloss: 0.719587\tvalid's multi_logloss: 0.918962\n",
      "[103]\ttrain's multi_logloss: 0.719016\tvalid's multi_logloss: 0.91925\n",
      "[104]\ttrain's multi_logloss: 0.7184\tvalid's multi_logloss: 0.91761\n",
      "[105]\ttrain's multi_logloss: 0.716961\tvalid's multi_logloss: 0.920185\n",
      "[106]\ttrain's multi_logloss: 0.715653\tvalid's multi_logloss: 0.915825\n",
      "[107]\ttrain's multi_logloss: 0.714495\tvalid's multi_logloss: 0.911693\n",
      "[108]\ttrain's multi_logloss: 0.713406\tvalid's multi_logloss: 0.91113\n",
      "[109]\ttrain's multi_logloss: 0.712147\tvalid's multi_logloss: 0.912278\n",
      "[110]\ttrain's multi_logloss: 0.711116\tvalid's multi_logloss: 0.914277\n",
      "[111]\ttrain's multi_logloss: 0.710308\tvalid's multi_logloss: 0.917408\n",
      "[112]\ttrain's multi_logloss: 0.709405\tvalid's multi_logloss: 0.915927\n",
      "[113]\ttrain's multi_logloss: 0.707571\tvalid's multi_logloss: 0.910523\n",
      "[114]\ttrain's multi_logloss: 0.705782\tvalid's multi_logloss: 0.906259\n",
      "[115]\ttrain's multi_logloss: 0.704425\tvalid's multi_logloss: 0.899541\n",
      "[116]\ttrain's multi_logloss: 0.703326\tvalid's multi_logloss: 0.894649\n",
      "[117]\ttrain's multi_logloss: 0.701448\tvalid's multi_logloss: 0.892002\n",
      "[118]\ttrain's multi_logloss: 0.699825\tvalid's multi_logloss: 0.889558\n",
      "[119]\ttrain's multi_logloss: 0.698365\tvalid's multi_logloss: 0.889493\n",
      "[120]\ttrain's multi_logloss: 0.697272\tvalid's multi_logloss: 0.884505\n",
      "[121]\ttrain's multi_logloss: 0.695664\tvalid's multi_logloss: 0.878744\n",
      "[122]\ttrain's multi_logloss: 0.694212\tvalid's multi_logloss: 0.87103\n",
      "[123]\ttrain's multi_logloss: 0.692793\tvalid's multi_logloss: 0.86858\n",
      "[124]\ttrain's multi_logloss: 0.691587\tvalid's multi_logloss: 0.861368\n",
      "[125]\ttrain's multi_logloss: 0.690572\tvalid's multi_logloss: 0.859693\n",
      "[126]\ttrain's multi_logloss: 0.69018\tvalid's multi_logloss: 0.86044\n",
      "[127]\ttrain's multi_logloss: 0.689749\tvalid's multi_logloss: 0.858247\n",
      "[128]\ttrain's multi_logloss: 0.689655\tvalid's multi_logloss: 0.85337\n",
      "[129]\ttrain's multi_logloss: 0.689082\tvalid's multi_logloss: 0.853927\n",
      "[130]\ttrain's multi_logloss: 0.688877\tvalid's multi_logloss: 0.858491\n",
      "[131]\ttrain's multi_logloss: 0.688836\tvalid's multi_logloss: 0.863205\n",
      "[132]\ttrain's multi_logloss: 0.689021\tvalid's multi_logloss: 0.867274\n",
      "[133]\ttrain's multi_logloss: 0.686969\tvalid's multi_logloss: 0.871561\n",
      "[134]\ttrain's multi_logloss: 0.685168\tvalid's multi_logloss: 0.870837\n",
      "[135]\ttrain's multi_logloss: 0.68394\tvalid's multi_logloss: 0.872846\n",
      "[136]\ttrain's multi_logloss: 0.682233\tvalid's multi_logloss: 0.872366\n",
      "[137]\ttrain's multi_logloss: 0.680527\tvalid's multi_logloss: 0.874339\n",
      "[138]\ttrain's multi_logloss: 0.67898\tvalid's multi_logloss: 0.876377\n",
      "[139]\ttrain's multi_logloss: 0.677703\tvalid's multi_logloss: 0.877426\n",
      "[140]\ttrain's multi_logloss: 0.676393\tvalid's multi_logloss: 0.877349\n",
      "[141]\ttrain's multi_logloss: 0.675395\tvalid's multi_logloss: 0.874958\n",
      "[142]\ttrain's multi_logloss: 0.674091\tvalid's multi_logloss: 0.878118\n",
      "[143]\ttrain's multi_logloss: 0.673158\tvalid's multi_logloss: 0.878517\n",
      "[144]\ttrain's multi_logloss: 0.67221\tvalid's multi_logloss: 0.881698\n",
      "[145]\ttrain's multi_logloss: 0.670315\tvalid's multi_logloss: 0.882996\n",
      "[146]\ttrain's multi_logloss: 0.669124\tvalid's multi_logloss: 0.885082\n",
      "[147]\ttrain's multi_logloss: 0.667416\tvalid's multi_logloss: 0.886521\n",
      "[148]\ttrain's multi_logloss: 0.666423\tvalid's multi_logloss: 0.888085\n",
      "[149]\ttrain's multi_logloss: 0.665601\tvalid's multi_logloss: 0.888185\n",
      "[150]\ttrain's multi_logloss: 0.664495\tvalid's multi_logloss: 0.886717\n",
      "[151]\ttrain's multi_logloss: 0.66393\tvalid's multi_logloss: 0.88723\n",
      "[152]\ttrain's multi_logloss: 0.66284\tvalid's multi_logloss: 0.887908\n",
      "[153]\ttrain's multi_logloss: 0.661498\tvalid's multi_logloss: 0.878714\n",
      "[154]\ttrain's multi_logloss: 0.66035\tvalid's multi_logloss: 0.869996\n",
      "[155]\ttrain's multi_logloss: 0.659486\tvalid's multi_logloss: 0.866122\n",
      "[156]\ttrain's multi_logloss: 0.658734\tvalid's multi_logloss: 0.862478\n",
      "[157]\ttrain's multi_logloss: 0.657807\tvalid's multi_logloss: 0.861494\n",
      "[158]\ttrain's multi_logloss: 0.656949\tvalid's multi_logloss: 0.860268\n",
      "[159]\ttrain's multi_logloss: 0.65671\tvalid's multi_logloss: 0.859324\n",
      "[160]\ttrain's multi_logloss: 0.656638\tvalid's multi_logloss: 0.858367\n",
      "[161]\ttrain's multi_logloss: 0.65452\tvalid's multi_logloss: 0.85398\n",
      "[162]\ttrain's multi_logloss: 0.651522\tvalid's multi_logloss: 0.852893\n",
      "[163]\ttrain's multi_logloss: 0.64846\tvalid's multi_logloss: 0.848352\n",
      "[164]\ttrain's multi_logloss: 0.646002\tvalid's multi_logloss: 0.84266\n",
      "[165]\ttrain's multi_logloss: 0.644068\tvalid's multi_logloss: 0.838177\n",
      "[166]\ttrain's multi_logloss: 0.643419\tvalid's multi_logloss: 0.832363\n",
      "[167]\ttrain's multi_logloss: 0.64209\tvalid's multi_logloss: 0.830832\n",
      "[168]\ttrain's multi_logloss: 0.640605\tvalid's multi_logloss: 0.827659\n",
      "[169]\ttrain's multi_logloss: 0.639129\tvalid's multi_logloss: 0.826045\n",
      "[170]\ttrain's multi_logloss: 0.637997\tvalid's multi_logloss: 0.824602\n",
      "[171]\ttrain's multi_logloss: 0.637012\tvalid's multi_logloss: 0.823311\n",
      "[172]\ttrain's multi_logloss: 0.635968\tvalid's multi_logloss: 0.818933\n",
      "[173]\ttrain's multi_logloss: 0.63502\tvalid's multi_logloss: 0.813498\n",
      "[174]\ttrain's multi_logloss: 0.634259\tvalid's multi_logloss: 0.808324\n",
      "[175]\ttrain's multi_logloss: 0.633366\tvalid's multi_logloss: 0.808047\n",
      "[176]\ttrain's multi_logloss: 0.632502\tvalid's multi_logloss: 0.806351\n",
      "[177]\ttrain's multi_logloss: 0.630845\tvalid's multi_logloss: 0.808784\n",
      "[178]\ttrain's multi_logloss: 0.629079\tvalid's multi_logloss: 0.815645\n",
      "[179]\ttrain's multi_logloss: 0.627783\tvalid's multi_logloss: 0.81256\n",
      "[180]\ttrain's multi_logloss: 0.626558\tvalid's multi_logloss: 0.812918\n",
      "[181]\ttrain's multi_logloss: 0.62593\tvalid's multi_logloss: 0.807725\n",
      "[182]\ttrain's multi_logloss: 0.625315\tvalid's multi_logloss: 0.807621\n",
      "[183]\ttrain's multi_logloss: 0.624229\tvalid's multi_logloss: 0.807531\n",
      "[184]\ttrain's multi_logloss: 0.623279\tvalid's multi_logloss: 0.808019\n",
      "[185]\ttrain's multi_logloss: 0.622315\tvalid's multi_logloss: 0.805795\n",
      "[186]\ttrain's multi_logloss: 0.621422\tvalid's multi_logloss: 0.800051\n",
      "[187]\ttrain's multi_logloss: 0.621201\tvalid's multi_logloss: 0.799881\n",
      "[188]\ttrain's multi_logloss: 0.620481\tvalid's multi_logloss: 0.794614\n",
      "[189]\ttrain's multi_logloss: 0.618874\tvalid's multi_logloss: 0.793739\n",
      "[190]\ttrain's multi_logloss: 0.617491\tvalid's multi_logloss: 0.793092\n",
      "[191]\ttrain's multi_logloss: 0.616522\tvalid's multi_logloss: 0.79138\n",
      "[192]\ttrain's multi_logloss: 0.61551\tvalid's multi_logloss: 0.792444\n",
      "[193]\ttrain's multi_logloss: 0.614577\tvalid's multi_logloss: 0.788885\n",
      "[194]\ttrain's multi_logloss: 0.613796\tvalid's multi_logloss: 0.785559\n",
      "[195]\ttrain's multi_logloss: 0.613018\tvalid's multi_logloss: 0.785365\n",
      "[196]\ttrain's multi_logloss: 0.612478\tvalid's multi_logloss: 0.783128\n",
      "[197]\ttrain's multi_logloss: 0.61144\tvalid's multi_logloss: 0.78908\n",
      "[198]\ttrain's multi_logloss: 0.610589\tvalid's multi_logloss: 0.78892\n",
      "[199]\ttrain's multi_logloss: 0.610083\tvalid's multi_logloss: 0.79135\n",
      "[200]\ttrain's multi_logloss: 0.609803\tvalid's multi_logloss: 0.787265\n",
      "[201]\ttrain's multi_logloss: 0.609116\tvalid's multi_logloss: 0.78767\n",
      "[202]\ttrain's multi_logloss: 0.608289\tvalid's multi_logloss: 0.785987\n",
      "[203]\ttrain's multi_logloss: 0.607539\tvalid's multi_logloss: 0.784395\n",
      "[204]\ttrain's multi_logloss: 0.60704\tvalid's multi_logloss: 0.784825\n",
      "[205]\ttrain's multi_logloss: 0.606495\tvalid's multi_logloss: 0.782669\n",
      "[206]\ttrain's multi_logloss: 0.605486\tvalid's multi_logloss: 0.778108\n",
      "[207]\ttrain's multi_logloss: 0.604335\tvalid's multi_logloss: 0.773026\n",
      "[208]\ttrain's multi_logloss: 0.603584\tvalid's multi_logloss: 0.769006\n",
      "[209]\ttrain's multi_logloss: 0.602919\tvalid's multi_logloss: 0.768458\n",
      "[210]\ttrain's multi_logloss: 0.602308\tvalid's multi_logloss: 0.769242\n",
      "[211]\ttrain's multi_logloss: 0.601578\tvalid's multi_logloss: 0.767835\n",
      "[212]\ttrain's multi_logloss: 0.601655\tvalid's multi_logloss: 0.767379\n",
      "[213]\ttrain's multi_logloss: 0.600298\tvalid's multi_logloss: 0.769794\n",
      "[214]\ttrain's multi_logloss: 0.599078\tvalid's multi_logloss: 0.771776\n",
      "[215]\ttrain's multi_logloss: 0.59777\tvalid's multi_logloss: 0.774908\n",
      "[216]\ttrain's multi_logloss: 0.596653\tvalid's multi_logloss: 0.778067\n",
      "[217]\ttrain's multi_logloss: 0.595968\tvalid's multi_logloss: 0.779201\n",
      "[218]\ttrain's multi_logloss: 0.595375\tvalid's multi_logloss: 0.77961\n",
      "[219]\ttrain's multi_logloss: 0.594699\tvalid's multi_logloss: 0.779458\n",
      "[220]\ttrain's multi_logloss: 0.59488\tvalid's multi_logloss: 0.776033\n",
      "[221]\ttrain's multi_logloss: 0.593881\tvalid's multi_logloss: 0.777845\n",
      "[222]\ttrain's multi_logloss: 0.593137\tvalid's multi_logloss: 0.777299\n",
      "[223]\ttrain's multi_logloss: 0.592529\tvalid's multi_logloss: 0.777333\n",
      "[224]\ttrain's multi_logloss: 0.59237\tvalid's multi_logloss: 0.776422\n",
      "[225]\ttrain's multi_logloss: 0.591307\tvalid's multi_logloss: 0.775215\n",
      "[226]\ttrain's multi_logloss: 0.590727\tvalid's multi_logloss: 0.776134\n",
      "[227]\ttrain's multi_logloss: 0.590237\tvalid's multi_logloss: 0.777514\n",
      "[228]\ttrain's multi_logloss: 0.589936\tvalid's multi_logloss: 0.779256\n",
      "[229]\ttrain's multi_logloss: 0.589638\tvalid's multi_logloss: 0.778353\n",
      "[230]\ttrain's multi_logloss: 0.588726\tvalid's multi_logloss: 0.778063\n",
      "[231]\ttrain's multi_logloss: 0.587964\tvalid's multi_logloss: 0.780826\n",
      "[232]\ttrain's multi_logloss: 0.587639\tvalid's multi_logloss: 0.782551\n",
      "[233]\ttrain's multi_logloss: 0.587092\tvalid's multi_logloss: 0.783045\n",
      "[234]\ttrain's multi_logloss: 0.586268\tvalid's multi_logloss: 0.784567\n",
      "[235]\ttrain's multi_logloss: 0.585391\tvalid's multi_logloss: 0.786727\n",
      "[236]\ttrain's multi_logloss: 0.584949\tvalid's multi_logloss: 0.790541\n",
      "[237]\ttrain's multi_logloss: 0.584949\tvalid's multi_logloss: 0.790541\n",
      "[238]\ttrain's multi_logloss: 0.584075\tvalid's multi_logloss: 0.792333\n",
      "[239]\ttrain's multi_logloss: 0.583422\tvalid's multi_logloss: 0.793433\n",
      "[240]\ttrain's multi_logloss: 0.582951\tvalid's multi_logloss: 0.791572\n",
      "[241]\ttrain's multi_logloss: 0.582447\tvalid's multi_logloss: 0.790448\n",
      "[242]\ttrain's multi_logloss: 0.581771\tvalid's multi_logloss: 0.790361\n",
      "[243]\ttrain's multi_logloss: 0.58103\tvalid's multi_logloss: 0.786939\n",
      "[244]\ttrain's multi_logloss: 0.58075\tvalid's multi_logloss: 0.787498\n",
      "[245]\ttrain's multi_logloss: 0.580331\tvalid's multi_logloss: 0.783617\n",
      "[246]\ttrain's multi_logloss: 0.579533\tvalid's multi_logloss: 0.785243\n",
      "[247]\ttrain's multi_logloss: 0.578955\tvalid's multi_logloss: 0.784415\n",
      "[248]\ttrain's multi_logloss: 0.578332\tvalid's multi_logloss: 0.786154\n",
      "[249]\ttrain's multi_logloss: 0.577911\tvalid's multi_logloss: 0.785474\n",
      "[250]\ttrain's multi_logloss: 0.57726\tvalid's multi_logloss: 0.793315\n",
      "[251]\ttrain's multi_logloss: 0.576449\tvalid's multi_logloss: 0.803473\n",
      "[252]\ttrain's multi_logloss: 0.575877\tvalid's multi_logloss: 0.808876\n",
      "[253]\ttrain's multi_logloss: 0.575425\tvalid's multi_logloss: 0.812797\n",
      "[254]\ttrain's multi_logloss: 0.574298\tvalid's multi_logloss: 0.813483\n",
      "[255]\ttrain's multi_logloss: 0.573204\tvalid's multi_logloss: 0.8192\n",
      "[256]\ttrain's multi_logloss: 0.572274\tvalid's multi_logloss: 0.825423\n",
      "[257]\ttrain's multi_logloss: 0.571623\tvalid's multi_logloss: 0.826897\n",
      "[258]\ttrain's multi_logloss: 0.570818\tvalid's multi_logloss: 0.830402\n",
      "[259]\ttrain's multi_logloss: 0.569759\tvalid's multi_logloss: 0.834029\n",
      "[260]\ttrain's multi_logloss: 0.56912\tvalid's multi_logloss: 0.835228\n",
      "[261]\ttrain's multi_logloss: 0.568392\tvalid's multi_logloss: 0.83629\n",
      "[262]\ttrain's multi_logloss: 0.568105\tvalid's multi_logloss: 0.840946\n",
      "[263]\ttrain's multi_logloss: 0.567292\tvalid's multi_logloss: 0.843967\n",
      "[264]\ttrain's multi_logloss: 0.566819\tvalid's multi_logloss: 0.849309\n",
      "[265]\ttrain's multi_logloss: 0.566408\tvalid's multi_logloss: 0.854518\n",
      "[266]\ttrain's multi_logloss: 0.565716\tvalid's multi_logloss: 0.84722\n",
      "[267]\ttrain's multi_logloss: 0.565097\tvalid's multi_logloss: 0.846768\n",
      "[268]\ttrain's multi_logloss: 0.564581\tvalid's multi_logloss: 0.848085\n",
      "[269]\ttrain's multi_logloss: 0.564144\tvalid's multi_logloss: 0.850027\n",
      "[270]\ttrain's multi_logloss: 0.563955\tvalid's multi_logloss: 0.846947\n",
      "[271]\ttrain's multi_logloss: 0.56377\tvalid's multi_logloss: 0.849814\n",
      "[272]\ttrain's multi_logloss: 0.563408\tvalid's multi_logloss: 0.850957\n",
      "[273]\ttrain's multi_logloss: 0.563287\tvalid's multi_logloss: 0.8485\n",
      "[274]\ttrain's multi_logloss: 0.563187\tvalid's multi_logloss: 0.844006\n",
      "[275]\ttrain's multi_logloss: 0.562028\tvalid's multi_logloss: 0.834649\n",
      "[276]\ttrain's multi_logloss: 0.561063\tvalid's multi_logloss: 0.825697\n",
      "[277]\ttrain's multi_logloss: 0.560277\tvalid's multi_logloss: 0.817127\n",
      "[278]\ttrain's multi_logloss: 0.559722\tvalid's multi_logloss: 0.813859\n",
      "[279]\ttrain's multi_logloss: 0.558566\tvalid's multi_logloss: 0.820431\n",
      "[280]\ttrain's multi_logloss: 0.557669\tvalid's multi_logloss: 0.82554\n",
      "[281]\ttrain's multi_logloss: 0.556979\tvalid's multi_logloss: 0.832186\n",
      "[282]\ttrain's multi_logloss: 0.556632\tvalid's multi_logloss: 0.835128\n",
      "[283]\ttrain's multi_logloss: 0.555993\tvalid's multi_logloss: 0.838191\n",
      "[284]\ttrain's multi_logloss: 0.555689\tvalid's multi_logloss: 0.842184\n",
      "[285]\ttrain's multi_logloss: 0.555289\tvalid's multi_logloss: 0.845261\n",
      "[286]\ttrain's multi_logloss: 0.554293\tvalid's multi_logloss: 0.844243\n",
      "[287]\ttrain's multi_logloss: 0.552895\tvalid's multi_logloss: 0.845858\n",
      "[288]\ttrain's multi_logloss: 0.551426\tvalid's multi_logloss: 0.843263\n",
      "[289]\ttrain's multi_logloss: 0.55022\tvalid's multi_logloss: 0.843898\n",
      "[290]\ttrain's multi_logloss: 0.5493\tvalid's multi_logloss: 0.840384\n",
      "[291]\ttrain's multi_logloss: 0.548168\tvalid's multi_logloss: 0.839415\n",
      "[292]\ttrain's multi_logloss: 0.547401\tvalid's multi_logloss: 0.839513\n",
      "[293]\ttrain's multi_logloss: 0.547017\tvalid's multi_logloss: 0.839032\n",
      "[294]\ttrain's multi_logloss: 0.546263\tvalid's multi_logloss: 0.840825\n",
      "[295]\ttrain's multi_logloss: 0.545452\tvalid's multi_logloss: 0.843227\n",
      "[296]\ttrain's multi_logloss: 0.544954\tvalid's multi_logloss: 0.845311\n",
      "[297]\ttrain's multi_logloss: 0.544268\tvalid's multi_logloss: 0.847681\n",
      "[298]\ttrain's multi_logloss: 0.543365\tvalid's multi_logloss: 0.838717\n",
      "[299]\ttrain's multi_logloss: 0.542575\tvalid's multi_logloss: 0.830069\n",
      "[300]\ttrain's multi_logloss: 0.542338\tvalid's multi_logloss: 0.822952\n",
      "[301]\ttrain's multi_logloss: 0.541712\tvalid's multi_logloss: 0.815019\n",
      "[302]\ttrain's multi_logloss: 0.54146\tvalid's multi_logloss: 0.813621\n",
      "[303]\ttrain's multi_logloss: 0.540769\tvalid's multi_logloss: 0.813865\n",
      "[304]\ttrain's multi_logloss: 0.540147\tvalid's multi_logloss: 0.814166\n",
      "[305]\ttrain's multi_logloss: 0.539589\tvalid's multi_logloss: 0.814519\n",
      "[306]\ttrain's multi_logloss: 0.539182\tvalid's multi_logloss: 0.816439\n",
      "[307]\ttrain's multi_logloss: 0.538759\tvalid's multi_logloss: 0.821649\n",
      "[308]\ttrain's multi_logloss: 0.538177\tvalid's multi_logloss: 0.825223\n",
      "[309]\ttrain's multi_logloss: 0.537737\tvalid's multi_logloss: 0.827769\n",
      "[310]\ttrain's multi_logloss: 0.53723\tvalid's multi_logloss: 0.830525\n",
      "[311]\ttrain's multi_logloss: 0.536991\tvalid's multi_logloss: 0.836111\n",
      "[312]\ttrain's multi_logloss: 0.536835\tvalid's multi_logloss: 0.841571\n",
      "Early stopping, best iteration is:\n",
      "[212]\ttrain's multi_logloss: 0.601655\tvalid's multi_logloss: 0.767379\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.111622\n",
      "1   feature2    0.100409\n",
      "2   feature3    0.066229\n",
      "3   feature4    0.072476\n",
      "4   feature5    0.148150\n",
      "5   feature6    0.050821\n",
      "6   feature7    0.076391\n",
      "7   feature8    0.127728\n",
      "8   feature9    0.162783\n",
      "9  feature10    0.083391\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08995\tvalid's my_multiclass_logloss: 1.08203\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07918\tvalid's my_multiclass_logloss: 1.0774\n",
      "[3]\ttrain's my_multiclass_logloss: 1.06856\tvalid's my_multiclass_logloss: 1.0732\n",
      "[4]\ttrain's my_multiclass_logloss: 1.06101\tvalid's my_multiclass_logloss: 1.06912\n",
      "[5]\ttrain's my_multiclass_logloss: 1.05045\tvalid's my_multiclass_logloss: 1.06039\n",
      "[6]\ttrain's my_multiclass_logloss: 1.03924\tvalid's my_multiclass_logloss: 1.05529\n",
      "[7]\ttrain's my_multiclass_logloss: 1.03132\tvalid's my_multiclass_logloss: 1.05269\n",
      "[8]\ttrain's my_multiclass_logloss: 1.02136\tvalid's my_multiclass_logloss: 1.05307\n",
      "[9]\ttrain's my_multiclass_logloss: 1.01594\tvalid's my_multiclass_logloss: 1.04161\n",
      "[10]\ttrain's my_multiclass_logloss: 1.00854\tvalid's my_multiclass_logloss: 1.03559\n",
      "[11]\ttrain's my_multiclass_logloss: 1.00181\tvalid's my_multiclass_logloss: 1.03021\n",
      "[12]\ttrain's my_multiclass_logloss: 0.997554\tvalid's my_multiclass_logloss: 1.02162\n",
      "[13]\ttrain's my_multiclass_logloss: 0.990825\tvalid's my_multiclass_logloss: 1.01141\n",
      "[14]\ttrain's my_multiclass_logloss: 0.9843\tvalid's my_multiclass_logloss: 1.00906\n",
      "[15]\ttrain's my_multiclass_logloss: 0.97705\tvalid's my_multiclass_logloss: 1.00185\n",
      "[16]\ttrain's my_multiclass_logloss: 0.971053\tvalid's my_multiclass_logloss: 0.997073\n",
      "[17]\ttrain's my_multiclass_logloss: 0.964094\tvalid's my_multiclass_logloss: 0.988797\n",
      "[18]\ttrain's my_multiclass_logloss: 0.958142\tvalid's my_multiclass_logloss: 0.979521\n",
      "[19]\ttrain's my_multiclass_logloss: 0.952484\tvalid's my_multiclass_logloss: 0.972816\n",
      "[20]\ttrain's my_multiclass_logloss: 0.9479\tvalid's my_multiclass_logloss: 0.965316\n",
      "[21]\ttrain's my_multiclass_logloss: 0.94119\tvalid's my_multiclass_logloss: 0.967846\n",
      "[22]\ttrain's my_multiclass_logloss: 0.936159\tvalid's my_multiclass_logloss: 0.968013\n",
      "[23]\ttrain's my_multiclass_logloss: 0.931305\tvalid's my_multiclass_logloss: 0.967552\n",
      "[24]\ttrain's my_multiclass_logloss: 0.927437\tvalid's my_multiclass_logloss: 0.975165\n",
      "[25]\ttrain's my_multiclass_logloss: 0.921024\tvalid's my_multiclass_logloss: 0.970292\n",
      "[26]\ttrain's my_multiclass_logloss: 0.916999\tvalid's my_multiclass_logloss: 0.971656\n",
      "[27]\ttrain's my_multiclass_logloss: 0.912727\tvalid's my_multiclass_logloss: 0.977282\n",
      "[28]\ttrain's my_multiclass_logloss: 0.908305\tvalid's my_multiclass_logloss: 0.970852\n",
      "[29]\ttrain's my_multiclass_logloss: 0.90495\tvalid's my_multiclass_logloss: 0.975945\n",
      "[30]\ttrain's my_multiclass_logloss: 0.901514\tvalid's my_multiclass_logloss: 0.976226\n",
      "[31]\ttrain's my_multiclass_logloss: 0.897932\tvalid's my_multiclass_logloss: 0.973064\n",
      "[32]\ttrain's my_multiclass_logloss: 0.895988\tvalid's my_multiclass_logloss: 0.978631\n",
      "[33]\ttrain's my_multiclass_logloss: 0.891695\tvalid's my_multiclass_logloss: 0.970981\n",
      "[34]\ttrain's my_multiclass_logloss: 0.888592\tvalid's my_multiclass_logloss: 0.960264\n",
      "[35]\ttrain's my_multiclass_logloss: 0.885211\tvalid's my_multiclass_logloss: 0.957064\n",
      "[36]\ttrain's my_multiclass_logloss: 0.882155\tvalid's my_multiclass_logloss: 0.949963\n",
      "[37]\ttrain's my_multiclass_logloss: 0.879271\tvalid's my_multiclass_logloss: 0.948413\n",
      "[38]\ttrain's my_multiclass_logloss: 0.876171\tvalid's my_multiclass_logloss: 0.950512\n",
      "[39]\ttrain's my_multiclass_logloss: 0.873781\tvalid's my_multiclass_logloss: 0.951332\n",
      "[40]\ttrain's my_multiclass_logloss: 0.872121\tvalid's my_multiclass_logloss: 0.953453\n",
      "[41]\ttrain's my_multiclass_logloss: 0.867321\tvalid's my_multiclass_logloss: 0.955977\n",
      "[42]\ttrain's my_multiclass_logloss: 0.864357\tvalid's my_multiclass_logloss: 0.959549\n",
      "[43]\ttrain's my_multiclass_logloss: 0.861196\tvalid's my_multiclass_logloss: 0.956728\n",
      "[44]\ttrain's my_multiclass_logloss: 0.858534\tvalid's my_multiclass_logloss: 0.960886\n",
      "[45]\ttrain's my_multiclass_logloss: 0.854444\tvalid's my_multiclass_logloss: 0.955931\n",
      "[46]\ttrain's my_multiclass_logloss: 0.85311\tvalid's my_multiclass_logloss: 0.951828\n",
      "[47]\ttrain's my_multiclass_logloss: 0.849551\tvalid's my_multiclass_logloss: 0.947394\n",
      "[48]\ttrain's my_multiclass_logloss: 0.846347\tvalid's my_multiclass_logloss: 0.943299\n",
      "[49]\ttrain's my_multiclass_logloss: 0.843925\tvalid's my_multiclass_logloss: 0.946786\n",
      "[50]\ttrain's my_multiclass_logloss: 0.841583\tvalid's my_multiclass_logloss: 0.947341\n",
      "[51]\ttrain's my_multiclass_logloss: 0.839651\tvalid's my_multiclass_logloss: 0.945193\n",
      "[52]\ttrain's my_multiclass_logloss: 0.837402\tvalid's my_multiclass_logloss: 0.942472\n",
      "[53]\ttrain's my_multiclass_logloss: 0.834576\tvalid's my_multiclass_logloss: 0.948803\n",
      "[54]\ttrain's my_multiclass_logloss: 0.832295\tvalid's my_multiclass_logloss: 0.947459\n",
      "[55]\ttrain's my_multiclass_logloss: 0.829992\tvalid's my_multiclass_logloss: 0.948884\n",
      "[56]\ttrain's my_multiclass_logloss: 0.828061\tvalid's my_multiclass_logloss: 0.947701\n",
      "[57]\ttrain's my_multiclass_logloss: 0.82462\tvalid's my_multiclass_logloss: 0.943695\n",
      "[58]\ttrain's my_multiclass_logloss: 0.820898\tvalid's my_multiclass_logloss: 0.937659\n",
      "[59]\ttrain's my_multiclass_logloss: 0.818004\tvalid's my_multiclass_logloss: 0.93112\n",
      "[60]\ttrain's my_multiclass_logloss: 0.814779\tvalid's my_multiclass_logloss: 0.925766\n",
      "[61]\ttrain's my_multiclass_logloss: 0.811657\tvalid's my_multiclass_logloss: 0.930764\n",
      "[62]\ttrain's my_multiclass_logloss: 0.810108\tvalid's my_multiclass_logloss: 0.936609\n",
      "[63]\ttrain's my_multiclass_logloss: 0.807731\tvalid's my_multiclass_logloss: 0.941219\n",
      "[64]\ttrain's my_multiclass_logloss: 0.80615\tvalid's my_multiclass_logloss: 0.947179\n",
      "[65]\ttrain's my_multiclass_logloss: 0.804052\tvalid's my_multiclass_logloss: 0.940084\n",
      "[66]\ttrain's my_multiclass_logloss: 0.802079\tvalid's my_multiclass_logloss: 0.935102\n",
      "[67]\ttrain's my_multiclass_logloss: 0.800341\tvalid's my_multiclass_logloss: 0.930387\n",
      "[68]\ttrain's my_multiclass_logloss: 0.798491\tvalid's my_multiclass_logloss: 0.927919\n",
      "[69]\ttrain's my_multiclass_logloss: 0.797784\tvalid's my_multiclass_logloss: 0.926902\n",
      "[70]\ttrain's my_multiclass_logloss: 0.795914\tvalid's my_multiclass_logloss: 0.922896\n",
      "[71]\ttrain's my_multiclass_logloss: 0.795576\tvalid's my_multiclass_logloss: 0.927445\n",
      "[72]\ttrain's my_multiclass_logloss: 0.794571\tvalid's my_multiclass_logloss: 0.925539\n",
      "[73]\ttrain's my_multiclass_logloss: 0.792649\tvalid's my_multiclass_logloss: 0.930816\n",
      "[74]\ttrain's my_multiclass_logloss: 0.791269\tvalid's my_multiclass_logloss: 0.930711\n",
      "[75]\ttrain's my_multiclass_logloss: 0.789467\tvalid's my_multiclass_logloss: 0.930628\n",
      "[76]\ttrain's my_multiclass_logloss: 0.788393\tvalid's my_multiclass_logloss: 0.929175\n",
      "[77]\ttrain's my_multiclass_logloss: 0.787114\tvalid's my_multiclass_logloss: 0.930736\n",
      "[78]\ttrain's my_multiclass_logloss: 0.78592\tvalid's my_multiclass_logloss: 0.93686\n",
      "[79]\ttrain's my_multiclass_logloss: 0.78466\tvalid's my_multiclass_logloss: 0.94082\n",
      "[80]\ttrain's my_multiclass_logloss: 0.783743\tvalid's my_multiclass_logloss: 0.94243\n",
      "[81]\ttrain's my_multiclass_logloss: 0.781577\tvalid's my_multiclass_logloss: 0.944236\n",
      "[82]\ttrain's my_multiclass_logloss: 0.779811\tvalid's my_multiclass_logloss: 0.945273\n",
      "[83]\ttrain's my_multiclass_logloss: 0.778296\tvalid's my_multiclass_logloss: 0.950626\n",
      "[84]\ttrain's my_multiclass_logloss: 0.776885\tvalid's my_multiclass_logloss: 0.952407\n",
      "[85]\ttrain's my_multiclass_logloss: 0.775172\tvalid's my_multiclass_logloss: 0.952393\n",
      "[86]\ttrain's my_multiclass_logloss: 0.773647\tvalid's my_multiclass_logloss: 0.947681\n",
      "[87]\ttrain's my_multiclass_logloss: 0.772528\tvalid's my_multiclass_logloss: 0.952223\n",
      "[88]\ttrain's my_multiclass_logloss: 0.771175\tvalid's my_multiclass_logloss: 0.959653\n",
      "[89]\ttrain's my_multiclass_logloss: 0.770027\tvalid's my_multiclass_logloss: 0.959477\n",
      "[90]\ttrain's my_multiclass_logloss: 0.769155\tvalid's my_multiclass_logloss: 0.959581\n",
      "[91]\ttrain's my_multiclass_logloss: 0.768783\tvalid's my_multiclass_logloss: 0.959011\n",
      "[92]\ttrain's my_multiclass_logloss: 0.768581\tvalid's my_multiclass_logloss: 0.958646\n",
      "[93]\ttrain's my_multiclass_logloss: 0.767017\tvalid's my_multiclass_logloss: 0.957147\n",
      "[94]\ttrain's my_multiclass_logloss: 0.765886\tvalid's my_multiclass_logloss: 0.949557\n",
      "[95]\ttrain's my_multiclass_logloss: 0.764855\tvalid's my_multiclass_logloss: 0.948605\n",
      "[96]\ttrain's my_multiclass_logloss: 0.764118\tvalid's my_multiclass_logloss: 0.941797\n",
      "[97]\ttrain's my_multiclass_logloss: 0.762475\tvalid's my_multiclass_logloss: 0.939751\n",
      "[98]\ttrain's my_multiclass_logloss: 0.761075\tvalid's my_multiclass_logloss: 0.941161\n",
      "[99]\ttrain's my_multiclass_logloss: 0.76029\tvalid's my_multiclass_logloss: 0.942115\n",
      "[100]\ttrain's my_multiclass_logloss: 0.75976\tvalid's my_multiclass_logloss: 0.942158\n",
      "[101]\ttrain's my_multiclass_logloss: 0.759691\tvalid's my_multiclass_logloss: 0.943762\n",
      "[102]\ttrain's my_multiclass_logloss: 0.759416\tvalid's my_multiclass_logloss: 0.942777\n",
      "[103]\ttrain's my_multiclass_logloss: 0.759273\tvalid's my_multiclass_logloss: 0.941904\n",
      "[104]\ttrain's my_multiclass_logloss: 0.759229\tvalid's my_multiclass_logloss: 0.941142\n",
      "[105]\ttrain's my_multiclass_logloss: 0.758626\tvalid's my_multiclass_logloss: 0.937817\n",
      "[106]\ttrain's my_multiclass_logloss: 0.757964\tvalid's my_multiclass_logloss: 0.938475\n",
      "[107]\ttrain's my_multiclass_logloss: 0.756912\tvalid's my_multiclass_logloss: 0.941319\n",
      "[108]\ttrain's my_multiclass_logloss: 0.756472\tvalid's my_multiclass_logloss: 0.933765\n",
      "[109]\ttrain's my_multiclass_logloss: 0.756225\tvalid's my_multiclass_logloss: 0.936286\n",
      "[110]\ttrain's my_multiclass_logloss: 0.755902\tvalid's my_multiclass_logloss: 0.941159\n",
      "[111]\ttrain's my_multiclass_logloss: 0.75532\tvalid's my_multiclass_logloss: 0.944576\n",
      "[112]\ttrain's my_multiclass_logloss: 0.755173\tvalid's my_multiclass_logloss: 0.94822\n",
      "[113]\ttrain's my_multiclass_logloss: 0.753789\tvalid's my_multiclass_logloss: 0.938535\n",
      "[114]\ttrain's my_multiclass_logloss: 0.752863\tvalid's my_multiclass_logloss: 0.932703\n",
      "[115]\ttrain's my_multiclass_logloss: 0.751737\tvalid's my_multiclass_logloss: 0.926954\n",
      "[116]\ttrain's my_multiclass_logloss: 0.750634\tvalid's my_multiclass_logloss: 0.919479\n",
      "[117]\ttrain's my_multiclass_logloss: 0.749335\tvalid's my_multiclass_logloss: 0.9138\n",
      "[118]\ttrain's my_multiclass_logloss: 0.748081\tvalid's my_multiclass_logloss: 0.908947\n",
      "[119]\ttrain's my_multiclass_logloss: 0.747548\tvalid's my_multiclass_logloss: 0.906054\n",
      "[120]\ttrain's my_multiclass_logloss: 0.74681\tvalid's my_multiclass_logloss: 0.903169\n",
      "[121]\ttrain's my_multiclass_logloss: 0.745061\tvalid's my_multiclass_logloss: 0.894339\n",
      "[122]\ttrain's my_multiclass_logloss: 0.74378\tvalid's my_multiclass_logloss: 0.889485\n",
      "[123]\ttrain's my_multiclass_logloss: 0.74268\tvalid's my_multiclass_logloss: 0.885622\n",
      "[124]\ttrain's my_multiclass_logloss: 0.741476\tvalid's my_multiclass_logloss: 0.879138\n",
      "[125]\ttrain's my_multiclass_logloss: 0.741127\tvalid's my_multiclass_logloss: 0.880433\n",
      "[126]\ttrain's my_multiclass_logloss: 0.740681\tvalid's my_multiclass_logloss: 0.882747\n",
      "[127]\ttrain's my_multiclass_logloss: 0.740379\tvalid's my_multiclass_logloss: 0.887149\n",
      "[128]\ttrain's my_multiclass_logloss: 0.740404\tvalid's my_multiclass_logloss: 0.886369\n",
      "[129]\ttrain's my_multiclass_logloss: 0.739801\tvalid's my_multiclass_logloss: 0.888165\n",
      "[130]\ttrain's my_multiclass_logloss: 0.73905\tvalid's my_multiclass_logloss: 0.895378\n",
      "[131]\ttrain's my_multiclass_logloss: 0.739319\tvalid's my_multiclass_logloss: 0.899643\n",
      "[132]\ttrain's my_multiclass_logloss: 0.739452\tvalid's my_multiclass_logloss: 0.907036\n",
      "[133]\ttrain's my_multiclass_logloss: 0.737957\tvalid's my_multiclass_logloss: 0.90199\n",
      "[134]\ttrain's my_multiclass_logloss: 0.737652\tvalid's my_multiclass_logloss: 0.901032\n",
      "[135]\ttrain's my_multiclass_logloss: 0.736355\tvalid's my_multiclass_logloss: 0.909055\n",
      "[136]\ttrain's my_multiclass_logloss: 0.735451\tvalid's my_multiclass_logloss: 0.912461\n",
      "[137]\ttrain's my_multiclass_logloss: 0.73433\tvalid's my_multiclass_logloss: 0.916015\n",
      "[138]\ttrain's my_multiclass_logloss: 0.733157\tvalid's my_multiclass_logloss: 0.913948\n",
      "[139]\ttrain's my_multiclass_logloss: 0.732473\tvalid's my_multiclass_logloss: 0.914142\n",
      "[140]\ttrain's my_multiclass_logloss: 0.73176\tvalid's my_multiclass_logloss: 0.915876\n",
      "[141]\ttrain's my_multiclass_logloss: 0.730571\tvalid's my_multiclass_logloss: 0.91808\n",
      "[142]\ttrain's my_multiclass_logloss: 0.729868\tvalid's my_multiclass_logloss: 0.913953\n",
      "[143]\ttrain's my_multiclass_logloss: 0.729657\tvalid's my_multiclass_logloss: 0.905207\n",
      "[144]\ttrain's my_multiclass_logloss: 0.729432\tvalid's my_multiclass_logloss: 0.906871\n",
      "[145]\ttrain's my_multiclass_logloss: 0.728459\tvalid's my_multiclass_logloss: 0.907844\n",
      "[146]\ttrain's my_multiclass_logloss: 0.727846\tvalid's my_multiclass_logloss: 0.905322\n",
      "[147]\ttrain's my_multiclass_logloss: 0.726878\tvalid's my_multiclass_logloss: 0.906589\n",
      "[148]\ttrain's my_multiclass_logloss: 0.725644\tvalid's my_multiclass_logloss: 0.907902\n",
      "[149]\ttrain's my_multiclass_logloss: 0.724349\tvalid's my_multiclass_logloss: 0.905903\n",
      "[150]\ttrain's my_multiclass_logloss: 0.723349\tvalid's my_multiclass_logloss: 0.905597\n",
      "[151]\ttrain's my_multiclass_logloss: 0.722441\tvalid's my_multiclass_logloss: 0.905855\n",
      "[152]\ttrain's my_multiclass_logloss: 0.721659\tvalid's my_multiclass_logloss: 0.90616\n",
      "[153]\ttrain's my_multiclass_logloss: 0.720696\tvalid's my_multiclass_logloss: 0.897883\n",
      "[154]\ttrain's my_multiclass_logloss: 0.71996\tvalid's my_multiclass_logloss: 0.890244\n",
      "[155]\ttrain's my_multiclass_logloss: 0.719276\tvalid's my_multiclass_logloss: 0.886641\n",
      "[156]\ttrain's my_multiclass_logloss: 0.71911\tvalid's my_multiclass_logloss: 0.883395\n",
      "[157]\ttrain's my_multiclass_logloss: 0.718868\tvalid's my_multiclass_logloss: 0.882953\n",
      "[158]\ttrain's my_multiclass_logloss: 0.718424\tvalid's my_multiclass_logloss: 0.882295\n",
      "[159]\ttrain's my_multiclass_logloss: 0.717969\tvalid's my_multiclass_logloss: 0.881586\n",
      "[160]\ttrain's my_multiclass_logloss: 0.717655\tvalid's my_multiclass_logloss: 0.881026\n",
      "[161]\ttrain's my_multiclass_logloss: 0.715801\tvalid's my_multiclass_logloss: 0.880228\n",
      "[162]\ttrain's my_multiclass_logloss: 0.713043\tvalid's my_multiclass_logloss: 0.876264\n",
      "[163]\ttrain's my_multiclass_logloss: 0.711314\tvalid's my_multiclass_logloss: 0.873904\n",
      "[164]\ttrain's my_multiclass_logloss: 0.709316\tvalid's my_multiclass_logloss: 0.872204\n",
      "[165]\ttrain's my_multiclass_logloss: 0.707964\tvalid's my_multiclass_logloss: 0.868741\n",
      "[166]\ttrain's my_multiclass_logloss: 0.706898\tvalid's my_multiclass_logloss: 0.868956\n",
      "[167]\ttrain's my_multiclass_logloss: 0.706264\tvalid's my_multiclass_logloss: 0.865079\n",
      "[168]\ttrain's my_multiclass_logloss: 0.705721\tvalid's my_multiclass_logloss: 0.865324\n",
      "[169]\ttrain's my_multiclass_logloss: 0.704856\tvalid's my_multiclass_logloss: 0.866972\n",
      "[170]\ttrain's my_multiclass_logloss: 0.704241\tvalid's my_multiclass_logloss: 0.864635\n",
      "[171]\ttrain's my_multiclass_logloss: 0.703523\tvalid's my_multiclass_logloss: 0.866336\n",
      "[172]\ttrain's my_multiclass_logloss: 0.702733\tvalid's my_multiclass_logloss: 0.865436\n",
      "[173]\ttrain's my_multiclass_logloss: 0.701813\tvalid's my_multiclass_logloss: 0.859723\n",
      "[174]\ttrain's my_multiclass_logloss: 0.701134\tvalid's my_multiclass_logloss: 0.860699\n",
      "[175]\ttrain's my_multiclass_logloss: 0.700789\tvalid's my_multiclass_logloss: 0.856855\n",
      "[176]\ttrain's my_multiclass_logloss: 0.700215\tvalid's my_multiclass_logloss: 0.857961\n",
      "[177]\ttrain's my_multiclass_logloss: 0.699168\tvalid's my_multiclass_logloss: 0.855498\n",
      "[178]\ttrain's my_multiclass_logloss: 0.698344\tvalid's my_multiclass_logloss: 0.854367\n",
      "[179]\ttrain's my_multiclass_logloss: 0.697738\tvalid's my_multiclass_logloss: 0.858032\n",
      "[180]\ttrain's my_multiclass_logloss: 0.696782\tvalid's my_multiclass_logloss: 0.862762\n",
      "[181]\ttrain's my_multiclass_logloss: 0.696492\tvalid's my_multiclass_logloss: 0.861561\n",
      "[182]\ttrain's my_multiclass_logloss: 0.69612\tvalid's my_multiclass_logloss: 0.861018\n",
      "[183]\ttrain's my_multiclass_logloss: 0.695923\tvalid's my_multiclass_logloss: 0.858025\n",
      "[184]\ttrain's my_multiclass_logloss: 0.695454\tvalid's my_multiclass_logloss: 0.85758\n",
      "[185]\ttrain's my_multiclass_logloss: 0.695441\tvalid's my_multiclass_logloss: 0.858671\n",
      "[186]\ttrain's my_multiclass_logloss: 0.695569\tvalid's my_multiclass_logloss: 0.863097\n",
      "[187]\ttrain's my_multiclass_logloss: 0.696141\tvalid's my_multiclass_logloss: 0.867683\n",
      "[188]\ttrain's my_multiclass_logloss: 0.69559\tvalid's my_multiclass_logloss: 0.862159\n",
      "[189]\ttrain's my_multiclass_logloss: 0.694554\tvalid's my_multiclass_logloss: 0.86207\n",
      "[190]\ttrain's my_multiclass_logloss: 0.693361\tvalid's my_multiclass_logloss: 0.859982\n",
      "[191]\ttrain's my_multiclass_logloss: 0.693356\tvalid's my_multiclass_logloss: 0.860349\n",
      "[192]\ttrain's my_multiclass_logloss: 0.692659\tvalid's my_multiclass_logloss: 0.860059\n",
      "[193]\ttrain's my_multiclass_logloss: 0.692041\tvalid's my_multiclass_logloss: 0.860186\n",
      "[194]\ttrain's my_multiclass_logloss: 0.691469\tvalid's my_multiclass_logloss: 0.860098\n",
      "[195]\ttrain's my_multiclass_logloss: 0.691193\tvalid's my_multiclass_logloss: 0.861726\n",
      "[196]\ttrain's my_multiclass_logloss: 0.690833\tvalid's my_multiclass_logloss: 0.858652\n",
      "[197]\ttrain's my_multiclass_logloss: 0.689573\tvalid's my_multiclass_logloss: 0.865612\n",
      "[198]\ttrain's my_multiclass_logloss: 0.689194\tvalid's my_multiclass_logloss: 0.861668\n",
      "[199]\ttrain's my_multiclass_logloss: 0.688584\tvalid's my_multiclass_logloss: 0.862602\n",
      "[200]\ttrain's my_multiclass_logloss: 0.688095\tvalid's my_multiclass_logloss: 0.862316\n",
      "[201]\ttrain's my_multiclass_logloss: 0.687232\tvalid's my_multiclass_logloss: 0.863915\n",
      "[202]\ttrain's my_multiclass_logloss: 0.686226\tvalid's my_multiclass_logloss: 0.866195\n",
      "[203]\ttrain's my_multiclass_logloss: 0.68487\tvalid's my_multiclass_logloss: 0.862008\n",
      "[204]\ttrain's my_multiclass_logloss: 0.683942\tvalid's my_multiclass_logloss: 0.857413\n",
      "[205]\ttrain's my_multiclass_logloss: 0.683154\tvalid's my_multiclass_logloss: 0.85286\n",
      "[206]\ttrain's my_multiclass_logloss: 0.682587\tvalid's my_multiclass_logloss: 0.84872\n",
      "[207]\ttrain's my_multiclass_logloss: 0.681645\tvalid's my_multiclass_logloss: 0.845672\n",
      "[208]\ttrain's my_multiclass_logloss: 0.680972\tvalid's my_multiclass_logloss: 0.845021\n",
      "[209]\ttrain's my_multiclass_logloss: 0.680574\tvalid's my_multiclass_logloss: 0.846459\n",
      "[210]\ttrain's my_multiclass_logloss: 0.6804\tvalid's my_multiclass_logloss: 0.847917\n",
      "[211]\ttrain's my_multiclass_logloss: 0.680423\tvalid's my_multiclass_logloss: 0.849388\n",
      "[212]\ttrain's my_multiclass_logloss: 0.680229\tvalid's my_multiclass_logloss: 0.853015\n",
      "[213]\ttrain's my_multiclass_logloss: 0.678573\tvalid's my_multiclass_logloss: 0.854629\n",
      "[214]\ttrain's my_multiclass_logloss: 0.677276\tvalid's my_multiclass_logloss: 0.85642\n",
      "[215]\ttrain's my_multiclass_logloss: 0.676392\tvalid's my_multiclass_logloss: 0.860616\n",
      "[216]\ttrain's my_multiclass_logloss: 0.675728\tvalid's my_multiclass_logloss: 0.860033\n",
      "[217]\ttrain's my_multiclass_logloss: 0.674677\tvalid's my_multiclass_logloss: 0.859072\n",
      "[218]\ttrain's my_multiclass_logloss: 0.67383\tvalid's my_multiclass_logloss: 0.857775\n",
      "[219]\ttrain's my_multiclass_logloss: 0.673178\tvalid's my_multiclass_logloss: 0.857117\n",
      "[220]\ttrain's my_multiclass_logloss: 0.672576\tvalid's my_multiclass_logloss: 0.857645\n",
      "[221]\ttrain's my_multiclass_logloss: 0.671732\tvalid's my_multiclass_logloss: 0.857364\n",
      "[222]\ttrain's my_multiclass_logloss: 0.671136\tvalid's my_multiclass_logloss: 0.857328\n",
      "[223]\ttrain's my_multiclass_logloss: 0.670856\tvalid's my_multiclass_logloss: 0.858522\n",
      "[224]\ttrain's my_multiclass_logloss: 0.67059\tvalid's my_multiclass_logloss: 0.858809\n",
      "[225]\ttrain's my_multiclass_logloss: 0.669932\tvalid's my_multiclass_logloss: 0.86429\n",
      "[226]\ttrain's my_multiclass_logloss: 0.669467\tvalid's my_multiclass_logloss: 0.865849\n",
      "[227]\ttrain's my_multiclass_logloss: 0.66934\tvalid's my_multiclass_logloss: 0.863409\n",
      "[228]\ttrain's my_multiclass_logloss: 0.669081\tvalid's my_multiclass_logloss: 0.86917\n",
      "[229]\ttrain's my_multiclass_logloss: 0.668663\tvalid's my_multiclass_logloss: 0.86909\n",
      "[230]\ttrain's my_multiclass_logloss: 0.667712\tvalid's my_multiclass_logloss: 0.870722\n",
      "[231]\ttrain's my_multiclass_logloss: 0.66695\tvalid's my_multiclass_logloss: 0.87244\n",
      "[232]\ttrain's my_multiclass_logloss: 0.666045\tvalid's my_multiclass_logloss: 0.87388\n",
      "[233]\ttrain's my_multiclass_logloss: 0.66504\tvalid's my_multiclass_logloss: 0.872641\n",
      "[234]\ttrain's my_multiclass_logloss: 0.664181\tvalid's my_multiclass_logloss: 0.874503\n",
      "[235]\ttrain's my_multiclass_logloss: 0.663686\tvalid's my_multiclass_logloss: 0.876238\n",
      "[236]\ttrain's my_multiclass_logloss: 0.663178\tvalid's my_multiclass_logloss: 0.878236\n",
      "[237]\ttrain's my_multiclass_logloss: 0.663178\tvalid's my_multiclass_logloss: 0.878236\n",
      "[238]\ttrain's my_multiclass_logloss: 0.662383\tvalid's my_multiclass_logloss: 0.879214\n",
      "[239]\ttrain's my_multiclass_logloss: 0.661891\tvalid's my_multiclass_logloss: 0.880324\n",
      "[240]\ttrain's my_multiclass_logloss: 0.661498\tvalid's my_multiclass_logloss: 0.881471\n",
      "[241]\ttrain's my_multiclass_logloss: 0.661241\tvalid's my_multiclass_logloss: 0.881413\n",
      "[242]\ttrain's my_multiclass_logloss: 0.660477\tvalid's my_multiclass_logloss: 0.877093\n",
      "[243]\ttrain's my_multiclass_logloss: 0.65967\tvalid's my_multiclass_logloss: 0.877514\n",
      "[244]\ttrain's my_multiclass_logloss: 0.659004\tvalid's my_multiclass_logloss: 0.878058\n",
      "[245]\ttrain's my_multiclass_logloss: 0.658363\tvalid's my_multiclass_logloss: 0.872101\n",
      "[246]\ttrain's my_multiclass_logloss: 0.657479\tvalid's my_multiclass_logloss: 0.872697\n",
      "[247]\ttrain's my_multiclass_logloss: 0.656887\tvalid's my_multiclass_logloss: 0.874728\n",
      "[248]\ttrain's my_multiclass_logloss: 0.656311\tvalid's my_multiclass_logloss: 0.875382\n",
      "[249]\ttrain's my_multiclass_logloss: 0.655856\tvalid's my_multiclass_logloss: 0.876072\n",
      "[250]\ttrain's my_multiclass_logloss: 0.655516\tvalid's my_multiclass_logloss: 0.879935\n",
      "[251]\ttrain's my_multiclass_logloss: 0.654956\tvalid's my_multiclass_logloss: 0.890816\n",
      "[252]\ttrain's my_multiclass_logloss: 0.654557\tvalid's my_multiclass_logloss: 0.894738\n",
      "[253]\ttrain's my_multiclass_logloss: 0.654561\tvalid's my_multiclass_logloss: 0.898317\n",
      "[254]\ttrain's my_multiclass_logloss: 0.653673\tvalid's my_multiclass_logloss: 0.903606\n",
      "[255]\ttrain's my_multiclass_logloss: 0.653024\tvalid's my_multiclass_logloss: 0.90883\n",
      "[256]\ttrain's my_multiclass_logloss: 0.652786\tvalid's my_multiclass_logloss: 0.911159\n",
      "[257]\ttrain's my_multiclass_logloss: 0.652132\tvalid's my_multiclass_logloss: 0.917645\n",
      "[258]\ttrain's my_multiclass_logloss: 0.651479\tvalid's my_multiclass_logloss: 0.918652\n",
      "[259]\ttrain's my_multiclass_logloss: 0.651201\tvalid's my_multiclass_logloss: 0.919568\n",
      "[260]\ttrain's my_multiclass_logloss: 0.650852\tvalid's my_multiclass_logloss: 0.917501\n",
      "[261]\ttrain's my_multiclass_logloss: 0.650669\tvalid's my_multiclass_logloss: 0.920743\n",
      "[262]\ttrain's my_multiclass_logloss: 0.649898\tvalid's my_multiclass_logloss: 0.927476\n",
      "[263]\ttrain's my_multiclass_logloss: 0.649573\tvalid's my_multiclass_logloss: 0.928817\n",
      "[264]\ttrain's my_multiclass_logloss: 0.648969\tvalid's my_multiclass_logloss: 0.935176\n",
      "[265]\ttrain's my_multiclass_logloss: 0.648454\tvalid's my_multiclass_logloss: 0.941343\n",
      "[266]\ttrain's my_multiclass_logloss: 0.647864\tvalid's my_multiclass_logloss: 0.933902\n",
      "[267]\ttrain's my_multiclass_logloss: 0.647378\tvalid's my_multiclass_logloss: 0.933627\n",
      "[268]\ttrain's my_multiclass_logloss: 0.647188\tvalid's my_multiclass_logloss: 0.932632\n",
      "[269]\ttrain's my_multiclass_logloss: 0.646856\tvalid's my_multiclass_logloss: 0.925006\n",
      "[270]\ttrain's my_multiclass_logloss: 0.646715\tvalid's my_multiclass_logloss: 0.932669\n",
      "[271]\ttrain's my_multiclass_logloss: 0.646693\tvalid's my_multiclass_logloss: 0.940008\n",
      "[272]\ttrain's my_multiclass_logloss: 0.646504\tvalid's my_multiclass_logloss: 0.941371\n",
      "[273]\ttrain's my_multiclass_logloss: 0.646006\tvalid's my_multiclass_logloss: 0.938226\n",
      "[274]\ttrain's my_multiclass_logloss: 0.645504\tvalid's my_multiclass_logloss: 0.931966\n",
      "[275]\ttrain's my_multiclass_logloss: 0.644728\tvalid's my_multiclass_logloss: 0.922838\n",
      "[276]\ttrain's my_multiclass_logloss: 0.643585\tvalid's my_multiclass_logloss: 0.919196\n",
      "[277]\ttrain's my_multiclass_logloss: 0.642889\tvalid's my_multiclass_logloss: 0.916389\n",
      "[278]\ttrain's my_multiclass_logloss: 0.642543\tvalid's my_multiclass_logloss: 0.916586\n",
      "[279]\ttrain's my_multiclass_logloss: 0.64193\tvalid's my_multiclass_logloss: 0.916771\n",
      "[280]\ttrain's my_multiclass_logloss: 0.641309\tvalid's my_multiclass_logloss: 0.92276\n",
      "[281]\ttrain's my_multiclass_logloss: 0.640887\tvalid's my_multiclass_logloss: 0.928906\n",
      "[282]\ttrain's my_multiclass_logloss: 0.640414\tvalid's my_multiclass_logloss: 0.932454\n",
      "[283]\ttrain's my_multiclass_logloss: 0.639746\tvalid's my_multiclass_logloss: 0.934795\n",
      "[284]\ttrain's my_multiclass_logloss: 0.639279\tvalid's my_multiclass_logloss: 0.937182\n",
      "[285]\ttrain's my_multiclass_logloss: 0.639358\tvalid's my_multiclass_logloss: 0.938183\n",
      "[286]\ttrain's my_multiclass_logloss: 0.638164\tvalid's my_multiclass_logloss: 0.936151\n",
      "[287]\ttrain's my_multiclass_logloss: 0.637619\tvalid's my_multiclass_logloss: 0.932929\n",
      "[288]\ttrain's my_multiclass_logloss: 0.636692\tvalid's my_multiclass_logloss: 0.929029\n",
      "[289]\ttrain's my_multiclass_logloss: 0.635281\tvalid's my_multiclass_logloss: 0.923403\n",
      "[290]\ttrain's my_multiclass_logloss: 0.634311\tvalid's my_multiclass_logloss: 0.922656\n",
      "[291]\ttrain's my_multiclass_logloss: 0.633663\tvalid's my_multiclass_logloss: 0.920211\n",
      "[292]\ttrain's my_multiclass_logloss: 0.633189\tvalid's my_multiclass_logloss: 0.915841\n",
      "[293]\ttrain's my_multiclass_logloss: 0.632907\tvalid's my_multiclass_logloss: 0.916259\n",
      "[294]\ttrain's my_multiclass_logloss: 0.632322\tvalid's my_multiclass_logloss: 0.916678\n",
      "[295]\ttrain's my_multiclass_logloss: 0.631844\tvalid's my_multiclass_logloss: 0.917139\n",
      "[296]\ttrain's my_multiclass_logloss: 0.63152\tvalid's my_multiclass_logloss: 0.917215\n",
      "[297]\ttrain's my_multiclass_logloss: 0.63119\tvalid's my_multiclass_logloss: 0.917723\n",
      "[298]\ttrain's my_multiclass_logloss: 0.630428\tvalid's my_multiclass_logloss: 0.912857\n",
      "[299]\ttrain's my_multiclass_logloss: 0.629804\tvalid's my_multiclass_logloss: 0.908239\n",
      "[300]\ttrain's my_multiclass_logloss: 0.629482\tvalid's my_multiclass_logloss: 0.909034\n",
      "[301]\ttrain's my_multiclass_logloss: 0.629073\tvalid's my_multiclass_logloss: 0.904743\n",
      "[302]\ttrain's my_multiclass_logloss: 0.628703\tvalid's my_multiclass_logloss: 0.902371\n",
      "[303]\ttrain's my_multiclass_logloss: 0.628377\tvalid's my_multiclass_logloss: 0.898188\n",
      "[304]\ttrain's my_multiclass_logloss: 0.628117\tvalid's my_multiclass_logloss: 0.894261\n",
      "[305]\ttrain's my_multiclass_logloss: 0.627878\tvalid's my_multiclass_logloss: 0.890458\n",
      "[306]\ttrain's my_multiclass_logloss: 0.62732\tvalid's my_multiclass_logloss: 0.894147\n",
      "[307]\ttrain's my_multiclass_logloss: 0.626616\tvalid's my_multiclass_logloss: 0.899308\n",
      "[308]\ttrain's my_multiclass_logloss: 0.626055\tvalid's my_multiclass_logloss: 0.8983\n",
      "Early stopping, best iteration is:\n",
      "[208]\ttrain's my_multiclass_logloss: 0.680972\tvalid's my_multiclass_logloss: 0.845021\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.6363636363636364\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.111622\n",
      "1   feature2    0.100409\n",
      "2   feature3    0.066229\n",
      "3   feature4    0.072476\n",
      "4   feature5    0.148150\n",
      "5   feature6    0.050821\n",
      "6   feature7    0.076391\n",
      "7   feature8    0.127728\n",
      "8   feature9    0.162783\n",
      "9  feature10    0.083391\n",
      "     feature  importance\n",
      "0   feature1    0.113598\n",
      "1   feature2    0.102947\n",
      "2   feature3    0.071390\n",
      "3   feature4    0.072587\n",
      "4   feature5    0.149705\n",
      "5   feature6    0.055574\n",
      "6   feature7    0.086583\n",
      "7   feature8    0.113555\n",
      "8   feature9    0.149420\n",
      "9  feature10    0.084641\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.035895\n",
      "1   feature2    0.046078\n",
      "2   feature3    0.093700\n",
      "3   feature4    0.002018\n",
      "4   feature5    0.028239\n",
      "5   feature6    0.086315\n",
      "6   feature7    0.185064\n",
      "7   feature8   -0.257348\n",
      "8   feature9   -0.242652\n",
      "9  feature10    0.022690\n",
      "-------------------- 3 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05157\tvalid's multi_logloss: 1.06428\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.04366\tvalid's multi_logloss: 1.06074\n",
      "[3]\ttrain's multi_logloss: 1.03456\tvalid's multi_logloss: 1.05426\n",
      "[4]\ttrain's multi_logloss: 1.02603\tvalid's multi_logloss: 1.05285\n",
      "[5]\ttrain's multi_logloss: 1.01941\tvalid's multi_logloss: 1.05117\n",
      "[6]\ttrain's multi_logloss: 1.01529\tvalid's multi_logloss: 1.05021\n",
      "[7]\ttrain's multi_logloss: 1.00756\tvalid's multi_logloss: 1.04115\n",
      "[8]\ttrain's multi_logloss: 0.999315\tvalid's multi_logloss: 1.0361\n",
      "[9]\ttrain's multi_logloss: 0.993373\tvalid's multi_logloss: 1.02507\n",
      "[10]\ttrain's multi_logloss: 0.987099\tvalid's multi_logloss: 1.02266\n",
      "[11]\ttrain's multi_logloss: 0.981697\tvalid's multi_logloss: 1.01242\n",
      "[12]\ttrain's multi_logloss: 0.976618\tvalid's multi_logloss: 1.00287\n",
      "[13]\ttrain's multi_logloss: 0.968986\tvalid's multi_logloss: 1.00794\n",
      "[14]\ttrain's multi_logloss: 0.961124\tvalid's multi_logloss: 1.00587\n",
      "[15]\ttrain's multi_logloss: 0.953889\tvalid's multi_logloss: 1.00258\n",
      "[16]\ttrain's multi_logloss: 0.946664\tvalid's multi_logloss: 1.00093\n",
      "[17]\ttrain's multi_logloss: 0.939414\tvalid's multi_logloss: 0.998763\n",
      "[18]\ttrain's multi_logloss: 0.932212\tvalid's multi_logloss: 1.00143\n",
      "[19]\ttrain's multi_logloss: 0.926674\tvalid's multi_logloss: 0.993699\n",
      "[20]\ttrain's multi_logloss: 0.92155\tvalid's multi_logloss: 0.9931\n",
      "[21]\ttrain's multi_logloss: 0.917565\tvalid's multi_logloss: 0.986989\n",
      "[22]\ttrain's multi_logloss: 0.91253\tvalid's multi_logloss: 0.98893\n",
      "[23]\ttrain's multi_logloss: 0.909342\tvalid's multi_logloss: 0.989238\n",
      "[24]\ttrain's multi_logloss: 0.903297\tvalid's multi_logloss: 0.986017\n",
      "[25]\ttrain's multi_logloss: 0.897898\tvalid's multi_logloss: 0.99005\n",
      "[26]\ttrain's multi_logloss: 0.892645\tvalid's multi_logloss: 0.988516\n",
      "[27]\ttrain's multi_logloss: 0.888383\tvalid's multi_logloss: 0.989738\n",
      "[28]\ttrain's multi_logloss: 0.884998\tvalid's multi_logloss: 0.986544\n",
      "[29]\ttrain's multi_logloss: 0.87963\tvalid's multi_logloss: 0.989986\n",
      "[30]\ttrain's multi_logloss: 0.875934\tvalid's multi_logloss: 0.995823\n",
      "[31]\ttrain's multi_logloss: 0.870802\tvalid's multi_logloss: 0.995356\n",
      "[32]\ttrain's multi_logloss: 0.868135\tvalid's multi_logloss: 0.992956\n",
      "[33]\ttrain's multi_logloss: 0.863819\tvalid's multi_logloss: 0.99865\n",
      "[34]\ttrain's multi_logloss: 0.858171\tvalid's multi_logloss: 1.00608\n",
      "[35]\ttrain's multi_logloss: 0.854855\tvalid's multi_logloss: 1.00371\n",
      "[36]\ttrain's multi_logloss: 0.84984\tvalid's multi_logloss: 1.01115\n",
      "[37]\ttrain's multi_logloss: 0.845321\tvalid's multi_logloss: 1.01278\n",
      "[38]\ttrain's multi_logloss: 0.841636\tvalid's multi_logloss: 1.01384\n",
      "[39]\ttrain's multi_logloss: 0.83766\tvalid's multi_logloss: 1.01571\n",
      "[40]\ttrain's multi_logloss: 0.833949\tvalid's multi_logloss: 1.01771\n",
      "[41]\ttrain's multi_logloss: 0.83046\tvalid's multi_logloss: 1.01597\n",
      "[42]\ttrain's multi_logloss: 0.828398\tvalid's multi_logloss: 1.0104\n",
      "[43]\ttrain's multi_logloss: 0.826527\tvalid's multi_logloss: 1.00508\n",
      "[44]\ttrain's multi_logloss: 0.823752\tvalid's multi_logloss: 0.998353\n",
      "[45]\ttrain's multi_logloss: 0.820064\tvalid's multi_logloss: 1.00061\n",
      "[46]\ttrain's multi_logloss: 0.816564\tvalid's multi_logloss: 0.995945\n",
      "[47]\ttrain's multi_logloss: 0.812607\tvalid's multi_logloss: 1.00224\n",
      "[48]\ttrain's multi_logloss: 0.80846\tvalid's multi_logloss: 1.00475\n",
      "[49]\ttrain's multi_logloss: 0.803426\tvalid's multi_logloss: 1.00105\n",
      "[50]\ttrain's multi_logloss: 0.799368\tvalid's multi_logloss: 0.999187\n",
      "[51]\ttrain's multi_logloss: 0.795992\tvalid's multi_logloss: 0.997949\n",
      "[52]\ttrain's multi_logloss: 0.792875\tvalid's multi_logloss: 0.996859\n",
      "[53]\ttrain's multi_logloss: 0.79031\tvalid's multi_logloss: 0.990693\n",
      "[54]\ttrain's multi_logloss: 0.788425\tvalid's multi_logloss: 0.986372\n",
      "[55]\ttrain's multi_logloss: 0.78669\tvalid's multi_logloss: 0.982232\n",
      "[56]\ttrain's multi_logloss: 0.78498\tvalid's multi_logloss: 0.977951\n",
      "[57]\ttrain's multi_logloss: 0.781696\tvalid's multi_logloss: 0.977906\n",
      "[58]\ttrain's multi_logloss: 0.779273\tvalid's multi_logloss: 0.9795\n",
      "[59]\ttrain's multi_logloss: 0.776474\tvalid's multi_logloss: 0.984329\n",
      "[60]\ttrain's multi_logloss: 0.773263\tvalid's multi_logloss: 0.989262\n",
      "[61]\ttrain's multi_logloss: 0.770682\tvalid's multi_logloss: 0.995392\n",
      "[62]\ttrain's multi_logloss: 0.768238\tvalid's multi_logloss: 0.997496\n",
      "[63]\ttrain's multi_logloss: 0.76596\tvalid's multi_logloss: 0.998868\n",
      "[64]\ttrain's multi_logloss: 0.764614\tvalid's multi_logloss: 0.998571\n",
      "[65]\ttrain's multi_logloss: 0.762318\tvalid's multi_logloss: 1.00179\n",
      "[66]\ttrain's multi_logloss: 0.759375\tvalid's multi_logloss: 1.00908\n",
      "[67]\ttrain's multi_logloss: 0.756795\tvalid's multi_logloss: 1.01475\n",
      "[68]\ttrain's multi_logloss: 0.754401\tvalid's multi_logloss: 1.02036\n",
      "[69]\ttrain's multi_logloss: 0.752764\tvalid's multi_logloss: 1.0198\n",
      "[70]\ttrain's multi_logloss: 0.751185\tvalid's multi_logloss: 1.01934\n",
      "[71]\ttrain's multi_logloss: 0.74983\tvalid's multi_logloss: 1.01908\n",
      "[72]\ttrain's multi_logloss: 0.748662\tvalid's multi_logloss: 1.01569\n",
      "[73]\ttrain's multi_logloss: 0.745881\tvalid's multi_logloss: 1.0094\n",
      "[74]\ttrain's multi_logloss: 0.743829\tvalid's multi_logloss: 1.00284\n",
      "[75]\ttrain's multi_logloss: 0.742612\tvalid's multi_logloss: 0.999494\n",
      "[76]\ttrain's multi_logloss: 0.741383\tvalid's multi_logloss: 0.993737\n",
      "[77]\ttrain's multi_logloss: 0.739459\tvalid's multi_logloss: 0.993918\n",
      "[78]\ttrain's multi_logloss: 0.737579\tvalid's multi_logloss: 0.995326\n",
      "[79]\ttrain's multi_logloss: 0.735864\tvalid's multi_logloss: 0.996814\n",
      "[80]\ttrain's multi_logloss: 0.734667\tvalid's multi_logloss: 0.995614\n",
      "[81]\ttrain's multi_logloss: 0.732992\tvalid's multi_logloss: 0.998386\n",
      "[82]\ttrain's multi_logloss: 0.73044\tvalid's multi_logloss: 0.995517\n",
      "[83]\ttrain's multi_logloss: 0.727824\tvalid's multi_logloss: 0.99524\n",
      "[84]\ttrain's multi_logloss: 0.726447\tvalid's multi_logloss: 0.998105\n",
      "[85]\ttrain's multi_logloss: 0.723701\tvalid's multi_logloss: 0.994123\n",
      "[86]\ttrain's multi_logloss: 0.721897\tvalid's multi_logloss: 0.98944\n",
      "[87]\ttrain's multi_logloss: 0.720099\tvalid's multi_logloss: 0.987051\n",
      "[88]\ttrain's multi_logloss: 0.718842\tvalid's multi_logloss: 0.983407\n",
      "[89]\ttrain's multi_logloss: 0.717362\tvalid's multi_logloss: 0.987852\n",
      "[90]\ttrain's multi_logloss: 0.715746\tvalid's multi_logloss: 0.98665\n",
      "[91]\ttrain's multi_logloss: 0.71452\tvalid's multi_logloss: 0.986222\n",
      "[92]\ttrain's multi_logloss: 0.713769\tvalid's multi_logloss: 0.985546\n",
      "[93]\ttrain's multi_logloss: 0.712455\tvalid's multi_logloss: 0.983556\n",
      "[94]\ttrain's multi_logloss: 0.711367\tvalid's multi_logloss: 0.983407\n",
      "[95]\ttrain's multi_logloss: 0.710415\tvalid's multi_logloss: 0.981919\n",
      "[96]\ttrain's multi_logloss: 0.70942\tvalid's multi_logloss: 0.979501\n",
      "[97]\ttrain's multi_logloss: 0.708492\tvalid's multi_logloss: 0.973393\n",
      "[98]\ttrain's multi_logloss: 0.707237\tvalid's multi_logloss: 0.970107\n",
      "[99]\ttrain's multi_logloss: 0.706388\tvalid's multi_logloss: 0.965054\n",
      "[100]\ttrain's multi_logloss: 0.705616\tvalid's multi_logloss: 0.960173\n",
      "[101]\ttrain's multi_logloss: 0.704019\tvalid's multi_logloss: 0.960701\n",
      "[102]\ttrain's multi_logloss: 0.702203\tvalid's multi_logloss: 0.963892\n",
      "[103]\ttrain's multi_logloss: 0.7005\tvalid's multi_logloss: 0.96709\n",
      "[104]\ttrain's multi_logloss: 0.698931\tvalid's multi_logloss: 0.972425\n",
      "[105]\ttrain's multi_logloss: 0.697631\tvalid's multi_logloss: 0.970721\n",
      "[106]\ttrain's multi_logloss: 0.6964\tvalid's multi_logloss: 0.967949\n",
      "[107]\ttrain's multi_logloss: 0.6953\tvalid's multi_logloss: 0.965362\n",
      "[108]\ttrain's multi_logloss: 0.69428\tvalid's multi_logloss: 0.96196\n",
      "[109]\ttrain's multi_logloss: 0.693194\tvalid's multi_logloss: 0.963401\n",
      "[110]\ttrain's multi_logloss: 0.69209\tvalid's multi_logloss: 0.960876\n",
      "[111]\ttrain's multi_logloss: 0.691055\tvalid's multi_logloss: 0.965093\n",
      "[112]\ttrain's multi_logloss: 0.690064\tvalid's multi_logloss: 0.962743\n",
      "[113]\ttrain's multi_logloss: 0.688293\tvalid's multi_logloss: 0.967212\n",
      "[114]\ttrain's multi_logloss: 0.686718\tvalid's multi_logloss: 0.971767\n",
      "[115]\ttrain's multi_logloss: 0.685323\tvalid's multi_logloss: 0.976392\n",
      "[116]\ttrain's multi_logloss: 0.684093\tvalid's multi_logloss: 0.981074\n",
      "[117]\ttrain's multi_logloss: 0.682213\tvalid's multi_logloss: 0.983925\n",
      "[118]\ttrain's multi_logloss: 0.680963\tvalid's multi_logloss: 0.987748\n",
      "[119]\ttrain's multi_logloss: 0.679849\tvalid's multi_logloss: 0.991578\n",
      "[120]\ttrain's multi_logloss: 0.678622\tvalid's multi_logloss: 0.997156\n",
      "[121]\ttrain's multi_logloss: 0.676982\tvalid's multi_logloss: 0.991554\n",
      "[122]\ttrain's multi_logloss: 0.675111\tvalid's multi_logloss: 0.987009\n",
      "[123]\ttrain's multi_logloss: 0.673433\tvalid's multi_logloss: 0.983317\n",
      "[124]\ttrain's multi_logloss: 0.671997\tvalid's multi_logloss: 0.979442\n",
      "[125]\ttrain's multi_logloss: 0.670345\tvalid's multi_logloss: 0.98053\n",
      "[126]\ttrain's multi_logloss: 0.668866\tvalid's multi_logloss: 0.981732\n",
      "[127]\ttrain's multi_logloss: 0.667571\tvalid's multi_logloss: 0.979419\n",
      "[128]\ttrain's multi_logloss: 0.666622\tvalid's multi_logloss: 0.978789\n",
      "[129]\ttrain's multi_logloss: 0.665239\tvalid's multi_logloss: 0.981592\n",
      "[130]\ttrain's multi_logloss: 0.664005\tvalid's multi_logloss: 0.984442\n",
      "[131]\ttrain's multi_logloss: 0.663434\tvalid's multi_logloss: 0.981518\n",
      "[132]\ttrain's multi_logloss: 0.662969\tvalid's multi_logloss: 0.978738\n",
      "[133]\ttrain's multi_logloss: 0.661673\tvalid's multi_logloss: 0.976859\n",
      "[134]\ttrain's multi_logloss: 0.660363\tvalid's multi_logloss: 0.973821\n",
      "[135]\ttrain's multi_logloss: 0.659457\tvalid's multi_logloss: 0.971485\n",
      "[136]\ttrain's multi_logloss: 0.658701\tvalid's multi_logloss: 0.972103\n",
      "[137]\ttrain's multi_logloss: 0.658169\tvalid's multi_logloss: 0.976143\n",
      "[138]\ttrain's multi_logloss: 0.657204\tvalid's multi_logloss: 0.978895\n",
      "[139]\ttrain's multi_logloss: 0.656324\tvalid's multi_logloss: 0.983067\n",
      "[140]\ttrain's multi_logloss: 0.655236\tvalid's multi_logloss: 0.984715\n",
      "[141]\ttrain's multi_logloss: 0.653896\tvalid's multi_logloss: 0.986253\n",
      "[142]\ttrain's multi_logloss: 0.652857\tvalid's multi_logloss: 0.988798\n",
      "[143]\ttrain's multi_logloss: 0.651733\tvalid's multi_logloss: 0.987301\n",
      "[144]\ttrain's multi_logloss: 0.650806\tvalid's multi_logloss: 0.987072\n",
      "[145]\ttrain's multi_logloss: 0.649963\tvalid's multi_logloss: 0.9916\n",
      "[146]\ttrain's multi_logloss: 0.649074\tvalid's multi_logloss: 0.997027\n",
      "[147]\ttrain's multi_logloss: 0.648545\tvalid's multi_logloss: 1.00038\n",
      "[148]\ttrain's multi_logloss: 0.648215\tvalid's multi_logloss: 1.00124\n",
      "[149]\ttrain's multi_logloss: 0.647345\tvalid's multi_logloss: 1.00377\n",
      "[150]\ttrain's multi_logloss: 0.645657\tvalid's multi_logloss: 0.999369\n",
      "[151]\ttrain's multi_logloss: 0.644748\tvalid's multi_logloss: 1.00292\n",
      "[152]\ttrain's multi_logloss: 0.643548\tvalid's multi_logloss: 1.00386\n",
      "[153]\ttrain's multi_logloss: 0.642941\tvalid's multi_logloss: 1.00431\n",
      "[154]\ttrain's multi_logloss: 0.642019\tvalid's multi_logloss: 0.998908\n",
      "[155]\ttrain's multi_logloss: 0.641513\tvalid's multi_logloss: 0.999557\n",
      "[156]\ttrain's multi_logloss: 0.64085\tvalid's multi_logloss: 0.998156\n",
      "[157]\ttrain's multi_logloss: 0.639675\tvalid's multi_logloss: 1.00032\n",
      "[158]\ttrain's multi_logloss: 0.639055\tvalid's multi_logloss: 1.00319\n",
      "[159]\ttrain's multi_logloss: 0.638573\tvalid's multi_logloss: 1.00494\n",
      "[160]\ttrain's multi_logloss: 0.637775\tvalid's multi_logloss: 1.00778\n",
      "[161]\ttrain's multi_logloss: 0.634611\tvalid's multi_logloss: 1.00725\n",
      "[162]\ttrain's multi_logloss: 0.631442\tvalid's multi_logloss: 1.00572\n",
      "[163]\ttrain's multi_logloss: 0.628899\tvalid's multi_logloss: 1.00555\n",
      "[164]\ttrain's multi_logloss: 0.62691\tvalid's multi_logloss: 1.00899\n",
      "[165]\ttrain's multi_logloss: 0.625485\tvalid's multi_logloss: 0.997666\n",
      "[166]\ttrain's multi_logloss: 0.623826\tvalid's multi_logloss: 0.990032\n",
      "[167]\ttrain's multi_logloss: 0.623026\tvalid's multi_logloss: 0.986264\n",
      "[168]\ttrain's multi_logloss: 0.62205\tvalid's multi_logloss: 0.986087\n",
      "[169]\ttrain's multi_logloss: 0.621192\tvalid's multi_logloss: 0.982836\n",
      "[170]\ttrain's multi_logloss: 0.620141\tvalid's multi_logloss: 0.979272\n",
      "[171]\ttrain's multi_logloss: 0.619414\tvalid's multi_logloss: 0.9774\n",
      "[172]\ttrain's multi_logloss: 0.618877\tvalid's multi_logloss: 0.974593\n",
      "[173]\ttrain's multi_logloss: 0.618072\tvalid's multi_logloss: 0.970201\n",
      "[174]\ttrain's multi_logloss: 0.617193\tvalid's multi_logloss: 0.966522\n",
      "[175]\ttrain's multi_logloss: 0.616708\tvalid's multi_logloss: 0.963438\n",
      "[176]\ttrain's multi_logloss: 0.616161\tvalid's multi_logloss: 0.959532\n",
      "[177]\ttrain's multi_logloss: 0.61469\tvalid's multi_logloss: 0.958299\n",
      "[178]\ttrain's multi_logloss: 0.613221\tvalid's multi_logloss: 0.958249\n",
      "[179]\ttrain's multi_logloss: 0.61209\tvalid's multi_logloss: 0.955589\n",
      "[180]\ttrain's multi_logloss: 0.610953\tvalid's multi_logloss: 0.9547\n",
      "[181]\ttrain's multi_logloss: 0.610237\tvalid's multi_logloss: 0.958449\n",
      "[182]\ttrain's multi_logloss: 0.609638\tvalid's multi_logloss: 0.962186\n",
      "[183]\ttrain's multi_logloss: 0.60839\tvalid's multi_logloss: 0.961854\n",
      "[184]\ttrain's multi_logloss: 0.607325\tvalid's multi_logloss: 0.965005\n",
      "[185]\ttrain's multi_logloss: 0.605876\tvalid's multi_logloss: 0.965927\n",
      "[186]\ttrain's multi_logloss: 0.604626\tvalid's multi_logloss: 0.96624\n",
      "[187]\ttrain's multi_logloss: 0.603746\tvalid's multi_logloss: 0.963922\n",
      "[188]\ttrain's multi_logloss: 0.602651\tvalid's multi_logloss: 0.965184\n",
      "[189]\ttrain's multi_logloss: 0.601357\tvalid's multi_logloss: 0.969292\n",
      "[190]\ttrain's multi_logloss: 0.600474\tvalid's multi_logloss: 0.973736\n",
      "[191]\ttrain's multi_logloss: 0.599652\tvalid's multi_logloss: 0.978445\n",
      "[192]\ttrain's multi_logloss: 0.599195\tvalid's multi_logloss: 0.982381\n",
      "[193]\ttrain's multi_logloss: 0.598436\tvalid's multi_logloss: 0.979525\n",
      "[194]\ttrain's multi_logloss: 0.597795\tvalid's multi_logloss: 0.979397\n",
      "[195]\ttrain's multi_logloss: 0.59731\tvalid's multi_logloss: 0.979862\n",
      "[196]\ttrain's multi_logloss: 0.59666\tvalid's multi_logloss: 0.980678\n",
      "[197]\ttrain's multi_logloss: 0.596087\tvalid's multi_logloss: 0.979996\n",
      "[198]\ttrain's multi_logloss: 0.595641\tvalid's multi_logloss: 0.977528\n",
      "[199]\ttrain's multi_logloss: 0.595081\tvalid's multi_logloss: 0.979882\n",
      "[200]\ttrain's multi_logloss: 0.594348\tvalid's multi_logloss: 0.983368\n",
      "[201]\ttrain's multi_logloss: 0.593399\tvalid's multi_logloss: 0.98837\n",
      "[202]\ttrain's multi_logloss: 0.592886\tvalid's multi_logloss: 0.98825\n",
      "[203]\ttrain's multi_logloss: 0.592185\tvalid's multi_logloss: 0.992111\n",
      "[204]\ttrain's multi_logloss: 0.591507\tvalid's multi_logloss: 0.996997\n",
      "[205]\ttrain's multi_logloss: 0.591001\tvalid's multi_logloss: 0.998661\n",
      "[206]\ttrain's multi_logloss: 0.590387\tvalid's multi_logloss: 0.998786\n",
      "[207]\ttrain's multi_logloss: 0.590293\tvalid's multi_logloss: 0.99953\n",
      "[208]\ttrain's multi_logloss: 0.589818\tvalid's multi_logloss: 0.999651\n",
      "[209]\ttrain's multi_logloss: 0.589231\tvalid's multi_logloss: 1.00149\n",
      "[210]\ttrain's multi_logloss: 0.588355\tvalid's multi_logloss: 1.00016\n",
      "[211]\ttrain's multi_logloss: 0.587694\tvalid's multi_logloss: 0.998581\n",
      "[212]\ttrain's multi_logloss: 0.587449\tvalid's multi_logloss: 0.996313\n",
      "[213]\ttrain's multi_logloss: 0.586287\tvalid's multi_logloss: 0.994495\n",
      "[214]\ttrain's multi_logloss: 0.585198\tvalid's multi_logloss: 0.991087\n",
      "[215]\ttrain's multi_logloss: 0.584121\tvalid's multi_logloss: 0.986925\n",
      "[216]\ttrain's multi_logloss: 0.583254\tvalid's multi_logloss: 0.985491\n",
      "[217]\ttrain's multi_logloss: 0.58198\tvalid's multi_logloss: 0.988809\n",
      "[218]\ttrain's multi_logloss: 0.580817\tvalid's multi_logloss: 0.996257\n",
      "[219]\ttrain's multi_logloss: 0.579938\tvalid's multi_logloss: 1.00206\n",
      "[220]\ttrain's multi_logloss: 0.578857\tvalid's multi_logloss: 1.00376\n",
      "[221]\ttrain's multi_logloss: 0.578326\tvalid's multi_logloss: 1.00694\n",
      "[222]\ttrain's multi_logloss: 0.577387\tvalid's multi_logloss: 1.00566\n",
      "[223]\ttrain's multi_logloss: 0.577235\tvalid's multi_logloss: 1.0054\n",
      "[224]\ttrain's multi_logloss: 0.577194\tvalid's multi_logloss: 1.00529\n",
      "[225]\ttrain's multi_logloss: 0.575792\tvalid's multi_logloss: 1.01162\n",
      "[226]\ttrain's multi_logloss: 0.574555\tvalid's multi_logloss: 1.01659\n",
      "[227]\ttrain's multi_logloss: 0.573825\tvalid's multi_logloss: 1.02221\n",
      "[228]\ttrain's multi_logloss: 0.573058\tvalid's multi_logloss: 1.02431\n",
      "[229]\ttrain's multi_logloss: 0.572342\tvalid's multi_logloss: 1.02916\n",
      "[230]\ttrain's multi_logloss: 0.571433\tvalid's multi_logloss: 1.0292\n",
      "[231]\ttrain's multi_logloss: 0.570773\tvalid's multi_logloss: 1.02875\n",
      "[232]\ttrain's multi_logloss: 0.570181\tvalid's multi_logloss: 1.02839\n",
      "[233]\ttrain's multi_logloss: 0.569456\tvalid's multi_logloss: 1.02552\n",
      "[234]\ttrain's multi_logloss: 0.568709\tvalid's multi_logloss: 1.02491\n",
      "[235]\ttrain's multi_logloss: 0.567936\tvalid's multi_logloss: 1.02364\n",
      "[236]\ttrain's multi_logloss: 0.567539\tvalid's multi_logloss: 1.02178\n",
      "[237]\ttrain's multi_logloss: 0.567539\tvalid's multi_logloss: 1.02178\n",
      "[238]\ttrain's multi_logloss: 0.566702\tvalid's multi_logloss: 1.01529\n",
      "[239]\ttrain's multi_logloss: 0.566034\tvalid's multi_logloss: 1.01134\n",
      "[240]\ttrain's multi_logloss: 0.565428\tvalid's multi_logloss: 1.00533\n",
      "[241]\ttrain's multi_logloss: 0.564707\tvalid's multi_logloss: 1.00204\n",
      "[242]\ttrain's multi_logloss: 0.563335\tvalid's multi_logloss: 1.00487\n",
      "[243]\ttrain's multi_logloss: 0.562137\tvalid's multi_logloss: 1.00774\n",
      "[244]\ttrain's multi_logloss: 0.561018\tvalid's multi_logloss: 1.00975\n",
      "[245]\ttrain's multi_logloss: 0.560119\tvalid's multi_logloss: 1.01331\n",
      "[246]\ttrain's multi_logloss: 0.559796\tvalid's multi_logloss: 1.01192\n",
      "[247]\ttrain's multi_logloss: 0.559597\tvalid's multi_logloss: 1.01066\n",
      "[248]\ttrain's multi_logloss: 0.559513\tvalid's multi_logloss: 1.00953\n",
      "[249]\ttrain's multi_logloss: 0.559352\tvalid's multi_logloss: 1.01063\n",
      "[250]\ttrain's multi_logloss: 0.558636\tvalid's multi_logloss: 1.01462\n",
      "[251]\ttrain's multi_logloss: 0.557997\tvalid's multi_logloss: 1.01507\n",
      "[252]\ttrain's multi_logloss: 0.557821\tvalid's multi_logloss: 1.01433\n",
      "[253]\ttrain's multi_logloss: 0.557318\tvalid's multi_logloss: 1.01398\n",
      "[254]\ttrain's multi_logloss: 0.556441\tvalid's multi_logloss: 1.0088\n",
      "[255]\ttrain's multi_logloss: 0.555753\tvalid's multi_logloss: 1.0039\n",
      "[256]\ttrain's multi_logloss: 0.555513\tvalid's multi_logloss: 1.00413\n",
      "[257]\ttrain's multi_logloss: 0.554918\tvalid's multi_logloss: 1.00422\n",
      "[258]\ttrain's multi_logloss: 0.554264\tvalid's multi_logloss: 1.00218\n",
      "[259]\ttrain's multi_logloss: 0.553593\tvalid's multi_logloss: 1.00095\n",
      "[260]\ttrain's multi_logloss: 0.553113\tvalid's multi_logloss: 0.999107\n",
      "[261]\ttrain's multi_logloss: 0.552637\tvalid's multi_logloss: 0.998081\n",
      "[262]\ttrain's multi_logloss: 0.551747\tvalid's multi_logloss: 0.999566\n",
      "[263]\ttrain's multi_logloss: 0.55076\tvalid's multi_logloss: 1.00916\n",
      "[264]\ttrain's multi_logloss: 0.54983\tvalid's multi_logloss: 1.01583\n",
      "[265]\ttrain's multi_logloss: 0.549061\tvalid's multi_logloss: 1.02212\n",
      "[266]\ttrain's multi_logloss: 0.548067\tvalid's multi_logloss: 1.02549\n",
      "[267]\ttrain's multi_logloss: 0.54763\tvalid's multi_logloss: 1.02302\n",
      "[268]\ttrain's multi_logloss: 0.547324\tvalid's multi_logloss: 1.02077\n",
      "[269]\ttrain's multi_logloss: 0.546982\tvalid's multi_logloss: 1.02542\n",
      "[270]\ttrain's multi_logloss: 0.546067\tvalid's multi_logloss: 1.03077\n",
      "[271]\ttrain's multi_logloss: 0.545249\tvalid's multi_logloss: 1.02999\n",
      "[272]\ttrain's multi_logloss: 0.544681\tvalid's multi_logloss: 1.03533\n",
      "[273]\ttrain's multi_logloss: 0.544289\tvalid's multi_logloss: 1.04203\n",
      "[274]\ttrain's multi_logloss: 0.543775\tvalid's multi_logloss: 1.04004\n",
      "[275]\ttrain's multi_logloss: 0.543317\tvalid's multi_logloss: 1.03783\n",
      "[276]\ttrain's multi_logloss: 0.542992\tvalid's multi_logloss: 1.03194\n",
      "[277]\ttrain's multi_logloss: 0.542925\tvalid's multi_logloss: 1.03211\n",
      "[278]\ttrain's multi_logloss: 0.542335\tvalid's multi_logloss: 1.02905\n",
      "[279]\ttrain's multi_logloss: 0.541537\tvalid's multi_logloss: 1.02491\n",
      "[280]\ttrain's multi_logloss: 0.541482\tvalid's multi_logloss: 1.02121\n",
      "Early stopping, best iteration is:\n",
      "[180]\ttrain's multi_logloss: 0.610953\tvalid's multi_logloss: 0.9547\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.5454545454545454\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.076327\n",
      "1   feature2    0.120998\n",
      "2   feature3    0.031617\n",
      "3   feature4    0.056653\n",
      "4   feature5    0.096483\n",
      "5   feature6    0.072270\n",
      "6   feature7    0.057909\n",
      "7   feature8    0.230036\n",
      "8   feature9    0.140776\n",
      "9  feature10    0.116931\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08229\tvalid's my_multiclass_logloss: 1.08462\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.06989\tvalid's my_multiclass_logloss: 1.08773\n",
      "[3]\ttrain's my_multiclass_logloss: 1.05634\tvalid's my_multiclass_logloss: 1.08164\n",
      "[4]\ttrain's my_multiclass_logloss: 1.04583\tvalid's my_multiclass_logloss: 1.07538\n",
      "[5]\ttrain's my_multiclass_logloss: 1.03591\tvalid's my_multiclass_logloss: 1.0714\n",
      "[6]\ttrain's my_multiclass_logloss: 1.02481\tvalid's my_multiclass_logloss: 1.06373\n",
      "[7]\ttrain's my_multiclass_logloss: 1.02033\tvalid's my_multiclass_logloss: 1.06259\n",
      "[8]\ttrain's my_multiclass_logloss: 1.01132\tvalid's my_multiclass_logloss: 1.05898\n",
      "[9]\ttrain's my_multiclass_logloss: 1.00404\tvalid's my_multiclass_logloss: 1.05252\n",
      "[10]\ttrain's my_multiclass_logloss: 0.997043\tvalid's my_multiclass_logloss: 1.04402\n",
      "[11]\ttrain's my_multiclass_logloss: 0.989635\tvalid's my_multiclass_logloss: 1.04953\n",
      "[12]\ttrain's my_multiclass_logloss: 0.983334\tvalid's my_multiclass_logloss: 1.05152\n",
      "[13]\ttrain's my_multiclass_logloss: 0.97621\tvalid's my_multiclass_logloss: 1.04689\n",
      "[14]\ttrain's my_multiclass_logloss: 0.97011\tvalid's my_multiclass_logloss: 1.05081\n",
      "[15]\ttrain's my_multiclass_logloss: 0.962786\tvalid's my_multiclass_logloss: 1.04816\n",
      "[16]\ttrain's my_multiclass_logloss: 0.956829\tvalid's my_multiclass_logloss: 1.05392\n",
      "[17]\ttrain's my_multiclass_logloss: 0.949322\tvalid's my_multiclass_logloss: 1.04405\n",
      "[18]\ttrain's my_multiclass_logloss: 0.94153\tvalid's my_multiclass_logloss: 1.04283\n",
      "[19]\ttrain's my_multiclass_logloss: 0.934684\tvalid's my_multiclass_logloss: 1.04144\n",
      "[20]\ttrain's my_multiclass_logloss: 0.928527\tvalid's my_multiclass_logloss: 1.04069\n",
      "[21]\ttrain's my_multiclass_logloss: 0.923762\tvalid's my_multiclass_logloss: 1.03209\n",
      "[22]\ttrain's my_multiclass_logloss: 0.920185\tvalid's my_multiclass_logloss: 1.02769\n",
      "[23]\ttrain's my_multiclass_logloss: 0.915056\tvalid's my_multiclass_logloss: 1.02484\n",
      "[24]\ttrain's my_multiclass_logloss: 0.910541\tvalid's my_multiclass_logloss: 1.0275\n",
      "[25]\ttrain's my_multiclass_logloss: 0.905199\tvalid's my_multiclass_logloss: 1.03272\n",
      "[26]\ttrain's my_multiclass_logloss: 0.900749\tvalid's my_multiclass_logloss: 1.03471\n",
      "[27]\ttrain's my_multiclass_logloss: 0.89649\tvalid's my_multiclass_logloss: 1.03458\n",
      "[28]\ttrain's my_multiclass_logloss: 0.891717\tvalid's my_multiclass_logloss: 1.03324\n",
      "[29]\ttrain's my_multiclass_logloss: 0.888826\tvalid's my_multiclass_logloss: 1.03576\n",
      "[30]\ttrain's my_multiclass_logloss: 0.884318\tvalid's my_multiclass_logloss: 1.04114\n",
      "[31]\ttrain's my_multiclass_logloss: 0.880763\tvalid's my_multiclass_logloss: 1.0422\n",
      "[32]\ttrain's my_multiclass_logloss: 0.876487\tvalid's my_multiclass_logloss: 1.04821\n",
      "[33]\ttrain's my_multiclass_logloss: 0.871056\tvalid's my_multiclass_logloss: 1.05841\n",
      "[34]\ttrain's my_multiclass_logloss: 0.866513\tvalid's my_multiclass_logloss: 1.05876\n",
      "[35]\ttrain's my_multiclass_logloss: 0.862877\tvalid's my_multiclass_logloss: 1.06037\n",
      "[36]\ttrain's my_multiclass_logloss: 0.859422\tvalid's my_multiclass_logloss: 1.07177\n",
      "[37]\ttrain's my_multiclass_logloss: 0.855697\tvalid's my_multiclass_logloss: 1.07611\n",
      "[38]\ttrain's my_multiclass_logloss: 0.852818\tvalid's my_multiclass_logloss: 1.07858\n",
      "[39]\ttrain's my_multiclass_logloss: 0.850249\tvalid's my_multiclass_logloss: 1.08117\n",
      "[40]\ttrain's my_multiclass_logloss: 0.848476\tvalid's my_multiclass_logloss: 1.07746\n",
      "[41]\ttrain's my_multiclass_logloss: 0.846669\tvalid's my_multiclass_logloss: 1.07125\n",
      "[42]\ttrain's my_multiclass_logloss: 0.844913\tvalid's my_multiclass_logloss: 1.06618\n",
      "[43]\ttrain's my_multiclass_logloss: 0.842765\tvalid's my_multiclass_logloss: 1.05854\n",
      "[44]\ttrain's my_multiclass_logloss: 0.842277\tvalid's my_multiclass_logloss: 1.06174\n",
      "[45]\ttrain's my_multiclass_logloss: 0.838746\tvalid's my_multiclass_logloss: 1.0694\n",
      "[46]\ttrain's my_multiclass_logloss: 0.835188\tvalid's my_multiclass_logloss: 1.06709\n",
      "[47]\ttrain's my_multiclass_logloss: 0.833105\tvalid's my_multiclass_logloss: 1.07182\n",
      "[48]\ttrain's my_multiclass_logloss: 0.830518\tvalid's my_multiclass_logloss: 1.06757\n",
      "[49]\ttrain's my_multiclass_logloss: 0.827627\tvalid's my_multiclass_logloss: 1.06708\n",
      "[50]\ttrain's my_multiclass_logloss: 0.825123\tvalid's my_multiclass_logloss: 1.06311\n",
      "[51]\ttrain's my_multiclass_logloss: 0.823327\tvalid's my_multiclass_logloss: 1.06281\n",
      "[52]\ttrain's my_multiclass_logloss: 0.820474\tvalid's my_multiclass_logloss: 1.05841\n",
      "[53]\ttrain's my_multiclass_logloss: 0.818449\tvalid's my_multiclass_logloss: 1.05178\n",
      "[54]\ttrain's my_multiclass_logloss: 0.816681\tvalid's my_multiclass_logloss: 1.04554\n",
      "[55]\ttrain's my_multiclass_logloss: 0.815143\tvalid's my_multiclass_logloss: 1.03967\n",
      "[56]\ttrain's my_multiclass_logloss: 0.813849\tvalid's my_multiclass_logloss: 1.03645\n",
      "[57]\ttrain's my_multiclass_logloss: 0.811504\tvalid's my_multiclass_logloss: 1.03479\n",
      "[58]\ttrain's my_multiclass_logloss: 0.809035\tvalid's my_multiclass_logloss: 1.03237\n",
      "[59]\ttrain's my_multiclass_logloss: 0.807159\tvalid's my_multiclass_logloss: 1.03939\n",
      "[60]\ttrain's my_multiclass_logloss: 0.805245\tvalid's my_multiclass_logloss: 1.04424\n",
      "[61]\ttrain's my_multiclass_logloss: 0.804103\tvalid's my_multiclass_logloss: 1.04744\n",
      "[62]\ttrain's my_multiclass_logloss: 0.802686\tvalid's my_multiclass_logloss: 1.05259\n",
      "[63]\ttrain's my_multiclass_logloss: 0.801057\tvalid's my_multiclass_logloss: 1.04949\n",
      "[64]\ttrain's my_multiclass_logloss: 0.800009\tvalid's my_multiclass_logloss: 1.05068\n",
      "[65]\ttrain's my_multiclass_logloss: 0.797614\tvalid's my_multiclass_logloss: 1.05914\n",
      "[66]\ttrain's my_multiclass_logloss: 0.795714\tvalid's my_multiclass_logloss: 1.06561\n",
      "[67]\ttrain's my_multiclass_logloss: 0.793812\tvalid's my_multiclass_logloss: 1.07376\n",
      "[68]\ttrain's my_multiclass_logloss: 0.791982\tvalid's my_multiclass_logloss: 1.07006\n",
      "[69]\ttrain's my_multiclass_logloss: 0.790199\tvalid's my_multiclass_logloss: 1.06979\n",
      "[70]\ttrain's my_multiclass_logloss: 0.788929\tvalid's my_multiclass_logloss: 1.06474\n",
      "[71]\ttrain's my_multiclass_logloss: 0.787454\tvalid's my_multiclass_logloss: 1.06478\n",
      "[72]\ttrain's my_multiclass_logloss: 0.78667\tvalid's my_multiclass_logloss: 1.06112\n",
      "[73]\ttrain's my_multiclass_logloss: 0.785505\tvalid's my_multiclass_logloss: 1.05464\n",
      "[74]\ttrain's my_multiclass_logloss: 0.78351\tvalid's my_multiclass_logloss: 1.05494\n",
      "[75]\ttrain's my_multiclass_logloss: 0.781899\tvalid's my_multiclass_logloss: 1.04881\n",
      "[76]\ttrain's my_multiclass_logloss: 0.780705\tvalid's my_multiclass_logloss: 1.04955\n",
      "[77]\ttrain's my_multiclass_logloss: 0.779774\tvalid's my_multiclass_logloss: 1.04735\n",
      "[78]\ttrain's my_multiclass_logloss: 0.778977\tvalid's my_multiclass_logloss: 1.04948\n",
      "[79]\ttrain's my_multiclass_logloss: 0.778674\tvalid's my_multiclass_logloss: 1.04726\n",
      "[80]\ttrain's my_multiclass_logloss: 0.778391\tvalid's my_multiclass_logloss: 1.04592\n",
      "[81]\ttrain's my_multiclass_logloss: 0.776805\tvalid's my_multiclass_logloss: 1.04707\n",
      "[82]\ttrain's my_multiclass_logloss: 0.775515\tvalid's my_multiclass_logloss: 1.04835\n",
      "[83]\ttrain's my_multiclass_logloss: 0.773657\tvalid's my_multiclass_logloss: 1.04307\n",
      "[84]\ttrain's my_multiclass_logloss: 0.772596\tvalid's my_multiclass_logloss: 1.04013\n",
      "[85]\ttrain's my_multiclass_logloss: 0.77202\tvalid's my_multiclass_logloss: 1.03421\n",
      "[86]\ttrain's my_multiclass_logloss: 0.770719\tvalid's my_multiclass_logloss: 1.02933\n",
      "[87]\ttrain's my_multiclass_logloss: 0.76937\tvalid's my_multiclass_logloss: 1.02421\n",
      "[88]\ttrain's my_multiclass_logloss: 0.768937\tvalid's my_multiclass_logloss: 1.02069\n",
      "[89]\ttrain's my_multiclass_logloss: 0.767276\tvalid's my_multiclass_logloss: 1.02537\n",
      "[90]\ttrain's my_multiclass_logloss: 0.765571\tvalid's my_multiclass_logloss: 1.02902\n",
      "[91]\ttrain's my_multiclass_logloss: 0.764057\tvalid's my_multiclass_logloss: 1.02816\n",
      "[92]\ttrain's my_multiclass_logloss: 0.762409\tvalid's my_multiclass_logloss: 1.02748\n",
      "[93]\ttrain's my_multiclass_logloss: 0.760972\tvalid's my_multiclass_logloss: 1.02549\n",
      "[94]\ttrain's my_multiclass_logloss: 0.76017\tvalid's my_multiclass_logloss: 1.02417\n",
      "[95]\ttrain's my_multiclass_logloss: 0.759233\tvalid's my_multiclass_logloss: 1.0222\n",
      "[96]\ttrain's my_multiclass_logloss: 0.758438\tvalid's my_multiclass_logloss: 1.02107\n",
      "[97]\ttrain's my_multiclass_logloss: 0.756811\tvalid's my_multiclass_logloss: 1.01253\n",
      "[98]\ttrain's my_multiclass_logloss: 0.75578\tvalid's my_multiclass_logloss: 1.00596\n",
      "[99]\ttrain's my_multiclass_logloss: 0.754737\tvalid's my_multiclass_logloss: 1.00201\n",
      "[100]\ttrain's my_multiclass_logloss: 0.754199\tvalid's my_multiclass_logloss: 0.998493\n",
      "[101]\ttrain's my_multiclass_logloss: 0.752573\tvalid's my_multiclass_logloss: 1.00013\n",
      "[102]\ttrain's my_multiclass_logloss: 0.750817\tvalid's my_multiclass_logloss: 1.00008\n",
      "[103]\ttrain's my_multiclass_logloss: 0.749375\tvalid's my_multiclass_logloss: 0.997516\n",
      "[104]\ttrain's my_multiclass_logloss: 0.747945\tvalid's my_multiclass_logloss: 1.00368\n",
      "[105]\ttrain's my_multiclass_logloss: 0.746774\tvalid's my_multiclass_logloss: 1.00021\n",
      "[106]\ttrain's my_multiclass_logloss: 0.745898\tvalid's my_multiclass_logloss: 0.996703\n",
      "[107]\ttrain's my_multiclass_logloss: 0.745161\tvalid's my_multiclass_logloss: 0.993439\n",
      "[108]\ttrain's my_multiclass_logloss: 0.744827\tvalid's my_multiclass_logloss: 0.996638\n",
      "[109]\ttrain's my_multiclass_logloss: 0.743715\tvalid's my_multiclass_logloss: 0.995661\n",
      "[110]\ttrain's my_multiclass_logloss: 0.742866\tvalid's my_multiclass_logloss: 1.00022\n",
      "[111]\ttrain's my_multiclass_logloss: 0.741968\tvalid's my_multiclass_logloss: 0.998121\n",
      "[112]\ttrain's my_multiclass_logloss: 0.741363\tvalid's my_multiclass_logloss: 0.996933\n",
      "[113]\ttrain's my_multiclass_logloss: 0.739557\tvalid's my_multiclass_logloss: 0.997329\n",
      "[114]\ttrain's my_multiclass_logloss: 0.737968\tvalid's my_multiclass_logloss: 0.997798\n",
      "[115]\ttrain's my_multiclass_logloss: 0.737341\tvalid's my_multiclass_logloss: 0.999227\n",
      "[116]\ttrain's my_multiclass_logloss: 0.737067\tvalid's my_multiclass_logloss: 0.999835\n",
      "[117]\ttrain's my_multiclass_logloss: 0.735912\tvalid's my_multiclass_logloss: 1.00076\n",
      "[118]\ttrain's my_multiclass_logloss: 0.73502\tvalid's my_multiclass_logloss: 1.00228\n",
      "[119]\ttrain's my_multiclass_logloss: 0.733986\tvalid's my_multiclass_logloss: 1.00198\n",
      "[120]\ttrain's my_multiclass_logloss: 0.733204\tvalid's my_multiclass_logloss: 1.00553\n",
      "[121]\ttrain's my_multiclass_logloss: 0.732281\tvalid's my_multiclass_logloss: 1.00053\n",
      "[122]\ttrain's my_multiclass_logloss: 0.73161\tvalid's my_multiclass_logloss: 0.997367\n",
      "[123]\ttrain's my_multiclass_logloss: 0.730694\tvalid's my_multiclass_logloss: 0.995779\n",
      "[124]\ttrain's my_multiclass_logloss: 0.72943\tvalid's my_multiclass_logloss: 0.994321\n",
      "[125]\ttrain's my_multiclass_logloss: 0.728192\tvalid's my_multiclass_logloss: 0.994942\n",
      "[126]\ttrain's my_multiclass_logloss: 0.726818\tvalid's my_multiclass_logloss: 0.995306\n",
      "[127]\ttrain's my_multiclass_logloss: 0.725719\tvalid's my_multiclass_logloss: 0.99883\n",
      "[128]\ttrain's my_multiclass_logloss: 0.725148\tvalid's my_multiclass_logloss: 0.997363\n",
      "[129]\ttrain's my_multiclass_logloss: 0.724089\tvalid's my_multiclass_logloss: 1.00382\n",
      "[130]\ttrain's my_multiclass_logloss: 0.723182\tvalid's my_multiclass_logloss: 1.01066\n",
      "[131]\ttrain's my_multiclass_logloss: 0.722477\tvalid's my_multiclass_logloss: 1.01448\n",
      "[132]\ttrain's my_multiclass_logloss: 0.721859\tvalid's my_multiclass_logloss: 1.02114\n",
      "[133]\ttrain's my_multiclass_logloss: 0.72054\tvalid's my_multiclass_logloss: 1.01951\n",
      "[134]\ttrain's my_multiclass_logloss: 0.719651\tvalid's my_multiclass_logloss: 1.01749\n",
      "[135]\ttrain's my_multiclass_logloss: 0.718739\tvalid's my_multiclass_logloss: 1.01499\n",
      "[136]\ttrain's my_multiclass_logloss: 0.718097\tvalid's my_multiclass_logloss: 1.01409\n",
      "[137]\ttrain's my_multiclass_logloss: 0.717387\tvalid's my_multiclass_logloss: 1.02073\n",
      "[138]\ttrain's my_multiclass_logloss: 0.716933\tvalid's my_multiclass_logloss: 1.02534\n",
      "[139]\ttrain's my_multiclass_logloss: 0.716113\tvalid's my_multiclass_logloss: 1.02839\n",
      "[140]\ttrain's my_multiclass_logloss: 0.715801\tvalid's my_multiclass_logloss: 1.02563\n",
      "[141]\ttrain's my_multiclass_logloss: 0.714365\tvalid's my_multiclass_logloss: 1.02811\n",
      "[142]\ttrain's my_multiclass_logloss: 0.713118\tvalid's my_multiclass_logloss: 1.03066\n",
      "[143]\ttrain's my_multiclass_logloss: 0.712043\tvalid's my_multiclass_logloss: 1.03324\n",
      "[144]\ttrain's my_multiclass_logloss: 0.711126\tvalid's my_multiclass_logloss: 1.03586\n",
      "[145]\ttrain's my_multiclass_logloss: 0.710369\tvalid's my_multiclass_logloss: 1.0373\n",
      "[146]\ttrain's my_multiclass_logloss: 0.709644\tvalid's my_multiclass_logloss: 1.03849\n",
      "[147]\ttrain's my_multiclass_logloss: 0.70898\tvalid's my_multiclass_logloss: 1.04365\n",
      "[148]\ttrain's my_multiclass_logloss: 0.708578\tvalid's my_multiclass_logloss: 1.04395\n",
      "[149]\ttrain's my_multiclass_logloss: 0.707068\tvalid's my_multiclass_logloss: 1.04733\n",
      "[150]\ttrain's my_multiclass_logloss: 0.706453\tvalid's my_multiclass_logloss: 1.04875\n",
      "[151]\ttrain's my_multiclass_logloss: 0.705228\tvalid's my_multiclass_logloss: 1.05212\n",
      "[152]\ttrain's my_multiclass_logloss: 0.704347\tvalid's my_multiclass_logloss: 1.05327\n",
      "[153]\ttrain's my_multiclass_logloss: 0.703861\tvalid's my_multiclass_logloss: 1.04927\n",
      "[154]\ttrain's my_multiclass_logloss: 0.703898\tvalid's my_multiclass_logloss: 1.04769\n",
      "[155]\ttrain's my_multiclass_logloss: 0.703729\tvalid's my_multiclass_logloss: 1.04421\n",
      "[156]\ttrain's my_multiclass_logloss: 0.704023\tvalid's my_multiclass_logloss: 1.04584\n",
      "[157]\ttrain's my_multiclass_logloss: 0.703355\tvalid's my_multiclass_logloss: 1.04593\n",
      "[158]\ttrain's my_multiclass_logloss: 0.702576\tvalid's my_multiclass_logloss: 1.04907\n",
      "[159]\ttrain's my_multiclass_logloss: 0.701958\tvalid's my_multiclass_logloss: 1.05223\n",
      "[160]\ttrain's my_multiclass_logloss: 0.701522\tvalid's my_multiclass_logloss: 1.05557\n",
      "[161]\ttrain's my_multiclass_logloss: 0.698341\tvalid's my_multiclass_logloss: 1.05595\n",
      "[162]\ttrain's my_multiclass_logloss: 0.695837\tvalid's my_multiclass_logloss: 1.05787\n",
      "[163]\ttrain's my_multiclass_logloss: 0.693595\tvalid's my_multiclass_logloss: 1.05959\n",
      "[164]\ttrain's my_multiclass_logloss: 0.690971\tvalid's my_multiclass_logloss: 1.06137\n",
      "[165]\ttrain's my_multiclass_logloss: 0.690126\tvalid's my_multiclass_logloss: 1.05658\n",
      "[166]\ttrain's my_multiclass_logloss: 0.689782\tvalid's my_multiclass_logloss: 1.05192\n",
      "[167]\ttrain's my_multiclass_logloss: 0.689257\tvalid's my_multiclass_logloss: 1.04768\n",
      "[168]\ttrain's my_multiclass_logloss: 0.688913\tvalid's my_multiclass_logloss: 1.04378\n",
      "[169]\ttrain's my_multiclass_logloss: 0.689018\tvalid's my_multiclass_logloss: 1.0383\n",
      "[170]\ttrain's my_multiclass_logloss: 0.688553\tvalid's my_multiclass_logloss: 1.03466\n",
      "[171]\ttrain's my_multiclass_logloss: 0.688058\tvalid's my_multiclass_logloss: 1.03064\n",
      "[172]\ttrain's my_multiclass_logloss: 0.687762\tvalid's my_multiclass_logloss: 1.02859\n",
      "[173]\ttrain's my_multiclass_logloss: 0.687408\tvalid's my_multiclass_logloss: 1.02448\n",
      "[174]\ttrain's my_multiclass_logloss: 0.687039\tvalid's my_multiclass_logloss: 1.02081\n",
      "[175]\ttrain's my_multiclass_logloss: 0.686728\tvalid's my_multiclass_logloss: 1.01982\n",
      "[176]\ttrain's my_multiclass_logloss: 0.686483\tvalid's my_multiclass_logloss: 1.01519\n",
      "[177]\ttrain's my_multiclass_logloss: 0.685522\tvalid's my_multiclass_logloss: 1.01225\n",
      "[178]\ttrain's my_multiclass_logloss: 0.684231\tvalid's my_multiclass_logloss: 1.01234\n",
      "[179]\ttrain's my_multiclass_logloss: 0.683819\tvalid's my_multiclass_logloss: 1.00604\n",
      "[180]\ttrain's my_multiclass_logloss: 0.682651\tvalid's my_multiclass_logloss: 1.00976\n",
      "[181]\ttrain's my_multiclass_logloss: 0.682349\tvalid's my_multiclass_logloss: 1.00971\n",
      "[182]\ttrain's my_multiclass_logloss: 0.681936\tvalid's my_multiclass_logloss: 1.01285\n",
      "[183]\ttrain's my_multiclass_logloss: 0.681412\tvalid's my_multiclass_logloss: 1.01345\n",
      "[184]\ttrain's my_multiclass_logloss: 0.681006\tvalid's my_multiclass_logloss: 1.01191\n",
      "[185]\ttrain's my_multiclass_logloss: 0.680133\tvalid's my_multiclass_logloss: 1.00919\n",
      "[186]\ttrain's my_multiclass_logloss: 0.679982\tvalid's my_multiclass_logloss: 1.00983\n",
      "[187]\ttrain's my_multiclass_logloss: 0.679747\tvalid's my_multiclass_logloss: 1.00817\n",
      "[188]\ttrain's my_multiclass_logloss: 0.678923\tvalid's my_multiclass_logloss: 1.00834\n",
      "[189]\ttrain's my_multiclass_logloss: 0.677933\tvalid's my_multiclass_logloss: 1.01008\n",
      "[190]\ttrain's my_multiclass_logloss: 0.676978\tvalid's my_multiclass_logloss: 1.01282\n",
      "[191]\ttrain's my_multiclass_logloss: 0.675875\tvalid's my_multiclass_logloss: 1.01646\n",
      "[192]\ttrain's my_multiclass_logloss: 0.675131\tvalid's my_multiclass_logloss: 1.02171\n",
      "[193]\ttrain's my_multiclass_logloss: 0.674782\tvalid's my_multiclass_logloss: 1.01973\n",
      "[194]\ttrain's my_multiclass_logloss: 0.67435\tvalid's my_multiclass_logloss: 1.01791\n",
      "[195]\ttrain's my_multiclass_logloss: 0.674071\tvalid's my_multiclass_logloss: 1.0163\n",
      "[196]\ttrain's my_multiclass_logloss: 0.674076\tvalid's my_multiclass_logloss: 1.01543\n",
      "[197]\ttrain's my_multiclass_logloss: 0.673548\tvalid's my_multiclass_logloss: 1.01591\n",
      "[198]\ttrain's my_multiclass_logloss: 0.673258\tvalid's my_multiclass_logloss: 1.01175\n",
      "[199]\ttrain's my_multiclass_logloss: 0.672805\tvalid's my_multiclass_logloss: 1.01087\n",
      "[200]\ttrain's my_multiclass_logloss: 0.672659\tvalid's my_multiclass_logloss: 1.01194\n",
      "[201]\ttrain's my_multiclass_logloss: 0.672189\tvalid's my_multiclass_logloss: 1.01655\n",
      "[202]\ttrain's my_multiclass_logloss: 0.671455\tvalid's my_multiclass_logloss: 1.02166\n",
      "[203]\ttrain's my_multiclass_logloss: 0.670896\tvalid's my_multiclass_logloss: 1.02666\n",
      "[204]\ttrain's my_multiclass_logloss: 0.670532\tvalid's my_multiclass_logloss: 1.02823\n",
      "[205]\ttrain's my_multiclass_logloss: 0.670171\tvalid's my_multiclass_logloss: 1.03\n",
      "[206]\ttrain's my_multiclass_logloss: 0.669679\tvalid's my_multiclass_logloss: 1.0274\n",
      "[207]\ttrain's my_multiclass_logloss: 0.669407\tvalid's my_multiclass_logloss: 1.02696\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttrain's my_multiclass_logloss: 0.745161\tvalid's my_multiclass_logloss: 0.993439\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.45454545454545453\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.076327\n",
      "1   feature2    0.120998\n",
      "2   feature3    0.031617\n",
      "3   feature4    0.056653\n",
      "4   feature5    0.096483\n",
      "5   feature6    0.072270\n",
      "6   feature7    0.057909\n",
      "7   feature8    0.230036\n",
      "8   feature9    0.140776\n",
      "9  feature10    0.116931\n",
      "     feature  importance\n",
      "0   feature1    0.087477\n",
      "1   feature2    0.128141\n",
      "2   feature3    0.023735\n",
      "3   feature4    0.052742\n",
      "4   feature5    0.120472\n",
      "5   feature6    0.044235\n",
      "6   feature7    0.061642\n",
      "7   feature8    0.235370\n",
      "8   feature9    0.128020\n",
      "9  feature10    0.118166\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.106020\n",
      "1   feature2    0.067916\n",
      "2   feature3   -0.074942\n",
      "3   feature4   -0.037190\n",
      "4   feature5    0.228100\n",
      "5   feature6   -0.266570\n",
      "6   feature7    0.035502\n",
      "7   feature8    0.050718\n",
      "8   feature9   -0.121298\n",
      "9  feature10    0.011744\n",
      "-------------------- 4 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05296\tvalid's multi_logloss: 1.06974\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.04498\tvalid's multi_logloss: 1.0601\n",
      "[3]\ttrain's multi_logloss: 1.03622\tvalid's multi_logloss: 1.05787\n",
      "[4]\ttrain's multi_logloss: 1.0288\tvalid's multi_logloss: 1.06149\n",
      "[5]\ttrain's multi_logloss: 1.02466\tvalid's multi_logloss: 1.04791\n",
      "[6]\ttrain's multi_logloss: 1.02063\tvalid's multi_logloss: 1.03968\n",
      "[7]\ttrain's multi_logloss: 1.01701\tvalid's multi_logloss: 1.02904\n",
      "[8]\ttrain's multi_logloss: 1.01363\tvalid's multi_logloss: 1.01883\n",
      "[9]\ttrain's multi_logloss: 1.00752\tvalid's multi_logloss: 1.01544\n",
      "[10]\ttrain's multi_logloss: 1.00331\tvalid's multi_logloss: 1.01546\n",
      "[11]\ttrain's multi_logloss: 0.997753\tvalid's multi_logloss: 1.01251\n",
      "[12]\ttrain's multi_logloss: 0.992558\tvalid's multi_logloss: 1.00987\n",
      "[13]\ttrain's multi_logloss: 0.986478\tvalid's multi_logloss: 1.00889\n",
      "[14]\ttrain's multi_logloss: 0.981101\tvalid's multi_logloss: 1.00479\n",
      "[15]\ttrain's multi_logloss: 0.977734\tvalid's multi_logloss: 1.00126\n",
      "[16]\ttrain's multi_logloss: 0.972921\tvalid's multi_logloss: 0.997633\n",
      "[17]\ttrain's multi_logloss: 0.966562\tvalid's multi_logloss: 0.994485\n",
      "[18]\ttrain's multi_logloss: 0.960756\tvalid's multi_logloss: 0.991768\n",
      "[19]\ttrain's multi_logloss: 0.954887\tvalid's multi_logloss: 0.988155\n",
      "[20]\ttrain's multi_logloss: 0.949162\tvalid's multi_logloss: 0.979245\n",
      "[21]\ttrain's multi_logloss: 0.943006\tvalid's multi_logloss: 0.976328\n",
      "[22]\ttrain's multi_logloss: 0.937201\tvalid's multi_logloss: 0.973031\n",
      "[23]\ttrain's multi_logloss: 0.932215\tvalid's multi_logloss: 0.975474\n",
      "[24]\ttrain's multi_logloss: 0.927795\tvalid's multi_logloss: 0.967953\n",
      "[25]\ttrain's multi_logloss: 0.922583\tvalid's multi_logloss: 0.964105\n",
      "[26]\ttrain's multi_logloss: 0.918044\tvalid's multi_logloss: 0.957738\n",
      "[27]\ttrain's multi_logloss: 0.914339\tvalid's multi_logloss: 0.960278\n",
      "[28]\ttrain's multi_logloss: 0.910942\tvalid's multi_logloss: 0.96292\n",
      "[29]\ttrain's multi_logloss: 0.905949\tvalid's multi_logloss: 0.954541\n",
      "[30]\ttrain's multi_logloss: 0.900659\tvalid's multi_logloss: 0.954635\n",
      "[31]\ttrain's multi_logloss: 0.897243\tvalid's multi_logloss: 0.949293\n",
      "[32]\ttrain's multi_logloss: 0.892467\tvalid's multi_logloss: 0.949646\n",
      "[33]\ttrain's multi_logloss: 0.889133\tvalid's multi_logloss: 0.956459\n",
      "[34]\ttrain's multi_logloss: 0.886816\tvalid's multi_logloss: 0.960941\n",
      "[35]\ttrain's multi_logloss: 0.882935\tvalid's multi_logloss: 0.967225\n",
      "[36]\ttrain's multi_logloss: 0.880214\tvalid's multi_logloss: 0.974047\n",
      "[37]\ttrain's multi_logloss: 0.876093\tvalid's multi_logloss: 0.965546\n",
      "[38]\ttrain's multi_logloss: 0.873831\tvalid's multi_logloss: 0.96456\n",
      "[39]\ttrain's multi_logloss: 0.870895\tvalid's multi_logloss: 0.966202\n",
      "[40]\ttrain's multi_logloss: 0.868357\tvalid's multi_logloss: 0.964192\n",
      "[41]\ttrain's multi_logloss: 0.863533\tvalid's multi_logloss: 0.953948\n",
      "[42]\ttrain's multi_logloss: 0.860404\tvalid's multi_logloss: 0.943609\n",
      "[43]\ttrain's multi_logloss: 0.85753\tvalid's multi_logloss: 0.943198\n",
      "[44]\ttrain's multi_logloss: 0.853404\tvalid's multi_logloss: 0.933826\n",
      "[45]\ttrain's multi_logloss: 0.851218\tvalid's multi_logloss: 0.929549\n",
      "[46]\ttrain's multi_logloss: 0.848864\tvalid's multi_logloss: 0.924259\n",
      "[47]\ttrain's multi_logloss: 0.846917\tvalid's multi_logloss: 0.921093\n",
      "[48]\ttrain's multi_logloss: 0.844875\tvalid's multi_logloss: 0.9137\n",
      "[49]\ttrain's multi_logloss: 0.841255\tvalid's multi_logloss: 0.913012\n",
      "[50]\ttrain's multi_logloss: 0.837813\tvalid's multi_logloss: 0.916783\n",
      "[51]\ttrain's multi_logloss: 0.834624\tvalid's multi_logloss: 0.920655\n",
      "[52]\ttrain's multi_logloss: 0.831674\tvalid's multi_logloss: 0.924616\n",
      "[53]\ttrain's multi_logloss: 0.826847\tvalid's multi_logloss: 0.919173\n",
      "[54]\ttrain's multi_logloss: 0.82248\tvalid's multi_logloss: 0.915373\n",
      "[55]\ttrain's multi_logloss: 0.818518\tvalid's multi_logloss: 0.910736\n",
      "[56]\ttrain's multi_logloss: 0.816385\tvalid's multi_logloss: 0.914049\n",
      "[57]\ttrain's multi_logloss: 0.813779\tvalid's multi_logloss: 0.910889\n",
      "[58]\ttrain's multi_logloss: 0.811158\tvalid's multi_logloss: 0.910824\n",
      "[59]\ttrain's multi_logloss: 0.807778\tvalid's multi_logloss: 0.903746\n",
      "[60]\ttrain's multi_logloss: 0.804415\tvalid's multi_logloss: 0.896052\n",
      "[61]\ttrain's multi_logloss: 0.802392\tvalid's multi_logloss: 0.894419\n",
      "[62]\ttrain's multi_logloss: 0.80037\tvalid's multi_logloss: 0.896562\n",
      "[63]\ttrain's multi_logloss: 0.798295\tvalid's multi_logloss: 0.896118\n",
      "[64]\ttrain's multi_logloss: 0.796866\tvalid's multi_logloss: 0.890749\n",
      "[65]\ttrain's multi_logloss: 0.795282\tvalid's multi_logloss: 0.88919\n",
      "[66]\ttrain's multi_logloss: 0.793242\tvalid's multi_logloss: 0.886713\n",
      "[67]\ttrain's multi_logloss: 0.791516\tvalid's multi_logloss: 0.881515\n",
      "[68]\ttrain's multi_logloss: 0.789964\tvalid's multi_logloss: 0.880181\n",
      "[69]\ttrain's multi_logloss: 0.78671\tvalid's multi_logloss: 0.875386\n",
      "[70]\ttrain's multi_logloss: 0.783377\tvalid's multi_logloss: 0.86873\n",
      "[71]\ttrain's multi_logloss: 0.781297\tvalid's multi_logloss: 0.868839\n",
      "[72]\ttrain's multi_logloss: 0.778798\tvalid's multi_logloss: 0.870234\n",
      "[73]\ttrain's multi_logloss: 0.776363\tvalid's multi_logloss: 0.856232\n",
      "[74]\ttrain's multi_logloss: 0.773888\tvalid's multi_logloss: 0.847022\n",
      "[75]\ttrain's multi_logloss: 0.772403\tvalid's multi_logloss: 0.840934\n",
      "[76]\ttrain's multi_logloss: 0.771264\tvalid's multi_logloss: 0.835268\n",
      "[77]\ttrain's multi_logloss: 0.769668\tvalid's multi_logloss: 0.83262\n",
      "[78]\ttrain's multi_logloss: 0.767845\tvalid's multi_logloss: 0.82987\n",
      "[79]\ttrain's multi_logloss: 0.766149\tvalid's multi_logloss: 0.825457\n",
      "[80]\ttrain's multi_logloss: 0.764862\tvalid's multi_logloss: 0.823237\n",
      "[81]\ttrain's multi_logloss: 0.763338\tvalid's multi_logloss: 0.827757\n",
      "[82]\ttrain's multi_logloss: 0.761582\tvalid's multi_logloss: 0.823572\n",
      "[83]\ttrain's multi_logloss: 0.759416\tvalid's multi_logloss: 0.812032\n",
      "[84]\ttrain's multi_logloss: 0.757497\tvalid's multi_logloss: 0.817539\n",
      "[85]\ttrain's multi_logloss: 0.756977\tvalid's multi_logloss: 0.813165\n",
      "[86]\ttrain's multi_logloss: 0.755988\tvalid's multi_logloss: 0.811371\n",
      "[87]\ttrain's multi_logloss: 0.755699\tvalid's multi_logloss: 0.807178\n",
      "[88]\ttrain's multi_logloss: 0.755504\tvalid's multi_logloss: 0.803549\n",
      "[89]\ttrain's multi_logloss: 0.75418\tvalid's multi_logloss: 0.801651\n",
      "[90]\ttrain's multi_logloss: 0.753482\tvalid's multi_logloss: 0.800312\n",
      "[91]\ttrain's multi_logloss: 0.752277\tvalid's multi_logloss: 0.799549\n",
      "[92]\ttrain's multi_logloss: 0.751384\tvalid's multi_logloss: 0.800257\n",
      "[93]\ttrain's multi_logloss: 0.749751\tvalid's multi_logloss: 0.797821\n",
      "[94]\ttrain's multi_logloss: 0.7482\tvalid's multi_logloss: 0.800449\n",
      "[95]\ttrain's multi_logloss: 0.746798\tvalid's multi_logloss: 0.800076\n",
      "[96]\ttrain's multi_logloss: 0.74502\tvalid's multi_logloss: 0.800354\n",
      "[97]\ttrain's multi_logloss: 0.743555\tvalid's multi_logloss: 0.793365\n",
      "[98]\ttrain's multi_logloss: 0.74258\tvalid's multi_logloss: 0.789272\n",
      "[99]\ttrain's multi_logloss: 0.741074\tvalid's multi_logloss: 0.784151\n",
      "[100]\ttrain's multi_logloss: 0.73971\tvalid's multi_logloss: 0.779289\n",
      "[101]\ttrain's multi_logloss: 0.738518\tvalid's multi_logloss: 0.778279\n",
      "[102]\ttrain's multi_logloss: 0.737142\tvalid's multi_logloss: 0.778843\n",
      "[103]\ttrain's multi_logloss: 0.736005\tvalid's multi_logloss: 0.78046\n",
      "[104]\ttrain's multi_logloss: 0.73491\tvalid's multi_logloss: 0.781193\n",
      "[105]\ttrain's multi_logloss: 0.733581\tvalid's multi_logloss: 0.783714\n",
      "[106]\ttrain's multi_logloss: 0.732493\tvalid's multi_logloss: 0.787831\n",
      "[107]\ttrain's multi_logloss: 0.731605\tvalid's multi_logloss: 0.792063\n",
      "[108]\ttrain's multi_logloss: 0.730822\tvalid's multi_logloss: 0.795588\n",
      "[109]\ttrain's multi_logloss: 0.729836\tvalid's multi_logloss: 0.794488\n",
      "[110]\ttrain's multi_logloss: 0.729424\tvalid's multi_logloss: 0.791745\n",
      "[111]\ttrain's multi_logloss: 0.728446\tvalid's multi_logloss: 0.790212\n",
      "[112]\ttrain's multi_logloss: 0.72763\tvalid's multi_logloss: 0.789677\n",
      "[113]\ttrain's multi_logloss: 0.725293\tvalid's multi_logloss: 0.791066\n",
      "[114]\ttrain's multi_logloss: 0.723191\tvalid's multi_logloss: 0.790711\n",
      "[115]\ttrain's multi_logloss: 0.721581\tvalid's multi_logloss: 0.791143\n",
      "[116]\ttrain's multi_logloss: 0.720585\tvalid's multi_logloss: 0.788362\n",
      "[117]\ttrain's multi_logloss: 0.719372\tvalid's multi_logloss: 0.783577\n",
      "[118]\ttrain's multi_logloss: 0.71779\tvalid's multi_logloss: 0.779154\n",
      "[119]\ttrain's multi_logloss: 0.717006\tvalid's multi_logloss: 0.778343\n",
      "[120]\ttrain's multi_logloss: 0.715851\tvalid's multi_logloss: 0.779463\n",
      "[121]\ttrain's multi_logloss: 0.713727\tvalid's multi_logloss: 0.778554\n",
      "[122]\ttrain's multi_logloss: 0.711726\tvalid's multi_logloss: 0.777757\n",
      "[123]\ttrain's multi_logloss: 0.709843\tvalid's multi_logloss: 0.777064\n",
      "[124]\ttrain's multi_logloss: 0.708072\tvalid's multi_logloss: 0.776468\n",
      "[125]\ttrain's multi_logloss: 0.706314\tvalid's multi_logloss: 0.780013\n",
      "[126]\ttrain's multi_logloss: 0.7049\tvalid's multi_logloss: 0.785439\n",
      "[127]\ttrain's multi_logloss: 0.703703\tvalid's multi_logloss: 0.790831\n",
      "[128]\ttrain's multi_logloss: 0.702969\tvalid's multi_logloss: 0.795417\n",
      "[129]\ttrain's multi_logloss: 0.702352\tvalid's multi_logloss: 0.793937\n",
      "[130]\ttrain's multi_logloss: 0.702325\tvalid's multi_logloss: 0.795932\n",
      "[131]\ttrain's multi_logloss: 0.702407\tvalid's multi_logloss: 0.797993\n",
      "[132]\ttrain's multi_logloss: 0.702596\tvalid's multi_logloss: 0.798108\n",
      "[133]\ttrain's multi_logloss: 0.701665\tvalid's multi_logloss: 0.797026\n",
      "[134]\ttrain's multi_logloss: 0.700463\tvalid's multi_logloss: 0.794536\n",
      "[135]\ttrain's multi_logloss: 0.699386\tvalid's multi_logloss: 0.79509\n",
      "[136]\ttrain's multi_logloss: 0.698946\tvalid's multi_logloss: 0.793104\n",
      "[137]\ttrain's multi_logloss: 0.69748\tvalid's multi_logloss: 0.791252\n",
      "[138]\ttrain's multi_logloss: 0.696176\tvalid's multi_logloss: 0.789715\n",
      "[139]\ttrain's multi_logloss: 0.695033\tvalid's multi_logloss: 0.788921\n",
      "[140]\ttrain's multi_logloss: 0.694252\tvalid's multi_logloss: 0.788609\n",
      "[141]\ttrain's multi_logloss: 0.693097\tvalid's multi_logloss: 0.789096\n",
      "[142]\ttrain's multi_logloss: 0.692262\tvalid's multi_logloss: 0.787407\n",
      "[143]\ttrain's multi_logloss: 0.691639\tvalid's multi_logloss: 0.790208\n",
      "[144]\ttrain's multi_logloss: 0.691144\tvalid's multi_logloss: 0.787063\n",
      "[145]\ttrain's multi_logloss: 0.690117\tvalid's multi_logloss: 0.787074\n",
      "[146]\ttrain's multi_logloss: 0.688993\tvalid's multi_logloss: 0.786203\n",
      "[147]\ttrain's multi_logloss: 0.687823\tvalid's multi_logloss: 0.784434\n",
      "[148]\ttrain's multi_logloss: 0.686876\tvalid's multi_logloss: 0.784963\n",
      "[149]\ttrain's multi_logloss: 0.685222\tvalid's multi_logloss: 0.785745\n",
      "[150]\ttrain's multi_logloss: 0.683698\tvalid's multi_logloss: 0.787873\n",
      "[151]\ttrain's multi_logloss: 0.682219\tvalid's multi_logloss: 0.789751\n",
      "[152]\ttrain's multi_logloss: 0.680554\tvalid's multi_logloss: 0.787999\n",
      "[153]\ttrain's multi_logloss: 0.679629\tvalid's multi_logloss: 0.787486\n",
      "[154]\ttrain's multi_logloss: 0.678835\tvalid's multi_logloss: 0.782958\n",
      "[155]\ttrain's multi_logloss: 0.678118\tvalid's multi_logloss: 0.780365\n",
      "[156]\ttrain's multi_logloss: 0.677494\tvalid's multi_logloss: 0.777523\n",
      "[157]\ttrain's multi_logloss: 0.676479\tvalid's multi_logloss: 0.776163\n",
      "[158]\ttrain's multi_logloss: 0.676061\tvalid's multi_logloss: 0.774228\n",
      "[159]\ttrain's multi_logloss: 0.675707\tvalid's multi_logloss: 0.776192\n",
      "[160]\ttrain's multi_logloss: 0.675416\tvalid's multi_logloss: 0.774455\n",
      "[161]\ttrain's multi_logloss: 0.673771\tvalid's multi_logloss: 0.77459\n",
      "[162]\ttrain's multi_logloss: 0.672319\tvalid's multi_logloss: 0.772266\n",
      "[163]\ttrain's multi_logloss: 0.670922\tvalid's multi_logloss: 0.770223\n",
      "[164]\ttrain's multi_logloss: 0.669463\tvalid's multi_logloss: 0.770976\n",
      "[165]\ttrain's multi_logloss: 0.668117\tvalid's multi_logloss: 0.766413\n",
      "[166]\ttrain's multi_logloss: 0.667587\tvalid's multi_logloss: 0.762353\n",
      "[167]\ttrain's multi_logloss: 0.667189\tvalid's multi_logloss: 0.75656\n",
      "[168]\ttrain's multi_logloss: 0.666464\tvalid's multi_logloss: 0.750507\n",
      "[169]\ttrain's multi_logloss: 0.665386\tvalid's multi_logloss: 0.751852\n",
      "[170]\ttrain's multi_logloss: 0.664603\tvalid's multi_logloss: 0.749701\n",
      "[171]\ttrain's multi_logloss: 0.66402\tvalid's multi_logloss: 0.750076\n",
      "[172]\ttrain's multi_logloss: 0.663362\tvalid's multi_logloss: 0.751574\n",
      "[173]\ttrain's multi_logloss: 0.662157\tvalid's multi_logloss: 0.748104\n",
      "[174]\ttrain's multi_logloss: 0.660812\tvalid's multi_logloss: 0.747287\n",
      "[175]\ttrain's multi_logloss: 0.65951\tvalid's multi_logloss: 0.747011\n",
      "[176]\ttrain's multi_logloss: 0.658352\tvalid's multi_logloss: 0.746306\n",
      "[177]\ttrain's multi_logloss: 0.656913\tvalid's multi_logloss: 0.748259\n",
      "[178]\ttrain's multi_logloss: 0.65568\tvalid's multi_logloss: 0.748594\n",
      "[179]\ttrain's multi_logloss: 0.654823\tvalid's multi_logloss: 0.751706\n",
      "[180]\ttrain's multi_logloss: 0.653835\tvalid's multi_logloss: 0.754557\n",
      "[181]\ttrain's multi_logloss: 0.653007\tvalid's multi_logloss: 0.753609\n",
      "[182]\ttrain's multi_logloss: 0.652439\tvalid's multi_logloss: 0.752417\n",
      "[183]\ttrain's multi_logloss: 0.651566\tvalid's multi_logloss: 0.755862\n",
      "[184]\ttrain's multi_logloss: 0.650594\tvalid's multi_logloss: 0.757517\n",
      "[185]\ttrain's multi_logloss: 0.64915\tvalid's multi_logloss: 0.754558\n",
      "[186]\ttrain's multi_logloss: 0.647926\tvalid's multi_logloss: 0.749836\n",
      "[187]\ttrain's multi_logloss: 0.647321\tvalid's multi_logloss: 0.741743\n",
      "[188]\ttrain's multi_logloss: 0.64661\tvalid's multi_logloss: 0.739834\n",
      "[189]\ttrain's multi_logloss: 0.644955\tvalid's multi_logloss: 0.740712\n",
      "[190]\ttrain's multi_logloss: 0.642936\tvalid's multi_logloss: 0.747158\n",
      "[191]\ttrain's multi_logloss: 0.641615\tvalid's multi_logloss: 0.750564\n",
      "[192]\ttrain's multi_logloss: 0.641198\tvalid's multi_logloss: 0.749676\n",
      "[193]\ttrain's multi_logloss: 0.640719\tvalid's multi_logloss: 0.747153\n",
      "[194]\ttrain's multi_logloss: 0.640339\tvalid's multi_logloss: 0.744833\n",
      "[195]\ttrain's multi_logloss: 0.640211\tvalid's multi_logloss: 0.743998\n",
      "[196]\ttrain's multi_logloss: 0.640114\tvalid's multi_logloss: 0.740588\n",
      "[197]\ttrain's multi_logloss: 0.639764\tvalid's multi_logloss: 0.742816\n",
      "[198]\ttrain's multi_logloss: 0.639193\tvalid's multi_logloss: 0.742872\n",
      "[199]\ttrain's multi_logloss: 0.638511\tvalid's multi_logloss: 0.742797\n",
      "[200]\ttrain's multi_logloss: 0.63815\tvalid's multi_logloss: 0.746405\n",
      "[201]\ttrain's multi_logloss: 0.637152\tvalid's multi_logloss: 0.74507\n",
      "[202]\ttrain's multi_logloss: 0.636963\tvalid's multi_logloss: 0.74124\n",
      "[203]\ttrain's multi_logloss: 0.636321\tvalid's multi_logloss: 0.740718\n",
      "[204]\ttrain's multi_logloss: 0.635978\tvalid's multi_logloss: 0.736455\n",
      "[205]\ttrain's multi_logloss: 0.635497\tvalid's multi_logloss: 0.733344\n",
      "[206]\ttrain's multi_logloss: 0.634679\tvalid's multi_logloss: 0.731513\n",
      "[207]\ttrain's multi_logloss: 0.634087\tvalid's multi_logloss: 0.728961\n",
      "[208]\ttrain's multi_logloss: 0.633602\tvalid's multi_logloss: 0.726622\n",
      "[209]\ttrain's multi_logloss: 0.632882\tvalid's multi_logloss: 0.72211\n",
      "[210]\ttrain's multi_logloss: 0.631623\tvalid's multi_logloss: 0.716949\n",
      "[211]\ttrain's multi_logloss: 0.630514\tvalid's multi_logloss: 0.711976\n",
      "[212]\ttrain's multi_logloss: 0.629545\tvalid's multi_logloss: 0.707184\n",
      "[213]\ttrain's multi_logloss: 0.629004\tvalid's multi_logloss: 0.701791\n",
      "[214]\ttrain's multi_logloss: 0.627997\tvalid's multi_logloss: 0.702844\n",
      "[215]\ttrain's multi_logloss: 0.627412\tvalid's multi_logloss: 0.699186\n",
      "[216]\ttrain's multi_logloss: 0.626912\tvalid's multi_logloss: 0.69759\n",
      "[217]\ttrain's multi_logloss: 0.625946\tvalid's multi_logloss: 0.696397\n",
      "[218]\ttrain's multi_logloss: 0.625173\tvalid's multi_logloss: 0.695301\n",
      "[219]\ttrain's multi_logloss: 0.624853\tvalid's multi_logloss: 0.694007\n",
      "[220]\ttrain's multi_logloss: 0.62425\tvalid's multi_logloss: 0.693172\n",
      "[221]\ttrain's multi_logloss: 0.624141\tvalid's multi_logloss: 0.694784\n",
      "[222]\ttrain's multi_logloss: 0.624116\tvalid's multi_logloss: 0.696406\n",
      "[223]\ttrain's multi_logloss: 0.623826\tvalid's multi_logloss: 0.697682\n",
      "[224]\ttrain's multi_logloss: 0.623075\tvalid's multi_logloss: 0.700699\n",
      "[225]\ttrain's multi_logloss: 0.62222\tvalid's multi_logloss: 0.702007\n",
      "[226]\ttrain's multi_logloss: 0.621079\tvalid's multi_logloss: 0.702847\n",
      "[227]\ttrain's multi_logloss: 0.620559\tvalid's multi_logloss: 0.700484\n",
      "[228]\ttrain's multi_logloss: 0.619563\tvalid's multi_logloss: 0.70031\n",
      "[229]\ttrain's multi_logloss: 0.619034\tvalid's multi_logloss: 0.701849\n",
      "[230]\ttrain's multi_logloss: 0.618264\tvalid's multi_logloss: 0.705958\n",
      "[231]\ttrain's multi_logloss: 0.617624\tvalid's multi_logloss: 0.70705\n",
      "[232]\ttrain's multi_logloss: 0.616825\tvalid's multi_logloss: 0.708588\n",
      "[233]\ttrain's multi_logloss: 0.615915\tvalid's multi_logloss: 0.71229\n",
      "[234]\ttrain's multi_logloss: 0.615083\tvalid's multi_logloss: 0.716764\n",
      "[235]\ttrain's multi_logloss: 0.614363\tvalid's multi_logloss: 0.722431\n",
      "[236]\ttrain's multi_logloss: 0.613717\tvalid's multi_logloss: 0.727019\n",
      "[237]\ttrain's multi_logloss: 0.613717\tvalid's multi_logloss: 0.727019\n",
      "[238]\ttrain's multi_logloss: 0.612948\tvalid's multi_logloss: 0.728035\n",
      "[239]\ttrain's multi_logloss: 0.612323\tvalid's multi_logloss: 0.727402\n",
      "[240]\ttrain's multi_logloss: 0.61182\tvalid's multi_logloss: 0.727656\n",
      "[241]\ttrain's multi_logloss: 0.611507\tvalid's multi_logloss: 0.723706\n",
      "[242]\ttrain's multi_logloss: 0.610387\tvalid's multi_logloss: 0.723837\n",
      "[243]\ttrain's multi_logloss: 0.609673\tvalid's multi_logloss: 0.722156\n",
      "[244]\ttrain's multi_logloss: 0.609388\tvalid's multi_logloss: 0.720472\n",
      "[245]\ttrain's multi_logloss: 0.60919\tvalid's multi_logloss: 0.718944\n",
      "[246]\ttrain's multi_logloss: 0.608775\tvalid's multi_logloss: 0.718497\n",
      "[247]\ttrain's multi_logloss: 0.608592\tvalid's multi_logloss: 0.716788\n",
      "[248]\ttrain's multi_logloss: 0.60784\tvalid's multi_logloss: 0.717447\n",
      "[249]\ttrain's multi_logloss: 0.607421\tvalid's multi_logloss: 0.71662\n",
      "[250]\ttrain's multi_logloss: 0.60661\tvalid's multi_logloss: 0.714086\n",
      "[251]\ttrain's multi_logloss: 0.606053\tvalid's multi_logloss: 0.712059\n",
      "[252]\ttrain's multi_logloss: 0.605768\tvalid's multi_logloss: 0.709265\n",
      "[253]\ttrain's multi_logloss: 0.605321\tvalid's multi_logloss: 0.707401\n",
      "[254]\ttrain's multi_logloss: 0.604638\tvalid's multi_logloss: 0.709434\n",
      "[255]\ttrain's multi_logloss: 0.604162\tvalid's multi_logloss: 0.704767\n",
      "[256]\ttrain's multi_logloss: 0.603572\tvalid's multi_logloss: 0.702104\n",
      "[257]\ttrain's multi_logloss: 0.60309\tvalid's multi_logloss: 0.703648\n",
      "[258]\ttrain's multi_logloss: 0.602639\tvalid's multi_logloss: 0.697786\n",
      "[259]\ttrain's multi_logloss: 0.602286\tvalid's multi_logloss: 0.69319\n",
      "[260]\ttrain's multi_logloss: 0.601787\tvalid's multi_logloss: 0.691592\n",
      "[261]\ttrain's multi_logloss: 0.60146\tvalid's multi_logloss: 0.686301\n",
      "[262]\ttrain's multi_logloss: 0.600331\tvalid's multi_logloss: 0.68888\n",
      "[263]\ttrain's multi_logloss: 0.599735\tvalid's multi_logloss: 0.691006\n",
      "[264]\ttrain's multi_logloss: 0.599259\tvalid's multi_logloss: 0.693141\n",
      "[265]\ttrain's multi_logloss: 0.598321\tvalid's multi_logloss: 0.69639\n",
      "[266]\ttrain's multi_logloss: 0.597741\tvalid's multi_logloss: 0.695846\n",
      "[267]\ttrain's multi_logloss: 0.597244\tvalid's multi_logloss: 0.695697\n",
      "[268]\ttrain's multi_logloss: 0.596727\tvalid's multi_logloss: 0.696543\n",
      "[269]\ttrain's multi_logloss: 0.596532\tvalid's multi_logloss: 0.69603\n",
      "[270]\ttrain's multi_logloss: 0.59547\tvalid's multi_logloss: 0.701524\n",
      "[271]\ttrain's multi_logloss: 0.594627\tvalid's multi_logloss: 0.70459\n",
      "[272]\ttrain's multi_logloss: 0.593536\tvalid's multi_logloss: 0.707693\n",
      "[273]\ttrain's multi_logloss: 0.592795\tvalid's multi_logloss: 0.709841\n",
      "[274]\ttrain's multi_logloss: 0.591769\tvalid's multi_logloss: 0.712924\n",
      "[275]\ttrain's multi_logloss: 0.590742\tvalid's multi_logloss: 0.718246\n",
      "[276]\ttrain's multi_logloss: 0.589884\tvalid's multi_logloss: 0.721283\n",
      "[277]\ttrain's multi_logloss: 0.589116\tvalid's multi_logloss: 0.724315\n",
      "[278]\ttrain's multi_logloss: 0.588779\tvalid's multi_logloss: 0.72635\n",
      "[279]\ttrain's multi_logloss: 0.587897\tvalid's multi_logloss: 0.726169\n",
      "[280]\ttrain's multi_logloss: 0.587099\tvalid's multi_logloss: 0.726048\n",
      "[281]\ttrain's multi_logloss: 0.586276\tvalid's multi_logloss: 0.724612\n",
      "[282]\ttrain's multi_logloss: 0.585674\tvalid's multi_logloss: 0.72196\n",
      "[283]\ttrain's multi_logloss: 0.585142\tvalid's multi_logloss: 0.719416\n",
      "[284]\ttrain's multi_logloss: 0.584472\tvalid's multi_logloss: 0.716488\n",
      "[285]\ttrain's multi_logloss: 0.583799\tvalid's multi_logloss: 0.714313\n",
      "[286]\ttrain's multi_logloss: 0.582616\tvalid's multi_logloss: 0.71466\n",
      "[287]\ttrain's multi_logloss: 0.581958\tvalid's multi_logloss: 0.714457\n",
      "[288]\ttrain's multi_logloss: 0.580836\tvalid's multi_logloss: 0.71074\n",
      "[289]\ttrain's multi_logloss: 0.580278\tvalid's multi_logloss: 0.711481\n",
      "[290]\ttrain's multi_logloss: 0.579628\tvalid's multi_logloss: 0.70777\n",
      "[291]\ttrain's multi_logloss: 0.57874\tvalid's multi_logloss: 0.703072\n",
      "[292]\ttrain's multi_logloss: 0.57798\tvalid's multi_logloss: 0.699304\n",
      "[293]\ttrain's multi_logloss: 0.577251\tvalid's multi_logloss: 0.6949\n",
      "[294]\ttrain's multi_logloss: 0.576748\tvalid's multi_logloss: 0.695966\n",
      "[295]\ttrain's multi_logloss: 0.576474\tvalid's multi_logloss: 0.695162\n",
      "[296]\ttrain's multi_logloss: 0.575931\tvalid's multi_logloss: 0.696262\n",
      "[297]\ttrain's multi_logloss: 0.575539\tvalid's multi_logloss: 0.699115\n",
      "[298]\ttrain's multi_logloss: 0.57508\tvalid's multi_logloss: 0.699761\n",
      "[299]\ttrain's multi_logloss: 0.574711\tvalid's multi_logloss: 0.69995\n",
      "[300]\ttrain's multi_logloss: 0.57439\tvalid's multi_logloss: 0.698736\n",
      "[301]\ttrain's multi_logloss: 0.574123\tvalid's multi_logloss: 0.69906\n",
      "[302]\ttrain's multi_logloss: 0.573797\tvalid's multi_logloss: 0.699262\n",
      "[303]\ttrain's multi_logloss: 0.57322\tvalid's multi_logloss: 0.700823\n",
      "[304]\ttrain's multi_logloss: 0.573019\tvalid's multi_logloss: 0.701064\n",
      "[305]\ttrain's multi_logloss: 0.572753\tvalid's multi_logloss: 0.701225\n",
      "[306]\ttrain's multi_logloss: 0.571665\tvalid's multi_logloss: 0.700888\n",
      "[307]\ttrain's multi_logloss: 0.57084\tvalid's multi_logloss: 0.699197\n",
      "[308]\ttrain's multi_logloss: 0.569932\tvalid's multi_logloss: 0.698999\n",
      "[309]\ttrain's multi_logloss: 0.569315\tvalid's multi_logloss: 0.696555\n",
      "[310]\ttrain's multi_logloss: 0.568785\tvalid's multi_logloss: 0.694373\n",
      "[311]\ttrain's multi_logloss: 0.568167\tvalid's multi_logloss: 0.696062\n",
      "[312]\ttrain's multi_logloss: 0.567752\tvalid's multi_logloss: 0.69257\n",
      "[313]\ttrain's multi_logloss: 0.567374\tvalid's multi_logloss: 0.690658\n",
      "[314]\ttrain's multi_logloss: 0.566523\tvalid's multi_logloss: 0.690242\n",
      "[315]\ttrain's multi_logloss: 0.56603\tvalid's multi_logloss: 0.68963\n",
      "[316]\ttrain's multi_logloss: 0.565482\tvalid's multi_logloss: 0.688504\n",
      "[317]\ttrain's multi_logloss: 0.565027\tvalid's multi_logloss: 0.68963\n",
      "[318]\ttrain's multi_logloss: 0.564623\tvalid's multi_logloss: 0.688011\n",
      "[319]\ttrain's multi_logloss: 0.564326\tvalid's multi_logloss: 0.686749\n",
      "[320]\ttrain's multi_logloss: 0.564108\tvalid's multi_logloss: 0.686428\n",
      "[321]\ttrain's multi_logloss: 0.56356\tvalid's multi_logloss: 0.687667\n",
      "[322]\ttrain's multi_logloss: 0.562979\tvalid's multi_logloss: 0.690779\n",
      "[323]\ttrain's multi_logloss: 0.562548\tvalid's multi_logloss: 0.693876\n",
      "[324]\ttrain's multi_logloss: 0.562337\tvalid's multi_logloss: 0.694469\n",
      "[325]\ttrain's multi_logloss: 0.562095\tvalid's multi_logloss: 0.697535\n",
      "[326]\ttrain's multi_logloss: 0.561663\tvalid's multi_logloss: 0.695415\n",
      "[327]\ttrain's multi_logloss: 0.561179\tvalid's multi_logloss: 0.693009\n",
      "[328]\ttrain's multi_logloss: 0.560695\tvalid's multi_logloss: 0.690668\n",
      "[329]\ttrain's multi_logloss: 0.560453\tvalid's multi_logloss: 0.688876\n",
      "[330]\ttrain's multi_logloss: 0.559414\tvalid's multi_logloss: 0.68966\n",
      "[331]\ttrain's multi_logloss: 0.558608\tvalid's multi_logloss: 0.691491\n",
      "[332]\ttrain's multi_logloss: 0.558054\tvalid's multi_logloss: 0.692316\n",
      "[333]\ttrain's multi_logloss: 0.557388\tvalid's multi_logloss: 0.689807\n",
      "[334]\ttrain's multi_logloss: 0.556797\tvalid's multi_logloss: 0.688982\n",
      "[335]\ttrain's multi_logloss: 0.556389\tvalid's multi_logloss: 0.689393\n",
      "[336]\ttrain's multi_logloss: 0.555956\tvalid's multi_logloss: 0.686022\n",
      "[337]\ttrain's multi_logloss: 0.555503\tvalid's multi_logloss: 0.685369\n",
      "[338]\ttrain's multi_logloss: 0.554835\tvalid's multi_logloss: 0.683148\n",
      "[339]\ttrain's multi_logloss: 0.554156\tvalid's multi_logloss: 0.67989\n",
      "[340]\ttrain's multi_logloss: 0.553568\tvalid's multi_logloss: 0.67675\n",
      "[341]\ttrain's multi_logloss: 0.553274\tvalid's multi_logloss: 0.673241\n",
      "[342]\ttrain's multi_logloss: 0.552937\tvalid's multi_logloss: 0.672515\n",
      "[343]\ttrain's multi_logloss: 0.551987\tvalid's multi_logloss: 0.67166\n",
      "[344]\ttrain's multi_logloss: 0.550942\tvalid's multi_logloss: 0.670261\n",
      "[345]\ttrain's multi_logloss: 0.550364\tvalid's multi_logloss: 0.668889\n",
      "[346]\ttrain's multi_logloss: 0.550347\tvalid's multi_logloss: 0.668184\n",
      "[347]\ttrain's multi_logloss: 0.549997\tvalid's multi_logloss: 0.669907\n",
      "[348]\ttrain's multi_logloss: 0.549775\tvalid's multi_logloss: 0.667773\n",
      "[349]\ttrain's multi_logloss: 0.549224\tvalid's multi_logloss: 0.670406\n",
      "[350]\ttrain's multi_logloss: 0.548586\tvalid's multi_logloss: 0.663435\n",
      "[351]\ttrain's multi_logloss: 0.548357\tvalid's multi_logloss: 0.65849\n",
      "[352]\ttrain's multi_logloss: 0.547997\tvalid's multi_logloss: 0.656646\n",
      "[353]\ttrain's multi_logloss: 0.547449\tvalid's multi_logloss: 0.653139\n",
      "[354]\ttrain's multi_logloss: 0.547258\tvalid's multi_logloss: 0.652406\n",
      "[355]\ttrain's multi_logloss: 0.547095\tvalid's multi_logloss: 0.654658\n",
      "[356]\ttrain's multi_logloss: 0.547112\tvalid's multi_logloss: 0.658071\n",
      "[357]\ttrain's multi_logloss: 0.547138\tvalid's multi_logloss: 0.658851\n",
      "[358]\ttrain's multi_logloss: 0.546586\tvalid's multi_logloss: 0.660943\n",
      "[359]\ttrain's multi_logloss: 0.54648\tvalid's multi_logloss: 0.663803\n",
      "[360]\ttrain's multi_logloss: 0.545979\tvalid's multi_logloss: 0.666279\n",
      "[361]\ttrain's multi_logloss: 0.545362\tvalid's multi_logloss: 0.669378\n",
      "[362]\ttrain's multi_logloss: 0.544552\tvalid's multi_logloss: 0.665416\n",
      "[363]\ttrain's multi_logloss: 0.543712\tvalid's multi_logloss: 0.662295\n",
      "[364]\ttrain's multi_logloss: 0.543068\tvalid's multi_logloss: 0.661685\n",
      "[365]\ttrain's multi_logloss: 0.542271\tvalid's multi_logloss: 0.658369\n",
      "[366]\ttrain's multi_logloss: 0.541542\tvalid's multi_logloss: 0.652495\n",
      "[367]\ttrain's multi_logloss: 0.54085\tvalid's multi_logloss: 0.650677\n",
      "[368]\ttrain's multi_logloss: 0.540041\tvalid's multi_logloss: 0.647484\n",
      "[369]\ttrain's multi_logloss: 0.539382\tvalid's multi_logloss: 0.644525\n",
      "[370]\ttrain's multi_logloss: 0.539318\tvalid's multi_logloss: 0.64422\n",
      "[371]\ttrain's multi_logloss: 0.538949\tvalid's multi_logloss: 0.647165\n",
      "[372]\ttrain's multi_logloss: 0.538661\tvalid's multi_logloss: 0.650068\n",
      "[373]\ttrain's multi_logloss: 0.538357\tvalid's multi_logloss: 0.6499\n",
      "[374]\ttrain's multi_logloss: 0.538079\tvalid's multi_logloss: 0.648214\n",
      "[375]\ttrain's multi_logloss: 0.537651\tvalid's multi_logloss: 0.650975\n",
      "[376]\ttrain's multi_logloss: 0.53721\tvalid's multi_logloss: 0.653941\n",
      "[377]\ttrain's multi_logloss: 0.537025\tvalid's multi_logloss: 0.652262\n",
      "[378]\ttrain's multi_logloss: 0.53688\tvalid's multi_logloss: 0.650299\n",
      "[379]\ttrain's multi_logloss: 0.536437\tvalid's multi_logloss: 0.650384\n",
      "[380]\ttrain's multi_logloss: 0.535911\tvalid's multi_logloss: 0.65123\n",
      "[381]\ttrain's multi_logloss: 0.535231\tvalid's multi_logloss: 0.656114\n",
      "[382]\ttrain's multi_logloss: 0.534303\tvalid's multi_logloss: 0.654352\n",
      "[383]\ttrain's multi_logloss: 0.533011\tvalid's multi_logloss: 0.655917\n",
      "[384]\ttrain's multi_logloss: 0.531812\tvalid's multi_logloss: 0.657807\n",
      "[385]\ttrain's multi_logloss: 0.531688\tvalid's multi_logloss: 0.658489\n",
      "[386]\ttrain's multi_logloss: 0.531411\tvalid's multi_logloss: 0.655048\n",
      "[387]\ttrain's multi_logloss: 0.531249\tvalid's multi_logloss: 0.651805\n",
      "[388]\ttrain's multi_logloss: 0.531304\tvalid's multi_logloss: 0.646297\n",
      "[389]\ttrain's multi_logloss: 0.531482\tvalid's multi_logloss: 0.641087\n",
      "[390]\ttrain's multi_logloss: 0.530797\tvalid's multi_logloss: 0.640788\n",
      "[391]\ttrain's multi_logloss: 0.53032\tvalid's multi_logloss: 0.644436\n",
      "[392]\ttrain's multi_logloss: 0.529829\tvalid's multi_logloss: 0.64592\n",
      "[393]\ttrain's multi_logloss: 0.529514\tvalid's multi_logloss: 0.649453\n",
      "[394]\ttrain's multi_logloss: 0.529008\tvalid's multi_logloss: 0.652025\n",
      "[395]\ttrain's multi_logloss: 0.528357\tvalid's multi_logloss: 0.653727\n",
      "[396]\ttrain's multi_logloss: 0.528075\tvalid's multi_logloss: 0.655949\n",
      "[397]\ttrain's multi_logloss: 0.527546\tvalid's multi_logloss: 0.657562\n",
      "[398]\ttrain's multi_logloss: 0.527104\tvalid's multi_logloss: 0.656147\n",
      "[399]\ttrain's multi_logloss: 0.526643\tvalid's multi_logloss: 0.653784\n",
      "[400]\ttrain's multi_logloss: 0.526254\tvalid's multi_logloss: 0.654581\n",
      "[401]\ttrain's multi_logloss: 0.525808\tvalid's multi_logloss: 0.65567\n",
      "[402]\ttrain's multi_logloss: 0.525539\tvalid's multi_logloss: 0.658251\n",
      "[403]\ttrain's multi_logloss: 0.525421\tvalid's multi_logloss: 0.659965\n",
      "[404]\ttrain's multi_logloss: 0.525412\tvalid's multi_logloss: 0.660833\n",
      "[405]\ttrain's multi_logloss: 0.525422\tvalid's multi_logloss: 0.66187\n",
      "[406]\ttrain's multi_logloss: 0.525125\tvalid's multi_logloss: 0.659589\n",
      "[407]\ttrain's multi_logloss: 0.525393\tvalid's multi_logloss: 0.661276\n",
      "[408]\ttrain's multi_logloss: 0.525234\tvalid's multi_logloss: 0.659845\n",
      "[409]\ttrain's multi_logloss: 0.525386\tvalid's multi_logloss: 0.661911\n",
      "[410]\ttrain's multi_logloss: 0.524802\tvalid's multi_logloss: 0.66406\n",
      "[411]\ttrain's multi_logloss: 0.524404\tvalid's multi_logloss: 0.66422\n",
      "[412]\ttrain's multi_logloss: 0.52339\tvalid's multi_logloss: 0.667319\n",
      "[413]\ttrain's multi_logloss: 0.522913\tvalid's multi_logloss: 0.671876\n",
      "[414]\ttrain's multi_logloss: 0.522096\tvalid's multi_logloss: 0.671756\n",
      "[415]\ttrain's multi_logloss: 0.521438\tvalid's multi_logloss: 0.670758\n",
      "[416]\ttrain's multi_logloss: 0.520685\tvalid's multi_logloss: 0.672616\n",
      "[417]\ttrain's multi_logloss: 0.520178\tvalid's multi_logloss: 0.675765\n",
      "[418]\ttrain's multi_logloss: 0.519181\tvalid's multi_logloss: 0.677141\n",
      "[419]\ttrain's multi_logloss: 0.518373\tvalid's multi_logloss: 0.681174\n",
      "[420]\ttrain's multi_logloss: 0.517644\tvalid's multi_logloss: 0.682689\n",
      "[421]\ttrain's multi_logloss: 0.516658\tvalid's multi_logloss: 0.683621\n",
      "[422]\ttrain's multi_logloss: 0.51582\tvalid's multi_logloss: 0.686979\n",
      "[423]\ttrain's multi_logloss: 0.515052\tvalid's multi_logloss: 0.690797\n",
      "[424]\ttrain's multi_logloss: 0.514444\tvalid's multi_logloss: 0.694193\n",
      "[425]\ttrain's multi_logloss: 0.514171\tvalid's multi_logloss: 0.699682\n",
      "[426]\ttrain's multi_logloss: 0.513756\tvalid's multi_logloss: 0.701415\n",
      "[427]\ttrain's multi_logloss: 0.513509\tvalid's multi_logloss: 0.705012\n",
      "[428]\ttrain's multi_logloss: 0.513179\tvalid's multi_logloss: 0.710318\n",
      "[429]\ttrain's multi_logloss: 0.512751\tvalid's multi_logloss: 0.716449\n",
      "[430]\ttrain's multi_logloss: 0.512028\tvalid's multi_logloss: 0.714531\n",
      "[431]\ttrain's multi_logloss: 0.511503\tvalid's multi_logloss: 0.711386\n",
      "[432]\ttrain's multi_logloss: 0.511486\tvalid's multi_logloss: 0.710941\n",
      "[433]\ttrain's multi_logloss: 0.511362\tvalid's multi_logloss: 0.709148\n",
      "[434]\ttrain's multi_logloss: 0.510918\tvalid's multi_logloss: 0.707755\n",
      "[435]\ttrain's multi_logloss: 0.510287\tvalid's multi_logloss: 0.706443\n",
      "[436]\ttrain's multi_logloss: 0.509859\tvalid's multi_logloss: 0.705559\n",
      "[437]\ttrain's multi_logloss: 0.509326\tvalid's multi_logloss: 0.704572\n",
      "[438]\ttrain's multi_logloss: 0.509224\tvalid's multi_logloss: 0.70926\n",
      "[439]\ttrain's multi_logloss: 0.509213\tvalid's multi_logloss: 0.710342\n",
      "[440]\ttrain's multi_logloss: 0.509119\tvalid's multi_logloss: 0.711241\n",
      "[441]\ttrain's multi_logloss: 0.509081\tvalid's multi_logloss: 0.712174\n",
      "[442]\ttrain's multi_logloss: 0.508433\tvalid's multi_logloss: 0.715948\n",
      "[443]\ttrain's multi_logloss: 0.507752\tvalid's multi_logloss: 0.716963\n",
      "[444]\ttrain's multi_logloss: 0.507161\tvalid's multi_logloss: 0.718017\n",
      "[445]\ttrain's multi_logloss: 0.506641\tvalid's multi_logloss: 0.716477\n",
      "[446]\ttrain's multi_logloss: 0.50616\tvalid's multi_logloss: 0.718639\n",
      "[447]\ttrain's multi_logloss: 0.505767\tvalid's multi_logloss: 0.720811\n",
      "[448]\ttrain's multi_logloss: 0.505398\tvalid's multi_logloss: 0.720533\n",
      "[449]\ttrain's multi_logloss: 0.505102\tvalid's multi_logloss: 0.723011\n",
      "[450]\ttrain's multi_logloss: 0.504316\tvalid's multi_logloss: 0.726636\n",
      "[451]\ttrain's multi_logloss: 0.503744\tvalid's multi_logloss: 0.727213\n",
      "[452]\ttrain's multi_logloss: 0.502956\tvalid's multi_logloss: 0.730944\n",
      "[453]\ttrain's multi_logloss: 0.502711\tvalid's multi_logloss: 0.732894\n",
      "[454]\ttrain's multi_logloss: 0.501584\tvalid's multi_logloss: 0.729928\n",
      "[455]\ttrain's multi_logloss: 0.501104\tvalid's multi_logloss: 0.729187\n",
      "[456]\ttrain's multi_logloss: 0.500511\tvalid's multi_logloss: 0.725356\n",
      "[457]\ttrain's multi_logloss: 0.499559\tvalid's multi_logloss: 0.725228\n",
      "[458]\ttrain's multi_logloss: 0.498951\tvalid's multi_logloss: 0.726593\n",
      "[459]\ttrain's multi_logloss: 0.498413\tvalid's multi_logloss: 0.72798\n",
      "[460]\ttrain's multi_logloss: 0.49814\tvalid's multi_logloss: 0.727158\n",
      "[461]\ttrain's multi_logloss: 0.497747\tvalid's multi_logloss: 0.728442\n",
      "[462]\ttrain's multi_logloss: 0.497126\tvalid's multi_logloss: 0.727719\n",
      "[463]\ttrain's multi_logloss: 0.496519\tvalid's multi_logloss: 0.726294\n",
      "[464]\ttrain's multi_logloss: 0.495892\tvalid's multi_logloss: 0.724385\n",
      "[465]\ttrain's multi_logloss: 0.495686\tvalid's multi_logloss: 0.723064\n",
      "[466]\ttrain's multi_logloss: 0.495485\tvalid's multi_logloss: 0.72595\n",
      "[467]\ttrain's multi_logloss: 0.495052\tvalid's multi_logloss: 0.727038\n",
      "[468]\ttrain's multi_logloss: 0.494888\tvalid's multi_logloss: 0.729332\n",
      "[469]\ttrain's multi_logloss: 0.494703\tvalid's multi_logloss: 0.729509\n",
      "[470]\ttrain's multi_logloss: 0.494125\tvalid's multi_logloss: 0.726731\n",
      "[471]\ttrain's multi_logloss: 0.493561\tvalid's multi_logloss: 0.727745\n",
      "[472]\ttrain's multi_logloss: 0.492786\tvalid's multi_logloss: 0.727169\n",
      "[473]\ttrain's multi_logloss: 0.492122\tvalid's multi_logloss: 0.723032\n",
      "[474]\ttrain's multi_logloss: 0.491475\tvalid's multi_logloss: 0.725372\n",
      "[475]\ttrain's multi_logloss: 0.490748\tvalid's multi_logloss: 0.723047\n",
      "[476]\ttrain's multi_logloss: 0.490134\tvalid's multi_logloss: 0.720855\n",
      "[477]\ttrain's multi_logloss: 0.489407\tvalid's multi_logloss: 0.721665\n",
      "[478]\ttrain's multi_logloss: 0.488917\tvalid's multi_logloss: 0.722731\n",
      "[479]\ttrain's multi_logloss: 0.488725\tvalid's multi_logloss: 0.724495\n",
      "[480]\ttrain's multi_logloss: 0.488432\tvalid's multi_logloss: 0.725625\n",
      "[481]\ttrain's multi_logloss: 0.488228\tvalid's multi_logloss: 0.726788\n",
      "[482]\ttrain's multi_logloss: 0.4879\tvalid's multi_logloss: 0.724117\n",
      "[483]\ttrain's multi_logloss: 0.487662\tvalid's multi_logloss: 0.724688\n",
      "[484]\ttrain's multi_logloss: 0.48745\tvalid's multi_logloss: 0.729009\n",
      "[485]\ttrain's multi_logloss: 0.487248\tvalid's multi_logloss: 0.726557\n",
      "[486]\ttrain's multi_logloss: 0.48696\tvalid's multi_logloss: 0.723958\n",
      "[487]\ttrain's multi_logloss: 0.486733\tvalid's multi_logloss: 0.72141\n",
      "[488]\ttrain's multi_logloss: 0.486457\tvalid's multi_logloss: 0.719934\n",
      "[489]\ttrain's multi_logloss: 0.486202\tvalid's multi_logloss: 0.717256\n",
      "[490]\ttrain's multi_logloss: 0.485699\tvalid's multi_logloss: 0.717642\n",
      "Early stopping, best iteration is:\n",
      "[390]\ttrain's multi_logloss: 0.530797\tvalid's multi_logloss: 0.640788\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.111356\n",
      "1   feature2    0.125119\n",
      "2   feature3    0.054172\n",
      "3   feature4    0.079876\n",
      "4   feature5    0.130345\n",
      "5   feature6    0.084560\n",
      "6   feature7    0.054571\n",
      "7   feature8    0.111218\n",
      "8   feature9    0.119003\n",
      "9  feature10    0.129781\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.0871\tvalid's my_multiclass_logloss: 1.09216\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07862\tvalid's my_multiclass_logloss: 1.08589\n",
      "[3]\ttrain's my_multiclass_logloss: 1.0689\tvalid's my_multiclass_logloss: 1.08346\n",
      "[4]\ttrain's my_multiclass_logloss: 1.06001\tvalid's my_multiclass_logloss: 1.08302\n",
      "[5]\ttrain's my_multiclass_logloss: 1.05379\tvalid's my_multiclass_logloss: 1.07329\n",
      "[6]\ttrain's my_multiclass_logloss: 1.04523\tvalid's my_multiclass_logloss: 1.06549\n",
      "[7]\ttrain's my_multiclass_logloss: 1.04001\tvalid's my_multiclass_logloss: 1.05702\n",
      "[8]\ttrain's my_multiclass_logloss: 1.03233\tvalid's my_multiclass_logloss: 1.04651\n",
      "[9]\ttrain's my_multiclass_logloss: 1.02591\tvalid's my_multiclass_logloss: 1.04168\n",
      "[10]\ttrain's my_multiclass_logloss: 1.01867\tvalid's my_multiclass_logloss: 1.03873\n",
      "[11]\ttrain's my_multiclass_logloss: 1.01246\tvalid's my_multiclass_logloss: 1.03329\n",
      "[12]\ttrain's my_multiclass_logloss: 1.00628\tvalid's my_multiclass_logloss: 1.02612\n",
      "[13]\ttrain's my_multiclass_logloss: 0.999596\tvalid's my_multiclass_logloss: 1.02275\n",
      "[14]\ttrain's my_multiclass_logloss: 0.993545\tvalid's my_multiclass_logloss: 1.02424\n",
      "[15]\ttrain's my_multiclass_logloss: 0.987122\tvalid's my_multiclass_logloss: 1.02469\n",
      "[16]\ttrain's my_multiclass_logloss: 0.982214\tvalid's my_multiclass_logloss: 1.0225\n",
      "[17]\ttrain's my_multiclass_logloss: 0.974563\tvalid's my_multiclass_logloss: 1.01781\n",
      "[18]\ttrain's my_multiclass_logloss: 0.967733\tvalid's my_multiclass_logloss: 1.01728\n",
      "[19]\ttrain's my_multiclass_logloss: 0.961769\tvalid's my_multiclass_logloss: 1.01099\n",
      "[20]\ttrain's my_multiclass_logloss: 0.956536\tvalid's my_multiclass_logloss: 1.00239\n",
      "[21]\ttrain's my_multiclass_logloss: 0.952401\tvalid's my_multiclass_logloss: 0.996188\n",
      "[22]\ttrain's my_multiclass_logloss: 0.947527\tvalid's my_multiclass_logloss: 0.990201\n",
      "[23]\ttrain's my_multiclass_logloss: 0.94214\tvalid's my_multiclass_logloss: 0.98712\n",
      "[24]\ttrain's my_multiclass_logloss: 0.937043\tvalid's my_multiclass_logloss: 0.985101\n",
      "[25]\ttrain's my_multiclass_logloss: 0.931782\tvalid's my_multiclass_logloss: 0.985037\n",
      "[26]\ttrain's my_multiclass_logloss: 0.926928\tvalid's my_multiclass_logloss: 0.979882\n",
      "[27]\ttrain's my_multiclass_logloss: 0.922408\tvalid's my_multiclass_logloss: 0.975159\n",
      "[28]\ttrain's my_multiclass_logloss: 0.91812\tvalid's my_multiclass_logloss: 0.96386\n",
      "[29]\ttrain's my_multiclass_logloss: 0.914016\tvalid's my_multiclass_logloss: 0.9646\n",
      "[30]\ttrain's my_multiclass_logloss: 0.910519\tvalid's my_multiclass_logloss: 0.960148\n",
      "[31]\ttrain's my_multiclass_logloss: 0.907277\tvalid's my_multiclass_logloss: 0.951163\n",
      "[32]\ttrain's my_multiclass_logloss: 0.904537\tvalid's my_multiclass_logloss: 0.9476\n",
      "[33]\ttrain's my_multiclass_logloss: 0.901496\tvalid's my_multiclass_logloss: 0.955291\n",
      "[34]\ttrain's my_multiclass_logloss: 0.898091\tvalid's my_multiclass_logloss: 0.961818\n",
      "[35]\ttrain's my_multiclass_logloss: 0.896452\tvalid's my_multiclass_logloss: 0.968452\n",
      "[36]\ttrain's my_multiclass_logloss: 0.894366\tvalid's my_multiclass_logloss: 0.975989\n",
      "[37]\ttrain's my_multiclass_logloss: 0.892904\tvalid's my_multiclass_logloss: 0.977063\n",
      "[38]\ttrain's my_multiclass_logloss: 0.890965\tvalid's my_multiclass_logloss: 0.978448\n",
      "[39]\ttrain's my_multiclass_logloss: 0.888107\tvalid's my_multiclass_logloss: 0.974041\n",
      "[40]\ttrain's my_multiclass_logloss: 0.886415\tvalid's my_multiclass_logloss: 0.970807\n",
      "[41]\ttrain's my_multiclass_logloss: 0.881983\tvalid's my_multiclass_logloss: 0.95548\n",
      "[42]\ttrain's my_multiclass_logloss: 0.879761\tvalid's my_multiclass_logloss: 0.945791\n",
      "[43]\ttrain's my_multiclass_logloss: 0.87526\tvalid's my_multiclass_logloss: 0.935537\n",
      "[44]\ttrain's my_multiclass_logloss: 0.872149\tvalid's my_multiclass_logloss: 0.922868\n",
      "[45]\ttrain's my_multiclass_logloss: 0.870117\tvalid's my_multiclass_logloss: 0.917251\n",
      "[46]\ttrain's my_multiclass_logloss: 0.868352\tvalid's my_multiclass_logloss: 0.918429\n",
      "[47]\ttrain's my_multiclass_logloss: 0.86672\tvalid's my_multiclass_logloss: 0.913255\n",
      "[48]\ttrain's my_multiclass_logloss: 0.865747\tvalid's my_multiclass_logloss: 0.909034\n",
      "[49]\ttrain's my_multiclass_logloss: 0.861905\tvalid's my_multiclass_logloss: 0.908825\n",
      "[50]\ttrain's my_multiclass_logloss: 0.85857\tvalid's my_multiclass_logloss: 0.915518\n",
      "[51]\ttrain's my_multiclass_logloss: 0.855557\tvalid's my_multiclass_logloss: 0.915632\n",
      "[52]\ttrain's my_multiclass_logloss: 0.854176\tvalid's my_multiclass_logloss: 0.914737\n",
      "[53]\ttrain's my_multiclass_logloss: 0.849899\tvalid's my_multiclass_logloss: 0.908747\n",
      "[54]\ttrain's my_multiclass_logloss: 0.846941\tvalid's my_multiclass_logloss: 0.8987\n",
      "[55]\ttrain's my_multiclass_logloss: 0.843226\tvalid's my_multiclass_logloss: 0.89761\n",
      "[56]\ttrain's my_multiclass_logloss: 0.84102\tvalid's my_multiclass_logloss: 0.888751\n",
      "[57]\ttrain's my_multiclass_logloss: 0.838615\tvalid's my_multiclass_logloss: 0.885348\n",
      "[58]\ttrain's my_multiclass_logloss: 0.835703\tvalid's my_multiclass_logloss: 0.878394\n",
      "[59]\ttrain's my_multiclass_logloss: 0.833227\tvalid's my_multiclass_logloss: 0.878141\n",
      "[60]\ttrain's my_multiclass_logloss: 0.830528\tvalid's my_multiclass_logloss: 0.874537\n",
      "[61]\ttrain's my_multiclass_logloss: 0.829405\tvalid's my_multiclass_logloss: 0.873115\n",
      "[62]\ttrain's my_multiclass_logloss: 0.828766\tvalid's my_multiclass_logloss: 0.875306\n",
      "[63]\ttrain's my_multiclass_logloss: 0.827541\tvalid's my_multiclass_logloss: 0.877602\n",
      "[64]\ttrain's my_multiclass_logloss: 0.825975\tvalid's my_multiclass_logloss: 0.874817\n",
      "[65]\ttrain's my_multiclass_logloss: 0.824085\tvalid's my_multiclass_logloss: 0.86964\n",
      "[66]\ttrain's my_multiclass_logloss: 0.823351\tvalid's my_multiclass_logloss: 0.8703\n",
      "[67]\ttrain's my_multiclass_logloss: 0.822401\tvalid's my_multiclass_logloss: 0.872371\n",
      "[68]\ttrain's my_multiclass_logloss: 0.821007\tvalid's my_multiclass_logloss: 0.868148\n",
      "[69]\ttrain's my_multiclass_logloss: 0.819135\tvalid's my_multiclass_logloss: 0.866993\n",
      "[70]\ttrain's my_multiclass_logloss: 0.816518\tvalid's my_multiclass_logloss: 0.861426\n",
      "[71]\ttrain's my_multiclass_logloss: 0.814494\tvalid's my_multiclass_logloss: 0.863923\n",
      "[72]\ttrain's my_multiclass_logloss: 0.812182\tvalid's my_multiclass_logloss: 0.860856\n",
      "[73]\ttrain's my_multiclass_logloss: 0.811009\tvalid's my_multiclass_logloss: 0.855763\n",
      "[74]\ttrain's my_multiclass_logloss: 0.810071\tvalid's my_multiclass_logloss: 0.848104\n",
      "[75]\ttrain's my_multiclass_logloss: 0.808705\tvalid's my_multiclass_logloss: 0.845707\n",
      "[76]\ttrain's my_multiclass_logloss: 0.807553\tvalid's my_multiclass_logloss: 0.843473\n",
      "[77]\ttrain's my_multiclass_logloss: 0.806366\tvalid's my_multiclass_logloss: 0.838328\n",
      "[78]\ttrain's my_multiclass_logloss: 0.804721\tvalid's my_multiclass_logloss: 0.834704\n",
      "[79]\ttrain's my_multiclass_logloss: 0.803016\tvalid's my_multiclass_logloss: 0.832344\n",
      "[80]\ttrain's my_multiclass_logloss: 0.801687\tvalid's my_multiclass_logloss: 0.829051\n",
      "[81]\ttrain's my_multiclass_logloss: 0.800087\tvalid's my_multiclass_logloss: 0.830843\n",
      "[82]\ttrain's my_multiclass_logloss: 0.798755\tvalid's my_multiclass_logloss: 0.832717\n",
      "[83]\ttrain's my_multiclass_logloss: 0.797473\tvalid's my_multiclass_logloss: 0.828808\n",
      "[84]\ttrain's my_multiclass_logloss: 0.796515\tvalid's my_multiclass_logloss: 0.829503\n",
      "[85]\ttrain's my_multiclass_logloss: 0.795408\tvalid's my_multiclass_logloss: 0.826772\n",
      "[86]\ttrain's my_multiclass_logloss: 0.795286\tvalid's my_multiclass_logloss: 0.824821\n",
      "[87]\ttrain's my_multiclass_logloss: 0.794483\tvalid's my_multiclass_logloss: 0.824098\n",
      "[88]\ttrain's my_multiclass_logloss: 0.7935\tvalid's my_multiclass_logloss: 0.825002\n",
      "[89]\ttrain's my_multiclass_logloss: 0.792557\tvalid's my_multiclass_logloss: 0.825838\n",
      "[90]\ttrain's my_multiclass_logloss: 0.791966\tvalid's my_multiclass_logloss: 0.826707\n",
      "[91]\ttrain's my_multiclass_logloss: 0.791721\tvalid's my_multiclass_logloss: 0.824719\n",
      "[92]\ttrain's my_multiclass_logloss: 0.791472\tvalid's my_multiclass_logloss: 0.820729\n",
      "[93]\ttrain's my_multiclass_logloss: 0.790588\tvalid's my_multiclass_logloss: 0.820062\n",
      "[94]\ttrain's my_multiclass_logloss: 0.789279\tvalid's my_multiclass_logloss: 0.818519\n",
      "[95]\ttrain's my_multiclass_logloss: 0.788128\tvalid's my_multiclass_logloss: 0.817133\n",
      "[96]\ttrain's my_multiclass_logloss: 0.78728\tvalid's my_multiclass_logloss: 0.817602\n",
      "[97]\ttrain's my_multiclass_logloss: 0.786153\tvalid's my_multiclass_logloss: 0.814202\n",
      "[98]\ttrain's my_multiclass_logloss: 0.785498\tvalid's my_multiclass_logloss: 0.810934\n",
      "[99]\ttrain's my_multiclass_logloss: 0.784108\tvalid's my_multiclass_logloss: 0.806243\n",
      "[100]\ttrain's my_multiclass_logloss: 0.782932\tvalid's my_multiclass_logloss: 0.801921\n",
      "[101]\ttrain's my_multiclass_logloss: 0.782012\tvalid's my_multiclass_logloss: 0.803397\n",
      "[102]\ttrain's my_multiclass_logloss: 0.780676\tvalid's my_multiclass_logloss: 0.805093\n",
      "[103]\ttrain's my_multiclass_logloss: 0.780256\tvalid's my_multiclass_logloss: 0.807679\n",
      "[104]\ttrain's my_multiclass_logloss: 0.779269\tvalid's my_multiclass_logloss: 0.80946\n",
      "[105]\ttrain's my_multiclass_logloss: 0.778719\tvalid's my_multiclass_logloss: 0.816133\n",
      "[106]\ttrain's my_multiclass_logloss: 0.778445\tvalid's my_multiclass_logloss: 0.822833\n",
      "[107]\ttrain's my_multiclass_logloss: 0.778414\tvalid's my_multiclass_logloss: 0.829537\n",
      "[108]\ttrain's my_multiclass_logloss: 0.777793\tvalid's my_multiclass_logloss: 0.829693\n",
      "[109]\ttrain's my_multiclass_logloss: 0.776216\tvalid's my_multiclass_logloss: 0.829981\n",
      "[110]\ttrain's my_multiclass_logloss: 0.774591\tvalid's my_multiclass_logloss: 0.829444\n",
      "[111]\ttrain's my_multiclass_logloss: 0.77386\tvalid's my_multiclass_logloss: 0.826152\n",
      "[112]\ttrain's my_multiclass_logloss: 0.773433\tvalid's my_multiclass_logloss: 0.826747\n",
      "[113]\ttrain's my_multiclass_logloss: 0.771378\tvalid's my_multiclass_logloss: 0.828945\n",
      "[114]\ttrain's my_multiclass_logloss: 0.769527\tvalid's my_multiclass_logloss: 0.829013\n",
      "[115]\ttrain's my_multiclass_logloss: 0.76739\tvalid's my_multiclass_logloss: 0.833086\n",
      "[116]\ttrain's my_multiclass_logloss: 0.766357\tvalid's my_multiclass_logloss: 0.832559\n",
      "[117]\ttrain's my_multiclass_logloss: 0.76576\tvalid's my_multiclass_logloss: 0.833121\n",
      "[118]\ttrain's my_multiclass_logloss: 0.765412\tvalid's my_multiclass_logloss: 0.834683\n",
      "[119]\ttrain's my_multiclass_logloss: 0.764748\tvalid's my_multiclass_logloss: 0.835946\n",
      "[120]\ttrain's my_multiclass_logloss: 0.764303\tvalid's my_multiclass_logloss: 0.838619\n",
      "[121]\ttrain's my_multiclass_logloss: 0.762609\tvalid's my_multiclass_logloss: 0.84155\n",
      "[122]\ttrain's my_multiclass_logloss: 0.760994\tvalid's my_multiclass_logloss: 0.84407\n",
      "[123]\ttrain's my_multiclass_logloss: 0.759315\tvalid's my_multiclass_logloss: 0.846243\n",
      "[124]\ttrain's my_multiclass_logloss: 0.757644\tvalid's my_multiclass_logloss: 0.848784\n",
      "[125]\ttrain's my_multiclass_logloss: 0.756921\tvalid's my_multiclass_logloss: 0.850601\n",
      "[126]\ttrain's my_multiclass_logloss: 0.75596\tvalid's my_multiclass_logloss: 0.855043\n",
      "[127]\ttrain's my_multiclass_logloss: 0.755628\tvalid's my_multiclass_logloss: 0.856247\n",
      "[128]\ttrain's my_multiclass_logloss: 0.754989\tvalid's my_multiclass_logloss: 0.861958\n",
      "[129]\ttrain's my_multiclass_logloss: 0.754949\tvalid's my_multiclass_logloss: 0.864928\n",
      "[130]\ttrain's my_multiclass_logloss: 0.754596\tvalid's my_multiclass_logloss: 0.863885\n",
      "[131]\ttrain's my_multiclass_logloss: 0.754583\tvalid's my_multiclass_logloss: 0.864585\n",
      "[132]\ttrain's my_multiclass_logloss: 0.755029\tvalid's my_multiclass_logloss: 0.866733\n",
      "[133]\ttrain's my_multiclass_logloss: 0.75424\tvalid's my_multiclass_logloss: 0.866157\n",
      "[134]\ttrain's my_multiclass_logloss: 0.753389\tvalid's my_multiclass_logloss: 0.865474\n",
      "[135]\ttrain's my_multiclass_logloss: 0.752998\tvalid's my_multiclass_logloss: 0.862495\n",
      "[136]\ttrain's my_multiclass_logloss: 0.752646\tvalid's my_multiclass_logloss: 0.859108\n",
      "[137]\ttrain's my_multiclass_logloss: 0.751795\tvalid's my_multiclass_logloss: 0.854714\n",
      "[138]\ttrain's my_multiclass_logloss: 0.750807\tvalid's my_multiclass_logloss: 0.851166\n",
      "[139]\ttrain's my_multiclass_logloss: 0.749745\tvalid's my_multiclass_logloss: 0.85164\n",
      "[140]\ttrain's my_multiclass_logloss: 0.748781\tvalid's my_multiclass_logloss: 0.849196\n",
      "[141]\ttrain's my_multiclass_logloss: 0.748046\tvalid's my_multiclass_logloss: 0.850054\n",
      "[142]\ttrain's my_multiclass_logloss: 0.747534\tvalid's my_multiclass_logloss: 0.85104\n",
      "[143]\ttrain's my_multiclass_logloss: 0.747095\tvalid's my_multiclass_logloss: 0.850993\n",
      "[144]\ttrain's my_multiclass_logloss: 0.746928\tvalid's my_multiclass_logloss: 0.852153\n",
      "[145]\ttrain's my_multiclass_logloss: 0.745807\tvalid's my_multiclass_logloss: 0.853044\n",
      "[146]\ttrain's my_multiclass_logloss: 0.745028\tvalid's my_multiclass_logloss: 0.8571\n",
      "[147]\ttrain's my_multiclass_logloss: 0.743596\tvalid's my_multiclass_logloss: 0.857012\n",
      "[148]\ttrain's my_multiclass_logloss: 0.742247\tvalid's my_multiclass_logloss: 0.854735\n",
      "[149]\ttrain's my_multiclass_logloss: 0.740639\tvalid's my_multiclass_logloss: 0.852784\n",
      "[150]\ttrain's my_multiclass_logloss: 0.739381\tvalid's my_multiclass_logloss: 0.852875\n",
      "[151]\ttrain's my_multiclass_logloss: 0.738042\tvalid's my_multiclass_logloss: 0.851235\n",
      "[152]\ttrain's my_multiclass_logloss: 0.73669\tvalid's my_multiclass_logloss: 0.852242\n",
      "[153]\ttrain's my_multiclass_logloss: 0.73539\tvalid's my_multiclass_logloss: 0.848281\n",
      "[154]\ttrain's my_multiclass_logloss: 0.734431\tvalid's my_multiclass_logloss: 0.844759\n",
      "[155]\ttrain's my_multiclass_logloss: 0.733754\tvalid's my_multiclass_logloss: 0.844553\n",
      "[156]\ttrain's my_multiclass_logloss: 0.733338\tvalid's my_multiclass_logloss: 0.842011\n",
      "[157]\ttrain's my_multiclass_logloss: 0.732709\tvalid's my_multiclass_logloss: 0.839837\n",
      "[158]\ttrain's my_multiclass_logloss: 0.731855\tvalid's my_multiclass_logloss: 0.835476\n",
      "[159]\ttrain's my_multiclass_logloss: 0.731173\tvalid's my_multiclass_logloss: 0.831442\n",
      "[160]\ttrain's my_multiclass_logloss: 0.730645\tvalid's my_multiclass_logloss: 0.827706\n",
      "[161]\ttrain's my_multiclass_logloss: 0.729464\tvalid's my_multiclass_logloss: 0.828876\n",
      "[162]\ttrain's my_multiclass_logloss: 0.728092\tvalid's my_multiclass_logloss: 0.827263\n",
      "[163]\ttrain's my_multiclass_logloss: 0.726567\tvalid's my_multiclass_logloss: 0.821551\n",
      "[164]\ttrain's my_multiclass_logloss: 0.724477\tvalid's my_multiclass_logloss: 0.815753\n",
      "[165]\ttrain's my_multiclass_logloss: 0.723275\tvalid's my_multiclass_logloss: 0.809793\n",
      "[166]\ttrain's my_multiclass_logloss: 0.722674\tvalid's my_multiclass_logloss: 0.806493\n",
      "[167]\ttrain's my_multiclass_logloss: 0.722149\tvalid's my_multiclass_logloss: 0.797311\n",
      "[168]\ttrain's my_multiclass_logloss: 0.721545\tvalid's my_multiclass_logloss: 0.792254\n",
      "[169]\ttrain's my_multiclass_logloss: 0.720455\tvalid's my_multiclass_logloss: 0.79296\n",
      "[170]\ttrain's my_multiclass_logloss: 0.71997\tvalid's my_multiclass_logloss: 0.794821\n",
      "[171]\ttrain's my_multiclass_logloss: 0.719604\tvalid's my_multiclass_logloss: 0.794091\n",
      "[172]\ttrain's my_multiclass_logloss: 0.719017\tvalid's my_multiclass_logloss: 0.795038\n",
      "[173]\ttrain's my_multiclass_logloss: 0.718633\tvalid's my_multiclass_logloss: 0.798385\n",
      "[174]\ttrain's my_multiclass_logloss: 0.718246\tvalid's my_multiclass_logloss: 0.79609\n",
      "[175]\ttrain's my_multiclass_logloss: 0.717696\tvalid's my_multiclass_logloss: 0.795809\n",
      "[176]\ttrain's my_multiclass_logloss: 0.717275\tvalid's my_multiclass_logloss: 0.795608\n",
      "[177]\ttrain's my_multiclass_logloss: 0.716327\tvalid's my_multiclass_logloss: 0.800674\n",
      "[178]\ttrain's my_multiclass_logloss: 0.715294\tvalid's my_multiclass_logloss: 0.80245\n",
      "[179]\ttrain's my_multiclass_logloss: 0.714621\tvalid's my_multiclass_logloss: 0.801593\n",
      "[180]\ttrain's my_multiclass_logloss: 0.713667\tvalid's my_multiclass_logloss: 0.803377\n",
      "[181]\ttrain's my_multiclass_logloss: 0.712789\tvalid's my_multiclass_logloss: 0.809687\n",
      "[182]\ttrain's my_multiclass_logloss: 0.712192\tvalid's my_multiclass_logloss: 0.810158\n",
      "[183]\ttrain's my_multiclass_logloss: 0.712388\tvalid's my_multiclass_logloss: 0.805211\n",
      "[184]\ttrain's my_multiclass_logloss: 0.711917\tvalid's my_multiclass_logloss: 0.811524\n",
      "[185]\ttrain's my_multiclass_logloss: 0.711151\tvalid's my_multiclass_logloss: 0.80748\n",
      "[186]\ttrain's my_multiclass_logloss: 0.710578\tvalid's my_multiclass_logloss: 0.801009\n",
      "[187]\ttrain's my_multiclass_logloss: 0.710097\tvalid's my_multiclass_logloss: 0.800614\n",
      "[188]\ttrain's my_multiclass_logloss: 0.709667\tvalid's my_multiclass_logloss: 0.796433\n",
      "[189]\ttrain's my_multiclass_logloss: 0.709123\tvalid's my_multiclass_logloss: 0.79479\n",
      "[190]\ttrain's my_multiclass_logloss: 0.707886\tvalid's my_multiclass_logloss: 0.798341\n",
      "[191]\ttrain's my_multiclass_logloss: 0.707483\tvalid's my_multiclass_logloss: 0.797634\n",
      "[192]\ttrain's my_multiclass_logloss: 0.70729\tvalid's my_multiclass_logloss: 0.797118\n",
      "[193]\ttrain's my_multiclass_logloss: 0.706767\tvalid's my_multiclass_logloss: 0.792274\n",
      "[194]\ttrain's my_multiclass_logloss: 0.70635\tvalid's my_multiclass_logloss: 0.790156\n",
      "[195]\ttrain's my_multiclass_logloss: 0.706103\tvalid's my_multiclass_logloss: 0.787873\n",
      "[196]\ttrain's my_multiclass_logloss: 0.705811\tvalid's my_multiclass_logloss: 0.78601\n",
      "[197]\ttrain's my_multiclass_logloss: 0.705523\tvalid's my_multiclass_logloss: 0.788057\n",
      "[198]\ttrain's my_multiclass_logloss: 0.70527\tvalid's my_multiclass_logloss: 0.791999\n",
      "[199]\ttrain's my_multiclass_logloss: 0.704893\tvalid's my_multiclass_logloss: 0.796082\n",
      "[200]\ttrain's my_multiclass_logloss: 0.704794\tvalid's my_multiclass_logloss: 0.798219\n",
      "[201]\ttrain's my_multiclass_logloss: 0.704281\tvalid's my_multiclass_logloss: 0.802777\n",
      "[202]\ttrain's my_multiclass_logloss: 0.703791\tvalid's my_multiclass_logloss: 0.803171\n",
      "[203]\ttrain's my_multiclass_logloss: 0.703303\tvalid's my_multiclass_logloss: 0.802544\n",
      "[204]\ttrain's my_multiclass_logloss: 0.702932\tvalid's my_multiclass_logloss: 0.802031\n",
      "[205]\ttrain's my_multiclass_logloss: 0.702507\tvalid's my_multiclass_logloss: 0.795554\n",
      "[206]\ttrain's my_multiclass_logloss: 0.702104\tvalid's my_multiclass_logloss: 0.792516\n",
      "[207]\ttrain's my_multiclass_logloss: 0.701726\tvalid's my_multiclass_logloss: 0.790943\n",
      "[208]\ttrain's my_multiclass_logloss: 0.701354\tvalid's my_multiclass_logloss: 0.792427\n",
      "[209]\ttrain's my_multiclass_logloss: 0.700492\tvalid's my_multiclass_logloss: 0.785867\n",
      "[210]\ttrain's my_multiclass_logloss: 0.699802\tvalid's my_multiclass_logloss: 0.779723\n",
      "[211]\ttrain's my_multiclass_logloss: 0.699268\tvalid's my_multiclass_logloss: 0.773967\n",
      "[212]\ttrain's my_multiclass_logloss: 0.699204\tvalid's my_multiclass_logloss: 0.768879\n",
      "[213]\ttrain's my_multiclass_logloss: 0.698586\tvalid's my_multiclass_logloss: 0.768317\n",
      "[214]\ttrain's my_multiclass_logloss: 0.697854\tvalid's my_multiclass_logloss: 0.771923\n",
      "[215]\ttrain's my_multiclass_logloss: 0.697068\tvalid's my_multiclass_logloss: 0.770758\n",
      "[216]\ttrain's my_multiclass_logloss: 0.696388\tvalid's my_multiclass_logloss: 0.770323\n",
      "[217]\ttrain's my_multiclass_logloss: 0.69604\tvalid's my_multiclass_logloss: 0.768874\n",
      "[218]\ttrain's my_multiclass_logloss: 0.695653\tvalid's my_multiclass_logloss: 0.767744\n",
      "[219]\ttrain's my_multiclass_logloss: 0.695426\tvalid's my_multiclass_logloss: 0.769187\n",
      "[220]\ttrain's my_multiclass_logloss: 0.695364\tvalid's my_multiclass_logloss: 0.768931\n",
      "[221]\ttrain's my_multiclass_logloss: 0.694851\tvalid's my_multiclass_logloss: 0.770821\n",
      "[222]\ttrain's my_multiclass_logloss: 0.694477\tvalid's my_multiclass_logloss: 0.772739\n",
      "[223]\ttrain's my_multiclass_logloss: 0.694349\tvalid's my_multiclass_logloss: 0.77119\n",
      "[224]\ttrain's my_multiclass_logloss: 0.693965\tvalid's my_multiclass_logloss: 0.772021\n",
      "[225]\ttrain's my_multiclass_logloss: 0.693213\tvalid's my_multiclass_logloss: 0.77432\n",
      "[226]\ttrain's my_multiclass_logloss: 0.691511\tvalid's my_multiclass_logloss: 0.775125\n",
      "[227]\ttrain's my_multiclass_logloss: 0.690503\tvalid's my_multiclass_logloss: 0.77567\n",
      "[228]\ttrain's my_multiclass_logloss: 0.689753\tvalid's my_multiclass_logloss: 0.772403\n",
      "[229]\ttrain's my_multiclass_logloss: 0.688533\tvalid's my_multiclass_logloss: 0.776895\n",
      "[230]\ttrain's my_multiclass_logloss: 0.687571\tvalid's my_multiclass_logloss: 0.781144\n",
      "[231]\ttrain's my_multiclass_logloss: 0.687189\tvalid's my_multiclass_logloss: 0.782358\n",
      "[232]\ttrain's my_multiclass_logloss: 0.68657\tvalid's my_multiclass_logloss: 0.786594\n",
      "[233]\ttrain's my_multiclass_logloss: 0.685958\tvalid's my_multiclass_logloss: 0.790511\n",
      "[234]\ttrain's my_multiclass_logloss: 0.685582\tvalid's my_multiclass_logloss: 0.794387\n",
      "[235]\ttrain's my_multiclass_logloss: 0.685489\tvalid's my_multiclass_logloss: 0.798774\n",
      "[236]\ttrain's my_multiclass_logloss: 0.68496\tvalid's my_multiclass_logloss: 0.804914\n",
      "[237]\ttrain's my_multiclass_logloss: 0.68496\tvalid's my_multiclass_logloss: 0.804914\n",
      "[238]\ttrain's my_multiclass_logloss: 0.684455\tvalid's my_multiclass_logloss: 0.804871\n",
      "[239]\ttrain's my_multiclass_logloss: 0.684052\tvalid's my_multiclass_logloss: 0.805604\n",
      "[240]\ttrain's my_multiclass_logloss: 0.683925\tvalid's my_multiclass_logloss: 0.801245\n",
      "[241]\ttrain's my_multiclass_logloss: 0.683854\tvalid's my_multiclass_logloss: 0.797251\n",
      "[242]\ttrain's my_multiclass_logloss: 0.682879\tvalid's my_multiclass_logloss: 0.797792\n",
      "[243]\ttrain's my_multiclass_logloss: 0.682205\tvalid's my_multiclass_logloss: 0.795883\n",
      "[244]\ttrain's my_multiclass_logloss: 0.681504\tvalid's my_multiclass_logloss: 0.796627\n",
      "[245]\ttrain's my_multiclass_logloss: 0.68099\tvalid's my_multiclass_logloss: 0.797475\n",
      "[246]\ttrain's my_multiclass_logloss: 0.680531\tvalid's my_multiclass_logloss: 0.796203\n",
      "[247]\ttrain's my_multiclass_logloss: 0.680117\tvalid's my_multiclass_logloss: 0.794898\n",
      "[248]\ttrain's my_multiclass_logloss: 0.679849\tvalid's my_multiclass_logloss: 0.793845\n",
      "[249]\ttrain's my_multiclass_logloss: 0.67912\tvalid's my_multiclass_logloss: 0.795034\n",
      "[250]\ttrain's my_multiclass_logloss: 0.67852\tvalid's my_multiclass_logloss: 0.793694\n",
      "[251]\ttrain's my_multiclass_logloss: 0.678079\tvalid's my_multiclass_logloss: 0.796528\n",
      "[252]\ttrain's my_multiclass_logloss: 0.677637\tvalid's my_multiclass_logloss: 0.794274\n",
      "[253]\ttrain's my_multiclass_logloss: 0.677261\tvalid's my_multiclass_logloss: 0.797452\n",
      "[254]\ttrain's my_multiclass_logloss: 0.676258\tvalid's my_multiclass_logloss: 0.79031\n",
      "[255]\ttrain's my_multiclass_logloss: 0.67548\tvalid's my_multiclass_logloss: 0.783706\n",
      "[256]\ttrain's my_multiclass_logloss: 0.674712\tvalid's my_multiclass_logloss: 0.78018\n",
      "[257]\ttrain's my_multiclass_logloss: 0.674259\tvalid's my_multiclass_logloss: 0.77447\n",
      "[258]\ttrain's my_multiclass_logloss: 0.673838\tvalid's my_multiclass_logloss: 0.769255\n",
      "[259]\ttrain's my_multiclass_logloss: 0.673439\tvalid's my_multiclass_logloss: 0.7629\n",
      "[260]\ttrain's my_multiclass_logloss: 0.673202\tvalid's my_multiclass_logloss: 0.756934\n",
      "[261]\ttrain's my_multiclass_logloss: 0.673137\tvalid's my_multiclass_logloss: 0.752677\n",
      "[262]\ttrain's my_multiclass_logloss: 0.672629\tvalid's my_multiclass_logloss: 0.755353\n",
      "[263]\ttrain's my_multiclass_logloss: 0.672211\tvalid's my_multiclass_logloss: 0.758291\n",
      "[264]\ttrain's my_multiclass_logloss: 0.672114\tvalid's my_multiclass_logloss: 0.761903\n",
      "[265]\ttrain's my_multiclass_logloss: 0.671926\tvalid's my_multiclass_logloss: 0.764536\n",
      "[266]\ttrain's my_multiclass_logloss: 0.671929\tvalid's my_multiclass_logloss: 0.762968\n",
      "[267]\ttrain's my_multiclass_logloss: 0.671321\tvalid's my_multiclass_logloss: 0.764485\n",
      "[268]\ttrain's my_multiclass_logloss: 0.670961\tvalid's my_multiclass_logloss: 0.764444\n",
      "[269]\ttrain's my_multiclass_logloss: 0.670531\tvalid's my_multiclass_logloss: 0.769206\n",
      "[270]\ttrain's my_multiclass_logloss: 0.669461\tvalid's my_multiclass_logloss: 0.77183\n",
      "[271]\ttrain's my_multiclass_logloss: 0.668606\tvalid's my_multiclass_logloss: 0.774424\n",
      "[272]\ttrain's my_multiclass_logloss: 0.667956\tvalid's my_multiclass_logloss: 0.776168\n",
      "[273]\ttrain's my_multiclass_logloss: 0.66725\tvalid's my_multiclass_logloss: 0.777547\n",
      "[274]\ttrain's my_multiclass_logloss: 0.66624\tvalid's my_multiclass_logloss: 0.784253\n",
      "[275]\ttrain's my_multiclass_logloss: 0.665561\tvalid's my_multiclass_logloss: 0.790921\n",
      "[276]\ttrain's my_multiclass_logloss: 0.664618\tvalid's my_multiclass_logloss: 0.794863\n",
      "[277]\ttrain's my_multiclass_logloss: 0.663699\tvalid's my_multiclass_logloss: 0.795\n",
      "[278]\ttrain's my_multiclass_logloss: 0.663494\tvalid's my_multiclass_logloss: 0.797967\n",
      "[279]\ttrain's my_multiclass_logloss: 0.662957\tvalid's my_multiclass_logloss: 0.798285\n",
      "[280]\ttrain's my_multiclass_logloss: 0.662692\tvalid's my_multiclass_logloss: 0.799047\n",
      "[281]\ttrain's my_multiclass_logloss: 0.662297\tvalid's my_multiclass_logloss: 0.79743\n",
      "[282]\ttrain's my_multiclass_logloss: 0.66186\tvalid's my_multiclass_logloss: 0.793996\n",
      "[283]\ttrain's my_multiclass_logloss: 0.661791\tvalid's my_multiclass_logloss: 0.791251\n",
      "[284]\ttrain's my_multiclass_logloss: 0.661072\tvalid's my_multiclass_logloss: 0.788523\n",
      "[285]\ttrain's my_multiclass_logloss: 0.660462\tvalid's my_multiclass_logloss: 0.785989\n",
      "[286]\ttrain's my_multiclass_logloss: 0.660024\tvalid's my_multiclass_logloss: 0.787151\n",
      "[287]\ttrain's my_multiclass_logloss: 0.659311\tvalid's my_multiclass_logloss: 0.791567\n",
      "[288]\ttrain's my_multiclass_logloss: 0.658257\tvalid's my_multiclass_logloss: 0.792546\n",
      "[289]\ttrain's my_multiclass_logloss: 0.657867\tvalid's my_multiclass_logloss: 0.796145\n",
      "[290]\ttrain's my_multiclass_logloss: 0.656773\tvalid's my_multiclass_logloss: 0.79414\n",
      "[291]\ttrain's my_multiclass_logloss: 0.655894\tvalid's my_multiclass_logloss: 0.792376\n",
      "[292]\ttrain's my_multiclass_logloss: 0.655081\tvalid's my_multiclass_logloss: 0.787682\n",
      "[293]\ttrain's my_multiclass_logloss: 0.654533\tvalid's my_multiclass_logloss: 0.786342\n",
      "[294]\ttrain's my_multiclass_logloss: 0.654265\tvalid's my_multiclass_logloss: 0.786412\n",
      "[295]\ttrain's my_multiclass_logloss: 0.65376\tvalid's my_multiclass_logloss: 0.789211\n",
      "[296]\ttrain's my_multiclass_logloss: 0.65326\tvalid's my_multiclass_logloss: 0.793347\n",
      "[297]\ttrain's my_multiclass_logloss: 0.653143\tvalid's my_multiclass_logloss: 0.793539\n",
      "[298]\ttrain's my_multiclass_logloss: 0.652506\tvalid's my_multiclass_logloss: 0.791959\n",
      "[299]\ttrain's my_multiclass_logloss: 0.652049\tvalid's my_multiclass_logloss: 0.791181\n",
      "[300]\ttrain's my_multiclass_logloss: 0.651485\tvalid's my_multiclass_logloss: 0.78837\n",
      "[301]\ttrain's my_multiclass_logloss: 0.651078\tvalid's my_multiclass_logloss: 0.787642\n",
      "[302]\ttrain's my_multiclass_logloss: 0.650519\tvalid's my_multiclass_logloss: 0.787569\n",
      "[303]\ttrain's my_multiclass_logloss: 0.650218\tvalid's my_multiclass_logloss: 0.787565\n",
      "[304]\ttrain's my_multiclass_logloss: 0.649844\tvalid's my_multiclass_logloss: 0.787939\n",
      "[305]\ttrain's my_multiclass_logloss: 0.649531\tvalid's my_multiclass_logloss: 0.788618\n",
      "[306]\ttrain's my_multiclass_logloss: 0.648459\tvalid's my_multiclass_logloss: 0.786847\n",
      "[307]\ttrain's my_multiclass_logloss: 0.647695\tvalid's my_multiclass_logloss: 0.785457\n",
      "[308]\ttrain's my_multiclass_logloss: 0.647086\tvalid's my_multiclass_logloss: 0.783002\n",
      "[309]\ttrain's my_multiclass_logloss: 0.646597\tvalid's my_multiclass_logloss: 0.780726\n",
      "[310]\ttrain's my_multiclass_logloss: 0.646204\tvalid's my_multiclass_logloss: 0.779177\n",
      "[311]\ttrain's my_multiclass_logloss: 0.645826\tvalid's my_multiclass_logloss: 0.777391\n",
      "[312]\ttrain's my_multiclass_logloss: 0.645384\tvalid's my_multiclass_logloss: 0.778991\n",
      "[313]\ttrain's my_multiclass_logloss: 0.645084\tvalid's my_multiclass_logloss: 0.780202\n",
      "[314]\ttrain's my_multiclass_logloss: 0.64515\tvalid's my_multiclass_logloss: 0.780749\n",
      "[315]\ttrain's my_multiclass_logloss: 0.645061\tvalid's my_multiclass_logloss: 0.781256\n",
      "[316]\ttrain's my_multiclass_logloss: 0.645066\tvalid's my_multiclass_logloss: 0.782287\n",
      "[317]\ttrain's my_multiclass_logloss: 0.645194\tvalid's my_multiclass_logloss: 0.783361\n",
      "[318]\ttrain's my_multiclass_logloss: 0.644661\tvalid's my_multiclass_logloss: 0.785128\n",
      "[319]\ttrain's my_multiclass_logloss: 0.644487\tvalid's my_multiclass_logloss: 0.787279\n",
      "[320]\ttrain's my_multiclass_logloss: 0.643946\tvalid's my_multiclass_logloss: 0.786662\n",
      "[321]\ttrain's my_multiclass_logloss: 0.643458\tvalid's my_multiclass_logloss: 0.784621\n",
      "[322]\ttrain's my_multiclass_logloss: 0.643173\tvalid's my_multiclass_logloss: 0.787864\n",
      "[323]\ttrain's my_multiclass_logloss: 0.64304\tvalid's my_multiclass_logloss: 0.788119\n",
      "[324]\ttrain's my_multiclass_logloss: 0.642702\tvalid's my_multiclass_logloss: 0.790093\n",
      "[325]\ttrain's my_multiclass_logloss: 0.642412\tvalid's my_multiclass_logloss: 0.79058\n",
      "[326]\ttrain's my_multiclass_logloss: 0.641711\tvalid's my_multiclass_logloss: 0.789753\n",
      "[327]\ttrain's my_multiclass_logloss: 0.641291\tvalid's my_multiclass_logloss: 0.788221\n",
      "[328]\ttrain's my_multiclass_logloss: 0.64117\tvalid's my_multiclass_logloss: 0.787285\n",
      "[329]\ttrain's my_multiclass_logloss: 0.641004\tvalid's my_multiclass_logloss: 0.786532\n",
      "[330]\ttrain's my_multiclass_logloss: 0.639938\tvalid's my_multiclass_logloss: 0.786037\n",
      "[331]\ttrain's my_multiclass_logloss: 0.638917\tvalid's my_multiclass_logloss: 0.78547\n",
      "[332]\ttrain's my_multiclass_logloss: 0.638132\tvalid's my_multiclass_logloss: 0.785025\n",
      "[333]\ttrain's my_multiclass_logloss: 0.637412\tvalid's my_multiclass_logloss: 0.784281\n",
      "[334]\ttrain's my_multiclass_logloss: 0.636896\tvalid's my_multiclass_logloss: 0.783337\n",
      "[335]\ttrain's my_multiclass_logloss: 0.636417\tvalid's my_multiclass_logloss: 0.782915\n",
      "[336]\ttrain's my_multiclass_logloss: 0.636135\tvalid's my_multiclass_logloss: 0.783934\n",
      "[337]\ttrain's my_multiclass_logloss: 0.635764\tvalid's my_multiclass_logloss: 0.783447\n",
      "[338]\ttrain's my_multiclass_logloss: 0.635297\tvalid's my_multiclass_logloss: 0.786127\n",
      "[339]\ttrain's my_multiclass_logloss: 0.634804\tvalid's my_multiclass_logloss: 0.787951\n",
      "[340]\ttrain's my_multiclass_logloss: 0.63429\tvalid's my_multiclass_logloss: 0.790999\n",
      "[341]\ttrain's my_multiclass_logloss: 0.634288\tvalid's my_multiclass_logloss: 0.795202\n",
      "[342]\ttrain's my_multiclass_logloss: 0.633655\tvalid's my_multiclass_logloss: 0.792273\n",
      "[343]\ttrain's my_multiclass_logloss: 0.633242\tvalid's my_multiclass_logloss: 0.789129\n",
      "[344]\ttrain's my_multiclass_logloss: 0.632722\tvalid's my_multiclass_logloss: 0.789546\n",
      "[345]\ttrain's my_multiclass_logloss: 0.632346\tvalid's my_multiclass_logloss: 0.788099\n",
      "[346]\ttrain's my_multiclass_logloss: 0.631799\tvalid's my_multiclass_logloss: 0.790063\n",
      "[347]\ttrain's my_multiclass_logloss: 0.63123\tvalid's my_multiclass_logloss: 0.792262\n",
      "[348]\ttrain's my_multiclass_logloss: 0.630818\tvalid's my_multiclass_logloss: 0.795223\n",
      "[349]\ttrain's my_multiclass_logloss: 0.630726\tvalid's my_multiclass_logloss: 0.797516\n",
      "[350]\ttrain's my_multiclass_logloss: 0.630811\tvalid's my_multiclass_logloss: 0.796522\n",
      "[351]\ttrain's my_multiclass_logloss: 0.630371\tvalid's my_multiclass_logloss: 0.793491\n",
      "[352]\ttrain's my_multiclass_logloss: 0.630075\tvalid's my_multiclass_logloss: 0.788771\n",
      "[353]\ttrain's my_multiclass_logloss: 0.63004\tvalid's my_multiclass_logloss: 0.789492\n",
      "[354]\ttrain's my_multiclass_logloss: 0.63006\tvalid's my_multiclass_logloss: 0.79166\n",
      "[355]\ttrain's my_multiclass_logloss: 0.630126\tvalid's my_multiclass_logloss: 0.793464\n",
      "[356]\ttrain's my_multiclass_logloss: 0.63012\tvalid's my_multiclass_logloss: 0.797445\n",
      "[357]\ttrain's my_multiclass_logloss: 0.630079\tvalid's my_multiclass_logloss: 0.798242\n",
      "[358]\ttrain's my_multiclass_logloss: 0.629708\tvalid's my_multiclass_logloss: 0.801084\n",
      "[359]\ttrain's my_multiclass_logloss: 0.629106\tvalid's my_multiclass_logloss: 0.803225\n",
      "[360]\ttrain's my_multiclass_logloss: 0.628747\tvalid's my_multiclass_logloss: 0.807162\n",
      "[361]\ttrain's my_multiclass_logloss: 0.628637\tvalid's my_multiclass_logloss: 0.81005\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttrain's my_multiclass_logloss: 0.673137\tvalid's my_multiclass_logloss: 0.752677\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.7272727272727273\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.111356\n",
      "1   feature2    0.125119\n",
      "2   feature3    0.054172\n",
      "3   feature4    0.079876\n",
      "4   feature5    0.130345\n",
      "5   feature6    0.084560\n",
      "6   feature7    0.054571\n",
      "7   feature8    0.111218\n",
      "8   feature9    0.119003\n",
      "9  feature10    0.129781\n",
      "     feature  importance\n",
      "0   feature1    0.113814\n",
      "1   feature2    0.116803\n",
      "2   feature3    0.043031\n",
      "3   feature4    0.055233\n",
      "4   feature5    0.126344\n",
      "5   feature6    0.087416\n",
      "6   feature7    0.078476\n",
      "7   feature8    0.120852\n",
      "8   feature9    0.122848\n",
      "9  feature10    0.135184\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.025552\n",
      "1   feature2   -0.086442\n",
      "2   feature3   -0.115809\n",
      "3   feature4   -0.256160\n",
      "4   feature5   -0.041588\n",
      "5   feature6    0.029684\n",
      "6   feature7    0.248496\n",
      "7   feature8    0.100141\n",
      "8   feature9    0.039968\n",
      "9  feature10    0.056160\n",
      "-------------------- 5 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05781\tvalid's multi_logloss: 1.06952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05414\tvalid's multi_logloss: 1.07073\n",
      "[3]\ttrain's multi_logloss: 1.04829\tvalid's multi_logloss: 1.06516\n",
      "[4]\ttrain's multi_logloss: 1.04509\tvalid's multi_logloss: 1.06672\n",
      "[5]\ttrain's multi_logloss: 1.03871\tvalid's multi_logloss: 1.06199\n",
      "[6]\ttrain's multi_logloss: 1.03493\tvalid's multi_logloss: 1.06139\n",
      "[7]\ttrain's multi_logloss: 1.02778\tvalid's multi_logloss: 1.05919\n",
      "[8]\ttrain's multi_logloss: 1.02001\tvalid's multi_logloss: 1.05325\n",
      "[9]\ttrain's multi_logloss: 1.01368\tvalid's multi_logloss: 1.03776\n",
      "[10]\ttrain's multi_logloss: 1.00881\tvalid's multi_logloss: 1.02496\n",
      "[11]\ttrain's multi_logloss: 1.00187\tvalid's multi_logloss: 1.01476\n",
      "[12]\ttrain's multi_logloss: 0.996346\tvalid's multi_logloss: 1.00921\n",
      "[13]\ttrain's multi_logloss: 0.990964\tvalid's multi_logloss: 1.00626\n",
      "[14]\ttrain's multi_logloss: 0.9841\tvalid's multi_logloss: 0.995941\n",
      "[15]\ttrain's multi_logloss: 0.978199\tvalid's multi_logloss: 0.987781\n",
      "[16]\ttrain's multi_logloss: 0.972189\tvalid's multi_logloss: 0.97854\n",
      "[17]\ttrain's multi_logloss: 0.96458\tvalid's multi_logloss: 0.967034\n",
      "[18]\ttrain's multi_logloss: 0.957567\tvalid's multi_logloss: 0.956358\n",
      "[19]\ttrain's multi_logloss: 0.951696\tvalid's multi_logloss: 0.94557\n",
      "[20]\ttrain's multi_logloss: 0.946822\tvalid's multi_logloss: 0.938777\n",
      "[21]\ttrain's multi_logloss: 0.944168\tvalid's multi_logloss: 0.933839\n",
      "[22]\ttrain's multi_logloss: 0.939202\tvalid's multi_logloss: 0.928152\n",
      "[23]\ttrain's multi_logloss: 0.936291\tvalid's multi_logloss: 0.924437\n",
      "[24]\ttrain's multi_logloss: 0.9313\tvalid's multi_logloss: 0.922289\n",
      "[25]\ttrain's multi_logloss: 0.927948\tvalid's multi_logloss: 0.915119\n",
      "[26]\ttrain's multi_logloss: 0.922436\tvalid's multi_logloss: 0.903683\n",
      "[27]\ttrain's multi_logloss: 0.918138\tvalid's multi_logloss: 0.895813\n",
      "[28]\ttrain's multi_logloss: 0.915211\tvalid's multi_logloss: 0.897779\n",
      "[29]\ttrain's multi_logloss: 0.912129\tvalid's multi_logloss: 0.894934\n",
      "[30]\ttrain's multi_logloss: 0.910782\tvalid's multi_logloss: 0.895871\n",
      "[31]\ttrain's multi_logloss: 0.905747\tvalid's multi_logloss: 0.894877\n",
      "[32]\ttrain's multi_logloss: 0.901078\tvalid's multi_logloss: 0.891159\n",
      "[33]\ttrain's multi_logloss: 0.896368\tvalid's multi_logloss: 0.883093\n",
      "[34]\ttrain's multi_logloss: 0.892249\tvalid's multi_logloss: 0.878918\n",
      "[35]\ttrain's multi_logloss: 0.889684\tvalid's multi_logloss: 0.872719\n",
      "[36]\ttrain's multi_logloss: 0.886044\tvalid's multi_logloss: 0.869145\n",
      "[37]\ttrain's multi_logloss: 0.880423\tvalid's multi_logloss: 0.864298\n",
      "[38]\ttrain's multi_logloss: 0.877322\tvalid's multi_logloss: 0.860009\n",
      "[39]\ttrain's multi_logloss: 0.872258\tvalid's multi_logloss: 0.85571\n",
      "[40]\ttrain's multi_logloss: 0.867748\tvalid's multi_logloss: 0.856026\n",
      "[41]\ttrain's multi_logloss: 0.864596\tvalid's multi_logloss: 0.850795\n",
      "[42]\ttrain's multi_logloss: 0.862901\tvalid's multi_logloss: 0.846268\n",
      "[43]\ttrain's multi_logloss: 0.860908\tvalid's multi_logloss: 0.843892\n",
      "[44]\ttrain's multi_logloss: 0.858433\tvalid's multi_logloss: 0.839406\n",
      "[45]\ttrain's multi_logloss: 0.855145\tvalid's multi_logloss: 0.834187\n",
      "[46]\ttrain's multi_logloss: 0.853359\tvalid's multi_logloss: 0.827895\n",
      "[47]\ttrain's multi_logloss: 0.850323\tvalid's multi_logloss: 0.822997\n",
      "[48]\ttrain's multi_logloss: 0.847501\tvalid's multi_logloss: 0.813792\n",
      "[49]\ttrain's multi_logloss: 0.843938\tvalid's multi_logloss: 0.810371\n",
      "[50]\ttrain's multi_logloss: 0.84053\tvalid's multi_logloss: 0.807399\n",
      "[51]\ttrain's multi_logloss: 0.838121\tvalid's multi_logloss: 0.806547\n",
      "[52]\ttrain's multi_logloss: 0.835899\tvalid's multi_logloss: 0.805829\n",
      "[53]\ttrain's multi_logloss: 0.832302\tvalid's multi_logloss: 0.794664\n",
      "[54]\ttrain's multi_logloss: 0.829007\tvalid's multi_logloss: 0.784044\n",
      "[55]\ttrain's multi_logloss: 0.825989\tvalid's multi_logloss: 0.773935\n",
      "[56]\ttrain's multi_logloss: 0.823939\tvalid's multi_logloss: 0.770056\n",
      "[57]\ttrain's multi_logloss: 0.82184\tvalid's multi_logloss: 0.768813\n",
      "[58]\ttrain's multi_logloss: 0.819619\tvalid's multi_logloss: 0.762334\n",
      "[59]\ttrain's multi_logloss: 0.816617\tvalid's multi_logloss: 0.758977\n",
      "[60]\ttrain's multi_logloss: 0.813549\tvalid's multi_logloss: 0.754676\n",
      "[61]\ttrain's multi_logloss: 0.810804\tvalid's multi_logloss: 0.754628\n",
      "[62]\ttrain's multi_logloss: 0.807603\tvalid's multi_logloss: 0.756121\n",
      "[63]\ttrain's multi_logloss: 0.804625\tvalid's multi_logloss: 0.757736\n",
      "[64]\ttrain's multi_logloss: 0.801857\tvalid's multi_logloss: 0.75946\n",
      "[65]\ttrain's multi_logloss: 0.799109\tvalid's multi_logloss: 0.755268\n",
      "[66]\ttrain's multi_logloss: 0.796469\tvalid's multi_logloss: 0.748098\n",
      "[67]\ttrain's multi_logloss: 0.79472\tvalid's multi_logloss: 0.742222\n",
      "[68]\ttrain's multi_logloss: 0.793139\tvalid's multi_logloss: 0.736656\n",
      "[69]\ttrain's multi_logloss: 0.791339\tvalid's multi_logloss: 0.731387\n",
      "[70]\ttrain's multi_logloss: 0.789246\tvalid's multi_logloss: 0.736114\n",
      "[71]\ttrain's multi_logloss: 0.787713\tvalid's multi_logloss: 0.735379\n",
      "[72]\ttrain's multi_logloss: 0.786063\tvalid's multi_logloss: 0.734874\n",
      "[73]\ttrain's multi_logloss: 0.783389\tvalid's multi_logloss: 0.726879\n",
      "[74]\ttrain's multi_logloss: 0.781667\tvalid's multi_logloss: 0.722545\n",
      "[75]\ttrain's multi_logloss: 0.780152\tvalid's multi_logloss: 0.714349\n",
      "[76]\ttrain's multi_logloss: 0.778332\tvalid's multi_logloss: 0.71015\n",
      "[77]\ttrain's multi_logloss: 0.776157\tvalid's multi_logloss: 0.707869\n",
      "[78]\ttrain's multi_logloss: 0.773998\tvalid's multi_logloss: 0.703051\n",
      "[79]\ttrain's multi_logloss: 0.772309\tvalid's multi_logloss: 0.705703\n",
      "[80]\ttrain's multi_logloss: 0.77063\tvalid's multi_logloss: 0.70198\n",
      "[81]\ttrain's multi_logloss: 0.768839\tvalid's multi_logloss: 0.695607\n",
      "[82]\ttrain's multi_logloss: 0.766931\tvalid's multi_logloss: 0.691269\n",
      "[83]\ttrain's multi_logloss: 0.764883\tvalid's multi_logloss: 0.686035\n",
      "[84]\ttrain's multi_logloss: 0.763496\tvalid's multi_logloss: 0.681111\n",
      "[85]\ttrain's multi_logloss: 0.761927\tvalid's multi_logloss: 0.677567\n",
      "[86]\ttrain's multi_logloss: 0.760498\tvalid's multi_logloss: 0.674232\n",
      "[87]\ttrain's multi_logloss: 0.759198\tvalid's multi_logloss: 0.671093\n",
      "[88]\ttrain's multi_logloss: 0.757992\tvalid's multi_logloss: 0.669946\n",
      "[89]\ttrain's multi_logloss: 0.75616\tvalid's multi_logloss: 0.671265\n",
      "[90]\ttrain's multi_logloss: 0.754702\tvalid's multi_logloss: 0.668914\n",
      "[91]\ttrain's multi_logloss: 0.753422\tvalid's multi_logloss: 0.672073\n",
      "[92]\ttrain's multi_logloss: 0.751981\tvalid's multi_logloss: 0.673535\n",
      "[93]\ttrain's multi_logloss: 0.749467\tvalid's multi_logloss: 0.669266\n",
      "[94]\ttrain's multi_logloss: 0.747223\tvalid's multi_logloss: 0.665308\n",
      "[95]\ttrain's multi_logloss: 0.745226\tvalid's multi_logloss: 0.66164\n",
      "[96]\ttrain's multi_logloss: 0.744433\tvalid's multi_logloss: 0.661774\n",
      "[97]\ttrain's multi_logloss: 0.742716\tvalid's multi_logloss: 0.665348\n",
      "[98]\ttrain's multi_logloss: 0.741227\tvalid's multi_logloss: 0.671407\n",
      "[99]\ttrain's multi_logloss: 0.739492\tvalid's multi_logloss: 0.67696\n",
      "[100]\ttrain's multi_logloss: 0.738408\tvalid's multi_logloss: 0.679738\n",
      "[101]\ttrain's multi_logloss: 0.736763\tvalid's multi_logloss: 0.680589\n",
      "[102]\ttrain's multi_logloss: 0.735048\tvalid's multi_logloss: 0.683968\n",
      "[103]\ttrain's multi_logloss: 0.734203\tvalid's multi_logloss: 0.682404\n",
      "[104]\ttrain's multi_logloss: 0.7333\tvalid's multi_logloss: 0.679694\n",
      "[105]\ttrain's multi_logloss: 0.731522\tvalid's multi_logloss: 0.67953\n",
      "[106]\ttrain's multi_logloss: 0.729939\tvalid's multi_logloss: 0.679606\n",
      "[107]\ttrain's multi_logloss: 0.728537\tvalid's multi_logloss: 0.679902\n",
      "[108]\ttrain's multi_logloss: 0.727435\tvalid's multi_logloss: 0.678156\n",
      "[109]\ttrain's multi_logloss: 0.726137\tvalid's multi_logloss: 0.677778\n",
      "[110]\ttrain's multi_logloss: 0.725053\tvalid's multi_logloss: 0.679555\n",
      "[111]\ttrain's multi_logloss: 0.723917\tvalid's multi_logloss: 0.679288\n",
      "[112]\ttrain's multi_logloss: 0.723376\tvalid's multi_logloss: 0.680547\n",
      "[113]\ttrain's multi_logloss: 0.721713\tvalid's multi_logloss: 0.676965\n",
      "[114]\ttrain's multi_logloss: 0.720237\tvalid's multi_logloss: 0.673678\n",
      "[115]\ttrain's multi_logloss: 0.71838\tvalid's multi_logloss: 0.670678\n",
      "[116]\ttrain's multi_logloss: 0.716738\tvalid's multi_logloss: 0.666483\n",
      "[117]\ttrain's multi_logloss: 0.715651\tvalid's multi_logloss: 0.670099\n",
      "[118]\ttrain's multi_logloss: 0.714355\tvalid's multi_logloss: 0.672897\n",
      "[119]\ttrain's multi_logloss: 0.713311\tvalid's multi_logloss: 0.677551\n",
      "[120]\ttrain's multi_logloss: 0.712435\tvalid's multi_logloss: 0.677968\n",
      "[121]\ttrain's multi_logloss: 0.710858\tvalid's multi_logloss: 0.678477\n",
      "[122]\ttrain's multi_logloss: 0.709212\tvalid's multi_logloss: 0.676566\n",
      "[123]\ttrain's multi_logloss: 0.707898\tvalid's multi_logloss: 0.678125\n",
      "[124]\ttrain's multi_logloss: 0.706573\tvalid's multi_logloss: 0.67878\n",
      "[125]\ttrain's multi_logloss: 0.705146\tvalid's multi_logloss: 0.676327\n",
      "[126]\ttrain's multi_logloss: 0.703764\tvalid's multi_logloss: 0.675955\n",
      "[127]\ttrain's multi_logloss: 0.703161\tvalid's multi_logloss: 0.677413\n",
      "[128]\ttrain's multi_logloss: 0.702312\tvalid's multi_logloss: 0.678477\n",
      "[129]\ttrain's multi_logloss: 0.7016\tvalid's multi_logloss: 0.676356\n",
      "[130]\ttrain's multi_logloss: 0.700599\tvalid's multi_logloss: 0.673484\n",
      "[131]\ttrain's multi_logloss: 0.699845\tvalid's multi_logloss: 0.671244\n",
      "[132]\ttrain's multi_logloss: 0.698808\tvalid's multi_logloss: 0.66827\n",
      "[133]\ttrain's multi_logloss: 0.697683\tvalid's multi_logloss: 0.672436\n",
      "[134]\ttrain's multi_logloss: 0.696663\tvalid's multi_logloss: 0.674771\n",
      "[135]\ttrain's multi_logloss: 0.69607\tvalid's multi_logloss: 0.674916\n",
      "[136]\ttrain's multi_logloss: 0.694638\tvalid's multi_logloss: 0.677067\n",
      "[137]\ttrain's multi_logloss: 0.693663\tvalid's multi_logloss: 0.674661\n",
      "[138]\ttrain's multi_logloss: 0.692793\tvalid's multi_logloss: 0.672936\n",
      "[139]\ttrain's multi_logloss: 0.692137\tvalid's multi_logloss: 0.671331\n",
      "[140]\ttrain's multi_logloss: 0.691798\tvalid's multi_logloss: 0.67202\n",
      "[141]\ttrain's multi_logloss: 0.690267\tvalid's multi_logloss: 0.672549\n",
      "[142]\ttrain's multi_logloss: 0.688831\tvalid's multi_logloss: 0.674124\n",
      "[143]\ttrain's multi_logloss: 0.687861\tvalid's multi_logloss: 0.675945\n",
      "[144]\ttrain's multi_logloss: 0.686649\tvalid's multi_logloss: 0.674813\n",
      "[145]\ttrain's multi_logloss: 0.68537\tvalid's multi_logloss: 0.675009\n",
      "[146]\ttrain's multi_logloss: 0.684234\tvalid's multi_logloss: 0.675342\n",
      "[147]\ttrain's multi_logloss: 0.683296\tvalid's multi_logloss: 0.677541\n",
      "[148]\ttrain's multi_logloss: 0.682538\tvalid's multi_logloss: 0.678628\n",
      "[149]\ttrain's multi_logloss: 0.680496\tvalid's multi_logloss: 0.679095\n",
      "[150]\ttrain's multi_logloss: 0.679117\tvalid's multi_logloss: 0.676329\n",
      "[151]\ttrain's multi_logloss: 0.677832\tvalid's multi_logloss: 0.678292\n",
      "[152]\ttrain's multi_logloss: 0.676451\tvalid's multi_logloss: 0.678887\n",
      "[153]\ttrain's multi_logloss: 0.675441\tvalid's multi_logloss: 0.678692\n",
      "[154]\ttrain's multi_logloss: 0.674565\tvalid's multi_logloss: 0.674981\n",
      "[155]\ttrain's multi_logloss: 0.673603\tvalid's multi_logloss: 0.67965\n",
      "[156]\ttrain's multi_logloss: 0.672763\tvalid's multi_logloss: 0.683201\n",
      "[157]\ttrain's multi_logloss: 0.671299\tvalid's multi_logloss: 0.675665\n",
      "[158]\ttrain's multi_logloss: 0.669629\tvalid's multi_logloss: 0.66905\n",
      "[159]\ttrain's multi_logloss: 0.668889\tvalid's multi_logloss: 0.66701\n",
      "[160]\ttrain's multi_logloss: 0.668028\tvalid's multi_logloss: 0.663185\n",
      "[161]\ttrain's multi_logloss: 0.665365\tvalid's multi_logloss: 0.661654\n",
      "[162]\ttrain's multi_logloss: 0.663626\tvalid's multi_logloss: 0.663194\n",
      "[163]\ttrain's multi_logloss: 0.662095\tvalid's multi_logloss: 0.659225\n",
      "[164]\ttrain's multi_logloss: 0.660604\tvalid's multi_logloss: 0.657784\n",
      "[165]\ttrain's multi_logloss: 0.660077\tvalid's multi_logloss: 0.661172\n",
      "[166]\ttrain's multi_logloss: 0.658999\tvalid's multi_logloss: 0.666104\n",
      "[167]\ttrain's multi_logloss: 0.65828\tvalid's multi_logloss: 0.668237\n",
      "[168]\ttrain's multi_logloss: 0.657081\tvalid's multi_logloss: 0.666568\n",
      "[169]\ttrain's multi_logloss: 0.656314\tvalid's multi_logloss: 0.664085\n",
      "[170]\ttrain's multi_logloss: 0.655475\tvalid's multi_logloss: 0.662118\n",
      "[171]\ttrain's multi_logloss: 0.65464\tvalid's multi_logloss: 0.659366\n",
      "[172]\ttrain's multi_logloss: 0.654044\tvalid's multi_logloss: 0.657725\n",
      "[173]\ttrain's multi_logloss: 0.652494\tvalid's multi_logloss: 0.654198\n",
      "[174]\ttrain's multi_logloss: 0.65179\tvalid's multi_logloss: 0.654421\n",
      "[175]\ttrain's multi_logloss: 0.650474\tvalid's multi_logloss: 0.650251\n",
      "[176]\ttrain's multi_logloss: 0.649289\tvalid's multi_logloss: 0.647312\n",
      "[177]\ttrain's multi_logloss: 0.648122\tvalid's multi_logloss: 0.64257\n",
      "[178]\ttrain's multi_logloss: 0.647031\tvalid's multi_logloss: 0.636807\n",
      "[179]\ttrain's multi_logloss: 0.646343\tvalid's multi_logloss: 0.629168\n",
      "[180]\ttrain's multi_logloss: 0.645529\tvalid's multi_logloss: 0.624318\n",
      "[181]\ttrain's multi_logloss: 0.644401\tvalid's multi_logloss: 0.620385\n",
      "[182]\ttrain's multi_logloss: 0.643229\tvalid's multi_logloss: 0.615119\n",
      "[183]\ttrain's multi_logloss: 0.64205\tvalid's multi_logloss: 0.610791\n",
      "[184]\ttrain's multi_logloss: 0.64104\tvalid's multi_logloss: 0.60586\n",
      "[185]\ttrain's multi_logloss: 0.639634\tvalid's multi_logloss: 0.604895\n",
      "[186]\ttrain's multi_logloss: 0.63885\tvalid's multi_logloss: 0.603244\n",
      "[187]\ttrain's multi_logloss: 0.637999\tvalid's multi_logloss: 0.603939\n",
      "[188]\ttrain's multi_logloss: 0.63724\tvalid's multi_logloss: 0.597583\n",
      "[189]\ttrain's multi_logloss: 0.636806\tvalid's multi_logloss: 0.60296\n",
      "[190]\ttrain's multi_logloss: 0.635908\tvalid's multi_logloss: 0.605423\n",
      "[191]\ttrain's multi_logloss: 0.635339\tvalid's multi_logloss: 0.61016\n",
      "[192]\ttrain's multi_logloss: 0.634323\tvalid's multi_logloss: 0.607208\n",
      "[193]\ttrain's multi_logloss: 0.633439\tvalid's multi_logloss: 0.602508\n",
      "[194]\ttrain's multi_logloss: 0.632679\tvalid's multi_logloss: 0.59803\n",
      "[195]\ttrain's multi_logloss: 0.632004\tvalid's multi_logloss: 0.59137\n",
      "[196]\ttrain's multi_logloss: 0.630965\tvalid's multi_logloss: 0.589479\n",
      "[197]\ttrain's multi_logloss: 0.630438\tvalid's multi_logloss: 0.592048\n",
      "[198]\ttrain's multi_logloss: 0.629888\tvalid's multi_logloss: 0.595734\n",
      "[199]\ttrain's multi_logloss: 0.629497\tvalid's multi_logloss: 0.597953\n",
      "[200]\ttrain's multi_logloss: 0.629137\tvalid's multi_logloss: 0.598998\n",
      "[201]\ttrain's multi_logloss: 0.629023\tvalid's multi_logloss: 0.60301\n",
      "[202]\ttrain's multi_logloss: 0.629017\tvalid's multi_logloss: 0.605869\n",
      "[203]\ttrain's multi_logloss: 0.629083\tvalid's multi_logloss: 0.60986\n",
      "[204]\ttrain's multi_logloss: 0.629084\tvalid's multi_logloss: 0.609393\n",
      "[205]\ttrain's multi_logloss: 0.628732\tvalid's multi_logloss: 0.609194\n",
      "[206]\ttrain's multi_logloss: 0.628408\tvalid's multi_logloss: 0.608435\n",
      "[207]\ttrain's multi_logloss: 0.628279\tvalid's multi_logloss: 0.608286\n",
      "[208]\ttrain's multi_logloss: 0.627938\tvalid's multi_logloss: 0.606679\n",
      "[209]\ttrain's multi_logloss: 0.627372\tvalid's multi_logloss: 0.605357\n",
      "[210]\ttrain's multi_logloss: 0.626655\tvalid's multi_logloss: 0.60052\n",
      "[211]\ttrain's multi_logloss: 0.626017\tvalid's multi_logloss: 0.59848\n",
      "[212]\ttrain's multi_logloss: 0.625509\tvalid's multi_logloss: 0.594045\n",
      "[213]\ttrain's multi_logloss: 0.624355\tvalid's multi_logloss: 0.594542\n",
      "[214]\ttrain's multi_logloss: 0.622963\tvalid's multi_logloss: 0.59132\n",
      "[215]\ttrain's multi_logloss: 0.622081\tvalid's multi_logloss: 0.59142\n",
      "[216]\ttrain's multi_logloss: 0.620949\tvalid's multi_logloss: 0.588521\n",
      "[217]\ttrain's multi_logloss: 0.619945\tvalid's multi_logloss: 0.582766\n",
      "[218]\ttrain's multi_logloss: 0.619464\tvalid's multi_logloss: 0.578339\n",
      "[219]\ttrain's multi_logloss: 0.61882\tvalid's multi_logloss: 0.573311\n",
      "[220]\ttrain's multi_logloss: 0.618398\tvalid's multi_logloss: 0.567676\n",
      "[221]\ttrain's multi_logloss: 0.617577\tvalid's multi_logloss: 0.568016\n",
      "[222]\ttrain's multi_logloss: 0.616267\tvalid's multi_logloss: 0.568828\n",
      "[223]\ttrain's multi_logloss: 0.615712\tvalid's multi_logloss: 0.570022\n",
      "[224]\ttrain's multi_logloss: 0.615205\tvalid's multi_logloss: 0.572118\n",
      "[225]\ttrain's multi_logloss: 0.614616\tvalid's multi_logloss: 0.573123\n",
      "[226]\ttrain's multi_logloss: 0.614102\tvalid's multi_logloss: 0.573171\n",
      "[227]\ttrain's multi_logloss: 0.613337\tvalid's multi_logloss: 0.57478\n",
      "[228]\ttrain's multi_logloss: 0.612599\tvalid's multi_logloss: 0.575383\n",
      "[229]\ttrain's multi_logloss: 0.611704\tvalid's multi_logloss: 0.572248\n",
      "[230]\ttrain's multi_logloss: 0.611038\tvalid's multi_logloss: 0.57101\n",
      "[231]\ttrain's multi_logloss: 0.610304\tvalid's multi_logloss: 0.569842\n",
      "[232]\ttrain's multi_logloss: 0.609456\tvalid's multi_logloss: 0.568182\n",
      "[233]\ttrain's multi_logloss: 0.60914\tvalid's multi_logloss: 0.571124\n",
      "[234]\ttrain's multi_logloss: 0.608919\tvalid's multi_logloss: 0.574141\n",
      "[235]\ttrain's multi_logloss: 0.608376\tvalid's multi_logloss: 0.573354\n",
      "[236]\ttrain's multi_logloss: 0.608251\tvalid's multi_logloss: 0.57498\n",
      "[237]\ttrain's multi_logloss: 0.608251\tvalid's multi_logloss: 0.57498\n",
      "[238]\ttrain's multi_logloss: 0.607934\tvalid's multi_logloss: 0.574194\n",
      "[239]\ttrain's multi_logloss: 0.607517\tvalid's multi_logloss: 0.572277\n",
      "[240]\ttrain's multi_logloss: 0.607297\tvalid's multi_logloss: 0.570437\n",
      "[241]\ttrain's multi_logloss: 0.606967\tvalid's multi_logloss: 0.569174\n",
      "[242]\ttrain's multi_logloss: 0.605376\tvalid's multi_logloss: 0.564286\n",
      "[243]\ttrain's multi_logloss: 0.604313\tvalid's multi_logloss: 0.560492\n",
      "[244]\ttrain's multi_logloss: 0.603826\tvalid's multi_logloss: 0.560491\n",
      "[245]\ttrain's multi_logloss: 0.602851\tvalid's multi_logloss: 0.556852\n",
      "[246]\ttrain's multi_logloss: 0.60199\tvalid's multi_logloss: 0.561578\n",
      "[247]\ttrain's multi_logloss: 0.600864\tvalid's multi_logloss: 0.565938\n",
      "[248]\ttrain's multi_logloss: 0.599941\tvalid's multi_logloss: 0.569048\n",
      "[249]\ttrain's multi_logloss: 0.599068\tvalid's multi_logloss: 0.573439\n",
      "[250]\ttrain's multi_logloss: 0.598334\tvalid's multi_logloss: 0.567563\n",
      "[251]\ttrain's multi_logloss: 0.59781\tvalid's multi_logloss: 0.565397\n",
      "[252]\ttrain's multi_logloss: 0.597412\tvalid's multi_logloss: 0.564451\n",
      "[253]\ttrain's multi_logloss: 0.596906\tvalid's multi_logloss: 0.565127\n",
      "[254]\ttrain's multi_logloss: 0.59635\tvalid's multi_logloss: 0.563617\n",
      "[255]\ttrain's multi_logloss: 0.595649\tvalid's multi_logloss: 0.56258\n",
      "[256]\ttrain's multi_logloss: 0.594845\tvalid's multi_logloss: 0.564608\n",
      "[257]\ttrain's multi_logloss: 0.594212\tvalid's multi_logloss: 0.565986\n",
      "[258]\ttrain's multi_logloss: 0.594227\tvalid's multi_logloss: 0.564451\n",
      "[259]\ttrain's multi_logloss: 0.594035\tvalid's multi_logloss: 0.564761\n",
      "[260]\ttrain's multi_logloss: 0.594206\tvalid's multi_logloss: 0.563465\n",
      "[261]\ttrain's multi_logloss: 0.593791\tvalid's multi_logloss: 0.559417\n",
      "[262]\ttrain's multi_logloss: 0.592838\tvalid's multi_logloss: 0.559179\n",
      "[263]\ttrain's multi_logloss: 0.59168\tvalid's multi_logloss: 0.556693\n",
      "[264]\ttrain's multi_logloss: 0.590973\tvalid's multi_logloss: 0.55311\n",
      "[265]\ttrain's multi_logloss: 0.590093\tvalid's multi_logloss: 0.552134\n",
      "[266]\ttrain's multi_logloss: 0.589973\tvalid's multi_logloss: 0.554976\n",
      "[267]\ttrain's multi_logloss: 0.589748\tvalid's multi_logloss: 0.556281\n",
      "[268]\ttrain's multi_logloss: 0.589568\tvalid's multi_logloss: 0.558021\n",
      "[269]\ttrain's multi_logloss: 0.589545\tvalid's multi_logloss: 0.559357\n",
      "[270]\ttrain's multi_logloss: 0.588744\tvalid's multi_logloss: 0.556911\n",
      "[271]\ttrain's multi_logloss: 0.587957\tvalid's multi_logloss: 0.553808\n",
      "[272]\ttrain's multi_logloss: 0.587236\tvalid's multi_logloss: 0.550845\n",
      "[273]\ttrain's multi_logloss: 0.586797\tvalid's multi_logloss: 0.549313\n",
      "[274]\ttrain's multi_logloss: 0.585988\tvalid's multi_logloss: 0.542939\n",
      "[275]\ttrain's multi_logloss: 0.585529\tvalid's multi_logloss: 0.53952\n",
      "[276]\ttrain's multi_logloss: 0.584846\tvalid's multi_logloss: 0.535694\n",
      "[277]\ttrain's multi_logloss: 0.584264\tvalid's multi_logloss: 0.53202\n",
      "[278]\ttrain's multi_logloss: 0.583178\tvalid's multi_logloss: 0.531493\n",
      "[279]\ttrain's multi_logloss: 0.582273\tvalid's multi_logloss: 0.532344\n",
      "[280]\ttrain's multi_logloss: 0.58095\tvalid's multi_logloss: 0.535432\n",
      "[281]\ttrain's multi_logloss: 0.579882\tvalid's multi_logloss: 0.536482\n",
      "[282]\ttrain's multi_logloss: 0.579511\tvalid's multi_logloss: 0.541536\n",
      "[283]\ttrain's multi_logloss: 0.579468\tvalid's multi_logloss: 0.54525\n",
      "[284]\ttrain's multi_logloss: 0.579106\tvalid's multi_logloss: 0.548188\n",
      "[285]\ttrain's multi_logloss: 0.578895\tvalid's multi_logloss: 0.55276\n",
      "[286]\ttrain's multi_logloss: 0.578521\tvalid's multi_logloss: 0.553513\n",
      "[287]\ttrain's multi_logloss: 0.577799\tvalid's multi_logloss: 0.557556\n",
      "[288]\ttrain's multi_logloss: 0.57729\tvalid's multi_logloss: 0.559622\n",
      "[289]\ttrain's multi_logloss: 0.577135\tvalid's multi_logloss: 0.561368\n",
      "[290]\ttrain's multi_logloss: 0.576407\tvalid's multi_logloss: 0.560042\n",
      "[291]\ttrain's multi_logloss: 0.575654\tvalid's multi_logloss: 0.554933\n",
      "[292]\ttrain's multi_logloss: 0.575123\tvalid's multi_logloss: 0.552581\n",
      "[293]\ttrain's multi_logloss: 0.574687\tvalid's multi_logloss: 0.54891\n",
      "[294]\ttrain's multi_logloss: 0.574061\tvalid's multi_logloss: 0.550408\n",
      "[295]\ttrain's multi_logloss: 0.573517\tvalid's multi_logloss: 0.551934\n",
      "[296]\ttrain's multi_logloss: 0.573048\tvalid's multi_logloss: 0.553484\n",
      "[297]\ttrain's multi_logloss: 0.572649\tvalid's multi_logloss: 0.555054\n",
      "[298]\ttrain's multi_logloss: 0.572055\tvalid's multi_logloss: 0.556123\n",
      "[299]\ttrain's multi_logloss: 0.571534\tvalid's multi_logloss: 0.55699\n",
      "[300]\ttrain's multi_logloss: 0.571062\tvalid's multi_logloss: 0.554285\n",
      "[301]\ttrain's multi_logloss: 0.570556\tvalid's multi_logloss: 0.555378\n",
      "[302]\ttrain's multi_logloss: 0.570224\tvalid's multi_logloss: 0.55829\n",
      "[303]\ttrain's multi_logloss: 0.569994\tvalid's multi_logloss: 0.561199\n",
      "[304]\ttrain's multi_logloss: 0.569478\tvalid's multi_logloss: 0.561862\n",
      "[305]\ttrain's multi_logloss: 0.569649\tvalid's multi_logloss: 0.565526\n",
      "[306]\ttrain's multi_logloss: 0.568989\tvalid's multi_logloss: 0.566489\n",
      "[307]\ttrain's multi_logloss: 0.56844\tvalid's multi_logloss: 0.567482\n",
      "[308]\ttrain's multi_logloss: 0.568022\tvalid's multi_logloss: 0.568557\n",
      "[309]\ttrain's multi_logloss: 0.567462\tvalid's multi_logloss: 0.568976\n",
      "[310]\ttrain's multi_logloss: 0.567513\tvalid's multi_logloss: 0.570444\n",
      "[311]\ttrain's multi_logloss: 0.567216\tvalid's multi_logloss: 0.569923\n",
      "[312]\ttrain's multi_logloss: 0.566969\tvalid's multi_logloss: 0.569423\n",
      "[313]\ttrain's multi_logloss: 0.566619\tvalid's multi_logloss: 0.568956\n",
      "[314]\ttrain's multi_logloss: 0.566379\tvalid's multi_logloss: 0.564761\n",
      "[315]\ttrain's multi_logloss: 0.566218\tvalid's multi_logloss: 0.562067\n",
      "[316]\ttrain's multi_logloss: 0.566169\tvalid's multi_logloss: 0.561256\n",
      "[317]\ttrain's multi_logloss: 0.566363\tvalid's multi_logloss: 0.557483\n",
      "[318]\ttrain's multi_logloss: 0.565242\tvalid's multi_logloss: 0.555923\n",
      "[319]\ttrain's multi_logloss: 0.564229\tvalid's multi_logloss: 0.554356\n",
      "[320]\ttrain's multi_logloss: 0.563231\tvalid's multi_logloss: 0.552948\n",
      "[321]\ttrain's multi_logloss: 0.562207\tvalid's multi_logloss: 0.550551\n",
      "[322]\ttrain's multi_logloss: 0.561978\tvalid's multi_logloss: 0.552269\n",
      "[323]\ttrain's multi_logloss: 0.561884\tvalid's multi_logloss: 0.553254\n",
      "[324]\ttrain's multi_logloss: 0.561609\tvalid's multi_logloss: 0.553188\n",
      "[325]\ttrain's multi_logloss: 0.561387\tvalid's multi_logloss: 0.553169\n",
      "[326]\ttrain's multi_logloss: 0.560657\tvalid's multi_logloss: 0.550001\n",
      "[327]\ttrain's multi_logloss: 0.560026\tvalid's multi_logloss: 0.547034\n",
      "[328]\ttrain's multi_logloss: 0.559446\tvalid's multi_logloss: 0.546241\n",
      "[329]\ttrain's multi_logloss: 0.558652\tvalid's multi_logloss: 0.542211\n",
      "[330]\ttrain's multi_logloss: 0.557615\tvalid's multi_logloss: 0.539594\n",
      "[331]\ttrain's multi_logloss: 0.556468\tvalid's multi_logloss: 0.537099\n",
      "[332]\ttrain's multi_logloss: 0.555596\tvalid's multi_logloss: 0.535675\n",
      "[333]\ttrain's multi_logloss: 0.554772\tvalid's multi_logloss: 0.532081\n",
      "[334]\ttrain's multi_logloss: 0.553765\tvalid's multi_logloss: 0.531139\n",
      "[335]\ttrain's multi_logloss: 0.553616\tvalid's multi_logloss: 0.52702\n",
      "[336]\ttrain's multi_logloss: 0.552968\tvalid's multi_logloss: 0.523573\n",
      "[337]\ttrain's multi_logloss: 0.552432\tvalid's multi_logloss: 0.520317\n",
      "[338]\ttrain's multi_logloss: 0.552198\tvalid's multi_logloss: 0.523345\n",
      "[339]\ttrain's multi_logloss: 0.552018\tvalid's multi_logloss: 0.526606\n",
      "[340]\ttrain's multi_logloss: 0.551906\tvalid's multi_logloss: 0.529165\n",
      "[341]\ttrain's multi_logloss: 0.551876\tvalid's multi_logloss: 0.531728\n",
      "[342]\ttrain's multi_logloss: 0.550593\tvalid's multi_logloss: 0.532572\n",
      "[343]\ttrain's multi_logloss: 0.549462\tvalid's multi_logloss: 0.533474\n",
      "[344]\ttrain's multi_logloss: 0.548542\tvalid's multi_logloss: 0.530724\n",
      "[345]\ttrain's multi_logloss: 0.54786\tvalid's multi_logloss: 0.531166\n",
      "[346]\ttrain's multi_logloss: 0.547776\tvalid's multi_logloss: 0.529727\n",
      "[347]\ttrain's multi_logloss: 0.547395\tvalid's multi_logloss: 0.526223\n",
      "[348]\ttrain's multi_logloss: 0.547235\tvalid's multi_logloss: 0.525466\n",
      "[349]\ttrain's multi_logloss: 0.547055\tvalid's multi_logloss: 0.524561\n",
      "[350]\ttrain's multi_logloss: 0.546595\tvalid's multi_logloss: 0.524755\n",
      "[351]\ttrain's multi_logloss: 0.546194\tvalid's multi_logloss: 0.527818\n",
      "[352]\ttrain's multi_logloss: 0.545866\tvalid's multi_logloss: 0.524495\n",
      "[353]\ttrain's multi_logloss: 0.545663\tvalid's multi_logloss: 0.521422\n",
      "[354]\ttrain's multi_logloss: 0.545506\tvalid's multi_logloss: 0.528632\n",
      "[355]\ttrain's multi_logloss: 0.545427\tvalid's multi_logloss: 0.53568\n",
      "[356]\ttrain's multi_logloss: 0.545427\tvalid's multi_logloss: 0.539322\n",
      "[357]\ttrain's multi_logloss: 0.545227\tvalid's multi_logloss: 0.541576\n",
      "[358]\ttrain's multi_logloss: 0.544952\tvalid's multi_logloss: 0.542856\n",
      "[359]\ttrain's multi_logloss: 0.54494\tvalid's multi_logloss: 0.544239\n",
      "[360]\ttrain's multi_logloss: 0.544613\tvalid's multi_logloss: 0.545406\n",
      "[361]\ttrain's multi_logloss: 0.544345\tvalid's multi_logloss: 0.546611\n",
      "[362]\ttrain's multi_logloss: 0.543759\tvalid's multi_logloss: 0.544953\n",
      "[363]\ttrain's multi_logloss: 0.543188\tvalid's multi_logloss: 0.541639\n",
      "[364]\ttrain's multi_logloss: 0.542692\tvalid's multi_logloss: 0.538466\n",
      "[365]\ttrain's multi_logloss: 0.542265\tvalid's multi_logloss: 0.535428\n",
      "[366]\ttrain's multi_logloss: 0.542127\tvalid's multi_logloss: 0.535729\n",
      "[367]\ttrain's multi_logloss: 0.541946\tvalid's multi_logloss: 0.535797\n",
      "[368]\ttrain's multi_logloss: 0.541884\tvalid's multi_logloss: 0.533339\n",
      "[369]\ttrain's multi_logloss: 0.541772\tvalid's multi_logloss: 0.531896\n",
      "[370]\ttrain's multi_logloss: 0.541057\tvalid's multi_logloss: 0.528622\n",
      "[371]\ttrain's multi_logloss: 0.540441\tvalid's multi_logloss: 0.527339\n",
      "[372]\ttrain's multi_logloss: 0.53966\tvalid's multi_logloss: 0.526196\n",
      "[373]\ttrain's multi_logloss: 0.538975\tvalid's multi_logloss: 0.525148\n",
      "[374]\ttrain's multi_logloss: 0.538479\tvalid's multi_logloss: 0.524622\n",
      "[375]\ttrain's multi_logloss: 0.53825\tvalid's multi_logloss: 0.523986\n",
      "[376]\ttrain's multi_logloss: 0.537852\tvalid's multi_logloss: 0.523643\n",
      "[377]\ttrain's multi_logloss: 0.537771\tvalid's multi_logloss: 0.523171\n",
      "[378]\ttrain's multi_logloss: 0.536821\tvalid's multi_logloss: 0.518986\n",
      "[379]\ttrain's multi_logloss: 0.53628\tvalid's multi_logloss: 0.512501\n",
      "[380]\ttrain's multi_logloss: 0.535918\tvalid's multi_logloss: 0.511749\n",
      "[381]\ttrain's multi_logloss: 0.535471\tvalid's multi_logloss: 0.509908\n",
      "[382]\ttrain's multi_logloss: 0.53467\tvalid's multi_logloss: 0.511173\n",
      "[383]\ttrain's multi_logloss: 0.533973\tvalid's multi_logloss: 0.5125\n",
      "[384]\ttrain's multi_logloss: 0.533413\tvalid's multi_logloss: 0.514885\n",
      "[385]\ttrain's multi_logloss: 0.533098\tvalid's multi_logloss: 0.516367\n",
      "[386]\ttrain's multi_logloss: 0.53295\tvalid's multi_logloss: 0.518026\n",
      "[387]\ttrain's multi_logloss: 0.533011\tvalid's multi_logloss: 0.51914\n",
      "[388]\ttrain's multi_logloss: 0.533191\tvalid's multi_logloss: 0.521303\n",
      "[389]\ttrain's multi_logloss: 0.533355\tvalid's multi_logloss: 0.522492\n",
      "[390]\ttrain's multi_logloss: 0.532741\tvalid's multi_logloss: 0.520541\n",
      "[391]\ttrain's multi_logloss: 0.532386\tvalid's multi_logloss: 0.517012\n",
      "[392]\ttrain's multi_logloss: 0.531911\tvalid's multi_logloss: 0.512506\n",
      "[393]\ttrain's multi_logloss: 0.531505\tvalid's multi_logloss: 0.508213\n",
      "[394]\ttrain's multi_logloss: 0.531042\tvalid's multi_logloss: 0.508913\n",
      "[395]\ttrain's multi_logloss: 0.530632\tvalid's multi_logloss: 0.510783\n",
      "[396]\ttrain's multi_logloss: 0.530312\tvalid's multi_logloss: 0.511268\n",
      "[397]\ttrain's multi_logloss: 0.530115\tvalid's multi_logloss: 0.514221\n",
      "[398]\ttrain's multi_logloss: 0.5298\tvalid's multi_logloss: 0.507654\n",
      "[399]\ttrain's multi_logloss: 0.529737\tvalid's multi_logloss: 0.501729\n",
      "[400]\ttrain's multi_logloss: 0.529739\tvalid's multi_logloss: 0.495954\n",
      "[401]\ttrain's multi_logloss: 0.529796\tvalid's multi_logloss: 0.497972\n",
      "[402]\ttrain's multi_logloss: 0.529615\tvalid's multi_logloss: 0.498406\n",
      "[403]\ttrain's multi_logloss: 0.52977\tvalid's multi_logloss: 0.494867\n",
      "[404]\ttrain's multi_logloss: 0.529528\tvalid's multi_logloss: 0.490329\n",
      "[405]\ttrain's multi_logloss: 0.529817\tvalid's multi_logloss: 0.487131\n",
      "[406]\ttrain's multi_logloss: 0.529449\tvalid's multi_logloss: 0.483119\n",
      "[407]\ttrain's multi_logloss: 0.52907\tvalid's multi_logloss: 0.480253\n",
      "[408]\ttrain's multi_logloss: 0.528562\tvalid's multi_logloss: 0.477385\n",
      "[409]\ttrain's multi_logloss: 0.528269\tvalid's multi_logloss: 0.474685\n",
      "[410]\ttrain's multi_logloss: 0.527551\tvalid's multi_logloss: 0.474584\n",
      "[411]\ttrain's multi_logloss: 0.52652\tvalid's multi_logloss: 0.474283\n",
      "[412]\ttrain's multi_logloss: 0.525674\tvalid's multi_logloss: 0.473524\n",
      "[413]\ttrain's multi_logloss: 0.525463\tvalid's multi_logloss: 0.474872\n",
      "[414]\ttrain's multi_logloss: 0.524873\tvalid's multi_logloss: 0.4813\n",
      "[415]\ttrain's multi_logloss: 0.524426\tvalid's multi_logloss: 0.486886\n",
      "[416]\ttrain's multi_logloss: 0.524219\tvalid's multi_logloss: 0.489598\n",
      "[417]\ttrain's multi_logloss: 0.523993\tvalid's multi_logloss: 0.49495\n",
      "[418]\ttrain's multi_logloss: 0.523462\tvalid's multi_logloss: 0.499975\n",
      "[419]\ttrain's multi_logloss: 0.523159\tvalid's multi_logloss: 0.503772\n",
      "[420]\ttrain's multi_logloss: 0.522802\tvalid's multi_logloss: 0.506062\n",
      "[421]\ttrain's multi_logloss: 0.522629\tvalid's multi_logloss: 0.507199\n",
      "[422]\ttrain's multi_logloss: 0.52203\tvalid's multi_logloss: 0.504883\n",
      "[423]\ttrain's multi_logloss: 0.521001\tvalid's multi_logloss: 0.501242\n",
      "[424]\ttrain's multi_logloss: 0.520379\tvalid's multi_logloss: 0.499527\n",
      "[425]\ttrain's multi_logloss: 0.519554\tvalid's multi_logloss: 0.49619\n",
      "[426]\ttrain's multi_logloss: 0.519583\tvalid's multi_logloss: 0.495654\n",
      "[427]\ttrain's multi_logloss: 0.519403\tvalid's multi_logloss: 0.495381\n",
      "[428]\ttrain's multi_logloss: 0.518854\tvalid's multi_logloss: 0.495433\n",
      "[429]\ttrain's multi_logloss: 0.518198\tvalid's multi_logloss: 0.496067\n",
      "[430]\ttrain's multi_logloss: 0.517322\tvalid's multi_logloss: 0.493438\n",
      "[431]\ttrain's multi_logloss: 0.516707\tvalid's multi_logloss: 0.49275\n",
      "[432]\ttrain's multi_logloss: 0.516537\tvalid's multi_logloss: 0.492373\n",
      "[433]\ttrain's multi_logloss: 0.516054\tvalid's multi_logloss: 0.491766\n",
      "[434]\ttrain's multi_logloss: 0.515239\tvalid's multi_logloss: 0.493193\n",
      "[435]\ttrain's multi_logloss: 0.514538\tvalid's multi_logloss: 0.494684\n",
      "[436]\ttrain's multi_logloss: 0.513943\tvalid's multi_logloss: 0.496232\n",
      "[437]\ttrain's multi_logloss: 0.513446\tvalid's multi_logloss: 0.497831\n",
      "[438]\ttrain's multi_logloss: 0.51285\tvalid's multi_logloss: 0.49302\n",
      "[439]\ttrain's multi_logloss: 0.51233\tvalid's multi_logloss: 0.48966\n",
      "[440]\ttrain's multi_logloss: 0.511877\tvalid's multi_logloss: 0.486414\n",
      "[441]\ttrain's multi_logloss: 0.511485\tvalid's multi_logloss: 0.483277\n",
      "[442]\ttrain's multi_logloss: 0.510939\tvalid's multi_logloss: 0.485473\n",
      "[443]\ttrain's multi_logloss: 0.510699\tvalid's multi_logloss: 0.488649\n",
      "[444]\ttrain's multi_logloss: 0.51016\tvalid's multi_logloss: 0.492261\n",
      "[445]\ttrain's multi_logloss: 0.509857\tvalid's multi_logloss: 0.491334\n",
      "[446]\ttrain's multi_logloss: 0.509015\tvalid's multi_logloss: 0.490384\n",
      "[447]\ttrain's multi_logloss: 0.50808\tvalid's multi_logloss: 0.491009\n",
      "[448]\ttrain's multi_logloss: 0.507231\tvalid's multi_logloss: 0.490795\n",
      "[449]\ttrain's multi_logloss: 0.506761\tvalid's multi_logloss: 0.490255\n",
      "[450]\ttrain's multi_logloss: 0.505956\tvalid's multi_logloss: 0.489874\n",
      "[451]\ttrain's multi_logloss: 0.505499\tvalid's multi_logloss: 0.488598\n",
      "[452]\ttrain's multi_logloss: 0.504654\tvalid's multi_logloss: 0.488122\n",
      "[453]\ttrain's multi_logloss: 0.504479\tvalid's multi_logloss: 0.488222\n",
      "[454]\ttrain's multi_logloss: 0.504004\tvalid's multi_logloss: 0.489603\n",
      "[455]\ttrain's multi_logloss: 0.50319\tvalid's multi_logloss: 0.487753\n",
      "[456]\ttrain's multi_logloss: 0.502848\tvalid's multi_logloss: 0.486675\n",
      "[457]\ttrain's multi_logloss: 0.502522\tvalid's multi_logloss: 0.488126\n",
      "[458]\ttrain's multi_logloss: 0.501884\tvalid's multi_logloss: 0.488641\n",
      "[459]\ttrain's multi_logloss: 0.501388\tvalid's multi_logloss: 0.487732\n",
      "[460]\ttrain's multi_logloss: 0.500913\tvalid's multi_logloss: 0.488292\n",
      "[461]\ttrain's multi_logloss: 0.500665\tvalid's multi_logloss: 0.488162\n",
      "[462]\ttrain's multi_logloss: 0.500433\tvalid's multi_logloss: 0.488937\n",
      "[463]\ttrain's multi_logloss: 0.500091\tvalid's multi_logloss: 0.490204\n",
      "[464]\ttrain's multi_logloss: 0.499809\tvalid's multi_logloss: 0.490501\n",
      "[465]\ttrain's multi_logloss: 0.499562\tvalid's multi_logloss: 0.490993\n",
      "[466]\ttrain's multi_logloss: 0.499216\tvalid's multi_logloss: 0.493191\n",
      "[467]\ttrain's multi_logloss: 0.498913\tvalid's multi_logloss: 0.495745\n",
      "[468]\ttrain's multi_logloss: 0.498702\tvalid's multi_logloss: 0.497895\n",
      "[469]\ttrain's multi_logloss: 0.498557\tvalid's multi_logloss: 0.500036\n",
      "[470]\ttrain's multi_logloss: 0.497937\tvalid's multi_logloss: 0.502474\n",
      "[471]\ttrain's multi_logloss: 0.49762\tvalid's multi_logloss: 0.503552\n",
      "[472]\ttrain's multi_logloss: 0.497091\tvalid's multi_logloss: 0.504446\n",
      "[473]\ttrain's multi_logloss: 0.496814\tvalid's multi_logloss: 0.5039\n",
      "[474]\ttrain's multi_logloss: 0.496398\tvalid's multi_logloss: 0.505986\n",
      "[475]\ttrain's multi_logloss: 0.496028\tvalid's multi_logloss: 0.509776\n",
      "[476]\ttrain's multi_logloss: 0.495389\tvalid's multi_logloss: 0.508456\n",
      "[477]\ttrain's multi_logloss: 0.495214\tvalid's multi_logloss: 0.511148\n",
      "[478]\ttrain's multi_logloss: 0.494637\tvalid's multi_logloss: 0.509402\n",
      "[479]\ttrain's multi_logloss: 0.494322\tvalid's multi_logloss: 0.509819\n",
      "[480]\ttrain's multi_logloss: 0.49406\tvalid's multi_logloss: 0.50995\n",
      "[481]\ttrain's multi_logloss: 0.493679\tvalid's multi_logloss: 0.506461\n",
      "[482]\ttrain's multi_logloss: 0.493043\tvalid's multi_logloss: 0.504952\n",
      "[483]\ttrain's multi_logloss: 0.492627\tvalid's multi_logloss: 0.505122\n",
      "[484]\ttrain's multi_logloss: 0.492273\tvalid's multi_logloss: 0.50534\n",
      "[485]\ttrain's multi_logloss: 0.491793\tvalid's multi_logloss: 0.50552\n",
      "[486]\ttrain's multi_logloss: 0.491533\tvalid's multi_logloss: 0.503905\n",
      "[487]\ttrain's multi_logloss: 0.491109\tvalid's multi_logloss: 0.505907\n",
      "[488]\ttrain's multi_logloss: 0.490698\tvalid's multi_logloss: 0.509124\n",
      "[489]\ttrain's multi_logloss: 0.490342\tvalid's multi_logloss: 0.512293\n",
      "[490]\ttrain's multi_logloss: 0.49025\tvalid's multi_logloss: 0.514772\n",
      "[491]\ttrain's multi_logloss: 0.489804\tvalid's multi_logloss: 0.514453\n",
      "[492]\ttrain's multi_logloss: 0.489249\tvalid's multi_logloss: 0.512894\n",
      "[493]\ttrain's multi_logloss: 0.489128\tvalid's multi_logloss: 0.512647\n",
      "[494]\ttrain's multi_logloss: 0.488562\tvalid's multi_logloss: 0.510901\n",
      "[495]\ttrain's multi_logloss: 0.488224\tvalid's multi_logloss: 0.505876\n",
      "[496]\ttrain's multi_logloss: 0.487621\tvalid's multi_logloss: 0.500874\n",
      "[497]\ttrain's multi_logloss: 0.487204\tvalid's multi_logloss: 0.495454\n",
      "[498]\ttrain's multi_logloss: 0.486806\tvalid's multi_logloss: 0.494744\n",
      "[499]\ttrain's multi_logloss: 0.486358\tvalid's multi_logloss: 0.495567\n",
      "[500]\ttrain's multi_logloss: 0.486016\tvalid's multi_logloss: 0.496513\n",
      "[501]\ttrain's multi_logloss: 0.485816\tvalid's multi_logloss: 0.497527\n",
      "[502]\ttrain's multi_logloss: 0.48568\tvalid's multi_logloss: 0.498719\n",
      "[503]\ttrain's multi_logloss: 0.485705\tvalid's multi_logloss: 0.497289\n",
      "[504]\ttrain's multi_logloss: 0.485788\tvalid's multi_logloss: 0.495942\n",
      "[505]\ttrain's multi_logloss: 0.485776\tvalid's multi_logloss: 0.497133\n",
      "[506]\ttrain's multi_logloss: 0.485675\tvalid's multi_logloss: 0.496094\n",
      "[507]\ttrain's multi_logloss: 0.485494\tvalid's multi_logloss: 0.498294\n",
      "[508]\ttrain's multi_logloss: 0.485108\tvalid's multi_logloss: 0.500065\n",
      "[509]\ttrain's multi_logloss: 0.485108\tvalid's multi_logloss: 0.500909\n",
      "[510]\ttrain's multi_logloss: 0.484776\tvalid's multi_logloss: 0.498469\n",
      "[511]\ttrain's multi_logloss: 0.484654\tvalid's multi_logloss: 0.496823\n",
      "[512]\ttrain's multi_logloss: 0.484301\tvalid's multi_logloss: 0.492966\n",
      "Early stopping, best iteration is:\n",
      "[412]\ttrain's multi_logloss: 0.525674\tvalid's multi_logloss: 0.473524\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.8181818181818182\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.092422\n",
      "1   feature2    0.106265\n",
      "2   feature3    0.069127\n",
      "3   feature4    0.097238\n",
      "4   feature5    0.113479\n",
      "5   feature6    0.067416\n",
      "6   feature7    0.090840\n",
      "7   feature8    0.124962\n",
      "8   feature9    0.135663\n",
      "9  feature10    0.102588\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08787\tvalid's my_multiclass_logloss: 1.09885\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07988\tvalid's my_multiclass_logloss: 1.09657\n",
      "[3]\ttrain's my_multiclass_logloss: 1.07154\tvalid's my_multiclass_logloss: 1.08788\n",
      "[4]\ttrain's my_multiclass_logloss: 1.065\tvalid's my_multiclass_logloss: 1.08867\n",
      "[5]\ttrain's my_multiclass_logloss: 1.05613\tvalid's my_multiclass_logloss: 1.08407\n",
      "[6]\ttrain's my_multiclass_logloss: 1.04543\tvalid's my_multiclass_logloss: 1.07616\n",
      "[7]\ttrain's my_multiclass_logloss: 1.04072\tvalid's my_multiclass_logloss: 1.07844\n",
      "[8]\ttrain's my_multiclass_logloss: 1.03357\tvalid's my_multiclass_logloss: 1.07262\n",
      "[9]\ttrain's my_multiclass_logloss: 1.02911\tvalid's my_multiclass_logloss: 1.06328\n",
      "[10]\ttrain's my_multiclass_logloss: 1.02342\tvalid's my_multiclass_logloss: 1.04759\n",
      "[11]\ttrain's my_multiclass_logloss: 1.01655\tvalid's my_multiclass_logloss: 1.04639\n",
      "[12]\ttrain's my_multiclass_logloss: 1.01164\tvalid's my_multiclass_logloss: 1.04115\n",
      "[13]\ttrain's my_multiclass_logloss: 1.00346\tvalid's my_multiclass_logloss: 1.03793\n",
      "[14]\ttrain's my_multiclass_logloss: 0.99839\tvalid's my_multiclass_logloss: 1.0215\n",
      "[15]\ttrain's my_multiclass_logloss: 0.991116\tvalid's my_multiclass_logloss: 1.01896\n",
      "[16]\ttrain's my_multiclass_logloss: 0.987046\tvalid's my_multiclass_logloss: 1.00478\n",
      "[17]\ttrain's my_multiclass_logloss: 0.978687\tvalid's my_multiclass_logloss: 0.991171\n",
      "[18]\ttrain's my_multiclass_logloss: 0.970376\tvalid's my_multiclass_logloss: 0.978385\n",
      "[19]\ttrain's my_multiclass_logloss: 0.963871\tvalid's my_multiclass_logloss: 0.971458\n",
      "[20]\ttrain's my_multiclass_logloss: 0.956869\tvalid's my_multiclass_logloss: 0.960427\n",
      "[21]\ttrain's my_multiclass_logloss: 0.952467\tvalid's my_multiclass_logloss: 0.956951\n",
      "[22]\ttrain's my_multiclass_logloss: 0.947655\tvalid's my_multiclass_logloss: 0.950254\n",
      "[23]\ttrain's my_multiclass_logloss: 0.943046\tvalid's my_multiclass_logloss: 0.943374\n",
      "[24]\ttrain's my_multiclass_logloss: 0.938311\tvalid's my_multiclass_logloss: 0.938004\n",
      "[25]\ttrain's my_multiclass_logloss: 0.932918\tvalid's my_multiclass_logloss: 0.934969\n",
      "[26]\ttrain's my_multiclass_logloss: 0.930174\tvalid's my_multiclass_logloss: 0.92753\n",
      "[27]\ttrain's my_multiclass_logloss: 0.927366\tvalid's my_multiclass_logloss: 0.92367\n",
      "[28]\ttrain's my_multiclass_logloss: 0.923081\tvalid's my_multiclass_logloss: 0.919581\n",
      "[29]\ttrain's my_multiclass_logloss: 0.921191\tvalid's my_multiclass_logloss: 0.92037\n",
      "[30]\ttrain's my_multiclass_logloss: 0.916879\tvalid's my_multiclass_logloss: 0.920687\n",
      "[31]\ttrain's my_multiclass_logloss: 0.913528\tvalid's my_multiclass_logloss: 0.921331\n",
      "[32]\ttrain's my_multiclass_logloss: 0.909089\tvalid's my_multiclass_logloss: 0.917698\n",
      "[33]\ttrain's my_multiclass_logloss: 0.904532\tvalid's my_multiclass_logloss: 0.911869\n",
      "[34]\ttrain's my_multiclass_logloss: 0.901956\tvalid's my_multiclass_logloss: 0.907814\n",
      "[35]\ttrain's my_multiclass_logloss: 0.898336\tvalid's my_multiclass_logloss: 0.903211\n",
      "[36]\ttrain's my_multiclass_logloss: 0.895273\tvalid's my_multiclass_logloss: 0.895853\n",
      "[37]\ttrain's my_multiclass_logloss: 0.89209\tvalid's my_multiclass_logloss: 0.892499\n",
      "[38]\ttrain's my_multiclass_logloss: 0.887563\tvalid's my_multiclass_logloss: 0.893144\n",
      "[39]\ttrain's my_multiclass_logloss: 0.885088\tvalid's my_multiclass_logloss: 0.889177\n",
      "[40]\ttrain's my_multiclass_logloss: 0.882369\tvalid's my_multiclass_logloss: 0.897095\n",
      "[41]\ttrain's my_multiclass_logloss: 0.8789\tvalid's my_multiclass_logloss: 0.888061\n",
      "[42]\ttrain's my_multiclass_logloss: 0.876355\tvalid's my_multiclass_logloss: 0.885635\n",
      "[43]\ttrain's my_multiclass_logloss: 0.873226\tvalid's my_multiclass_logloss: 0.879736\n",
      "[44]\ttrain's my_multiclass_logloss: 0.870383\tvalid's my_multiclass_logloss: 0.874168\n",
      "[45]\ttrain's my_multiclass_logloss: 0.867718\tvalid's my_multiclass_logloss: 0.865892\n",
      "[46]\ttrain's my_multiclass_logloss: 0.866077\tvalid's my_multiclass_logloss: 0.860165\n",
      "[47]\ttrain's my_multiclass_logloss: 0.864069\tvalid's my_multiclass_logloss: 0.8577\n",
      "[48]\ttrain's my_multiclass_logloss: 0.861761\tvalid's my_multiclass_logloss: 0.851362\n",
      "[49]\ttrain's my_multiclass_logloss: 0.85958\tvalid's my_multiclass_logloss: 0.845091\n",
      "[50]\ttrain's my_multiclass_logloss: 0.858279\tvalid's my_multiclass_logloss: 0.839742\n",
      "[51]\ttrain's my_multiclass_logloss: 0.856769\tvalid's my_multiclass_logloss: 0.839102\n",
      "[52]\ttrain's my_multiclass_logloss: 0.854577\tvalid's my_multiclass_logloss: 0.837182\n",
      "[53]\ttrain's my_multiclass_logloss: 0.851653\tvalid's my_multiclass_logloss: 0.826741\n",
      "[54]\ttrain's my_multiclass_logloss: 0.848934\tvalid's my_multiclass_logloss: 0.819619\n",
      "[55]\ttrain's my_multiclass_logloss: 0.846603\tvalid's my_multiclass_logloss: 0.810353\n",
      "[56]\ttrain's my_multiclass_logloss: 0.844345\tvalid's my_multiclass_logloss: 0.804039\n",
      "[57]\ttrain's my_multiclass_logloss: 0.841412\tvalid's my_multiclass_logloss: 0.800871\n",
      "[58]\ttrain's my_multiclass_logloss: 0.838426\tvalid's my_multiclass_logloss: 0.79667\n",
      "[59]\ttrain's my_multiclass_logloss: 0.836787\tvalid's my_multiclass_logloss: 0.795416\n",
      "[60]\ttrain's my_multiclass_logloss: 0.834653\tvalid's my_multiclass_logloss: 0.791636\n",
      "[61]\ttrain's my_multiclass_logloss: 0.831901\tvalid's my_multiclass_logloss: 0.791446\n",
      "[62]\ttrain's my_multiclass_logloss: 0.829946\tvalid's my_multiclass_logloss: 0.791705\n",
      "[63]\ttrain's my_multiclass_logloss: 0.827871\tvalid's my_multiclass_logloss: 0.793761\n",
      "[64]\ttrain's my_multiclass_logloss: 0.82546\tvalid's my_multiclass_logloss: 0.79557\n",
      "[65]\ttrain's my_multiclass_logloss: 0.823376\tvalid's my_multiclass_logloss: 0.789829\n",
      "[66]\ttrain's my_multiclass_logloss: 0.821586\tvalid's my_multiclass_logloss: 0.784593\n",
      "[67]\ttrain's my_multiclass_logloss: 0.820398\tvalid's my_multiclass_logloss: 0.784516\n",
      "[68]\ttrain's my_multiclass_logloss: 0.819042\tvalid's my_multiclass_logloss: 0.782488\n",
      "[69]\ttrain's my_multiclass_logloss: 0.817571\tvalid's my_multiclass_logloss: 0.782928\n",
      "[70]\ttrain's my_multiclass_logloss: 0.816543\tvalid's my_multiclass_logloss: 0.782343\n",
      "[71]\ttrain's my_multiclass_logloss: 0.815276\tvalid's my_multiclass_logloss: 0.778622\n",
      "[72]\ttrain's my_multiclass_logloss: 0.814314\tvalid's my_multiclass_logloss: 0.783174\n",
      "[73]\ttrain's my_multiclass_logloss: 0.812828\tvalid's my_multiclass_logloss: 0.777489\n",
      "[74]\ttrain's my_multiclass_logloss: 0.810995\tvalid's my_multiclass_logloss: 0.770318\n",
      "[75]\ttrain's my_multiclass_logloss: 0.809414\tvalid's my_multiclass_logloss: 0.763549\n",
      "[76]\ttrain's my_multiclass_logloss: 0.807969\tvalid's my_multiclass_logloss: 0.759483\n",
      "[77]\ttrain's my_multiclass_logloss: 0.807023\tvalid's my_multiclass_logloss: 0.760724\n",
      "[78]\ttrain's my_multiclass_logloss: 0.805527\tvalid's my_multiclass_logloss: 0.760758\n",
      "[79]\ttrain's my_multiclass_logloss: 0.804283\tvalid's my_multiclass_logloss: 0.760974\n",
      "[80]\ttrain's my_multiclass_logloss: 0.803421\tvalid's my_multiclass_logloss: 0.760843\n",
      "[81]\ttrain's my_multiclass_logloss: 0.801578\tvalid's my_multiclass_logloss: 0.750648\n",
      "[82]\ttrain's my_multiclass_logloss: 0.80003\tvalid's my_multiclass_logloss: 0.740646\n",
      "[83]\ttrain's my_multiclass_logloss: 0.798946\tvalid's my_multiclass_logloss: 0.736046\n",
      "[84]\ttrain's my_multiclass_logloss: 0.797905\tvalid's my_multiclass_logloss: 0.726441\n",
      "[85]\ttrain's my_multiclass_logloss: 0.79678\tvalid's my_multiclass_logloss: 0.725713\n",
      "[86]\ttrain's my_multiclass_logloss: 0.795835\tvalid's my_multiclass_logloss: 0.72177\n",
      "[87]\ttrain's my_multiclass_logloss: 0.794779\tvalid's my_multiclass_logloss: 0.724414\n",
      "[88]\ttrain's my_multiclass_logloss: 0.793921\tvalid's my_multiclass_logloss: 0.725901\n",
      "[89]\ttrain's my_multiclass_logloss: 0.792111\tvalid's my_multiclass_logloss: 0.723304\n",
      "[90]\ttrain's my_multiclass_logloss: 0.790495\tvalid's my_multiclass_logloss: 0.720934\n",
      "[91]\ttrain's my_multiclass_logloss: 0.789196\tvalid's my_multiclass_logloss: 0.720553\n",
      "[92]\ttrain's my_multiclass_logloss: 0.788218\tvalid's my_multiclass_logloss: 0.72234\n",
      "[93]\ttrain's my_multiclass_logloss: 0.786607\tvalid's my_multiclass_logloss: 0.717122\n",
      "[94]\ttrain's my_multiclass_logloss: 0.785225\tvalid's my_multiclass_logloss: 0.714125\n",
      "[95]\ttrain's my_multiclass_logloss: 0.784539\tvalid's my_multiclass_logloss: 0.712325\n",
      "[96]\ttrain's my_multiclass_logloss: 0.783476\tvalid's my_multiclass_logloss: 0.708093\n",
      "[97]\ttrain's my_multiclass_logloss: 0.781857\tvalid's my_multiclass_logloss: 0.716694\n",
      "[98]\ttrain's my_multiclass_logloss: 0.780703\tvalid's my_multiclass_logloss: 0.723019\n",
      "[99]\ttrain's my_multiclass_logloss: 0.779971\tvalid's my_multiclass_logloss: 0.726285\n",
      "[100]\ttrain's my_multiclass_logloss: 0.779624\tvalid's my_multiclass_logloss: 0.730675\n",
      "[101]\ttrain's my_multiclass_logloss: 0.77815\tvalid's my_multiclass_logloss: 0.734217\n",
      "[102]\ttrain's my_multiclass_logloss: 0.776914\tvalid's my_multiclass_logloss: 0.737601\n",
      "[103]\ttrain's my_multiclass_logloss: 0.776004\tvalid's my_multiclass_logloss: 0.739469\n",
      "[104]\ttrain's my_multiclass_logloss: 0.775\tvalid's my_multiclass_logloss: 0.742818\n",
      "[105]\ttrain's my_multiclass_logloss: 0.7734\tvalid's my_multiclass_logloss: 0.74302\n",
      "[106]\ttrain's my_multiclass_logloss: 0.772093\tvalid's my_multiclass_logloss: 0.743594\n",
      "[107]\ttrain's my_multiclass_logloss: 0.771028\tvalid's my_multiclass_logloss: 0.745128\n",
      "[108]\ttrain's my_multiclass_logloss: 0.770019\tvalid's my_multiclass_logloss: 0.743972\n",
      "[109]\ttrain's my_multiclass_logloss: 0.769359\tvalid's my_multiclass_logloss: 0.747163\n",
      "[110]\ttrain's my_multiclass_logloss: 0.768291\tvalid's my_multiclass_logloss: 0.746452\n",
      "[111]\ttrain's my_multiclass_logloss: 0.767079\tvalid's my_multiclass_logloss: 0.747717\n",
      "[112]\ttrain's my_multiclass_logloss: 0.766655\tvalid's my_multiclass_logloss: 0.748512\n",
      "[113]\ttrain's my_multiclass_logloss: 0.765551\tvalid's my_multiclass_logloss: 0.74875\n",
      "[114]\ttrain's my_multiclass_logloss: 0.764085\tvalid's my_multiclass_logloss: 0.746413\n",
      "[115]\ttrain's my_multiclass_logloss: 0.762844\tvalid's my_multiclass_logloss: 0.744355\n",
      "[116]\ttrain's my_multiclass_logloss: 0.762168\tvalid's my_multiclass_logloss: 0.740708\n",
      "[117]\ttrain's my_multiclass_logloss: 0.761275\tvalid's my_multiclass_logloss: 0.74504\n",
      "[118]\ttrain's my_multiclass_logloss: 0.760481\tvalid's my_multiclass_logloss: 0.748554\n",
      "[119]\ttrain's my_multiclass_logloss: 0.759848\tvalid's my_multiclass_logloss: 0.752058\n",
      "[120]\ttrain's my_multiclass_logloss: 0.758968\tvalid's my_multiclass_logloss: 0.758448\n",
      "[121]\ttrain's my_multiclass_logloss: 0.756519\tvalid's my_multiclass_logloss: 0.755731\n",
      "[122]\ttrain's my_multiclass_logloss: 0.754421\tvalid's my_multiclass_logloss: 0.75186\n",
      "[123]\ttrain's my_multiclass_logloss: 0.753282\tvalid's my_multiclass_logloss: 0.75128\n",
      "[124]\ttrain's my_multiclass_logloss: 0.751488\tvalid's my_multiclass_logloss: 0.751111\n",
      "[125]\ttrain's my_multiclass_logloss: 0.75041\tvalid's my_multiclass_logloss: 0.74315\n",
      "[126]\ttrain's my_multiclass_logloss: 0.7496\tvalid's my_multiclass_logloss: 0.74119\n",
      "[127]\ttrain's my_multiclass_logloss: 0.748766\tvalid's my_multiclass_logloss: 0.739517\n",
      "[128]\ttrain's my_multiclass_logloss: 0.748568\tvalid's my_multiclass_logloss: 0.740287\n",
      "[129]\ttrain's my_multiclass_logloss: 0.747324\tvalid's my_multiclass_logloss: 0.737448\n",
      "[130]\ttrain's my_multiclass_logloss: 0.747523\tvalid's my_multiclass_logloss: 0.742933\n",
      "[131]\ttrain's my_multiclass_logloss: 0.746543\tvalid's my_multiclass_logloss: 0.740462\n",
      "[132]\ttrain's my_multiclass_logloss: 0.74681\tvalid's my_multiclass_logloss: 0.745898\n",
      "[133]\ttrain's my_multiclass_logloss: 0.745188\tvalid's my_multiclass_logloss: 0.745225\n",
      "[134]\ttrain's my_multiclass_logloss: 0.744107\tvalid's my_multiclass_logloss: 0.746001\n",
      "[135]\ttrain's my_multiclass_logloss: 0.743281\tvalid's my_multiclass_logloss: 0.750767\n",
      "[136]\ttrain's my_multiclass_logloss: 0.742219\tvalid's my_multiclass_logloss: 0.753114\n",
      "[137]\ttrain's my_multiclass_logloss: 0.74146\tvalid's my_multiclass_logloss: 0.752215\n",
      "[138]\ttrain's my_multiclass_logloss: 0.740853\tvalid's my_multiclass_logloss: 0.751466\n",
      "[139]\ttrain's my_multiclass_logloss: 0.740198\tvalid's my_multiclass_logloss: 0.746179\n",
      "[140]\ttrain's my_multiclass_logloss: 0.739598\tvalid's my_multiclass_logloss: 0.74409\n",
      "[141]\ttrain's my_multiclass_logloss: 0.739459\tvalid's my_multiclass_logloss: 0.748968\n",
      "[142]\ttrain's my_multiclass_logloss: 0.738749\tvalid's my_multiclass_logloss: 0.751478\n",
      "[143]\ttrain's my_multiclass_logloss: 0.738152\tvalid's my_multiclass_logloss: 0.753429\n",
      "[144]\ttrain's my_multiclass_logloss: 0.737724\tvalid's my_multiclass_logloss: 0.75547\n",
      "[145]\ttrain's my_multiclass_logloss: 0.736278\tvalid's my_multiclass_logloss: 0.758328\n",
      "[146]\ttrain's my_multiclass_logloss: 0.735117\tvalid's my_multiclass_logloss: 0.761217\n",
      "[147]\ttrain's my_multiclass_logloss: 0.734207\tvalid's my_multiclass_logloss: 0.764117\n",
      "[148]\ttrain's my_multiclass_logloss: 0.73374\tvalid's my_multiclass_logloss: 0.766092\n",
      "[149]\ttrain's my_multiclass_logloss: 0.732542\tvalid's my_multiclass_logloss: 0.768207\n",
      "[150]\ttrain's my_multiclass_logloss: 0.731935\tvalid's my_multiclass_logloss: 0.769772\n",
      "[151]\ttrain's my_multiclass_logloss: 0.731091\tvalid's my_multiclass_logloss: 0.772031\n",
      "[152]\ttrain's my_multiclass_logloss: 0.730496\tvalid's my_multiclass_logloss: 0.771455\n",
      "[153]\ttrain's my_multiclass_logloss: 0.729513\tvalid's my_multiclass_logloss: 0.767514\n",
      "[154]\ttrain's my_multiclass_logloss: 0.728783\tvalid's my_multiclass_logloss: 0.770785\n",
      "[155]\ttrain's my_multiclass_logloss: 0.728382\tvalid's my_multiclass_logloss: 0.774315\n",
      "[156]\ttrain's my_multiclass_logloss: 0.727704\tvalid's my_multiclass_logloss: 0.774463\n",
      "[157]\ttrain's my_multiclass_logloss: 0.72713\tvalid's my_multiclass_logloss: 0.770377\n",
      "[158]\ttrain's my_multiclass_logloss: 0.726158\tvalid's my_multiclass_logloss: 0.764551\n",
      "[159]\ttrain's my_multiclass_logloss: 0.725408\tvalid's my_multiclass_logloss: 0.756081\n",
      "[160]\ttrain's my_multiclass_logloss: 0.724888\tvalid's my_multiclass_logloss: 0.753905\n",
      "[161]\ttrain's my_multiclass_logloss: 0.722212\tvalid's my_multiclass_logloss: 0.751747\n",
      "[162]\ttrain's my_multiclass_logloss: 0.719791\tvalid's my_multiclass_logloss: 0.748857\n",
      "[163]\ttrain's my_multiclass_logloss: 0.718556\tvalid's my_multiclass_logloss: 0.745645\n",
      "[164]\ttrain's my_multiclass_logloss: 0.717404\tvalid's my_multiclass_logloss: 0.74254\n",
      "[165]\ttrain's my_multiclass_logloss: 0.716483\tvalid's my_multiclass_logloss: 0.748293\n",
      "[166]\ttrain's my_multiclass_logloss: 0.716352\tvalid's my_multiclass_logloss: 0.749632\n",
      "[167]\ttrain's my_multiclass_logloss: 0.716255\tvalid's my_multiclass_logloss: 0.756155\n",
      "[168]\ttrain's my_multiclass_logloss: 0.716449\tvalid's my_multiclass_logloss: 0.759715\n",
      "[169]\ttrain's my_multiclass_logloss: 0.715112\tvalid's my_multiclass_logloss: 0.75944\n",
      "[170]\ttrain's my_multiclass_logloss: 0.714563\tvalid's my_multiclass_logloss: 0.753266\n",
      "[171]\ttrain's my_multiclass_logloss: 0.713791\tvalid's my_multiclass_logloss: 0.751379\n",
      "[172]\ttrain's my_multiclass_logloss: 0.713249\tvalid's my_multiclass_logloss: 0.749814\n",
      "[173]\ttrain's my_multiclass_logloss: 0.712734\tvalid's my_multiclass_logloss: 0.75245\n",
      "[174]\ttrain's my_multiclass_logloss: 0.711651\tvalid's my_multiclass_logloss: 0.755177\n",
      "[175]\ttrain's my_multiclass_logloss: 0.711047\tvalid's my_multiclass_logloss: 0.755911\n",
      "[176]\ttrain's my_multiclass_logloss: 0.710725\tvalid's my_multiclass_logloss: 0.756937\n",
      "[177]\ttrain's my_multiclass_logloss: 0.710215\tvalid's my_multiclass_logloss: 0.75153\n",
      "[178]\ttrain's my_multiclass_logloss: 0.709526\tvalid's my_multiclass_logloss: 0.74897\n",
      "[179]\ttrain's my_multiclass_logloss: 0.709105\tvalid's my_multiclass_logloss: 0.748956\n",
      "[180]\ttrain's my_multiclass_logloss: 0.708264\tvalid's my_multiclass_logloss: 0.743839\n",
      "[181]\ttrain's my_multiclass_logloss: 0.707194\tvalid's my_multiclass_logloss: 0.738673\n",
      "[182]\ttrain's my_multiclass_logloss: 0.70647\tvalid's my_multiclass_logloss: 0.733812\n",
      "[183]\ttrain's my_multiclass_logloss: 0.705681\tvalid's my_multiclass_logloss: 0.729522\n",
      "[184]\ttrain's my_multiclass_logloss: 0.705168\tvalid's my_multiclass_logloss: 0.723883\n",
      "[185]\ttrain's my_multiclass_logloss: 0.704193\tvalid's my_multiclass_logloss: 0.718496\n",
      "[186]\ttrain's my_multiclass_logloss: 0.703886\tvalid's my_multiclass_logloss: 0.720282\n",
      "[187]\ttrain's my_multiclass_logloss: 0.702948\tvalid's my_multiclass_logloss: 0.719702\n",
      "[188]\ttrain's my_multiclass_logloss: 0.702623\tvalid's my_multiclass_logloss: 0.716262\n",
      "[189]\ttrain's my_multiclass_logloss: 0.701771\tvalid's my_multiclass_logloss: 0.71518\n",
      "[190]\ttrain's my_multiclass_logloss: 0.700871\tvalid's my_multiclass_logloss: 0.716928\n",
      "[191]\ttrain's my_multiclass_logloss: 0.700532\tvalid's my_multiclass_logloss: 0.723314\n",
      "[192]\ttrain's my_multiclass_logloss: 0.700256\tvalid's my_multiclass_logloss: 0.727086\n",
      "[193]\ttrain's my_multiclass_logloss: 0.699038\tvalid's my_multiclass_logloss: 0.719805\n",
      "[194]\ttrain's my_multiclass_logloss: 0.69834\tvalid's my_multiclass_logloss: 0.718854\n",
      "[195]\ttrain's my_multiclass_logloss: 0.697437\tvalid's my_multiclass_logloss: 0.713989\n",
      "[196]\ttrain's my_multiclass_logloss: 0.696681\tvalid's my_multiclass_logloss: 0.709424\n",
      "Early stopping, best iteration is:\n",
      "[96]\ttrain's my_multiclass_logloss: 0.783476\tvalid's my_multiclass_logloss: 0.708093\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.8181818181818182\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.092422\n",
      "1   feature2    0.106265\n",
      "2   feature3    0.069127\n",
      "3   feature4    0.097238\n",
      "4   feature5    0.113479\n",
      "5   feature6    0.067416\n",
      "6   feature7    0.090840\n",
      "7   feature8    0.124962\n",
      "8   feature9    0.135663\n",
      "9  feature10    0.102588\n",
      "     feature  importance\n",
      "0   feature1    0.102522\n",
      "1   feature2    0.143452\n",
      "2   feature3    0.041258\n",
      "3   feature4    0.036509\n",
      "4   feature5    0.127799\n",
      "5   feature6    0.049734\n",
      "6   feature7    0.091460\n",
      "7   feature8    0.150019\n",
      "8   feature9    0.153363\n",
      "9  feature10    0.103884\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.047515\n",
      "1   feature2    0.174950\n",
      "2   feature3   -0.131108\n",
      "3   feature4   -0.285704\n",
      "4   feature5    0.067370\n",
      "5   feature6   -0.083188\n",
      "6   feature7    0.002918\n",
      "7   feature8    0.117883\n",
      "8   feature9    0.083269\n",
      "9  feature10    0.006095\n",
      "-------------------- 6 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05157\tvalid's multi_logloss: 1.06472\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.04182\tvalid's multi_logloss: 1.05278\n",
      "[3]\ttrain's multi_logloss: 1.03102\tvalid's multi_logloss: 1.04658\n",
      "[4]\ttrain's multi_logloss: 1.021\tvalid's multi_logloss: 1.04096\n",
      "[5]\ttrain's multi_logloss: 1.01454\tvalid's multi_logloss: 1.04149\n",
      "[6]\ttrain's multi_logloss: 1.01076\tvalid's multi_logloss: 1.04489\n",
      "[7]\ttrain's multi_logloss: 1.0054\tvalid's multi_logloss: 1.04108\n",
      "[8]\ttrain's multi_logloss: 0.999401\tvalid's multi_logloss: 1.04138\n",
      "[9]\ttrain's multi_logloss: 0.993937\tvalid's multi_logloss: 1.04615\n",
      "[10]\ttrain's multi_logloss: 0.988918\tvalid's multi_logloss: 1.05099\n",
      "[11]\ttrain's multi_logloss: 0.985222\tvalid's multi_logloss: 1.05391\n",
      "[12]\ttrain's multi_logloss: 0.980551\tvalid's multi_logloss: 1.0557\n",
      "[13]\ttrain's multi_logloss: 0.97104\tvalid's multi_logloss: 1.06011\n",
      "[14]\ttrain's multi_logloss: 0.962695\tvalid's multi_logloss: 1.05832\n",
      "[15]\ttrain's multi_logloss: 0.95628\tvalid's multi_logloss: 1.06156\n",
      "[16]\ttrain's multi_logloss: 0.948689\tvalid's multi_logloss: 1.06023\n",
      "[17]\ttrain's multi_logloss: 0.943365\tvalid's multi_logloss: 1.06068\n",
      "[18]\ttrain's multi_logloss: 0.93667\tvalid's multi_logloss: 1.06524\n",
      "[19]\ttrain's multi_logloss: 0.930606\tvalid's multi_logloss: 1.07197\n",
      "[20]\ttrain's multi_logloss: 0.924538\tvalid's multi_logloss: 1.07235\n",
      "[21]\ttrain's multi_logloss: 0.918954\tvalid's multi_logloss: 1.06996\n",
      "[22]\ttrain's multi_logloss: 0.914334\tvalid's multi_logloss: 1.07062\n",
      "[23]\ttrain's multi_logloss: 0.910118\tvalid's multi_logloss: 1.07357\n",
      "[24]\ttrain's multi_logloss: 0.904751\tvalid's multi_logloss: 1.07153\n",
      "[25]\ttrain's multi_logloss: 0.899428\tvalid's multi_logloss: 1.07667\n",
      "[26]\ttrain's multi_logloss: 0.893686\tvalid's multi_logloss: 1.08246\n",
      "[27]\ttrain's multi_logloss: 0.888998\tvalid's multi_logloss: 1.08776\n",
      "[28]\ttrain's multi_logloss: 0.884655\tvalid's multi_logloss: 1.09317\n",
      "[29]\ttrain's multi_logloss: 0.879366\tvalid's multi_logloss: 1.09328\n",
      "[30]\ttrain's multi_logloss: 0.876445\tvalid's multi_logloss: 1.09024\n",
      "[31]\ttrain's multi_logloss: 0.868825\tvalid's multi_logloss: 1.08939\n",
      "[32]\ttrain's multi_logloss: 0.863436\tvalid's multi_logloss: 1.08387\n",
      "[33]\ttrain's multi_logloss: 0.857865\tvalid's multi_logloss: 1.08316\n",
      "[34]\ttrain's multi_logloss: 0.853534\tvalid's multi_logloss: 1.08442\n",
      "[35]\ttrain's multi_logloss: 0.849388\tvalid's multi_logloss: 1.08543\n",
      "[36]\ttrain's multi_logloss: 0.84465\tvalid's multi_logloss: 1.08819\n",
      "[37]\ttrain's multi_logloss: 0.839689\tvalid's multi_logloss: 1.09212\n",
      "[38]\ttrain's multi_logloss: 0.835793\tvalid's multi_logloss: 1.09042\n",
      "[39]\ttrain's multi_logloss: 0.831556\tvalid's multi_logloss: 1.0948\n",
      "[40]\ttrain's multi_logloss: 0.829062\tvalid's multi_logloss: 1.08983\n",
      "[41]\ttrain's multi_logloss: 0.826218\tvalid's multi_logloss: 1.08896\n",
      "[42]\ttrain's multi_logloss: 0.824183\tvalid's multi_logloss: 1.09439\n",
      "[43]\ttrain's multi_logloss: 0.822804\tvalid's multi_logloss: 1.09889\n",
      "[44]\ttrain's multi_logloss: 0.820631\tvalid's multi_logloss: 1.09843\n",
      "[45]\ttrain's multi_logloss: 0.818274\tvalid's multi_logloss: 1.09674\n",
      "[46]\ttrain's multi_logloss: 0.815285\tvalid's multi_logloss: 1.09985\n",
      "[47]\ttrain's multi_logloss: 0.812907\tvalid's multi_logloss: 1.09955\n",
      "[48]\ttrain's multi_logloss: 0.810698\tvalid's multi_logloss: 1.09936\n",
      "[49]\ttrain's multi_logloss: 0.807318\tvalid's multi_logloss: 1.10173\n",
      "[50]\ttrain's multi_logloss: 0.804022\tvalid's multi_logloss: 1.10076\n",
      "[51]\ttrain's multi_logloss: 0.800957\tvalid's multi_logloss: 1.09998\n",
      "[52]\ttrain's multi_logloss: 0.798107\tvalid's multi_logloss: 1.09936\n",
      "[53]\ttrain's multi_logloss: 0.792599\tvalid's multi_logloss: 1.10145\n",
      "[54]\ttrain's multi_logloss: 0.789096\tvalid's multi_logloss: 1.10453\n",
      "[55]\ttrain's multi_logloss: 0.786041\tvalid's multi_logloss: 1.10781\n",
      "[56]\ttrain's multi_logloss: 0.78379\tvalid's multi_logloss: 1.10682\n",
      "[57]\ttrain's multi_logloss: 0.781249\tvalid's multi_logloss: 1.10564\n",
      "[58]\ttrain's multi_logloss: 0.777976\tvalid's multi_logloss: 1.10811\n",
      "[59]\ttrain's multi_logloss: 0.774356\tvalid's multi_logloss: 1.10814\n",
      "[60]\ttrain's multi_logloss: 0.770997\tvalid's multi_logloss: 1.10831\n",
      "[61]\ttrain's multi_logloss: 0.76751\tvalid's multi_logloss: 1.10579\n",
      "[62]\ttrain's multi_logloss: 0.764325\tvalid's multi_logloss: 1.10351\n",
      "[63]\ttrain's multi_logloss: 0.761943\tvalid's multi_logloss: 1.10222\n",
      "[64]\ttrain's multi_logloss: 0.760052\tvalid's multi_logloss: 1.10097\n",
      "[65]\ttrain's multi_logloss: 0.758172\tvalid's multi_logloss: 1.1021\n",
      "[66]\ttrain's multi_logloss: 0.756274\tvalid's multi_logloss: 1.10215\n",
      "[67]\ttrain's multi_logloss: 0.754299\tvalid's multi_logloss: 1.10219\n",
      "[68]\ttrain's multi_logloss: 0.752336\tvalid's multi_logloss: 1.1037\n",
      "[69]\ttrain's multi_logloss: 0.75042\tvalid's multi_logloss: 1.10559\n",
      "[70]\ttrain's multi_logloss: 0.748157\tvalid's multi_logloss: 1.11254\n",
      "[71]\ttrain's multi_logloss: 0.746312\tvalid's multi_logloss: 1.11852\n",
      "[72]\ttrain's multi_logloss: 0.744709\tvalid's multi_logloss: 1.12452\n",
      "[73]\ttrain's multi_logloss: 0.741872\tvalid's multi_logloss: 1.12952\n",
      "[74]\ttrain's multi_logloss: 0.739613\tvalid's multi_logloss: 1.13168\n",
      "[75]\ttrain's multi_logloss: 0.737375\tvalid's multi_logloss: 1.13949\n",
      "[76]\ttrain's multi_logloss: 0.735071\tvalid's multi_logloss: 1.14474\n",
      "[77]\ttrain's multi_logloss: 0.732554\tvalid's multi_logloss: 1.14765\n",
      "[78]\ttrain's multi_logloss: 0.730325\tvalid's multi_logloss: 1.15066\n",
      "[79]\ttrain's multi_logloss: 0.728736\tvalid's multi_logloss: 1.14847\n",
      "[80]\ttrain's multi_logloss: 0.726795\tvalid's multi_logloss: 1.15155\n",
      "[81]\ttrain's multi_logloss: 0.723931\tvalid's multi_logloss: 1.14861\n",
      "[82]\ttrain's multi_logloss: 0.721054\tvalid's multi_logloss: 1.15017\n",
      "[83]\ttrain's multi_logloss: 0.718486\tvalid's multi_logloss: 1.15192\n",
      "[84]\ttrain's multi_logloss: 0.716071\tvalid's multi_logloss: 1.14946\n",
      "[85]\ttrain's multi_logloss: 0.714887\tvalid's multi_logloss: 1.14903\n",
      "[86]\ttrain's multi_logloss: 0.713165\tvalid's multi_logloss: 1.14904\n",
      "[87]\ttrain's multi_logloss: 0.711507\tvalid's multi_logloss: 1.14471\n",
      "[88]\ttrain's multi_logloss: 0.710569\tvalid's multi_logloss: 1.14321\n",
      "[89]\ttrain's multi_logloss: 0.708155\tvalid's multi_logloss: 1.14625\n",
      "[90]\ttrain's multi_logloss: 0.706233\tvalid's multi_logloss: 1.15217\n",
      "[91]\ttrain's multi_logloss: 0.704536\tvalid's multi_logloss: 1.1586\n",
      "[92]\ttrain's multi_logloss: 0.702746\tvalid's multi_logloss: 1.16561\n",
      "[93]\ttrain's multi_logloss: 0.700475\tvalid's multi_logloss: 1.16389\n",
      "[94]\ttrain's multi_logloss: 0.698439\tvalid's multi_logloss: 1.16238\n",
      "[95]\ttrain's multi_logloss: 0.696618\tvalid's multi_logloss: 1.16106\n",
      "[96]\ttrain's multi_logloss: 0.695122\tvalid's multi_logloss: 1.16064\n",
      "[97]\ttrain's multi_logloss: 0.693449\tvalid's multi_logloss: 1.15781\n",
      "[98]\ttrain's multi_logloss: 0.69187\tvalid's multi_logloss: 1.15635\n",
      "[99]\ttrain's multi_logloss: 0.690412\tvalid's multi_logloss: 1.15553\n",
      "[100]\ttrain's multi_logloss: 0.688555\tvalid's multi_logloss: 1.15519\n",
      "[101]\ttrain's multi_logloss: 0.686551\tvalid's multi_logloss: 1.15613\n",
      "[102]\ttrain's multi_logloss: 0.68477\tvalid's multi_logloss: 1.15725\n",
      "[103]\ttrain's multi_logloss: 0.683965\tvalid's multi_logloss: 1.15585\n",
      "[104]\ttrain's multi_logloss: 0.68232\tvalid's multi_logloss: 1.1564\n",
      "Early stopping, best iteration is:\n",
      "[4]\ttrain's multi_logloss: 1.021\tvalid's multi_logloss: 1.04096\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.45454545454545453\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.000000\n",
      "1   feature2    0.447703\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5    0.392054\n",
      "5   feature6    0.000000\n",
      "6   feature7    0.000000\n",
      "7   feature8    0.000000\n",
      "8   feature9    0.160243\n",
      "9  feature10    0.000000\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08596\tvalid's my_multiclass_logloss: 1.08437\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07566\tvalid's my_multiclass_logloss: 1.07755\n",
      "[3]\ttrain's my_multiclass_logloss: 1.06136\tvalid's my_multiclass_logloss: 1.06949\n",
      "[4]\ttrain's my_multiclass_logloss: 1.04959\tvalid's my_multiclass_logloss: 1.06797\n",
      "[5]\ttrain's my_multiclass_logloss: 1.04102\tvalid's my_multiclass_logloss: 1.06694\n",
      "[6]\ttrain's my_multiclass_logloss: 1.03318\tvalid's my_multiclass_logloss: 1.06648\n",
      "[7]\ttrain's my_multiclass_logloss: 1.02628\tvalid's my_multiclass_logloss: 1.06902\n",
      "[8]\ttrain's my_multiclass_logloss: 1.01962\tvalid's my_multiclass_logloss: 1.07246\n",
      "[9]\ttrain's my_multiclass_logloss: 1.01321\tvalid's my_multiclass_logloss: 1.07181\n",
      "[10]\ttrain's my_multiclass_logloss: 1.00683\tvalid's my_multiclass_logloss: 1.07378\n",
      "[11]\ttrain's my_multiclass_logloss: 1.00166\tvalid's my_multiclass_logloss: 1.07564\n",
      "[12]\ttrain's my_multiclass_logloss: 0.995909\tvalid's my_multiclass_logloss: 1.08128\n",
      "[13]\ttrain's my_multiclass_logloss: 0.987635\tvalid's my_multiclass_logloss: 1.08314\n",
      "[14]\ttrain's my_multiclass_logloss: 0.978616\tvalid's my_multiclass_logloss: 1.08448\n",
      "[15]\ttrain's my_multiclass_logloss: 0.969485\tvalid's my_multiclass_logloss: 1.08278\n",
      "[16]\ttrain's my_multiclass_logloss: 0.960669\tvalid's my_multiclass_logloss: 1.09031\n",
      "[17]\ttrain's my_multiclass_logloss: 0.953266\tvalid's my_multiclass_logloss: 1.09355\n",
      "[18]\ttrain's my_multiclass_logloss: 0.946677\tvalid's my_multiclass_logloss: 1.10189\n",
      "[19]\ttrain's my_multiclass_logloss: 0.942285\tvalid's my_multiclass_logloss: 1.10167\n",
      "[20]\ttrain's my_multiclass_logloss: 0.938683\tvalid's my_multiclass_logloss: 1.10603\n",
      "[21]\ttrain's my_multiclass_logloss: 0.932354\tvalid's my_multiclass_logloss: 1.10055\n",
      "[22]\ttrain's my_multiclass_logloss: 0.926768\tvalid's my_multiclass_logloss: 1.0984\n",
      "[23]\ttrain's my_multiclass_logloss: 0.92101\tvalid's my_multiclass_logloss: 1.09649\n",
      "[24]\ttrain's my_multiclass_logloss: 0.91815\tvalid's my_multiclass_logloss: 1.09765\n",
      "[25]\ttrain's my_multiclass_logloss: 0.911782\tvalid's my_multiclass_logloss: 1.09948\n",
      "[26]\ttrain's my_multiclass_logloss: 0.905902\tvalid's my_multiclass_logloss: 1.10571\n",
      "[27]\ttrain's my_multiclass_logloss: 0.900523\tvalid's my_multiclass_logloss: 1.11153\n",
      "[28]\ttrain's my_multiclass_logloss: 0.895021\tvalid's my_multiclass_logloss: 1.11483\n",
      "[29]\ttrain's my_multiclass_logloss: 0.894368\tvalid's my_multiclass_logloss: 1.11065\n",
      "[30]\ttrain's my_multiclass_logloss: 0.887004\tvalid's my_multiclass_logloss: 1.1097\n",
      "[31]\ttrain's my_multiclass_logloss: 0.880928\tvalid's my_multiclass_logloss: 1.11103\n",
      "[32]\ttrain's my_multiclass_logloss: 0.877501\tvalid's my_multiclass_logloss: 1.10466\n",
      "[33]\ttrain's my_multiclass_logloss: 0.874403\tvalid's my_multiclass_logloss: 1.11158\n",
      "[34]\ttrain's my_multiclass_logloss: 0.872521\tvalid's my_multiclass_logloss: 1.11837\n",
      "[35]\ttrain's my_multiclass_logloss: 0.868863\tvalid's my_multiclass_logloss: 1.12304\n",
      "[36]\ttrain's my_multiclass_logloss: 0.865483\tvalid's my_multiclass_logloss: 1.12397\n",
      "[37]\ttrain's my_multiclass_logloss: 0.862328\tvalid's my_multiclass_logloss: 1.11866\n",
      "[38]\ttrain's my_multiclass_logloss: 0.859313\tvalid's my_multiclass_logloss: 1.11842\n",
      "[39]\ttrain's my_multiclass_logloss: 0.855923\tvalid's my_multiclass_logloss: 1.12495\n",
      "[40]\ttrain's my_multiclass_logloss: 0.853604\tvalid's my_multiclass_logloss: 1.12533\n",
      "[41]\ttrain's my_multiclass_logloss: 0.850383\tvalid's my_multiclass_logloss: 1.13037\n",
      "[42]\ttrain's my_multiclass_logloss: 0.847462\tvalid's my_multiclass_logloss: 1.13549\n",
      "[43]\ttrain's my_multiclass_logloss: 0.844735\tvalid's my_multiclass_logloss: 1.13088\n",
      "[44]\ttrain's my_multiclass_logloss: 0.842771\tvalid's my_multiclass_logloss: 1.13186\n",
      "[45]\ttrain's my_multiclass_logloss: 0.840155\tvalid's my_multiclass_logloss: 1.13318\n",
      "[46]\ttrain's my_multiclass_logloss: 0.837595\tvalid's my_multiclass_logloss: 1.13135\n",
      "[47]\ttrain's my_multiclass_logloss: 0.835647\tvalid's my_multiclass_logloss: 1.13314\n",
      "[48]\ttrain's my_multiclass_logloss: 0.833703\tvalid's my_multiclass_logloss: 1.13187\n",
      "[49]\ttrain's my_multiclass_logloss: 0.831019\tvalid's my_multiclass_logloss: 1.13181\n",
      "[50]\ttrain's my_multiclass_logloss: 0.828608\tvalid's my_multiclass_logloss: 1.13191\n",
      "[51]\ttrain's my_multiclass_logloss: 0.825695\tvalid's my_multiclass_logloss: 1.13158\n",
      "[52]\ttrain's my_multiclass_logloss: 0.823271\tvalid's my_multiclass_logloss: 1.12965\n",
      "[53]\ttrain's my_multiclass_logloss: 0.819313\tvalid's my_multiclass_logloss: 1.13104\n",
      "[54]\ttrain's my_multiclass_logloss: 0.816731\tvalid's my_multiclass_logloss: 1.13162\n",
      "[55]\ttrain's my_multiclass_logloss: 0.814574\tvalid's my_multiclass_logloss: 1.13471\n",
      "[56]\ttrain's my_multiclass_logloss: 0.813523\tvalid's my_multiclass_logloss: 1.13706\n",
      "[57]\ttrain's my_multiclass_logloss: 0.811269\tvalid's my_multiclass_logloss: 1.13478\n",
      "[58]\ttrain's my_multiclass_logloss: 0.808208\tvalid's my_multiclass_logloss: 1.13294\n",
      "[59]\ttrain's my_multiclass_logloss: 0.805964\tvalid's my_multiclass_logloss: 1.13153\n",
      "[60]\ttrain's my_multiclass_logloss: 0.80338\tvalid's my_multiclass_logloss: 1.13012\n",
      "[61]\ttrain's my_multiclass_logloss: 0.799708\tvalid's my_multiclass_logloss: 1.12612\n",
      "[62]\ttrain's my_multiclass_logloss: 0.798466\tvalid's my_multiclass_logloss: 1.12766\n",
      "[63]\ttrain's my_multiclass_logloss: 0.796245\tvalid's my_multiclass_logloss: 1.12984\n",
      "[64]\ttrain's my_multiclass_logloss: 0.793624\tvalid's my_multiclass_logloss: 1.1286\n",
      "[65]\ttrain's my_multiclass_logloss: 0.791845\tvalid's my_multiclass_logloss: 1.12884\n",
      "[66]\ttrain's my_multiclass_logloss: 0.790366\tvalid's my_multiclass_logloss: 1.12936\n",
      "[67]\ttrain's my_multiclass_logloss: 0.789158\tvalid's my_multiclass_logloss: 1.13012\n",
      "[68]\ttrain's my_multiclass_logloss: 0.787559\tvalid's my_multiclass_logloss: 1.1331\n",
      "[69]\ttrain's my_multiclass_logloss: 0.78578\tvalid's my_multiclass_logloss: 1.14018\n",
      "[70]\ttrain's my_multiclass_logloss: 0.784134\tvalid's my_multiclass_logloss: 1.14617\n",
      "[71]\ttrain's my_multiclass_logloss: 0.782835\tvalid's my_multiclass_logloss: 1.15312\n",
      "[72]\ttrain's my_multiclass_logloss: 0.781801\tvalid's my_multiclass_logloss: 1.15996\n",
      "[73]\ttrain's my_multiclass_logloss: 0.77953\tvalid's my_multiclass_logloss: 1.16223\n",
      "[74]\ttrain's my_multiclass_logloss: 0.777597\tvalid's my_multiclass_logloss: 1.16207\n",
      "[75]\ttrain's my_multiclass_logloss: 0.775708\tvalid's my_multiclass_logloss: 1.16845\n",
      "[76]\ttrain's my_multiclass_logloss: 0.773991\tvalid's my_multiclass_logloss: 1.17482\n",
      "[77]\ttrain's my_multiclass_logloss: 0.772317\tvalid's my_multiclass_logloss: 1.17304\n",
      "[78]\ttrain's my_multiclass_logloss: 0.770468\tvalid's my_multiclass_logloss: 1.17649\n",
      "[79]\ttrain's my_multiclass_logloss: 0.76861\tvalid's my_multiclass_logloss: 1.17216\n",
      "[80]\ttrain's my_multiclass_logloss: 0.767131\tvalid's my_multiclass_logloss: 1.17557\n",
      "[81]\ttrain's my_multiclass_logloss: 0.764539\tvalid's my_multiclass_logloss: 1.17533\n",
      "[82]\ttrain's my_multiclass_logloss: 0.762573\tvalid's my_multiclass_logloss: 1.17898\n",
      "[83]\ttrain's my_multiclass_logloss: 0.760672\tvalid's my_multiclass_logloss: 1.18206\n",
      "[84]\ttrain's my_multiclass_logloss: 0.759303\tvalid's my_multiclass_logloss: 1.18857\n",
      "[85]\ttrain's my_multiclass_logloss: 0.75785\tvalid's my_multiclass_logloss: 1.18548\n",
      "[86]\ttrain's my_multiclass_logloss: 0.756779\tvalid's my_multiclass_logloss: 1.18576\n",
      "[87]\ttrain's my_multiclass_logloss: 0.755929\tvalid's my_multiclass_logloss: 1.18607\n",
      "[88]\ttrain's my_multiclass_logloss: 0.7544\tvalid's my_multiclass_logloss: 1.1816\n",
      "[89]\ttrain's my_multiclass_logloss: 0.752363\tvalid's my_multiclass_logloss: 1.18253\n",
      "[90]\ttrain's my_multiclass_logloss: 0.750647\tvalid's my_multiclass_logloss: 1.1837\n",
      "[91]\ttrain's my_multiclass_logloss: 0.749252\tvalid's my_multiclass_logloss: 1.18256\n",
      "[92]\ttrain's my_multiclass_logloss: 0.748087\tvalid's my_multiclass_logloss: 1.18205\n",
      "[93]\ttrain's my_multiclass_logloss: 0.746509\tvalid's my_multiclass_logloss: 1.18127\n",
      "[94]\ttrain's my_multiclass_logloss: 0.745859\tvalid's my_multiclass_logloss: 1.18005\n",
      "[95]\ttrain's my_multiclass_logloss: 0.744326\tvalid's my_multiclass_logloss: 1.18135\n",
      "[96]\ttrain's my_multiclass_logloss: 0.743466\tvalid's my_multiclass_logloss: 1.17972\n",
      "[97]\ttrain's my_multiclass_logloss: 0.741546\tvalid's my_multiclass_logloss: 1.18178\n",
      "[98]\ttrain's my_multiclass_logloss: 0.739858\tvalid's my_multiclass_logloss: 1.18014\n",
      "[99]\ttrain's my_multiclass_logloss: 0.738695\tvalid's my_multiclass_logloss: 1.18356\n",
      "[100]\ttrain's my_multiclass_logloss: 0.737265\tvalid's my_multiclass_logloss: 1.18431\n",
      "[101]\ttrain's my_multiclass_logloss: 0.73593\tvalid's my_multiclass_logloss: 1.18561\n",
      "[102]\ttrain's my_multiclass_logloss: 0.733903\tvalid's my_multiclass_logloss: 1.19008\n",
      "[103]\ttrain's my_multiclass_logloss: 0.732934\tvalid's my_multiclass_logloss: 1.18842\n",
      "[104]\ttrain's my_multiclass_logloss: 0.731362\tvalid's my_multiclass_logloss: 1.19302\n",
      "[105]\ttrain's my_multiclass_logloss: 0.730344\tvalid's my_multiclass_logloss: 1.19541\n",
      "[106]\ttrain's my_multiclass_logloss: 0.729606\tvalid's my_multiclass_logloss: 1.19787\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttrain's my_multiclass_logloss: 1.03318\tvalid's my_multiclass_logloss: 1.06648\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.5454545454545454\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.000000\n",
      "1   feature2    0.447703\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5    0.392054\n",
      "5   feature6    0.000000\n",
      "6   feature7    0.000000\n",
      "7   feature8    0.000000\n",
      "8   feature9    0.160243\n",
      "9  feature10    0.000000\n",
      "     feature  importance\n",
      "0   feature1    0.000000\n",
      "1   feature2    0.513374\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5    0.058499\n",
      "5   feature6    0.000000\n",
      "6   feature7    0.000000\n",
      "7   feature8    0.228017\n",
      "8   feature9    0.109581\n",
      "9  feature10    0.090529\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.000000\n",
      "1   feature2    0.085462\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5   -0.434071\n",
      "5   feature6    0.000000\n",
      "6   feature7    0.000000\n",
      "7   feature8    0.296729\n",
      "8   feature9   -0.065929\n",
      "9  feature10    0.117810\n",
      "-------------------- 7 --------------------\n",
      "(97, 10) (97,)\n",
      "(11, 10) (11,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05454\tvalid's multi_logloss: 1.066\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.04705\tvalid's multi_logloss: 1.06051\n",
      "[3]\ttrain's multi_logloss: 1.03915\tvalid's multi_logloss: 1.05938\n",
      "[4]\ttrain's multi_logloss: 1.03256\tvalid's multi_logloss: 1.05455\n",
      "[5]\ttrain's multi_logloss: 1.02281\tvalid's multi_logloss: 1.04524\n",
      "[6]\ttrain's multi_logloss: 1.01589\tvalid's multi_logloss: 1.04061\n",
      "[7]\ttrain's multi_logloss: 1.00681\tvalid's multi_logloss: 1.03982\n",
      "[8]\ttrain's multi_logloss: 0.998258\tvalid's multi_logloss: 1.03152\n",
      "[9]\ttrain's multi_logloss: 0.990321\tvalid's multi_logloss: 1.03391\n",
      "[10]\ttrain's multi_logloss: 0.983363\tvalid's multi_logloss: 1.03836\n",
      "[11]\ttrain's multi_logloss: 0.976271\tvalid's multi_logloss: 1.03501\n",
      "[12]\ttrain's multi_logloss: 0.969779\tvalid's multi_logloss: 1.03215\n",
      "[13]\ttrain's multi_logloss: 0.963644\tvalid's multi_logloss: 1.04076\n",
      "[14]\ttrain's multi_logloss: 0.958813\tvalid's multi_logloss: 1.04455\n",
      "[15]\ttrain's multi_logloss: 0.954396\tvalid's multi_logloss: 1.04853\n",
      "[16]\ttrain's multi_logloss: 0.947968\tvalid's multi_logloss: 1.05172\n",
      "[17]\ttrain's multi_logloss: 0.93875\tvalid's multi_logloss: 1.0458\n",
      "[18]\ttrain's multi_logloss: 0.930714\tvalid's multi_logloss: 1.03925\n",
      "[19]\ttrain's multi_logloss: 0.923529\tvalid's multi_logloss: 1.0397\n",
      "[20]\ttrain's multi_logloss: 0.916148\tvalid's multi_logloss: 1.03668\n",
      "[21]\ttrain's multi_logloss: 0.910465\tvalid's multi_logloss: 1.03517\n",
      "[22]\ttrain's multi_logloss: 0.90522\tvalid's multi_logloss: 1.03455\n",
      "[23]\ttrain's multi_logloss: 0.901575\tvalid's multi_logloss: 1.03007\n",
      "[24]\ttrain's multi_logloss: 0.896169\tvalid's multi_logloss: 1.02898\n",
      "[25]\ttrain's multi_logloss: 0.890819\tvalid's multi_logloss: 1.02901\n",
      "[26]\ttrain's multi_logloss: 0.886447\tvalid's multi_logloss: 1.02557\n",
      "[27]\ttrain's multi_logloss: 0.881491\tvalid's multi_logloss: 1.02848\n",
      "[28]\ttrain's multi_logloss: 0.87676\tvalid's multi_logloss: 1.03604\n",
      "[29]\ttrain's multi_logloss: 0.872128\tvalid's multi_logloss: 1.0368\n",
      "[30]\ttrain's multi_logloss: 0.868855\tvalid's multi_logloss: 1.0361\n",
      "[31]\ttrain's multi_logloss: 0.864851\tvalid's multi_logloss: 1.03723\n",
      "[32]\ttrain's multi_logloss: 0.862428\tvalid's multi_logloss: 1.03758\n",
      "[33]\ttrain's multi_logloss: 0.857004\tvalid's multi_logloss: 1.04095\n",
      "[34]\ttrain's multi_logloss: 0.851366\tvalid's multi_logloss: 1.03929\n",
      "[35]\ttrain's multi_logloss: 0.846105\tvalid's multi_logloss: 1.04191\n",
      "[36]\ttrain's multi_logloss: 0.841056\tvalid's multi_logloss: 1.04067\n",
      "[37]\ttrain's multi_logloss: 0.83744\tvalid's multi_logloss: 1.03954\n",
      "[38]\ttrain's multi_logloss: 0.833523\tvalid's multi_logloss: 1.03589\n",
      "[39]\ttrain's multi_logloss: 0.830024\tvalid's multi_logloss: 1.03435\n",
      "[40]\ttrain's multi_logloss: 0.827112\tvalid's multi_logloss: 1.03301\n",
      "[41]\ttrain's multi_logloss: 0.823294\tvalid's multi_logloss: 1.03869\n",
      "[42]\ttrain's multi_logloss: 0.820002\tvalid's multi_logloss: 1.04617\n",
      "[43]\ttrain's multi_logloss: 0.818396\tvalid's multi_logloss: 1.04655\n",
      "[44]\ttrain's multi_logloss: 0.814861\tvalid's multi_logloss: 1.05023\n",
      "[45]\ttrain's multi_logloss: 0.813124\tvalid's multi_logloss: 1.04724\n",
      "[46]\ttrain's multi_logloss: 0.8105\tvalid's multi_logloss: 1.04831\n",
      "[47]\ttrain's multi_logloss: 0.808196\tvalid's multi_logloss: 1.0449\n",
      "[48]\ttrain's multi_logloss: 0.80601\tvalid's multi_logloss: 1.04643\n",
      "[49]\ttrain's multi_logloss: 0.801793\tvalid's multi_logloss: 1.043\n",
      "[50]\ttrain's multi_logloss: 0.797932\tvalid's multi_logloss: 1.04435\n",
      "[51]\ttrain's multi_logloss: 0.794985\tvalid's multi_logloss: 1.04771\n",
      "[52]\ttrain's multi_logloss: 0.792996\tvalid's multi_logloss: 1.05199\n",
      "[53]\ttrain's multi_logloss: 0.789151\tvalid's multi_logloss: 1.04491\n",
      "[54]\ttrain's multi_logloss: 0.785972\tvalid's multi_logloss: 1.04026\n",
      "[55]\ttrain's multi_logloss: 0.783059\tvalid's multi_logloss: 1.03596\n",
      "[56]\ttrain's multi_logloss: 0.780599\tvalid's multi_logloss: 1.03013\n",
      "[57]\ttrain's multi_logloss: 0.777701\tvalid's multi_logloss: 1.03432\n",
      "[58]\ttrain's multi_logloss: 0.774068\tvalid's multi_logloss: 1.04121\n",
      "[59]\ttrain's multi_logloss: 0.771616\tvalid's multi_logloss: 1.04562\n",
      "[60]\ttrain's multi_logloss: 0.767652\tvalid's multi_logloss: 1.04206\n",
      "[61]\ttrain's multi_logloss: 0.763443\tvalid's multi_logloss: 1.04394\n",
      "[62]\ttrain's multi_logloss: 0.759573\tvalid's multi_logloss: 1.04597\n",
      "[63]\ttrain's multi_logloss: 0.756257\tvalid's multi_logloss: 1.04691\n",
      "[64]\ttrain's multi_logloss: 0.752903\tvalid's multi_logloss: 1.04921\n",
      "[65]\ttrain's multi_logloss: 0.750009\tvalid's multi_logloss: 1.05277\n",
      "[66]\ttrain's multi_logloss: 0.747456\tvalid's multi_logloss: 1.05252\n",
      "[67]\ttrain's multi_logloss: 0.744724\tvalid's multi_logloss: 1.0523\n",
      "[68]\ttrain's multi_logloss: 0.74194\tvalid's multi_logloss: 1.05131\n",
      "[69]\ttrain's multi_logloss: 0.740635\tvalid's multi_logloss: 1.05526\n",
      "[70]\ttrain's multi_logloss: 0.738276\tvalid's multi_logloss: 1.06029\n",
      "[71]\ttrain's multi_logloss: 0.736438\tvalid's multi_logloss: 1.06503\n",
      "[72]\ttrain's multi_logloss: 0.734742\tvalid's multi_logloss: 1.06976\n",
      "[73]\ttrain's multi_logloss: 0.732337\tvalid's multi_logloss: 1.07228\n",
      "[74]\ttrain's multi_logloss: 0.729452\tvalid's multi_logloss: 1.07963\n",
      "[75]\ttrain's multi_logloss: 0.726864\tvalid's multi_logloss: 1.08686\n",
      "[76]\ttrain's multi_logloss: 0.725114\tvalid's multi_logloss: 1.08936\n",
      "[77]\ttrain's multi_logloss: 0.723114\tvalid's multi_logloss: 1.09614\n",
      "[78]\ttrain's multi_logloss: 0.721298\tvalid's multi_logloss: 1.10224\n",
      "[79]\ttrain's multi_logloss: 0.719356\tvalid's multi_logloss: 1.09907\n",
      "[80]\ttrain's multi_logloss: 0.717575\tvalid's multi_logloss: 1.10519\n",
      "[81]\ttrain's multi_logloss: 0.714711\tvalid's multi_logloss: 1.10395\n",
      "[82]\ttrain's multi_logloss: 0.712092\tvalid's multi_logloss: 1.10542\n",
      "[83]\ttrain's multi_logloss: 0.709731\tvalid's multi_logloss: 1.10365\n",
      "[84]\ttrain's multi_logloss: 0.707583\tvalid's multi_logloss: 1.10204\n",
      "[85]\ttrain's multi_logloss: 0.704889\tvalid's multi_logloss: 1.10712\n",
      "[86]\ttrain's multi_logloss: 0.703089\tvalid's multi_logloss: 1.10951\n",
      "[87]\ttrain's multi_logloss: 0.700803\tvalid's multi_logloss: 1.1191\n",
      "[88]\ttrain's multi_logloss: 0.698811\tvalid's multi_logloss: 1.12141\n",
      "[89]\ttrain's multi_logloss: 0.696849\tvalid's multi_logloss: 1.12414\n",
      "[90]\ttrain's multi_logloss: 0.695146\tvalid's multi_logloss: 1.12622\n",
      "[91]\ttrain's multi_logloss: 0.693136\tvalid's multi_logloss: 1.12794\n",
      "[92]\ttrain's multi_logloss: 0.691345\tvalid's multi_logloss: 1.12979\n",
      "[93]\ttrain's multi_logloss: 0.689334\tvalid's multi_logloss: 1.12682\n",
      "[94]\ttrain's multi_logloss: 0.687682\tvalid's multi_logloss: 1.13207\n",
      "[95]\ttrain's multi_logloss: 0.685569\tvalid's multi_logloss: 1.13063\n",
      "[96]\ttrain's multi_logloss: 0.683622\tvalid's multi_logloss: 1.12934\n",
      "[97]\ttrain's multi_logloss: 0.681912\tvalid's multi_logloss: 1.12657\n",
      "[98]\ttrain's multi_logloss: 0.680625\tvalid's multi_logloss: 1.12952\n",
      "[99]\ttrain's multi_logloss: 0.67959\tvalid's multi_logloss: 1.13328\n",
      "[100]\ttrain's multi_logloss: 0.678763\tvalid's multi_logloss: 1.13897\n",
      "[101]\ttrain's multi_logloss: 0.677438\tvalid's multi_logloss: 1.14433\n",
      "[102]\ttrain's multi_logloss: 0.676364\tvalid's multi_logloss: 1.1489\n",
      "[103]\ttrain's multi_logloss: 0.675045\tvalid's multi_logloss: 1.15461\n",
      "[104]\ttrain's multi_logloss: 0.674587\tvalid's multi_logloss: 1.15967\n",
      "[105]\ttrain's multi_logloss: 0.673071\tvalid's multi_logloss: 1.16831\n",
      "[106]\ttrain's multi_logloss: 0.671485\tvalid's multi_logloss: 1.16486\n",
      "[107]\ttrain's multi_logloss: 0.670103\tvalid's multi_logloss: 1.16956\n",
      "[108]\ttrain's multi_logloss: 0.668771\tvalid's multi_logloss: 1.16697\n",
      "[109]\ttrain's multi_logloss: 0.667776\tvalid's multi_logloss: 1.17014\n",
      "[110]\ttrain's multi_logloss: 0.666813\tvalid's multi_logloss: 1.17416\n",
      "[111]\ttrain's multi_logloss: 0.665773\tvalid's multi_logloss: 1.17755\n",
      "[112]\ttrain's multi_logloss: 0.664723\tvalid's multi_logloss: 1.18112\n",
      "[113]\ttrain's multi_logloss: 0.662841\tvalid's multi_logloss: 1.1844\n",
      "[114]\ttrain's multi_logloss: 0.66147\tvalid's multi_logloss: 1.18963\n",
      "[115]\ttrain's multi_logloss: 0.660013\tvalid's multi_logloss: 1.19918\n",
      "[116]\ttrain's multi_logloss: 0.658581\tvalid's multi_logloss: 1.2043\n",
      "[117]\ttrain's multi_logloss: 0.656738\tvalid's multi_logloss: 1.21235\n",
      "[118]\ttrain's multi_logloss: 0.655083\tvalid's multi_logloss: 1.21316\n",
      "[119]\ttrain's multi_logloss: 0.653367\tvalid's multi_logloss: 1.21809\n",
      "[120]\ttrain's multi_logloss: 0.651976\tvalid's multi_logloss: 1.22619\n",
      "[121]\ttrain's multi_logloss: 0.650081\tvalid's multi_logloss: 1.22102\n",
      "[122]\ttrain's multi_logloss: 0.64894\tvalid's multi_logloss: 1.21721\n",
      "[123]\ttrain's multi_logloss: 0.647451\tvalid's multi_logloss: 1.21534\n",
      "[124]\ttrain's multi_logloss: 0.646582\tvalid's multi_logloss: 1.21268\n",
      "[125]\ttrain's multi_logloss: 0.645451\tvalid's multi_logloss: 1.21072\n",
      "[126]\ttrain's multi_logloss: 0.644414\tvalid's multi_logloss: 1.21803\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttrain's multi_logloss: 0.886447\tvalid's multi_logloss: 1.02557\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.45454545454545453\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.109411\n",
      "1   feature2    0.172312\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5    0.119312\n",
      "5   feature6    0.000000\n",
      "6   feature7    0.156860\n",
      "7   feature8    0.131356\n",
      "8   feature9    0.247805\n",
      "9  feature10    0.062944\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08662\tvalid's my_multiclass_logloss: 1.09456\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07884\tvalid's my_multiclass_logloss: 1.08526\n",
      "[3]\ttrain's my_multiclass_logloss: 1.06894\tvalid's my_multiclass_logloss: 1.0826\n",
      "[4]\ttrain's my_multiclass_logloss: 1.06021\tvalid's my_multiclass_logloss: 1.08077\n",
      "[5]\ttrain's my_multiclass_logloss: 1.04976\tvalid's my_multiclass_logloss: 1.07815\n",
      "[6]\ttrain's my_multiclass_logloss: 1.03778\tvalid's my_multiclass_logloss: 1.06526\n",
      "[7]\ttrain's my_multiclass_logloss: 1.02935\tvalid's my_multiclass_logloss: 1.06803\n",
      "[8]\ttrain's my_multiclass_logloss: 1.01942\tvalid's my_multiclass_logloss: 1.05378\n",
      "[9]\ttrain's my_multiclass_logloss: 1.01055\tvalid's my_multiclass_logloss: 1.03976\n",
      "[10]\ttrain's my_multiclass_logloss: 1.00212\tvalid's my_multiclass_logloss: 1.03532\n",
      "[11]\ttrain's my_multiclass_logloss: 0.994826\tvalid's my_multiclass_logloss: 1.02339\n",
      "[12]\ttrain's my_multiclass_logloss: 0.987649\tvalid's my_multiclass_logloss: 1.02021\n",
      "[13]\ttrain's my_multiclass_logloss: 0.982969\tvalid's my_multiclass_logloss: 1.02038\n",
      "[14]\ttrain's my_multiclass_logloss: 0.976734\tvalid's my_multiclass_logloss: 1.02758\n",
      "[15]\ttrain's my_multiclass_logloss: 0.96984\tvalid's my_multiclass_logloss: 1.02629\n",
      "[16]\ttrain's my_multiclass_logloss: 0.963152\tvalid's my_multiclass_logloss: 1.02475\n",
      "[17]\ttrain's my_multiclass_logloss: 0.954242\tvalid's my_multiclass_logloss: 1.01964\n",
      "[18]\ttrain's my_multiclass_logloss: 0.946673\tvalid's my_multiclass_logloss: 1.01237\n",
      "[19]\ttrain's my_multiclass_logloss: 0.939275\tvalid's my_multiclass_logloss: 1.00809\n",
      "[20]\ttrain's my_multiclass_logloss: 0.932852\tvalid's my_multiclass_logloss: 1.00348\n",
      "[21]\ttrain's my_multiclass_logloss: 0.925788\tvalid's my_multiclass_logloss: 0.997144\n",
      "[22]\ttrain's my_multiclass_logloss: 0.919402\tvalid's my_multiclass_logloss: 0.991427\n",
      "[23]\ttrain's my_multiclass_logloss: 0.91363\tvalid's my_multiclass_logloss: 0.986275\n",
      "[24]\ttrain's my_multiclass_logloss: 0.90928\tvalid's my_multiclass_logloss: 0.984714\n",
      "[25]\ttrain's my_multiclass_logloss: 0.904421\tvalid's my_multiclass_logloss: 0.993613\n",
      "[26]\ttrain's my_multiclass_logloss: 0.898695\tvalid's my_multiclass_logloss: 0.996747\n",
      "[27]\ttrain's my_multiclass_logloss: 0.893465\tvalid's my_multiclass_logloss: 1.0001\n",
      "[28]\ttrain's my_multiclass_logloss: 0.888961\tvalid's my_multiclass_logloss: 1.00794\n",
      "[29]\ttrain's my_multiclass_logloss: 0.884851\tvalid's my_multiclass_logloss: 1.00967\n",
      "[30]\ttrain's my_multiclass_logloss: 0.882381\tvalid's my_multiclass_logloss: 1.01023\n",
      "[31]\ttrain's my_multiclass_logloss: 0.879057\tvalid's my_multiclass_logloss: 1.01243\n",
      "[32]\ttrain's my_multiclass_logloss: 0.87634\tvalid's my_multiclass_logloss: 1.01141\n",
      "[33]\ttrain's my_multiclass_logloss: 0.870024\tvalid's my_multiclass_logloss: 1.01414\n",
      "[34]\ttrain's my_multiclass_logloss: 0.864314\tvalid's my_multiclass_logloss: 1.02054\n",
      "[35]\ttrain's my_multiclass_logloss: 0.859064\tvalid's my_multiclass_logloss: 1.02396\n",
      "[36]\ttrain's my_multiclass_logloss: 0.854683\tvalid's my_multiclass_logloss: 1.02859\n",
      "[37]\ttrain's my_multiclass_logloss: 0.851811\tvalid's my_multiclass_logloss: 1.0307\n",
      "[38]\ttrain's my_multiclass_logloss: 0.849547\tvalid's my_multiclass_logloss: 1.02835\n",
      "[39]\ttrain's my_multiclass_logloss: 0.847121\tvalid's my_multiclass_logloss: 1.02892\n",
      "[40]\ttrain's my_multiclass_logloss: 0.844333\tvalid's my_multiclass_logloss: 1.02831\n",
      "[41]\ttrain's my_multiclass_logloss: 0.84157\tvalid's my_multiclass_logloss: 1.03792\n",
      "[42]\ttrain's my_multiclass_logloss: 0.83876\tvalid's my_multiclass_logloss: 1.03458\n",
      "[43]\ttrain's my_multiclass_logloss: 0.836724\tvalid's my_multiclass_logloss: 1.04228\n",
      "[44]\ttrain's my_multiclass_logloss: 0.834386\tvalid's my_multiclass_logloss: 1.05264\n",
      "[45]\ttrain's my_multiclass_logloss: 0.832167\tvalid's my_multiclass_logloss: 1.0489\n",
      "[46]\ttrain's my_multiclass_logloss: 0.830328\tvalid's my_multiclass_logloss: 1.05259\n",
      "[47]\ttrain's my_multiclass_logloss: 0.828127\tvalid's my_multiclass_logloss: 1.05064\n",
      "[48]\ttrain's my_multiclass_logloss: 0.826426\tvalid's my_multiclass_logloss: 1.05076\n",
      "[49]\ttrain's my_multiclass_logloss: 0.824279\tvalid's my_multiclass_logloss: 1.05661\n",
      "[50]\ttrain's my_multiclass_logloss: 0.822285\tvalid's my_multiclass_logloss: 1.06176\n",
      "[51]\ttrain's my_multiclass_logloss: 0.820464\tvalid's my_multiclass_logloss: 1.07133\n",
      "[52]\ttrain's my_multiclass_logloss: 0.818123\tvalid's my_multiclass_logloss: 1.07879\n",
      "[53]\ttrain's my_multiclass_logloss: 0.815109\tvalid's my_multiclass_logloss: 1.07847\n",
      "[54]\ttrain's my_multiclass_logloss: 0.813451\tvalid's my_multiclass_logloss: 1.08112\n",
      "[55]\ttrain's my_multiclass_logloss: 0.80988\tvalid's my_multiclass_logloss: 1.07916\n",
      "[56]\ttrain's my_multiclass_logloss: 0.808671\tvalid's my_multiclass_logloss: 1.07951\n",
      "[57]\ttrain's my_multiclass_logloss: 0.805582\tvalid's my_multiclass_logloss: 1.08416\n",
      "[58]\ttrain's my_multiclass_logloss: 0.803014\tvalid's my_multiclass_logloss: 1.0891\n",
      "[59]\ttrain's my_multiclass_logloss: 0.800904\tvalid's my_multiclass_logloss: 1.09425\n",
      "[60]\ttrain's my_multiclass_logloss: 0.797295\tvalid's my_multiclass_logloss: 1.08486\n",
      "[61]\ttrain's my_multiclass_logloss: 0.793325\tvalid's my_multiclass_logloss: 1.08771\n",
      "[62]\ttrain's my_multiclass_logloss: 0.790037\tvalid's my_multiclass_logloss: 1.08937\n",
      "[63]\ttrain's my_multiclass_logloss: 0.786568\tvalid's my_multiclass_logloss: 1.09309\n",
      "[64]\ttrain's my_multiclass_logloss: 0.783622\tvalid's my_multiclass_logloss: 1.09619\n",
      "[65]\ttrain's my_multiclass_logloss: 0.780629\tvalid's my_multiclass_logloss: 1.09507\n",
      "[66]\ttrain's my_multiclass_logloss: 0.778355\tvalid's my_multiclass_logloss: 1.09568\n",
      "[67]\ttrain's my_multiclass_logloss: 0.7764\tvalid's my_multiclass_logloss: 1.09646\n",
      "[68]\ttrain's my_multiclass_logloss: 0.77472\tvalid's my_multiclass_logloss: 1.10023\n",
      "[69]\ttrain's my_multiclass_logloss: 0.773143\tvalid's my_multiclass_logloss: 1.10681\n",
      "[70]\ttrain's my_multiclass_logloss: 0.771095\tvalid's my_multiclass_logloss: 1.11179\n",
      "[71]\ttrain's my_multiclass_logloss: 0.769916\tvalid's my_multiclass_logloss: 1.11827\n",
      "[72]\ttrain's my_multiclass_logloss: 0.768707\tvalid's my_multiclass_logloss: 1.11807\n",
      "[73]\ttrain's my_multiclass_logloss: 0.766427\tvalid's my_multiclass_logloss: 1.11983\n",
      "[74]\ttrain's my_multiclass_logloss: 0.765143\tvalid's my_multiclass_logloss: 1.12187\n",
      "[75]\ttrain's my_multiclass_logloss: 0.763006\tvalid's my_multiclass_logloss: 1.12986\n",
      "[76]\ttrain's my_multiclass_logloss: 0.76146\tvalid's my_multiclass_logloss: 1.13274\n",
      "[77]\ttrain's my_multiclass_logloss: 0.759779\tvalid's my_multiclass_logloss: 1.13501\n",
      "[78]\ttrain's my_multiclass_logloss: 0.758423\tvalid's my_multiclass_logloss: 1.14172\n",
      "[79]\ttrain's my_multiclass_logloss: 0.75678\tvalid's my_multiclass_logloss: 1.13823\n",
      "[80]\ttrain's my_multiclass_logloss: 0.755676\tvalid's my_multiclass_logloss: 1.14477\n",
      "[81]\ttrain's my_multiclass_logloss: 0.75391\tvalid's my_multiclass_logloss: 1.14147\n",
      "[82]\ttrain's my_multiclass_logloss: 0.752164\tvalid's my_multiclass_logloss: 1.14026\n",
      "[83]\ttrain's my_multiclass_logloss: 0.750857\tvalid's my_multiclass_logloss: 1.14065\n",
      "[84]\ttrain's my_multiclass_logloss: 0.749446\tvalid's my_multiclass_logloss: 1.14153\n",
      "[85]\ttrain's my_multiclass_logloss: 0.747471\tvalid's my_multiclass_logloss: 1.15192\n",
      "[86]\ttrain's my_multiclass_logloss: 0.745667\tvalid's my_multiclass_logloss: 1.15702\n",
      "[87]\ttrain's my_multiclass_logloss: 0.744598\tvalid's my_multiclass_logloss: 1.16715\n",
      "[88]\ttrain's my_multiclass_logloss: 0.743302\tvalid's my_multiclass_logloss: 1.17426\n",
      "[89]\ttrain's my_multiclass_logloss: 0.741915\tvalid's my_multiclass_logloss: 1.17942\n",
      "[90]\ttrain's my_multiclass_logloss: 0.74067\tvalid's my_multiclass_logloss: 1.1828\n",
      "[91]\ttrain's my_multiclass_logloss: 0.740098\tvalid's my_multiclass_logloss: 1.18589\n",
      "[92]\ttrain's my_multiclass_logloss: 0.73928\tvalid's my_multiclass_logloss: 1.18483\n",
      "[93]\ttrain's my_multiclass_logloss: 0.737304\tvalid's my_multiclass_logloss: 1.18298\n",
      "[94]\ttrain's my_multiclass_logloss: 0.735199\tvalid's my_multiclass_logloss: 1.18056\n",
      "[95]\ttrain's my_multiclass_logloss: 0.733841\tvalid's my_multiclass_logloss: 1.18037\n",
      "[96]\ttrain's my_multiclass_logloss: 0.732509\tvalid's my_multiclass_logloss: 1.18551\n",
      "[97]\ttrain's my_multiclass_logloss: 0.731088\tvalid's my_multiclass_logloss: 1.18376\n",
      "[98]\ttrain's my_multiclass_logloss: 0.729454\tvalid's my_multiclass_logloss: 1.18736\n",
      "[99]\ttrain's my_multiclass_logloss: 0.728225\tvalid's my_multiclass_logloss: 1.19298\n",
      "[100]\ttrain's my_multiclass_logloss: 0.727241\tvalid's my_multiclass_logloss: 1.19826\n",
      "[101]\ttrain's my_multiclass_logloss: 0.72679\tvalid's my_multiclass_logloss: 1.20473\n",
      "[102]\ttrain's my_multiclass_logloss: 0.726523\tvalid's my_multiclass_logloss: 1.21119\n",
      "[103]\ttrain's my_multiclass_logloss: 0.726628\tvalid's my_multiclass_logloss: 1.2143\n",
      "[104]\ttrain's my_multiclass_logloss: 0.726688\tvalid's my_multiclass_logloss: 1.21729\n",
      "[105]\ttrain's my_multiclass_logloss: 0.725926\tvalid's my_multiclass_logloss: 1.21938\n",
      "[106]\ttrain's my_multiclass_logloss: 0.725209\tvalid's my_multiclass_logloss: 1.22168\n",
      "[107]\ttrain's my_multiclass_logloss: 0.724567\tvalid's my_multiclass_logloss: 1.22528\n",
      "[108]\ttrain's my_multiclass_logloss: 0.723488\tvalid's my_multiclass_logloss: 1.22301\n",
      "[109]\ttrain's my_multiclass_logloss: 0.722924\tvalid's my_multiclass_logloss: 1.21968\n",
      "[110]\ttrain's my_multiclass_logloss: 0.722242\tvalid's my_multiclass_logloss: 1.22093\n",
      "[111]\ttrain's my_multiclass_logloss: 0.721713\tvalid's my_multiclass_logloss: 1.22034\n",
      "[112]\ttrain's my_multiclass_logloss: 0.721432\tvalid's my_multiclass_logloss: 1.22065\n",
      "[113]\ttrain's my_multiclass_logloss: 0.720007\tvalid's my_multiclass_logloss: 1.22237\n",
      "[114]\ttrain's my_multiclass_logloss: 0.718871\tvalid's my_multiclass_logloss: 1.22273\n",
      "[115]\ttrain's my_multiclass_logloss: 0.717707\tvalid's my_multiclass_logloss: 1.22443\n",
      "[116]\ttrain's my_multiclass_logloss: 0.716817\tvalid's my_multiclass_logloss: 1.22482\n",
      "[117]\ttrain's my_multiclass_logloss: 0.715709\tvalid's my_multiclass_logloss: 1.23542\n",
      "[118]\ttrain's my_multiclass_logloss: 0.714487\tvalid's my_multiclass_logloss: 1.23776\n",
      "[119]\ttrain's my_multiclass_logloss: 0.713401\tvalid's my_multiclass_logloss: 1.24561\n",
      "[120]\ttrain's my_multiclass_logloss: 0.712229\tvalid's my_multiclass_logloss: 1.25102\n",
      "[121]\ttrain's my_multiclass_logloss: 0.711212\tvalid's my_multiclass_logloss: 1.2523\n",
      "[122]\ttrain's my_multiclass_logloss: 0.710403\tvalid's my_multiclass_logloss: 1.25367\n",
      "[123]\ttrain's my_multiclass_logloss: 0.709171\tvalid's my_multiclass_logloss: 1.25126\n",
      "[124]\ttrain's my_multiclass_logloss: 0.708125\tvalid's my_multiclass_logloss: 1.24846\n",
      "Early stopping, best iteration is:\n",
      "[24]\ttrain's my_multiclass_logloss: 0.90928\tvalid's my_multiclass_logloss: 0.984714\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.5454545454545454\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.109411\n",
      "1   feature2    0.172312\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5    0.119312\n",
      "5   feature6    0.000000\n",
      "6   feature7    0.156860\n",
      "7   feature8    0.131356\n",
      "8   feature9    0.247805\n",
      "9  feature10    0.062944\n",
      "     feature  importance\n",
      "0   feature1    0.100366\n",
      "1   feature2    0.167033\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5    0.066783\n",
      "5   feature6    0.011188\n",
      "6   feature7    0.183639\n",
      "7   feature8    0.189840\n",
      "8   feature9    0.224150\n",
      "9  feature10    0.057001\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1   -0.046893\n",
      "1   feature2   -0.027366\n",
      "2   feature3    0.000000\n",
      "3   feature4    0.000000\n",
      "4   feature5   -0.272307\n",
      "5   feature6    0.058000\n",
      "6   feature7    0.138824\n",
      "7   feature8    0.303176\n",
      "8   feature9   -0.122629\n",
      "9  feature10   -0.030806\n",
      "-------------------- 8 --------------------\n",
      "(98, 10) (98,)\n",
      "(10, 10) (10,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05758\tvalid's multi_logloss: 1.02927\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.05051\tvalid's multi_logloss: 1.02147\n",
      "[3]\ttrain's multi_logloss: 1.04296\tvalid's multi_logloss: 1.0088\n",
      "[4]\ttrain's multi_logloss: 1.0359\tvalid's multi_logloss: 0.996837\n",
      "[5]\ttrain's multi_logloss: 1.02723\tvalid's multi_logloss: 0.995947\n",
      "[6]\ttrain's multi_logloss: 1.02145\tvalid's multi_logloss: 0.997264\n",
      "[7]\ttrain's multi_logloss: 1.01537\tvalid's multi_logloss: 0.995423\n",
      "[8]\ttrain's multi_logloss: 1.00843\tvalid's multi_logloss: 0.992585\n",
      "[9]\ttrain's multi_logloss: 1.00378\tvalid's multi_logloss: 0.979982\n",
      "[10]\ttrain's multi_logloss: 0.997415\tvalid's multi_logloss: 0.974486\n",
      "[11]\ttrain's multi_logloss: 0.990879\tvalid's multi_logloss: 0.968104\n",
      "[12]\ttrain's multi_logloss: 0.985787\tvalid's multi_logloss: 0.959421\n",
      "[13]\ttrain's multi_logloss: 0.980989\tvalid's multi_logloss: 0.950608\n",
      "[14]\ttrain's multi_logloss: 0.975989\tvalid's multi_logloss: 0.947394\n",
      "[15]\ttrain's multi_logloss: 0.971675\tvalid's multi_logloss: 0.948018\n",
      "[16]\ttrain's multi_logloss: 0.967097\tvalid's multi_logloss: 0.940707\n",
      "[17]\ttrain's multi_logloss: 0.961263\tvalid's multi_logloss: 0.93595\n",
      "[18]\ttrain's multi_logloss: 0.956617\tvalid's multi_logloss: 0.93731\n",
      "[19]\ttrain's multi_logloss: 0.951322\tvalid's multi_logloss: 0.93286\n",
      "[20]\ttrain's multi_logloss: 0.946365\tvalid's multi_logloss: 0.928669\n",
      "[21]\ttrain's multi_logloss: 0.94077\tvalid's multi_logloss: 0.925176\n",
      "[22]\ttrain's multi_logloss: 0.936549\tvalid's multi_logloss: 0.927644\n",
      "[23]\ttrain's multi_logloss: 0.933387\tvalid's multi_logloss: 0.924972\n",
      "[24]\ttrain's multi_logloss: 0.928141\tvalid's multi_logloss: 0.924724\n",
      "[25]\ttrain's multi_logloss: 0.923129\tvalid's multi_logloss: 0.916501\n",
      "[26]\ttrain's multi_logloss: 0.918633\tvalid's multi_logloss: 0.908755\n",
      "[27]\ttrain's multi_logloss: 0.913897\tvalid's multi_logloss: 0.90563\n",
      "[28]\ttrain's multi_logloss: 0.910523\tvalid's multi_logloss: 0.900697\n",
      "[29]\ttrain's multi_logloss: 0.90715\tvalid's multi_logloss: 0.896843\n",
      "[30]\ttrain's multi_logloss: 0.903969\tvalid's multi_logloss: 0.893148\n",
      "[31]\ttrain's multi_logloss: 0.900067\tvalid's multi_logloss: 0.888795\n",
      "[32]\ttrain's multi_logloss: 0.897755\tvalid's multi_logloss: 0.887165\n",
      "[33]\ttrain's multi_logloss: 0.893727\tvalid's multi_logloss: 0.886756\n",
      "[34]\ttrain's multi_logloss: 0.888949\tvalid's multi_logloss: 0.8821\n",
      "[35]\ttrain's multi_logloss: 0.884908\tvalid's multi_logloss: 0.883386\n",
      "[36]\ttrain's multi_logloss: 0.880498\tvalid's multi_logloss: 0.879057\n",
      "[37]\ttrain's multi_logloss: 0.875429\tvalid's multi_logloss: 0.876099\n",
      "[38]\ttrain's multi_logloss: 0.872721\tvalid's multi_logloss: 0.873592\n",
      "[39]\ttrain's multi_logloss: 0.870803\tvalid's multi_logloss: 0.873147\n",
      "[40]\ttrain's multi_logloss: 0.868113\tvalid's multi_logloss: 0.873573\n",
      "[41]\ttrain's multi_logloss: 0.865378\tvalid's multi_logloss: 0.877816\n",
      "[42]\ttrain's multi_logloss: 0.863513\tvalid's multi_logloss: 0.875491\n",
      "[43]\ttrain's multi_logloss: 0.861726\tvalid's multi_logloss: 0.874846\n",
      "[44]\ttrain's multi_logloss: 0.860322\tvalid's multi_logloss: 0.8783\n",
      "[45]\ttrain's multi_logloss: 0.857721\tvalid's multi_logloss: 0.878524\n",
      "[46]\ttrain's multi_logloss: 0.853477\tvalid's multi_logloss: 0.875204\n",
      "[47]\ttrain's multi_logloss: 0.851286\tvalid's multi_logloss: 0.875665\n",
      "[48]\ttrain's multi_logloss: 0.847693\tvalid's multi_logloss: 0.875075\n",
      "[49]\ttrain's multi_logloss: 0.843012\tvalid's multi_logloss: 0.871359\n",
      "[50]\ttrain's multi_logloss: 0.839474\tvalid's multi_logloss: 0.874156\n",
      "[51]\ttrain's multi_logloss: 0.835699\tvalid's multi_logloss: 0.870942\n",
      "[52]\ttrain's multi_logloss: 0.833081\tvalid's multi_logloss: 0.872399\n",
      "[53]\ttrain's multi_logloss: 0.830046\tvalid's multi_logloss: 0.871692\n",
      "[54]\ttrain's multi_logloss: 0.828486\tvalid's multi_logloss: 0.871682\n",
      "[55]\ttrain's multi_logloss: 0.8271\tvalid's multi_logloss: 0.871745\n",
      "[56]\ttrain's multi_logloss: 0.825038\tvalid's multi_logloss: 0.871654\n",
      "[57]\ttrain's multi_logloss: 0.822279\tvalid's multi_logloss: 0.866694\n",
      "[58]\ttrain's multi_logloss: 0.819744\tvalid's multi_logloss: 0.862072\n",
      "[59]\ttrain's multi_logloss: 0.817108\tvalid's multi_logloss: 0.858177\n",
      "[60]\ttrain's multi_logloss: 0.813693\tvalid's multi_logloss: 0.852399\n",
      "[61]\ttrain's multi_logloss: 0.811431\tvalid's multi_logloss: 0.853279\n",
      "[62]\ttrain's multi_logloss: 0.809285\tvalid's multi_logloss: 0.851569\n",
      "[63]\ttrain's multi_logloss: 0.80732\tvalid's multi_logloss: 0.850019\n",
      "[64]\ttrain's multi_logloss: 0.80521\tvalid's multi_logloss: 0.844493\n",
      "[65]\ttrain's multi_logloss: 0.803186\tvalid's multi_logloss: 0.843446\n",
      "[66]\ttrain's multi_logloss: 0.799891\tvalid's multi_logloss: 0.838636\n",
      "[67]\ttrain's multi_logloss: 0.797051\tvalid's multi_logloss: 0.834742\n",
      "[68]\ttrain's multi_logloss: 0.794756\tvalid's multi_logloss: 0.829712\n",
      "[69]\ttrain's multi_logloss: 0.792403\tvalid's multi_logloss: 0.827467\n",
      "[70]\ttrain's multi_logloss: 0.78974\tvalid's multi_logloss: 0.822583\n",
      "[71]\ttrain's multi_logloss: 0.787311\tvalid's multi_logloss: 0.818052\n",
      "[72]\ttrain's multi_logloss: 0.785602\tvalid's multi_logloss: 0.821059\n",
      "[73]\ttrain's multi_logloss: 0.782435\tvalid's multi_logloss: 0.81378\n",
      "[74]\ttrain's multi_logloss: 0.780207\tvalid's multi_logloss: 0.808609\n",
      "[75]\ttrain's multi_logloss: 0.777088\tvalid's multi_logloss: 0.804127\n",
      "[76]\ttrain's multi_logloss: 0.775304\tvalid's multi_logloss: 0.802084\n",
      "[77]\ttrain's multi_logloss: 0.773328\tvalid's multi_logloss: 0.803617\n",
      "[78]\ttrain's multi_logloss: 0.770851\tvalid's multi_logloss: 0.800659\n",
      "[79]\ttrain's multi_logloss: 0.768596\tvalid's multi_logloss: 0.797913\n",
      "[80]\ttrain's multi_logloss: 0.766748\tvalid's multi_logloss: 0.79455\n",
      "[81]\ttrain's multi_logloss: 0.764326\tvalid's multi_logloss: 0.791865\n",
      "[82]\ttrain's multi_logloss: 0.762357\tvalid's multi_logloss: 0.791658\n",
      "[83]\ttrain's multi_logloss: 0.760467\tvalid's multi_logloss: 0.788842\n",
      "[84]\ttrain's multi_logloss: 0.757953\tvalid's multi_logloss: 0.786709\n",
      "[85]\ttrain's multi_logloss: 0.755995\tvalid's multi_logloss: 0.787092\n",
      "[86]\ttrain's multi_logloss: 0.754318\tvalid's multi_logloss: 0.785598\n",
      "[87]\ttrain's multi_logloss: 0.752822\tvalid's multi_logloss: 0.786536\n",
      "[88]\ttrain's multi_logloss: 0.751607\tvalid's multi_logloss: 0.785242\n",
      "[89]\ttrain's multi_logloss: 0.749228\tvalid's multi_logloss: 0.786648\n",
      "[90]\ttrain's multi_logloss: 0.748006\tvalid's multi_logloss: 0.786297\n",
      "[91]\ttrain's multi_logloss: 0.746111\tvalid's multi_logloss: 0.786091\n",
      "[92]\ttrain's multi_logloss: 0.744167\tvalid's multi_logloss: 0.785117\n",
      "[93]\ttrain's multi_logloss: 0.741568\tvalid's multi_logloss: 0.783988\n",
      "[94]\ttrain's multi_logloss: 0.738951\tvalid's multi_logloss: 0.78365\n",
      "[95]\ttrain's multi_logloss: 0.736567\tvalid's multi_logloss: 0.783452\n",
      "[96]\ttrain's multi_logloss: 0.73469\tvalid's multi_logloss: 0.788616\n",
      "[97]\ttrain's multi_logloss: 0.733127\tvalid's multi_logloss: 0.784213\n",
      "[98]\ttrain's multi_logloss: 0.732284\tvalid's multi_logloss: 0.783421\n",
      "[99]\ttrain's multi_logloss: 0.730937\tvalid's multi_logloss: 0.781185\n",
      "[100]\ttrain's multi_logloss: 0.730652\tvalid's multi_logloss: 0.781051\n",
      "[101]\ttrain's multi_logloss: 0.728831\tvalid's multi_logloss: 0.779651\n",
      "[102]\ttrain's multi_logloss: 0.726931\tvalid's multi_logloss: 0.781315\n",
      "[103]\ttrain's multi_logloss: 0.725533\tvalid's multi_logloss: 0.783931\n",
      "[104]\ttrain's multi_logloss: 0.72466\tvalid's multi_logloss: 0.780927\n",
      "[105]\ttrain's multi_logloss: 0.723968\tvalid's multi_logloss: 0.781936\n",
      "[106]\ttrain's multi_logloss: 0.723179\tvalid's multi_logloss: 0.784934\n",
      "[107]\ttrain's multi_logloss: 0.722044\tvalid's multi_logloss: 0.783513\n",
      "[108]\ttrain's multi_logloss: 0.721468\tvalid's multi_logloss: 0.786739\n",
      "[109]\ttrain's multi_logloss: 0.720011\tvalid's multi_logloss: 0.786197\n",
      "[110]\ttrain's multi_logloss: 0.718386\tvalid's multi_logloss: 0.786113\n",
      "[111]\ttrain's multi_logloss: 0.716924\tvalid's multi_logloss: 0.786116\n",
      "[112]\ttrain's multi_logloss: 0.715516\tvalid's multi_logloss: 0.786643\n",
      "[113]\ttrain's multi_logloss: 0.71428\tvalid's multi_logloss: 0.788787\n",
      "[114]\ttrain's multi_logloss: 0.713048\tvalid's multi_logloss: 0.787053\n",
      "[115]\ttrain's multi_logloss: 0.712203\tvalid's multi_logloss: 0.784256\n",
      "[116]\ttrain's multi_logloss: 0.711227\tvalid's multi_logloss: 0.786557\n",
      "[117]\ttrain's multi_logloss: 0.709598\tvalid's multi_logloss: 0.785946\n",
      "[118]\ttrain's multi_logloss: 0.708121\tvalid's multi_logloss: 0.786895\n",
      "[119]\ttrain's multi_logloss: 0.706739\tvalid's multi_logloss: 0.787939\n",
      "[120]\ttrain's multi_logloss: 0.705343\tvalid's multi_logloss: 0.789979\n",
      "[121]\ttrain's multi_logloss: 0.703287\tvalid's multi_logloss: 0.786706\n",
      "[122]\ttrain's multi_logloss: 0.701176\tvalid's multi_logloss: 0.781115\n",
      "[123]\ttrain's multi_logloss: 0.699396\tvalid's multi_logloss: 0.778252\n",
      "[124]\ttrain's multi_logloss: 0.697239\tvalid's multi_logloss: 0.774957\n",
      "[125]\ttrain's multi_logloss: 0.695168\tvalid's multi_logloss: 0.776523\n",
      "[126]\ttrain's multi_logloss: 0.693308\tvalid's multi_logloss: 0.778921\n",
      "[127]\ttrain's multi_logloss: 0.692321\tvalid's multi_logloss: 0.779546\n",
      "[128]\ttrain's multi_logloss: 0.691097\tvalid's multi_logloss: 0.780821\n",
      "[129]\ttrain's multi_logloss: 0.689556\tvalid's multi_logloss: 0.774378\n",
      "[130]\ttrain's multi_logloss: 0.68822\tvalid's multi_logloss: 0.770977\n",
      "[131]\ttrain's multi_logloss: 0.686883\tvalid's multi_logloss: 0.764867\n",
      "[132]\ttrain's multi_logloss: 0.686093\tvalid's multi_logloss: 0.763678\n",
      "[133]\ttrain's multi_logloss: 0.685057\tvalid's multi_logloss: 0.76433\n",
      "[134]\ttrain's multi_logloss: 0.683688\tvalid's multi_logloss: 0.761033\n",
      "[135]\ttrain's multi_logloss: 0.682734\tvalid's multi_logloss: 0.759354\n",
      "[136]\ttrain's multi_logloss: 0.681258\tvalid's multi_logloss: 0.755262\n",
      "[137]\ttrain's multi_logloss: 0.680275\tvalid's multi_logloss: 0.752378\n",
      "[138]\ttrain's multi_logloss: 0.679457\tvalid's multi_logloss: 0.749695\n",
      "[139]\ttrain's multi_logloss: 0.678262\tvalid's multi_logloss: 0.753598\n",
      "[140]\ttrain's multi_logloss: 0.677123\tvalid's multi_logloss: 0.759138\n",
      "[141]\ttrain's multi_logloss: 0.675972\tvalid's multi_logloss: 0.753588\n",
      "[142]\ttrain's multi_logloss: 0.674879\tvalid's multi_logloss: 0.749591\n",
      "[143]\ttrain's multi_logloss: 0.673907\tvalid's multi_logloss: 0.745795\n",
      "[144]\ttrain's multi_logloss: 0.673056\tvalid's multi_logloss: 0.746\n",
      "[145]\ttrain's multi_logloss: 0.671636\tvalid's multi_logloss: 0.747996\n",
      "[146]\ttrain's multi_logloss: 0.670583\tvalid's multi_logloss: 0.748142\n",
      "[147]\ttrain's multi_logloss: 0.669683\tvalid's multi_logloss: 0.748777\n",
      "[148]\ttrain's multi_logloss: 0.668698\tvalid's multi_logloss: 0.748282\n",
      "[149]\ttrain's multi_logloss: 0.667673\tvalid's multi_logloss: 0.746381\n",
      "[150]\ttrain's multi_logloss: 0.666722\tvalid's multi_logloss: 0.745531\n",
      "[151]\ttrain's multi_logloss: 0.66587\tvalid's multi_logloss: 0.744761\n",
      "[152]\ttrain's multi_logloss: 0.665108\tvalid's multi_logloss: 0.744066\n",
      "[153]\ttrain's multi_logloss: 0.66463\tvalid's multi_logloss: 0.744479\n",
      "[154]\ttrain's multi_logloss: 0.664091\tvalid's multi_logloss: 0.744261\n",
      "[155]\ttrain's multi_logloss: 0.663909\tvalid's multi_logloss: 0.744415\n",
      "[156]\ttrain's multi_logloss: 0.663652\tvalid's multi_logloss: 0.745005\n",
      "[157]\ttrain's multi_logloss: 0.662448\tvalid's multi_logloss: 0.74778\n",
      "[158]\ttrain's multi_logloss: 0.661863\tvalid's multi_logloss: 0.748678\n",
      "[159]\ttrain's multi_logloss: 0.661389\tvalid's multi_logloss: 0.74961\n",
      "[160]\ttrain's multi_logloss: 0.660567\tvalid's multi_logloss: 0.752323\n",
      "[161]\ttrain's multi_logloss: 0.658902\tvalid's multi_logloss: 0.75095\n",
      "[162]\ttrain's multi_logloss: 0.657329\tvalid's multi_logloss: 0.750419\n",
      "[163]\ttrain's multi_logloss: 0.656079\tvalid's multi_logloss: 0.749979\n",
      "[164]\ttrain's multi_logloss: 0.655075\tvalid's multi_logloss: 0.75075\n",
      "[165]\ttrain's multi_logloss: 0.654644\tvalid's multi_logloss: 0.75197\n",
      "[166]\ttrain's multi_logloss: 0.654087\tvalid's multi_logloss: 0.755325\n",
      "[167]\ttrain's multi_logloss: 0.65365\tvalid's multi_logloss: 0.756746\n",
      "[168]\ttrain's multi_logloss: 0.653155\tvalid's multi_logloss: 0.75602\n",
      "[169]\ttrain's multi_logloss: 0.653029\tvalid's multi_logloss: 0.759971\n",
      "[170]\ttrain's multi_logloss: 0.652794\tvalid's multi_logloss: 0.763225\n",
      "[171]\ttrain's multi_logloss: 0.65266\tvalid's multi_logloss: 0.763057\n",
      "[172]\ttrain's multi_logloss: 0.652806\tvalid's multi_logloss: 0.766926\n",
      "[173]\ttrain's multi_logloss: 0.651255\tvalid's multi_logloss: 0.760694\n",
      "[174]\ttrain's multi_logloss: 0.649813\tvalid's multi_logloss: 0.75893\n",
      "[175]\ttrain's multi_logloss: 0.648252\tvalid's multi_logloss: 0.75864\n",
      "[176]\ttrain's multi_logloss: 0.647054\tvalid's multi_logloss: 0.752738\n",
      "[177]\ttrain's multi_logloss: 0.646085\tvalid's multi_logloss: 0.752402\n",
      "[178]\ttrain's multi_logloss: 0.645192\tvalid's multi_logloss: 0.752128\n",
      "[179]\ttrain's multi_logloss: 0.644793\tvalid's multi_logloss: 0.751123\n",
      "[180]\ttrain's multi_logloss: 0.643968\tvalid's multi_logloss: 0.751229\n",
      "[181]\ttrain's multi_logloss: 0.643293\tvalid's multi_logloss: 0.754335\n",
      "[182]\ttrain's multi_logloss: 0.642446\tvalid's multi_logloss: 0.754606\n",
      "[183]\ttrain's multi_logloss: 0.6419\tvalid's multi_logloss: 0.755985\n",
      "[184]\ttrain's multi_logloss: 0.64116\tvalid's multi_logloss: 0.758106\n",
      "[185]\ttrain's multi_logloss: 0.639594\tvalid's multi_logloss: 0.763584\n",
      "[186]\ttrain's multi_logloss: 0.638216\tvalid's multi_logloss: 0.766805\n",
      "[187]\ttrain's multi_logloss: 0.637221\tvalid's multi_logloss: 0.768624\n",
      "[188]\ttrain's multi_logloss: 0.636086\tvalid's multi_logloss: 0.769964\n",
      "[189]\ttrain's multi_logloss: 0.634728\tvalid's multi_logloss: 0.76812\n",
      "[190]\ttrain's multi_logloss: 0.633523\tvalid's multi_logloss: 0.7664\n",
      "[191]\ttrain's multi_logloss: 0.632439\tvalid's multi_logloss: 0.76407\n",
      "[192]\ttrain's multi_logloss: 0.631766\tvalid's multi_logloss: 0.764074\n",
      "[193]\ttrain's multi_logloss: 0.631627\tvalid's multi_logloss: 0.768244\n",
      "[194]\ttrain's multi_logloss: 0.631657\tvalid's multi_logloss: 0.773861\n",
      "[195]\ttrain's multi_logloss: 0.631201\tvalid's multi_logloss: 0.776979\n",
      "[196]\ttrain's multi_logloss: 0.631383\tvalid's multi_logloss: 0.782419\n",
      "[197]\ttrain's multi_logloss: 0.630873\tvalid's multi_logloss: 0.780684\n",
      "[198]\ttrain's multi_logloss: 0.630298\tvalid's multi_logloss: 0.780478\n",
      "[199]\ttrain's multi_logloss: 0.6296\tvalid's multi_logloss: 0.776906\n",
      "[200]\ttrain's multi_logloss: 0.628982\tvalid's multi_logloss: 0.775422\n",
      "[201]\ttrain's multi_logloss: 0.628485\tvalid's multi_logloss: 0.775976\n",
      "[202]\ttrain's multi_logloss: 0.62799\tvalid's multi_logloss: 0.780138\n",
      "[203]\ttrain's multi_logloss: 0.627637\tvalid's multi_logloss: 0.776797\n",
      "[204]\ttrain's multi_logloss: 0.627287\tvalid's multi_logloss: 0.77764\n",
      "[205]\ttrain's multi_logloss: 0.626876\tvalid's multi_logloss: 0.774583\n",
      "[206]\ttrain's multi_logloss: 0.626279\tvalid's multi_logloss: 0.769943\n",
      "[207]\ttrain's multi_logloss: 0.625832\tvalid's multi_logloss: 0.767123\n",
      "[208]\ttrain's multi_logloss: 0.625413\tvalid's multi_logloss: 0.763662\n",
      "[209]\ttrain's multi_logloss: 0.624862\tvalid's multi_logloss: 0.765886\n",
      "[210]\ttrain's multi_logloss: 0.624385\tvalid's multi_logloss: 0.767922\n",
      "[211]\ttrain's multi_logloss: 0.623599\tvalid's multi_logloss: 0.76724\n",
      "[212]\ttrain's multi_logloss: 0.623133\tvalid's multi_logloss: 0.768889\n",
      "[213]\ttrain's multi_logloss: 0.622594\tvalid's multi_logloss: 0.766385\n",
      "[214]\ttrain's multi_logloss: 0.62169\tvalid's multi_logloss: 0.765853\n",
      "[215]\ttrain's multi_logloss: 0.621322\tvalid's multi_logloss: 0.763485\n",
      "[216]\ttrain's multi_logloss: 0.620521\tvalid's multi_logloss: 0.763073\n",
      "[217]\ttrain's multi_logloss: 0.619702\tvalid's multi_logloss: 0.756217\n",
      "[218]\ttrain's multi_logloss: 0.619242\tvalid's multi_logloss: 0.75263\n",
      "[219]\ttrain's multi_logloss: 0.618907\tvalid's multi_logloss: 0.749279\n",
      "[220]\ttrain's multi_logloss: 0.618629\tvalid's multi_logloss: 0.747137\n",
      "[221]\ttrain's multi_logloss: 0.618014\tvalid's multi_logloss: 0.742004\n",
      "[222]\ttrain's multi_logloss: 0.617603\tvalid's multi_logloss: 0.740757\n",
      "[223]\ttrain's multi_logloss: 0.617271\tvalid's multi_logloss: 0.738824\n",
      "[224]\ttrain's multi_logloss: 0.616893\tvalid's multi_logloss: 0.734186\n",
      "[225]\ttrain's multi_logloss: 0.616206\tvalid's multi_logloss: 0.736935\n",
      "[226]\ttrain's multi_logloss: 0.615827\tvalid's multi_logloss: 0.742346\n",
      "[227]\ttrain's multi_logloss: 0.615551\tvalid's multi_logloss: 0.742751\n",
      "[228]\ttrain's multi_logloss: 0.615026\tvalid's multi_logloss: 0.741912\n",
      "[229]\ttrain's multi_logloss: 0.61406\tvalid's multi_logloss: 0.741859\n",
      "[230]\ttrain's multi_logloss: 0.613302\tvalid's multi_logloss: 0.744368\n",
      "[231]\ttrain's multi_logloss: 0.612259\tvalid's multi_logloss: 0.742425\n",
      "[232]\ttrain's multi_logloss: 0.611446\tvalid's multi_logloss: 0.740749\n",
      "[233]\ttrain's multi_logloss: 0.610784\tvalid's multi_logloss: 0.740857\n",
      "[234]\ttrain's multi_logloss: 0.610251\tvalid's multi_logloss: 0.741072\n",
      "[235]\ttrain's multi_logloss: 0.609632\tvalid's multi_logloss: 0.740536\n",
      "[236]\ttrain's multi_logloss: 0.60844\tvalid's multi_logloss: 0.739384\n",
      "[237]\ttrain's multi_logloss: 0.607781\tvalid's multi_logloss: 0.736329\n",
      "[238]\ttrain's multi_logloss: 0.607203\tvalid's multi_logloss: 0.733458\n",
      "[239]\ttrain's multi_logloss: 0.606702\tvalid's multi_logloss: 0.731865\n",
      "[240]\ttrain's multi_logloss: 0.60641\tvalid's multi_logloss: 0.7286\n",
      "[241]\ttrain's multi_logloss: 0.605988\tvalid's multi_logloss: 0.727932\n",
      "[242]\ttrain's multi_logloss: 0.605619\tvalid's multi_logloss: 0.725347\n",
      "[243]\ttrain's multi_logloss: 0.605097\tvalid's multi_logloss: 0.722727\n",
      "[244]\ttrain's multi_logloss: 0.604729\tvalid's multi_logloss: 0.718214\n",
      "[245]\ttrain's multi_logloss: 0.602897\tvalid's multi_logloss: 0.712439\n",
      "[246]\ttrain's multi_logloss: 0.601103\tvalid's multi_logloss: 0.70886\n",
      "[247]\ttrain's multi_logloss: 0.599504\tvalid's multi_logloss: 0.703474\n",
      "[248]\ttrain's multi_logloss: 0.598036\tvalid's multi_logloss: 0.698325\n",
      "[249]\ttrain's multi_logloss: 0.597267\tvalid's multi_logloss: 0.696203\n",
      "[250]\ttrain's multi_logloss: 0.596631\tvalid's multi_logloss: 0.694264\n",
      "[251]\ttrain's multi_logloss: 0.596116\tvalid's multi_logloss: 0.692496\n",
      "[252]\ttrain's multi_logloss: 0.595613\tvalid's multi_logloss: 0.69166\n",
      "[253]\ttrain's multi_logloss: 0.594994\tvalid's multi_logloss: 0.69273\n",
      "[254]\ttrain's multi_logloss: 0.594327\tvalid's multi_logloss: 0.690503\n",
      "[255]\ttrain's multi_logloss: 0.593495\tvalid's multi_logloss: 0.687698\n",
      "[256]\ttrain's multi_logloss: 0.592915\tvalid's multi_logloss: 0.683661\n",
      "[257]\ttrain's multi_logloss: 0.59243\tvalid's multi_logloss: 0.687217\n",
      "[258]\ttrain's multi_logloss: 0.591573\tvalid's multi_logloss: 0.690797\n",
      "[259]\ttrain's multi_logloss: 0.591016\tvalid's multi_logloss: 0.698002\n",
      "[260]\ttrain's multi_logloss: 0.590378\tvalid's multi_logloss: 0.702655\n",
      "[261]\ttrain's multi_logloss: 0.589563\tvalid's multi_logloss: 0.698359\n",
      "[262]\ttrain's multi_logloss: 0.588856\tvalid's multi_logloss: 0.69428\n",
      "[263]\ttrain's multi_logloss: 0.588346\tvalid's multi_logloss: 0.69039\n",
      "[264]\ttrain's multi_logloss: 0.587995\tvalid's multi_logloss: 0.687471\n",
      "[265]\ttrain's multi_logloss: 0.587679\tvalid's multi_logloss: 0.685438\n",
      "[266]\ttrain's multi_logloss: 0.587356\tvalid's multi_logloss: 0.684594\n",
      "[267]\ttrain's multi_logloss: 0.58694\tvalid's multi_logloss: 0.683264\n",
      "[268]\ttrain's multi_logloss: 0.586867\tvalid's multi_logloss: 0.683634\n",
      "[269]\ttrain's multi_logloss: 0.586482\tvalid's multi_logloss: 0.683809\n",
      "[270]\ttrain's multi_logloss: 0.585783\tvalid's multi_logloss: 0.679782\n",
      "[271]\ttrain's multi_logloss: 0.585216\tvalid's multi_logloss: 0.675931\n",
      "[272]\ttrain's multi_logloss: 0.585102\tvalid's multi_logloss: 0.676514\n",
      "[273]\ttrain's multi_logloss: 0.584407\tvalid's multi_logloss: 0.67437\n",
      "[274]\ttrain's multi_logloss: 0.583619\tvalid's multi_logloss: 0.673063\n",
      "[275]\ttrain's multi_logloss: 0.582788\tvalid's multi_logloss: 0.670021\n",
      "[276]\ttrain's multi_logloss: 0.582083\tvalid's multi_logloss: 0.666163\n",
      "[277]\ttrain's multi_logloss: 0.581176\tvalid's multi_logloss: 0.66814\n",
      "[278]\ttrain's multi_logloss: 0.580414\tvalid's multi_logloss: 0.668929\n",
      "[279]\ttrain's multi_logloss: 0.579638\tvalid's multi_logloss: 0.66887\n",
      "[280]\ttrain's multi_logloss: 0.579134\tvalid's multi_logloss: 0.671682\n",
      "[281]\ttrain's multi_logloss: 0.579063\tvalid's multi_logloss: 0.675298\n",
      "[282]\ttrain's multi_logloss: 0.579195\tvalid's multi_logloss: 0.679784\n",
      "[283]\ttrain's multi_logloss: 0.579022\tvalid's multi_logloss: 0.681636\n",
      "[284]\ttrain's multi_logloss: 0.578829\tvalid's multi_logloss: 0.680882\n",
      "[285]\ttrain's multi_logloss: 0.578087\tvalid's multi_logloss: 0.680665\n",
      "[286]\ttrain's multi_logloss: 0.577595\tvalid's multi_logloss: 0.681896\n",
      "[287]\ttrain's multi_logloss: 0.576996\tvalid's multi_logloss: 0.68077\n",
      "[288]\ttrain's multi_logloss: 0.576606\tvalid's multi_logloss: 0.681753\n",
      "[289]\ttrain's multi_logloss: 0.576096\tvalid's multi_logloss: 0.680975\n",
      "[290]\ttrain's multi_logloss: 0.575116\tvalid's multi_logloss: 0.677786\n",
      "[291]\ttrain's multi_logloss: 0.574685\tvalid's multi_logloss: 0.677289\n",
      "[292]\ttrain's multi_logloss: 0.573989\tvalid's multi_logloss: 0.676264\n",
      "[293]\ttrain's multi_logloss: 0.573529\tvalid's multi_logloss: 0.679223\n",
      "[294]\ttrain's multi_logloss: 0.572844\tvalid's multi_logloss: 0.680248\n",
      "[295]\ttrain's multi_logloss: 0.572253\tvalid's multi_logloss: 0.681246\n",
      "[296]\ttrain's multi_logloss: 0.571597\tvalid's multi_logloss: 0.683553\n",
      "[297]\ttrain's multi_logloss: 0.570878\tvalid's multi_logloss: 0.680726\n",
      "[298]\ttrain's multi_logloss: 0.570244\tvalid's multi_logloss: 0.67802\n",
      "[299]\ttrain's multi_logloss: 0.56969\tvalid's multi_logloss: 0.675428\n",
      "[300]\ttrain's multi_logloss: 0.569211\tvalid's multi_logloss: 0.672945\n",
      "[301]\ttrain's multi_logloss: 0.568815\tvalid's multi_logloss: 0.669955\n",
      "[302]\ttrain's multi_logloss: 0.568345\tvalid's multi_logloss: 0.668589\n",
      "[303]\ttrain's multi_logloss: 0.568158\tvalid's multi_logloss: 0.667968\n",
      "[304]\ttrain's multi_logloss: 0.567722\tvalid's multi_logloss: 0.667205\n",
      "[305]\ttrain's multi_logloss: 0.566131\tvalid's multi_logloss: 0.663479\n",
      "[306]\ttrain's multi_logloss: 0.564944\tvalid's multi_logloss: 0.661523\n",
      "[307]\ttrain's multi_logloss: 0.564609\tvalid's multi_logloss: 0.658204\n",
      "[308]\ttrain's multi_logloss: 0.56351\tvalid's multi_logloss: 0.654996\n",
      "[309]\ttrain's multi_logloss: 0.562961\tvalid's multi_logloss: 0.655606\n",
      "[310]\ttrain's multi_logloss: 0.562454\tvalid's multi_logloss: 0.656173\n",
      "[311]\ttrain's multi_logloss: 0.561839\tvalid's multi_logloss: 0.655646\n",
      "[312]\ttrain's multi_logloss: 0.561706\tvalid's multi_logloss: 0.657052\n",
      "[313]\ttrain's multi_logloss: 0.561522\tvalid's multi_logloss: 0.658019\n",
      "[314]\ttrain's multi_logloss: 0.561449\tvalid's multi_logloss: 0.659066\n",
      "[315]\ttrain's multi_logloss: 0.56116\tvalid's multi_logloss: 0.658761\n",
      "[316]\ttrain's multi_logloss: 0.561287\tvalid's multi_logloss: 0.659879\n",
      "[317]\ttrain's multi_logloss: 0.560525\tvalid's multi_logloss: 0.657481\n",
      "[318]\ttrain's multi_logloss: 0.559739\tvalid's multi_logloss: 0.651617\n",
      "[319]\ttrain's multi_logloss: 0.559104\tvalid's multi_logloss: 0.646052\n",
      "[320]\ttrain's multi_logloss: 0.558668\tvalid's multi_logloss: 0.643599\n",
      "[321]\ttrain's multi_logloss: 0.557934\tvalid's multi_logloss: 0.6436\n",
      "[322]\ttrain's multi_logloss: 0.557284\tvalid's multi_logloss: 0.642395\n",
      "[323]\ttrain's multi_logloss: 0.556746\tvalid's multi_logloss: 0.644329\n",
      "[324]\ttrain's multi_logloss: 0.556147\tvalid's multi_logloss: 0.643552\n",
      "[325]\ttrain's multi_logloss: 0.555108\tvalid's multi_logloss: 0.639808\n",
      "[326]\ttrain's multi_logloss: 0.554081\tvalid's multi_logloss: 0.638419\n",
      "[327]\ttrain's multi_logloss: 0.553276\tvalid's multi_logloss: 0.635018\n",
      "[328]\ttrain's multi_logloss: 0.552652\tvalid's multi_logloss: 0.634856\n",
      "[329]\ttrain's multi_logloss: 0.552062\tvalid's multi_logloss: 0.635483\n",
      "[330]\ttrain's multi_logloss: 0.551754\tvalid's multi_logloss: 0.63529\n",
      "[331]\ttrain's multi_logloss: 0.551468\tvalid's multi_logloss: 0.632689\n",
      "[332]\ttrain's multi_logloss: 0.551261\tvalid's multi_logloss: 0.632604\n",
      "[333]\ttrain's multi_logloss: 0.550707\tvalid's multi_logloss: 0.636653\n",
      "[334]\ttrain's multi_logloss: 0.550038\tvalid's multi_logloss: 0.641623\n",
      "[335]\ttrain's multi_logloss: 0.549466\tvalid's multi_logloss: 0.644501\n",
      "[336]\ttrain's multi_logloss: 0.549269\tvalid's multi_logloss: 0.647925\n",
      "[337]\ttrain's multi_logloss: 0.548784\tvalid's multi_logloss: 0.644456\n",
      "[338]\ttrain's multi_logloss: 0.548329\tvalid's multi_logloss: 0.642337\n",
      "[339]\ttrain's multi_logloss: 0.548037\tvalid's multi_logloss: 0.639922\n",
      "[340]\ttrain's multi_logloss: 0.547699\tvalid's multi_logloss: 0.638052\n",
      "[341]\ttrain's multi_logloss: 0.546945\tvalid's multi_logloss: 0.638932\n",
      "[342]\ttrain's multi_logloss: 0.546075\tvalid's multi_logloss: 0.638611\n",
      "[343]\ttrain's multi_logloss: 0.54523\tvalid's multi_logloss: 0.638067\n",
      "[344]\ttrain's multi_logloss: 0.544504\tvalid's multi_logloss: 0.637869\n",
      "[345]\ttrain's multi_logloss: 0.543879\tvalid's multi_logloss: 0.636495\n",
      "[346]\ttrain's multi_logloss: 0.543446\tvalid's multi_logloss: 0.63223\n",
      "[347]\ttrain's multi_logloss: 0.543124\tvalid's multi_logloss: 0.628189\n",
      "[348]\ttrain's multi_logloss: 0.542872\tvalid's multi_logloss: 0.625897\n",
      "[349]\ttrain's multi_logloss: 0.542621\tvalid's multi_logloss: 0.623566\n",
      "[350]\ttrain's multi_logloss: 0.542084\tvalid's multi_logloss: 0.620683\n",
      "[351]\ttrain's multi_logloss: 0.541585\tvalid's multi_logloss: 0.617036\n",
      "[352]\ttrain's multi_logloss: 0.541571\tvalid's multi_logloss: 0.612713\n",
      "[353]\ttrain's multi_logloss: 0.540909\tvalid's multi_logloss: 0.60964\n",
      "[354]\ttrain's multi_logloss: 0.540668\tvalid's multi_logloss: 0.609655\n",
      "[355]\ttrain's multi_logloss: 0.540132\tvalid's multi_logloss: 0.612195\n",
      "[356]\ttrain's multi_logloss: 0.539851\tvalid's multi_logloss: 0.609093\n",
      "[357]\ttrain's multi_logloss: 0.538622\tvalid's multi_logloss: 0.604456\n",
      "[358]\ttrain's multi_logloss: 0.538577\tvalid's multi_logloss: 0.602418\n",
      "[359]\ttrain's multi_logloss: 0.539089\tvalid's multi_logloss: 0.600659\n",
      "[360]\ttrain's multi_logloss: 0.538511\tvalid's multi_logloss: 0.597727\n",
      "[361]\ttrain's multi_logloss: 0.537656\tvalid's multi_logloss: 0.598032\n",
      "[362]\ttrain's multi_logloss: 0.53741\tvalid's multi_logloss: 0.598383\n",
      "[363]\ttrain's multi_logloss: 0.536803\tvalid's multi_logloss: 0.599224\n",
      "[364]\ttrain's multi_logloss: 0.536548\tvalid's multi_logloss: 0.600158\n",
      "[365]\ttrain's multi_logloss: 0.535876\tvalid's multi_logloss: 0.59956\n",
      "[366]\ttrain's multi_logloss: 0.535162\tvalid's multi_logloss: 0.602279\n",
      "[367]\ttrain's multi_logloss: 0.534525\tvalid's multi_logloss: 0.602604\n",
      "[368]\ttrain's multi_logloss: 0.533953\tvalid's multi_logloss: 0.605385\n",
      "[369]\ttrain's multi_logloss: 0.533692\tvalid's multi_logloss: 0.604032\n",
      "[370]\ttrain's multi_logloss: 0.533549\tvalid's multi_logloss: 0.604131\n",
      "[371]\ttrain's multi_logloss: 0.53348\tvalid's multi_logloss: 0.60419\n",
      "[372]\ttrain's multi_logloss: 0.533489\tvalid's multi_logloss: 0.60086\n",
      "[373]\ttrain's multi_logloss: 0.533101\tvalid's multi_logloss: 0.599313\n",
      "[374]\ttrain's multi_logloss: 0.532636\tvalid's multi_logloss: 0.599469\n",
      "[375]\ttrain's multi_logloss: 0.532134\tvalid's multi_logloss: 0.599265\n",
      "[376]\ttrain's multi_logloss: 0.531644\tvalid's multi_logloss: 0.598757\n",
      "[377]\ttrain's multi_logloss: 0.531558\tvalid's multi_logloss: 0.59656\n",
      "[378]\ttrain's multi_logloss: 0.531438\tvalid's multi_logloss: 0.5945\n",
      "[379]\ttrain's multi_logloss: 0.531218\tvalid's multi_logloss: 0.593448\n",
      "[380]\ttrain's multi_logloss: 0.530792\tvalid's multi_logloss: 0.595511\n",
      "[381]\ttrain's multi_logloss: 0.529275\tvalid's multi_logloss: 0.596173\n",
      "[382]\ttrain's multi_logloss: 0.527866\tvalid's multi_logloss: 0.596888\n",
      "[383]\ttrain's multi_logloss: 0.526368\tvalid's multi_logloss: 0.598471\n",
      "[384]\ttrain's multi_logloss: 0.525695\tvalid's multi_logloss: 0.599825\n",
      "[385]\ttrain's multi_logloss: 0.525431\tvalid's multi_logloss: 0.599427\n",
      "[386]\ttrain's multi_logloss: 0.525197\tvalid's multi_logloss: 0.600516\n",
      "[387]\ttrain's multi_logloss: 0.525053\tvalid's multi_logloss: 0.601639\n",
      "[388]\ttrain's multi_logloss: 0.524924\tvalid's multi_logloss: 0.601705\n",
      "[389]\ttrain's multi_logloss: 0.524157\tvalid's multi_logloss: 0.599344\n",
      "[390]\ttrain's multi_logloss: 0.52355\tvalid's multi_logloss: 0.598259\n",
      "[391]\ttrain's multi_logloss: 0.523145\tvalid's multi_logloss: 0.596769\n",
      "[392]\ttrain's multi_logloss: 0.522777\tvalid's multi_logloss: 0.597032\n",
      "[393]\ttrain's multi_logloss: 0.522304\tvalid's multi_logloss: 0.598234\n",
      "[394]\ttrain's multi_logloss: 0.521668\tvalid's multi_logloss: 0.598978\n",
      "[395]\ttrain's multi_logloss: 0.521209\tvalid's multi_logloss: 0.600283\n",
      "[396]\ttrain's multi_logloss: 0.52085\tvalid's multi_logloss: 0.600418\n",
      "[397]\ttrain's multi_logloss: 0.520537\tvalid's multi_logloss: 0.596565\n",
      "[398]\ttrain's multi_logloss: 0.520298\tvalid's multi_logloss: 0.59322\n",
      "[399]\ttrain's multi_logloss: 0.520227\tvalid's multi_logloss: 0.590662\n",
      "[400]\ttrain's multi_logloss: 0.520256\tvalid's multi_logloss: 0.58935\n",
      "[401]\ttrain's multi_logloss: 0.52039\tvalid's multi_logloss: 0.588704\n",
      "[402]\ttrain's multi_logloss: 0.519872\tvalid's multi_logloss: 0.589128\n",
      "[403]\ttrain's multi_logloss: 0.519586\tvalid's multi_logloss: 0.58957\n",
      "[404]\ttrain's multi_logloss: 0.519264\tvalid's multi_logloss: 0.590039\n",
      "[405]\ttrain's multi_logloss: 0.51851\tvalid's multi_logloss: 0.593008\n",
      "[406]\ttrain's multi_logloss: 0.518072\tvalid's multi_logloss: 0.594794\n",
      "[407]\ttrain's multi_logloss: 0.517742\tvalid's multi_logloss: 0.596644\n",
      "[408]\ttrain's multi_logloss: 0.517513\tvalid's multi_logloss: 0.596269\n",
      "[409]\ttrain's multi_logloss: 0.51696\tvalid's multi_logloss: 0.594344\n",
      "[410]\ttrain's multi_logloss: 0.516493\tvalid's multi_logloss: 0.593752\n",
      "[411]\ttrain's multi_logloss: 0.516298\tvalid's multi_logloss: 0.592817\n",
      "[412]\ttrain's multi_logloss: 0.515881\tvalid's multi_logloss: 0.591215\n",
      "[413]\ttrain's multi_logloss: 0.515795\tvalid's multi_logloss: 0.590723\n",
      "[414]\ttrain's multi_logloss: 0.515776\tvalid's multi_logloss: 0.592103\n",
      "[415]\ttrain's multi_logloss: 0.515696\tvalid's multi_logloss: 0.591988\n",
      "[416]\ttrain's multi_logloss: 0.515845\tvalid's multi_logloss: 0.593595\n",
      "[417]\ttrain's multi_logloss: 0.514948\tvalid's multi_logloss: 0.597332\n",
      "[418]\ttrain's multi_logloss: 0.513906\tvalid's multi_logloss: 0.600993\n",
      "[419]\ttrain's multi_logloss: 0.513159\tvalid's multi_logloss: 0.602054\n",
      "[420]\ttrain's multi_logloss: 0.513013\tvalid's multi_logloss: 0.60271\n",
      "[421]\ttrain's multi_logloss: 0.512507\tvalid's multi_logloss: 0.605152\n",
      "[422]\ttrain's multi_logloss: 0.512083\tvalid's multi_logloss: 0.607573\n",
      "[423]\ttrain's multi_logloss: 0.511846\tvalid's multi_logloss: 0.605021\n",
      "[424]\ttrain's multi_logloss: 0.511818\tvalid's multi_logloss: 0.606841\n",
      "[425]\ttrain's multi_logloss: 0.511165\tvalid's multi_logloss: 0.608369\n",
      "[426]\ttrain's multi_logloss: 0.510329\tvalid's multi_logloss: 0.609191\n",
      "[427]\ttrain's multi_logloss: 0.509605\tvalid's multi_logloss: 0.608346\n",
      "[428]\ttrain's multi_logloss: 0.509084\tvalid's multi_logloss: 0.609869\n",
      "[429]\ttrain's multi_logloss: 0.508799\tvalid's multi_logloss: 0.607808\n",
      "[430]\ttrain's multi_logloss: 0.508602\tvalid's multi_logloss: 0.607659\n",
      "[431]\ttrain's multi_logloss: 0.508355\tvalid's multi_logloss: 0.605875\n",
      "[432]\ttrain's multi_logloss: 0.508295\tvalid's multi_logloss: 0.603792\n",
      "[433]\ttrain's multi_logloss: 0.507508\tvalid's multi_logloss: 0.599359\n",
      "[434]\ttrain's multi_logloss: 0.506841\tvalid's multi_logloss: 0.596063\n",
      "[435]\ttrain's multi_logloss: 0.50627\tvalid's multi_logloss: 0.59296\n",
      "[436]\ttrain's multi_logloss: 0.505849\tvalid's multi_logloss: 0.588177\n",
      "[437]\ttrain's multi_logloss: 0.505029\tvalid's multi_logloss: 0.588417\n",
      "[438]\ttrain's multi_logloss: 0.504537\tvalid's multi_logloss: 0.588288\n",
      "[439]\ttrain's multi_logloss: 0.504149\tvalid's multi_logloss: 0.587624\n",
      "[440]\ttrain's multi_logloss: 0.503611\tvalid's multi_logloss: 0.587083\n",
      "[441]\ttrain's multi_logloss: 0.503313\tvalid's multi_logloss: 0.584378\n",
      "[442]\ttrain's multi_logloss: 0.503062\tvalid's multi_logloss: 0.583205\n",
      "[443]\ttrain's multi_logloss: 0.502637\tvalid's multi_logloss: 0.58415\n",
      "[444]\ttrain's multi_logloss: 0.502157\tvalid's multi_logloss: 0.582885\n",
      "[445]\ttrain's multi_logloss: 0.501705\tvalid's multi_logloss: 0.582741\n",
      "[446]\ttrain's multi_logloss: 0.501326\tvalid's multi_logloss: 0.583713\n",
      "[447]\ttrain's multi_logloss: 0.50105\tvalid's multi_logloss: 0.584436\n",
      "[448]\ttrain's multi_logloss: 0.500449\tvalid's multi_logloss: 0.582274\n",
      "[449]\ttrain's multi_logloss: 0.499643\tvalid's multi_logloss: 0.585362\n",
      "[450]\ttrain's multi_logloss: 0.499289\tvalid's multi_logloss: 0.586133\n",
      "[451]\ttrain's multi_logloss: 0.498817\tvalid's multi_logloss: 0.584743\n",
      "[452]\ttrain's multi_logloss: 0.498414\tvalid's multi_logloss: 0.583448\n",
      "[453]\ttrain's multi_logloss: 0.497972\tvalid's multi_logloss: 0.585132\n",
      "[454]\ttrain's multi_logloss: 0.497663\tvalid's multi_logloss: 0.586518\n",
      "[455]\ttrain's multi_logloss: 0.49737\tvalid's multi_logloss: 0.586566\n",
      "[456]\ttrain's multi_logloss: 0.497213\tvalid's multi_logloss: 0.588669\n",
      "[457]\ttrain's multi_logloss: 0.497003\tvalid's multi_logloss: 0.588267\n",
      "[458]\ttrain's multi_logloss: 0.497011\tvalid's multi_logloss: 0.587346\n",
      "[459]\ttrain's multi_logloss: 0.496623\tvalid's multi_logloss: 0.588827\n",
      "[460]\ttrain's multi_logloss: 0.496606\tvalid's multi_logloss: 0.588609\n",
      "[461]\ttrain's multi_logloss: 0.496337\tvalid's multi_logloss: 0.585777\n",
      "[462]\ttrain's multi_logloss: 0.495758\tvalid's multi_logloss: 0.585223\n",
      "[463]\ttrain's multi_logloss: 0.495588\tvalid's multi_logloss: 0.586489\n",
      "[464]\ttrain's multi_logloss: 0.495634\tvalid's multi_logloss: 0.585743\n",
      "[465]\ttrain's multi_logloss: 0.494825\tvalid's multi_logloss: 0.587831\n",
      "[466]\ttrain's multi_logloss: 0.494484\tvalid's multi_logloss: 0.587209\n",
      "[467]\ttrain's multi_logloss: 0.494078\tvalid's multi_logloss: 0.590129\n",
      "[468]\ttrain's multi_logloss: 0.493797\tvalid's multi_logloss: 0.59309\n",
      "[469]\ttrain's multi_logloss: 0.493386\tvalid's multi_logloss: 0.59327\n",
      "[470]\ttrain's multi_logloss: 0.49302\tvalid's multi_logloss: 0.5928\n",
      "[471]\ttrain's multi_logloss: 0.492741\tvalid's multi_logloss: 0.593127\n",
      "[472]\ttrain's multi_logloss: 0.492445\tvalid's multi_logloss: 0.592428\n",
      "[473]\ttrain's multi_logloss: 0.49219\tvalid's multi_logloss: 0.591525\n",
      "[474]\ttrain's multi_logloss: 0.491981\tvalid's multi_logloss: 0.590283\n",
      "[475]\ttrain's multi_logloss: 0.491821\tvalid's multi_logloss: 0.589462\n",
      "[476]\ttrain's multi_logloss: 0.491298\tvalid's multi_logloss: 0.586681\n",
      "[477]\ttrain's multi_logloss: 0.490744\tvalid's multi_logloss: 0.586023\n",
      "[478]\ttrain's multi_logloss: 0.490104\tvalid's multi_logloss: 0.587137\n",
      "[479]\ttrain's multi_logloss: 0.489756\tvalid's multi_logloss: 0.587313\n",
      "[480]\ttrain's multi_logloss: 0.489595\tvalid's multi_logloss: 0.588209\n",
      "[481]\ttrain's multi_logloss: 0.489054\tvalid's multi_logloss: 0.585771\n",
      "[482]\ttrain's multi_logloss: 0.488141\tvalid's multi_logloss: 0.582507\n",
      "[483]\ttrain's multi_logloss: 0.487547\tvalid's multi_logloss: 0.579663\n",
      "[484]\ttrain's multi_logloss: 0.487231\tvalid's multi_logloss: 0.577678\n",
      "[485]\ttrain's multi_logloss: 0.48686\tvalid's multi_logloss: 0.580175\n",
      "[486]\ttrain's multi_logloss: 0.486582\tvalid's multi_logloss: 0.582027\n",
      "[487]\ttrain's multi_logloss: 0.486004\tvalid's multi_logloss: 0.583019\n",
      "[488]\ttrain's multi_logloss: 0.485902\tvalid's multi_logloss: 0.584701\n",
      "[489]\ttrain's multi_logloss: 0.485542\tvalid's multi_logloss: 0.585872\n",
      "[490]\ttrain's multi_logloss: 0.485245\tvalid's multi_logloss: 0.587085\n",
      "[491]\ttrain's multi_logloss: 0.48497\tvalid's multi_logloss: 0.588417\n",
      "[492]\ttrain's multi_logloss: 0.48493\tvalid's multi_logloss: 0.590696\n",
      "[493]\ttrain's multi_logloss: 0.484608\tvalid's multi_logloss: 0.590524\n",
      "[494]\ttrain's multi_logloss: 0.484154\tvalid's multi_logloss: 0.59\n",
      "[495]\ttrain's multi_logloss: 0.483701\tvalid's multi_logloss: 0.591311\n",
      "[496]\ttrain's multi_logloss: 0.483294\tvalid's multi_logloss: 0.594907\n",
      "[497]\ttrain's multi_logloss: 0.482631\tvalid's multi_logloss: 0.597353\n",
      "[498]\ttrain's multi_logloss: 0.482069\tvalid's multi_logloss: 0.599865\n",
      "[499]\ttrain's multi_logloss: 0.481601\tvalid's multi_logloss: 0.602433\n",
      "[500]\ttrain's multi_logloss: 0.481297\tvalid's multi_logloss: 0.603924\n",
      "[501]\ttrain's multi_logloss: 0.480763\tvalid's multi_logloss: 0.606045\n",
      "[502]\ttrain's multi_logloss: 0.480331\tvalid's multi_logloss: 0.609363\n",
      "[503]\ttrain's multi_logloss: 0.479917\tvalid's multi_logloss: 0.612671\n",
      "[504]\ttrain's multi_logloss: 0.479555\tvalid's multi_logloss: 0.61507\n",
      "[505]\ttrain's multi_logloss: 0.479099\tvalid's multi_logloss: 0.618336\n",
      "[506]\ttrain's multi_logloss: 0.478911\tvalid's multi_logloss: 0.61981\n",
      "[507]\ttrain's multi_logloss: 0.478601\tvalid's multi_logloss: 0.622941\n",
      "[508]\ttrain's multi_logloss: 0.478525\tvalid's multi_logloss: 0.624395\n",
      "[509]\ttrain's multi_logloss: 0.478009\tvalid's multi_logloss: 0.626614\n",
      "[510]\ttrain's multi_logloss: 0.477383\tvalid's multi_logloss: 0.630149\n",
      "[511]\ttrain's multi_logloss: 0.476853\tvalid's multi_logloss: 0.633665\n",
      "[512]\ttrain's multi_logloss: 0.4764\tvalid's multi_logloss: 0.637009\n",
      "[513]\ttrain's multi_logloss: 0.476079\tvalid's multi_logloss: 0.640047\n",
      "[514]\ttrain's multi_logloss: 0.475787\tvalid's multi_logloss: 0.64336\n",
      "[515]\ttrain's multi_logloss: 0.475364\tvalid's multi_logloss: 0.643811\n",
      "[516]\ttrain's multi_logloss: 0.475134\tvalid's multi_logloss: 0.647104\n",
      "[517]\ttrain's multi_logloss: 0.474927\tvalid's multi_logloss: 0.648869\n",
      "[518]\ttrain's multi_logloss: 0.474713\tvalid's multi_logloss: 0.650311\n",
      "[519]\ttrain's multi_logloss: 0.474619\tvalid's multi_logloss: 0.651809\n",
      "[520]\ttrain's multi_logloss: 0.474633\tvalid's multi_logloss: 0.653353\n",
      "[521]\ttrain's multi_logloss: 0.473699\tvalid's multi_logloss: 0.649843\n",
      "[522]\ttrain's multi_logloss: 0.472847\tvalid's multi_logloss: 0.645869\n",
      "[523]\ttrain's multi_logloss: 0.472125\tvalid's multi_logloss: 0.642107\n",
      "[524]\ttrain's multi_logloss: 0.471552\tvalid's multi_logloss: 0.636767\n",
      "[525]\ttrain's multi_logloss: 0.471159\tvalid's multi_logloss: 0.635507\n",
      "[526]\ttrain's multi_logloss: 0.47079\tvalid's multi_logloss: 0.633562\n",
      "[527]\ttrain's multi_logloss: 0.470452\tvalid's multi_logloss: 0.634044\n",
      "[528]\ttrain's multi_logloss: 0.470163\tvalid's multi_logloss: 0.63222\n",
      "[529]\ttrain's multi_logloss: 0.469744\tvalid's multi_logloss: 0.63204\n",
      "[530]\ttrain's multi_logloss: 0.469526\tvalid's multi_logloss: 0.631182\n",
      "[531]\ttrain's multi_logloss: 0.469274\tvalid's multi_logloss: 0.631752\n",
      "[532]\ttrain's multi_logloss: 0.468926\tvalid's multi_logloss: 0.631098\n",
      "[533]\ttrain's multi_logloss: 0.468543\tvalid's multi_logloss: 0.636332\n",
      "[534]\ttrain's multi_logloss: 0.468362\tvalid's multi_logloss: 0.641626\n",
      "[535]\ttrain's multi_logloss: 0.468057\tvalid's multi_logloss: 0.646656\n",
      "[536]\ttrain's multi_logloss: 0.468168\tvalid's multi_logloss: 0.651939\n",
      "[537]\ttrain's multi_logloss: 0.467511\tvalid's multi_logloss: 0.653022\n",
      "[538]\ttrain's multi_logloss: 0.466511\tvalid's multi_logloss: 0.653752\n",
      "[539]\ttrain's multi_logloss: 0.465848\tvalid's multi_logloss: 0.654158\n",
      "[540]\ttrain's multi_logloss: 0.465145\tvalid's multi_logloss: 0.655182\n",
      "[541]\ttrain's multi_logloss: 0.464508\tvalid's multi_logloss: 0.655173\n",
      "[542]\ttrain's multi_logloss: 0.462071\tvalid's multi_logloss: 0.652866\n",
      "[543]\ttrain's multi_logloss: 0.46091\tvalid's multi_logloss: 0.649841\n",
      "[544]\ttrain's multi_logloss: 0.45912\tvalid's multi_logloss: 0.647862\n",
      "[545]\ttrain's multi_logloss: 0.45864\tvalid's multi_logloss: 0.649156\n",
      "[546]\ttrain's multi_logloss: 0.458288\tvalid's multi_logloss: 0.650169\n",
      "[547]\ttrain's multi_logloss: 0.457753\tvalid's multi_logloss: 0.649511\n",
      "[548]\ttrain's multi_logloss: 0.457553\tvalid's multi_logloss: 0.648485\n",
      "[549]\ttrain's multi_logloss: 0.4574\tvalid's multi_logloss: 0.648226\n",
      "[550]\ttrain's multi_logloss: 0.457253\tvalid's multi_logloss: 0.649097\n",
      "[551]\ttrain's multi_logloss: 0.4571\tvalid's multi_logloss: 0.649693\n",
      "[552]\ttrain's multi_logloss: 0.456917\tvalid's multi_logloss: 0.65152\n",
      "[553]\ttrain's multi_logloss: 0.456691\tvalid's multi_logloss: 0.651686\n",
      "[554]\ttrain's multi_logloss: 0.456361\tvalid's multi_logloss: 0.65047\n",
      "[555]\ttrain's multi_logloss: 0.456341\tvalid's multi_logloss: 0.651073\n",
      "[556]\ttrain's multi_logloss: 0.456223\tvalid's multi_logloss: 0.651233\n",
      "[557]\ttrain's multi_logloss: 0.456124\tvalid's multi_logloss: 0.652832\n",
      "[558]\ttrain's multi_logloss: 0.45615\tvalid's multi_logloss: 0.655183\n",
      "[559]\ttrain's multi_logloss: 0.456204\tvalid's multi_logloss: 0.657518\n",
      "[560]\ttrain's multi_logloss: 0.456124\tvalid's multi_logloss: 0.657765\n",
      "[561]\ttrain's multi_logloss: 0.455686\tvalid's multi_logloss: 0.658649\n",
      "[562]\ttrain's multi_logloss: 0.455342\tvalid's multi_logloss: 0.659578\n",
      "[563]\ttrain's multi_logloss: 0.454924\tvalid's multi_logloss: 0.658975\n",
      "[564]\ttrain's multi_logloss: 0.454737\tvalid's multi_logloss: 0.659966\n",
      "[565]\ttrain's multi_logloss: 0.454267\tvalid's multi_logloss: 0.659151\n",
      "[566]\ttrain's multi_logloss: 0.453828\tvalid's multi_logloss: 0.658153\n",
      "[567]\ttrain's multi_logloss: 0.453473\tvalid's multi_logloss: 0.65725\n",
      "[568]\ttrain's multi_logloss: 0.453157\tvalid's multi_logloss: 0.656424\n",
      "[569]\ttrain's multi_logloss: 0.452801\tvalid's multi_logloss: 0.653815\n",
      "[570]\ttrain's multi_logloss: 0.452481\tvalid's multi_logloss: 0.651306\n",
      "[571]\ttrain's multi_logloss: 0.452101\tvalid's multi_logloss: 0.649949\n",
      "[572]\ttrain's multi_logloss: 0.451845\tvalid's multi_logloss: 0.647614\n",
      "[573]\ttrain's multi_logloss: 0.451637\tvalid's multi_logloss: 0.646871\n",
      "[574]\ttrain's multi_logloss: 0.451367\tvalid's multi_logloss: 0.644677\n",
      "[575]\ttrain's multi_logloss: 0.451037\tvalid's multi_logloss: 0.64426\n",
      "[576]\ttrain's multi_logloss: 0.450921\tvalid's multi_logloss: 0.642533\n",
      "[577]\ttrain's multi_logloss: 0.450587\tvalid's multi_logloss: 0.638724\n",
      "[578]\ttrain's multi_logloss: 0.450303\tvalid's multi_logloss: 0.63503\n",
      "[579]\ttrain's multi_logloss: 0.450061\tvalid's multi_logloss: 0.633487\n",
      "[580]\ttrain's multi_logloss: 0.449872\tvalid's multi_logloss: 0.630005\n",
      "[581]\ttrain's multi_logloss: 0.449646\tvalid's multi_logloss: 0.63265\n",
      "[582]\ttrain's multi_logloss: 0.449263\tvalid's multi_logloss: 0.633453\n",
      "[583]\ttrain's multi_logloss: 0.448657\tvalid's multi_logloss: 0.631633\n",
      "[584]\ttrain's multi_logloss: 0.448332\tvalid's multi_logloss: 0.633784\n",
      "Early stopping, best iteration is:\n",
      "[484]\ttrain's multi_logloss: 0.487231\tvalid's multi_logloss: 0.577678\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.8\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.102282\n",
      "1   feature2    0.087443\n",
      "2   feature3    0.070223\n",
      "3   feature4    0.091995\n",
      "4   feature5    0.110166\n",
      "5   feature6    0.080610\n",
      "6   feature7    0.091926\n",
      "7   feature8    0.118097\n",
      "8   feature9    0.125351\n",
      "9  feature10    0.121908\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08616\tvalid's my_multiclass_logloss: 1.08344\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07596\tvalid's my_multiclass_logloss: 1.07203\n",
      "[3]\ttrain's my_multiclass_logloss: 1.06506\tvalid's my_multiclass_logloss: 1.05692\n",
      "[4]\ttrain's my_multiclass_logloss: 1.05711\tvalid's my_multiclass_logloss: 1.03977\n",
      "[5]\ttrain's my_multiclass_logloss: 1.04874\tvalid's my_multiclass_logloss: 1.03827\n",
      "[6]\ttrain's my_multiclass_logloss: 1.04104\tvalid's my_multiclass_logloss: 1.0313\n",
      "[7]\ttrain's my_multiclass_logloss: 1.034\tvalid's my_multiclass_logloss: 1.03069\n",
      "[8]\ttrain's my_multiclass_logloss: 1.02515\tvalid's my_multiclass_logloss: 1.02776\n",
      "[9]\ttrain's my_multiclass_logloss: 1.01652\tvalid's my_multiclass_logloss: 1.01975\n",
      "[10]\ttrain's my_multiclass_logloss: 1.01043\tvalid's my_multiclass_logloss: 1.00427\n",
      "[11]\ttrain's my_multiclass_logloss: 1.00378\tvalid's my_multiclass_logloss: 0.994234\n",
      "[12]\ttrain's my_multiclass_logloss: 0.997825\tvalid's my_multiclass_logloss: 0.984978\n",
      "[13]\ttrain's my_multiclass_logloss: 0.992171\tvalid's my_multiclass_logloss: 0.980878\n",
      "[14]\ttrain's my_multiclass_logloss: 0.986109\tvalid's my_multiclass_logloss: 0.976114\n",
      "[15]\ttrain's my_multiclass_logloss: 0.979589\tvalid's my_multiclass_logloss: 0.967238\n",
      "[16]\ttrain's my_multiclass_logloss: 0.975298\tvalid's my_multiclass_logloss: 0.9643\n",
      "[17]\ttrain's my_multiclass_logloss: 0.970432\tvalid's my_multiclass_logloss: 0.965863\n",
      "[18]\ttrain's my_multiclass_logloss: 0.965212\tvalid's my_multiclass_logloss: 0.966962\n",
      "[19]\ttrain's my_multiclass_logloss: 0.960056\tvalid's my_multiclass_logloss: 0.96569\n",
      "[20]\ttrain's my_multiclass_logloss: 0.95573\tvalid's my_multiclass_logloss: 0.964343\n",
      "[21]\ttrain's my_multiclass_logloss: 0.949845\tvalid's my_multiclass_logloss: 0.963111\n",
      "[22]\ttrain's my_multiclass_logloss: 0.94513\tvalid's my_multiclass_logloss: 0.960092\n",
      "[23]\ttrain's my_multiclass_logloss: 0.940748\tvalid's my_multiclass_logloss: 0.961012\n",
      "[24]\ttrain's my_multiclass_logloss: 0.936653\tvalid's my_multiclass_logloss: 0.954612\n",
      "[25]\ttrain's my_multiclass_logloss: 0.931713\tvalid's my_multiclass_logloss: 0.9482\n",
      "[26]\ttrain's my_multiclass_logloss: 0.926914\tvalid's my_multiclass_logloss: 0.940043\n",
      "[27]\ttrain's my_multiclass_logloss: 0.922547\tvalid's my_multiclass_logloss: 0.932217\n",
      "[28]\ttrain's my_multiclass_logloss: 0.918459\tvalid's my_multiclass_logloss: 0.926369\n",
      "[29]\ttrain's my_multiclass_logloss: 0.916712\tvalid's my_multiclass_logloss: 0.91769\n",
      "[30]\ttrain's my_multiclass_logloss: 0.912628\tvalid's my_multiclass_logloss: 0.915158\n",
      "[31]\ttrain's my_multiclass_logloss: 0.908806\tvalid's my_multiclass_logloss: 0.910995\n",
      "[32]\ttrain's my_multiclass_logloss: 0.905365\tvalid's my_multiclass_logloss: 0.909058\n",
      "[33]\ttrain's my_multiclass_logloss: 0.901197\tvalid's my_multiclass_logloss: 0.905337\n",
      "[34]\ttrain's my_multiclass_logloss: 0.897373\tvalid's my_multiclass_logloss: 0.901948\n",
      "[35]\ttrain's my_multiclass_logloss: 0.893988\tvalid's my_multiclass_logloss: 0.89991\n",
      "[36]\ttrain's my_multiclass_logloss: 0.890172\tvalid's my_multiclass_logloss: 0.896544\n",
      "[37]\ttrain's my_multiclass_logloss: 0.88628\tvalid's my_multiclass_logloss: 0.894796\n",
      "[38]\ttrain's my_multiclass_logloss: 0.881855\tvalid's my_multiclass_logloss: 0.892676\n",
      "[39]\ttrain's my_multiclass_logloss: 0.877996\tvalid's my_multiclass_logloss: 0.891118\n",
      "[40]\ttrain's my_multiclass_logloss: 0.874488\tvalid's my_multiclass_logloss: 0.888482\n",
      "[41]\ttrain's my_multiclass_logloss: 0.871558\tvalid's my_multiclass_logloss: 0.887482\n",
      "[42]\ttrain's my_multiclass_logloss: 0.86923\tvalid's my_multiclass_logloss: 0.885782\n",
      "[43]\ttrain's my_multiclass_logloss: 0.867075\tvalid's my_multiclass_logloss: 0.883259\n",
      "[44]\ttrain's my_multiclass_logloss: 0.866116\tvalid's my_multiclass_logloss: 0.887034\n",
      "[45]\ttrain's my_multiclass_logloss: 0.862065\tvalid's my_multiclass_logloss: 0.887527\n",
      "[46]\ttrain's my_multiclass_logloss: 0.859419\tvalid's my_multiclass_logloss: 0.883863\n",
      "[47]\ttrain's my_multiclass_logloss: 0.855993\tvalid's my_multiclass_logloss: 0.884768\n",
      "[48]\ttrain's my_multiclass_logloss: 0.852951\tvalid's my_multiclass_logloss: 0.885905\n",
      "[49]\ttrain's my_multiclass_logloss: 0.849959\tvalid's my_multiclass_logloss: 0.886316\n",
      "[50]\ttrain's my_multiclass_logloss: 0.847351\tvalid's my_multiclass_logloss: 0.887124\n",
      "[51]\ttrain's my_multiclass_logloss: 0.845086\tvalid's my_multiclass_logloss: 0.888267\n",
      "[52]\ttrain's my_multiclass_logloss: 0.843917\tvalid's my_multiclass_logloss: 0.892049\n",
      "[53]\ttrain's my_multiclass_logloss: 0.842226\tvalid's my_multiclass_logloss: 0.891991\n",
      "[54]\ttrain's my_multiclass_logloss: 0.841198\tvalid's my_multiclass_logloss: 0.892221\n",
      "[55]\ttrain's my_multiclass_logloss: 0.839376\tvalid's my_multiclass_logloss: 0.888957\n",
      "[56]\ttrain's my_multiclass_logloss: 0.838552\tvalid's my_multiclass_logloss: 0.888653\n",
      "[57]\ttrain's my_multiclass_logloss: 0.83592\tvalid's my_multiclass_logloss: 0.886963\n",
      "[58]\ttrain's my_multiclass_logloss: 0.833287\tvalid's my_multiclass_logloss: 0.882599\n",
      "[59]\ttrain's my_multiclass_logloss: 0.830957\tvalid's my_multiclass_logloss: 0.879981\n",
      "[60]\ttrain's my_multiclass_logloss: 0.828932\tvalid's my_multiclass_logloss: 0.87279\n",
      "[61]\ttrain's my_multiclass_logloss: 0.827468\tvalid's my_multiclass_logloss: 0.872923\n",
      "[62]\ttrain's my_multiclass_logloss: 0.826574\tvalid's my_multiclass_logloss: 0.871452\n",
      "[63]\ttrain's my_multiclass_logloss: 0.825026\tvalid's my_multiclass_logloss: 0.867515\n",
      "[64]\ttrain's my_multiclass_logloss: 0.823861\tvalid's my_multiclass_logloss: 0.866344\n",
      "[65]\ttrain's my_multiclass_logloss: 0.821398\tvalid's my_multiclass_logloss: 0.861478\n",
      "[66]\ttrain's my_multiclass_logloss: 0.819388\tvalid's my_multiclass_logloss: 0.856072\n",
      "[67]\ttrain's my_multiclass_logloss: 0.81823\tvalid's my_multiclass_logloss: 0.853461\n",
      "[68]\ttrain's my_multiclass_logloss: 0.816526\tvalid's my_multiclass_logloss: 0.854507\n",
      "[69]\ttrain's my_multiclass_logloss: 0.813682\tvalid's my_multiclass_logloss: 0.852917\n",
      "[70]\ttrain's my_multiclass_logloss: 0.812206\tvalid's my_multiclass_logloss: 0.849795\n",
      "[71]\ttrain's my_multiclass_logloss: 0.809944\tvalid's my_multiclass_logloss: 0.84897\n",
      "[72]\ttrain's my_multiclass_logloss: 0.809326\tvalid's my_multiclass_logloss: 0.847816\n",
      "[73]\ttrain's my_multiclass_logloss: 0.807698\tvalid's my_multiclass_logloss: 0.844051\n",
      "[74]\ttrain's my_multiclass_logloss: 0.806524\tvalid's my_multiclass_logloss: 0.84032\n",
      "[75]\ttrain's my_multiclass_logloss: 0.805545\tvalid's my_multiclass_logloss: 0.83689\n",
      "[76]\ttrain's my_multiclass_logloss: 0.803495\tvalid's my_multiclass_logloss: 0.830649\n",
      "[77]\ttrain's my_multiclass_logloss: 0.802063\tvalid's my_multiclass_logloss: 0.829454\n",
      "[78]\ttrain's my_multiclass_logloss: 0.800333\tvalid's my_multiclass_logloss: 0.825191\n",
      "[79]\ttrain's my_multiclass_logloss: 0.798479\tvalid's my_multiclass_logloss: 0.821382\n",
      "[80]\ttrain's my_multiclass_logloss: 0.796936\tvalid's my_multiclass_logloss: 0.816201\n",
      "[81]\ttrain's my_multiclass_logloss: 0.794993\tvalid's my_multiclass_logloss: 0.814439\n",
      "[82]\ttrain's my_multiclass_logloss: 0.793324\tvalid's my_multiclass_logloss: 0.812893\n",
      "[83]\ttrain's my_multiclass_logloss: 0.791569\tvalid's my_multiclass_logloss: 0.809877\n",
      "[84]\ttrain's my_multiclass_logloss: 0.790088\tvalid's my_multiclass_logloss: 0.809496\n",
      "[85]\ttrain's my_multiclass_logloss: 0.789376\tvalid's my_multiclass_logloss: 0.806605\n",
      "[86]\ttrain's my_multiclass_logloss: 0.788576\tvalid's my_multiclass_logloss: 0.808703\n",
      "[87]\ttrain's my_multiclass_logloss: 0.788276\tvalid's my_multiclass_logloss: 0.806088\n",
      "[88]\ttrain's my_multiclass_logloss: 0.787846\tvalid's my_multiclass_logloss: 0.806306\n",
      "[89]\ttrain's my_multiclass_logloss: 0.786507\tvalid's my_multiclass_logloss: 0.80241\n",
      "[90]\ttrain's my_multiclass_logloss: 0.785476\tvalid's my_multiclass_logloss: 0.802777\n",
      "[91]\ttrain's my_multiclass_logloss: 0.784527\tvalid's my_multiclass_logloss: 0.798185\n",
      "[92]\ttrain's my_multiclass_logloss: 0.783936\tvalid's my_multiclass_logloss: 0.794679\n",
      "[93]\ttrain's my_multiclass_logloss: 0.782681\tvalid's my_multiclass_logloss: 0.796171\n",
      "[94]\ttrain's my_multiclass_logloss: 0.780549\tvalid's my_multiclass_logloss: 0.796063\n",
      "[95]\ttrain's my_multiclass_logloss: 0.779331\tvalid's my_multiclass_logloss: 0.796291\n",
      "[96]\ttrain's my_multiclass_logloss: 0.777516\tvalid's my_multiclass_logloss: 0.799556\n",
      "[97]\ttrain's my_multiclass_logloss: 0.776735\tvalid's my_multiclass_logloss: 0.799622\n",
      "[98]\ttrain's my_multiclass_logloss: 0.776219\tvalid's my_multiclass_logloss: 0.798741\n",
      "[99]\ttrain's my_multiclass_logloss: 0.775433\tvalid's my_multiclass_logloss: 0.795497\n",
      "[100]\ttrain's my_multiclass_logloss: 0.774996\tvalid's my_multiclass_logloss: 0.793781\n",
      "[101]\ttrain's my_multiclass_logloss: 0.773332\tvalid's my_multiclass_logloss: 0.791579\n",
      "[102]\ttrain's my_multiclass_logloss: 0.772333\tvalid's my_multiclass_logloss: 0.791867\n",
      "[103]\ttrain's my_multiclass_logloss: 0.770773\tvalid's my_multiclass_logloss: 0.791006\n",
      "[104]\ttrain's my_multiclass_logloss: 0.770441\tvalid's my_multiclass_logloss: 0.788019\n",
      "[105]\ttrain's my_multiclass_logloss: 0.769499\tvalid's my_multiclass_logloss: 0.788393\n",
      "[106]\ttrain's my_multiclass_logloss: 0.769096\tvalid's my_multiclass_logloss: 0.784958\n",
      "[107]\ttrain's my_multiclass_logloss: 0.768277\tvalid's my_multiclass_logloss: 0.784148\n",
      "[108]\ttrain's my_multiclass_logloss: 0.767735\tvalid's my_multiclass_logloss: 0.78714\n",
      "[109]\ttrain's my_multiclass_logloss: 0.766728\tvalid's my_multiclass_logloss: 0.785271\n",
      "[110]\ttrain's my_multiclass_logloss: 0.764971\tvalid's my_multiclass_logloss: 0.787839\n",
      "[111]\ttrain's my_multiclass_logloss: 0.763488\tvalid's my_multiclass_logloss: 0.790474\n",
      "[112]\ttrain's my_multiclass_logloss: 0.762454\tvalid's my_multiclass_logloss: 0.790922\n",
      "[113]\ttrain's my_multiclass_logloss: 0.761808\tvalid's my_multiclass_logloss: 0.790431\n",
      "[114]\ttrain's my_multiclass_logloss: 0.760981\tvalid's my_multiclass_logloss: 0.784981\n",
      "[115]\ttrain's my_multiclass_logloss: 0.760574\tvalid's my_multiclass_logloss: 0.784771\n",
      "[116]\ttrain's my_multiclass_logloss: 0.76002\tvalid's my_multiclass_logloss: 0.785738\n",
      "[117]\ttrain's my_multiclass_logloss: 0.759\tvalid's my_multiclass_logloss: 0.786862\n",
      "[118]\ttrain's my_multiclass_logloss: 0.757921\tvalid's my_multiclass_logloss: 0.788185\n",
      "[119]\ttrain's my_multiclass_logloss: 0.757014\tvalid's my_multiclass_logloss: 0.790315\n",
      "[120]\ttrain's my_multiclass_logloss: 0.755692\tvalid's my_multiclass_logloss: 0.795029\n",
      "[121]\ttrain's my_multiclass_logloss: 0.753395\tvalid's my_multiclass_logloss: 0.792398\n",
      "[122]\ttrain's my_multiclass_logloss: 0.75137\tvalid's my_multiclass_logloss: 0.790036\n",
      "[123]\ttrain's my_multiclass_logloss: 0.749624\tvalid's my_multiclass_logloss: 0.788715\n",
      "[124]\ttrain's my_multiclass_logloss: 0.747445\tvalid's my_multiclass_logloss: 0.786991\n",
      "[125]\ttrain's my_multiclass_logloss: 0.746256\tvalid's my_multiclass_logloss: 0.788801\n",
      "[126]\ttrain's my_multiclass_logloss: 0.745363\tvalid's my_multiclass_logloss: 0.793529\n",
      "[127]\ttrain's my_multiclass_logloss: 0.744498\tvalid's my_multiclass_logloss: 0.795387\n",
      "[128]\ttrain's my_multiclass_logloss: 0.74421\tvalid's my_multiclass_logloss: 0.79846\n",
      "[129]\ttrain's my_multiclass_logloss: 0.743184\tvalid's my_multiclass_logloss: 0.794982\n",
      "[130]\ttrain's my_multiclass_logloss: 0.742481\tvalid's my_multiclass_logloss: 0.79199\n",
      "[131]\ttrain's my_multiclass_logloss: 0.741295\tvalid's my_multiclass_logloss: 0.78821\n",
      "[132]\ttrain's my_multiclass_logloss: 0.74083\tvalid's my_multiclass_logloss: 0.787094\n",
      "[133]\ttrain's my_multiclass_logloss: 0.739607\tvalid's my_multiclass_logloss: 0.783658\n",
      "[134]\ttrain's my_multiclass_logloss: 0.738845\tvalid's my_multiclass_logloss: 0.782671\n",
      "[135]\ttrain's my_multiclass_logloss: 0.738227\tvalid's my_multiclass_logloss: 0.785043\n",
      "[136]\ttrain's my_multiclass_logloss: 0.737471\tvalid's my_multiclass_logloss: 0.783172\n",
      "[137]\ttrain's my_multiclass_logloss: 0.736784\tvalid's my_multiclass_logloss: 0.784158\n",
      "[138]\ttrain's my_multiclass_logloss: 0.73688\tvalid's my_multiclass_logloss: 0.784258\n",
      "[139]\ttrain's my_multiclass_logloss: 0.736677\tvalid's my_multiclass_logloss: 0.78561\n",
      "[140]\ttrain's my_multiclass_logloss: 0.736126\tvalid's my_multiclass_logloss: 0.788617\n",
      "[141]\ttrain's my_multiclass_logloss: 0.735435\tvalid's my_multiclass_logloss: 0.785206\n",
      "[142]\ttrain's my_multiclass_logloss: 0.734212\tvalid's my_multiclass_logloss: 0.779359\n",
      "[143]\ttrain's my_multiclass_logloss: 0.732865\tvalid's my_multiclass_logloss: 0.774429\n",
      "[144]\ttrain's my_multiclass_logloss: 0.731904\tvalid's my_multiclass_logloss: 0.770174\n",
      "[145]\ttrain's my_multiclass_logloss: 0.730408\tvalid's my_multiclass_logloss: 0.773186\n",
      "[146]\ttrain's my_multiclass_logloss: 0.729448\tvalid's my_multiclass_logloss: 0.775108\n",
      "[147]\ttrain's my_multiclass_logloss: 0.728672\tvalid's my_multiclass_logloss: 0.77645\n",
      "[148]\ttrain's my_multiclass_logloss: 0.728018\tvalid's my_multiclass_logloss: 0.778477\n",
      "[149]\ttrain's my_multiclass_logloss: 0.726702\tvalid's my_multiclass_logloss: 0.776783\n",
      "[150]\ttrain's my_multiclass_logloss: 0.726017\tvalid's my_multiclass_logloss: 0.776047\n",
      "[151]\ttrain's my_multiclass_logloss: 0.724847\tvalid's my_multiclass_logloss: 0.773823\n",
      "[152]\ttrain's my_multiclass_logloss: 0.723718\tvalid's my_multiclass_logloss: 0.77051\n",
      "[153]\ttrain's my_multiclass_logloss: 0.72343\tvalid's my_multiclass_logloss: 0.773459\n",
      "[154]\ttrain's my_multiclass_logloss: 0.723366\tvalid's my_multiclass_logloss: 0.776417\n",
      "[155]\ttrain's my_multiclass_logloss: 0.723569\tvalid's my_multiclass_logloss: 0.778592\n",
      "[156]\ttrain's my_multiclass_logloss: 0.723302\tvalid's my_multiclass_logloss: 0.780298\n",
      "[157]\ttrain's my_multiclass_logloss: 0.722781\tvalid's my_multiclass_logloss: 0.781219\n",
      "[158]\ttrain's my_multiclass_logloss: 0.721994\tvalid's my_multiclass_logloss: 0.784031\n",
      "[159]\ttrain's my_multiclass_logloss: 0.721273\tvalid's my_multiclass_logloss: 0.782811\n",
      "[160]\ttrain's my_multiclass_logloss: 0.720934\tvalid's my_multiclass_logloss: 0.784763\n",
      "[161]\ttrain's my_multiclass_logloss: 0.718794\tvalid's my_multiclass_logloss: 0.779731\n",
      "[162]\ttrain's my_multiclass_logloss: 0.717724\tvalid's my_multiclass_logloss: 0.77702\n",
      "[163]\ttrain's my_multiclass_logloss: 0.716225\tvalid's my_multiclass_logloss: 0.774015\n",
      "[164]\ttrain's my_multiclass_logloss: 0.714834\tvalid's my_multiclass_logloss: 0.772426\n",
      "[165]\ttrain's my_multiclass_logloss: 0.714338\tvalid's my_multiclass_logloss: 0.775022\n",
      "[166]\ttrain's my_multiclass_logloss: 0.713954\tvalid's my_multiclass_logloss: 0.777407\n",
      "[167]\ttrain's my_multiclass_logloss: 0.713723\tvalid's my_multiclass_logloss: 0.779016\n",
      "[168]\ttrain's my_multiclass_logloss: 0.713591\tvalid's my_multiclass_logloss: 0.78232\n",
      "[169]\ttrain's my_multiclass_logloss: 0.713358\tvalid's my_multiclass_logloss: 0.781186\n",
      "[170]\ttrain's my_multiclass_logloss: 0.7137\tvalid's my_multiclass_logloss: 0.782852\n",
      "[171]\ttrain's my_multiclass_logloss: 0.71402\tvalid's my_multiclass_logloss: 0.784416\n",
      "[172]\ttrain's my_multiclass_logloss: 0.714321\tvalid's my_multiclass_logloss: 0.785985\n",
      "[173]\ttrain's my_multiclass_logloss: 0.713735\tvalid's my_multiclass_logloss: 0.787836\n",
      "[174]\ttrain's my_multiclass_logloss: 0.712996\tvalid's my_multiclass_logloss: 0.782324\n",
      "[175]\ttrain's my_multiclass_logloss: 0.712118\tvalid's my_multiclass_logloss: 0.783103\n",
      "[176]\ttrain's my_multiclass_logloss: 0.711717\tvalid's my_multiclass_logloss: 0.78334\n",
      "[177]\ttrain's my_multiclass_logloss: 0.710845\tvalid's my_multiclass_logloss: 0.782334\n",
      "[178]\ttrain's my_multiclass_logloss: 0.710172\tvalid's my_multiclass_logloss: 0.782251\n",
      "[179]\ttrain's my_multiclass_logloss: 0.709626\tvalid's my_multiclass_logloss: 0.781823\n",
      "[180]\ttrain's my_multiclass_logloss: 0.709513\tvalid's my_multiclass_logloss: 0.78434\n",
      "[181]\ttrain's my_multiclass_logloss: 0.709\tvalid's my_multiclass_logloss: 0.786667\n",
      "[182]\ttrain's my_multiclass_logloss: 0.708272\tvalid's my_multiclass_logloss: 0.78678\n",
      "[183]\ttrain's my_multiclass_logloss: 0.707509\tvalid's my_multiclass_logloss: 0.785675\n",
      "[184]\ttrain's my_multiclass_logloss: 0.707024\tvalid's my_multiclass_logloss: 0.785894\n",
      "[185]\ttrain's my_multiclass_logloss: 0.70576\tvalid's my_multiclass_logloss: 0.787919\n",
      "[186]\ttrain's my_multiclass_logloss: 0.704518\tvalid's my_multiclass_logloss: 0.788298\n",
      "[187]\ttrain's my_multiclass_logloss: 0.703806\tvalid's my_multiclass_logloss: 0.789894\n",
      "[188]\ttrain's my_multiclass_logloss: 0.702809\tvalid's my_multiclass_logloss: 0.7919\n",
      "[189]\ttrain's my_multiclass_logloss: 0.702378\tvalid's my_multiclass_logloss: 0.793503\n",
      "[190]\ttrain's my_multiclass_logloss: 0.701588\tvalid's my_multiclass_logloss: 0.792248\n",
      "[191]\ttrain's my_multiclass_logloss: 0.700896\tvalid's my_multiclass_logloss: 0.791421\n",
      "[192]\ttrain's my_multiclass_logloss: 0.699462\tvalid's my_multiclass_logloss: 0.787533\n",
      "[193]\ttrain's my_multiclass_logloss: 0.699377\tvalid's my_multiclass_logloss: 0.790961\n",
      "[194]\ttrain's my_multiclass_logloss: 0.699482\tvalid's my_multiclass_logloss: 0.796031\n",
      "[195]\ttrain's my_multiclass_logloss: 0.699425\tvalid's my_multiclass_logloss: 0.800043\n",
      "[196]\ttrain's my_multiclass_logloss: 0.699432\tvalid's my_multiclass_logloss: 0.803804\n",
      "[197]\ttrain's my_multiclass_logloss: 0.69854\tvalid's my_multiclass_logloss: 0.799312\n",
      "[198]\ttrain's my_multiclass_logloss: 0.698173\tvalid's my_multiclass_logloss: 0.797385\n",
      "[199]\ttrain's my_multiclass_logloss: 0.69755\tvalid's my_multiclass_logloss: 0.79356\n",
      "[200]\ttrain's my_multiclass_logloss: 0.697384\tvalid's my_multiclass_logloss: 0.792695\n",
      "[201]\ttrain's my_multiclass_logloss: 0.697184\tvalid's my_multiclass_logloss: 0.797165\n",
      "[202]\ttrain's my_multiclass_logloss: 0.697075\tvalid's my_multiclass_logloss: 0.795186\n",
      "[203]\ttrain's my_multiclass_logloss: 0.696967\tvalid's my_multiclass_logloss: 0.795377\n",
      "[204]\ttrain's my_multiclass_logloss: 0.696776\tvalid's my_multiclass_logloss: 0.792326\n",
      "[205]\ttrain's my_multiclass_logloss: 0.696287\tvalid's my_multiclass_logloss: 0.789602\n",
      "[206]\ttrain's my_multiclass_logloss: 0.695934\tvalid's my_multiclass_logloss: 0.787098\n",
      "[207]\ttrain's my_multiclass_logloss: 0.695827\tvalid's my_multiclass_logloss: 0.786215\n",
      "[208]\ttrain's my_multiclass_logloss: 0.695848\tvalid's my_multiclass_logloss: 0.784507\n",
      "[209]\ttrain's my_multiclass_logloss: 0.695071\tvalid's my_multiclass_logloss: 0.789038\n",
      "[210]\ttrain's my_multiclass_logloss: 0.694454\tvalid's my_multiclass_logloss: 0.79349\n",
      "[211]\ttrain's my_multiclass_logloss: 0.693981\tvalid's my_multiclass_logloss: 0.797927\n",
      "[212]\ttrain's my_multiclass_logloss: 0.693403\tvalid's my_multiclass_logloss: 0.799042\n",
      "[213]\ttrain's my_multiclass_logloss: 0.693178\tvalid's my_multiclass_logloss: 0.796934\n",
      "[214]\ttrain's my_multiclass_logloss: 0.692854\tvalid's my_multiclass_logloss: 0.794525\n",
      "[215]\ttrain's my_multiclass_logloss: 0.69274\tvalid's my_multiclass_logloss: 0.793999\n",
      "[216]\ttrain's my_multiclass_logloss: 0.692477\tvalid's my_multiclass_logloss: 0.790564\n",
      "[217]\ttrain's my_multiclass_logloss: 0.691476\tvalid's my_multiclass_logloss: 0.783715\n",
      "[218]\ttrain's my_multiclass_logloss: 0.690694\tvalid's my_multiclass_logloss: 0.780201\n",
      "[219]\ttrain's my_multiclass_logloss: 0.69008\tvalid's my_multiclass_logloss: 0.777023\n",
      "[220]\ttrain's my_multiclass_logloss: 0.6895\tvalid's my_multiclass_logloss: 0.772014\n",
      "[221]\ttrain's my_multiclass_logloss: 0.689\tvalid's my_multiclass_logloss: 0.769129\n",
      "[222]\ttrain's my_multiclass_logloss: 0.688766\tvalid's my_multiclass_logloss: 0.76654\n",
      "[223]\ttrain's my_multiclass_logloss: 0.688321\tvalid's my_multiclass_logloss: 0.763774\n",
      "[224]\ttrain's my_multiclass_logloss: 0.687866\tvalid's my_multiclass_logloss: 0.76128\n",
      "[225]\ttrain's my_multiclass_logloss: 0.68726\tvalid's my_multiclass_logloss: 0.760652\n",
      "[226]\ttrain's my_multiclass_logloss: 0.687147\tvalid's my_multiclass_logloss: 0.765196\n",
      "[227]\ttrain's my_multiclass_logloss: 0.687016\tvalid's my_multiclass_logloss: 0.768731\n",
      "[228]\ttrain's my_multiclass_logloss: 0.68703\tvalid's my_multiclass_logloss: 0.772291\n",
      "[229]\ttrain's my_multiclass_logloss: 0.686071\tvalid's my_multiclass_logloss: 0.769068\n",
      "[230]\ttrain's my_multiclass_logloss: 0.685174\tvalid's my_multiclass_logloss: 0.772562\n",
      "[231]\ttrain's my_multiclass_logloss: 0.684523\tvalid's my_multiclass_logloss: 0.773573\n",
      "[232]\ttrain's my_multiclass_logloss: 0.684151\tvalid's my_multiclass_logloss: 0.775159\n",
      "[233]\ttrain's my_multiclass_logloss: 0.68279\tvalid's my_multiclass_logloss: 0.774332\n",
      "[234]\ttrain's my_multiclass_logloss: 0.682233\tvalid's my_multiclass_logloss: 0.77438\n",
      "[235]\ttrain's my_multiclass_logloss: 0.681963\tvalid's my_multiclass_logloss: 0.775841\n",
      "[236]\ttrain's my_multiclass_logloss: 0.680907\tvalid's my_multiclass_logloss: 0.775099\n",
      "[237]\ttrain's my_multiclass_logloss: 0.680522\tvalid's my_multiclass_logloss: 0.770825\n",
      "[238]\ttrain's my_multiclass_logloss: 0.680291\tvalid's my_multiclass_logloss: 0.767155\n",
      "[239]\ttrain's my_multiclass_logloss: 0.680038\tvalid's my_multiclass_logloss: 0.766482\n",
      "[240]\ttrain's my_multiclass_logloss: 0.679821\tvalid's my_multiclass_logloss: 0.7646\n",
      "[241]\ttrain's my_multiclass_logloss: 0.679399\tvalid's my_multiclass_logloss: 0.761627\n",
      "[242]\ttrain's my_multiclass_logloss: 0.679026\tvalid's my_multiclass_logloss: 0.762329\n",
      "[243]\ttrain's my_multiclass_logloss: 0.678653\tvalid's my_multiclass_logloss: 0.756173\n",
      "[244]\ttrain's my_multiclass_logloss: 0.678566\tvalid's my_multiclass_logloss: 0.754358\n",
      "[245]\ttrain's my_multiclass_logloss: 0.677183\tvalid's my_multiclass_logloss: 0.751185\n",
      "[246]\ttrain's my_multiclass_logloss: 0.67589\tvalid's my_multiclass_logloss: 0.747375\n",
      "[247]\ttrain's my_multiclass_logloss: 0.674549\tvalid's my_multiclass_logloss: 0.743109\n",
      "[248]\ttrain's my_multiclass_logloss: 0.674029\tvalid's my_multiclass_logloss: 0.742366\n",
      "[249]\ttrain's my_multiclass_logloss: 0.673697\tvalid's my_multiclass_logloss: 0.740027\n",
      "[250]\ttrain's my_multiclass_logloss: 0.673433\tvalid's my_multiclass_logloss: 0.736957\n",
      "[251]\ttrain's my_multiclass_logloss: 0.673203\tvalid's my_multiclass_logloss: 0.73373\n",
      "[252]\ttrain's my_multiclass_logloss: 0.672955\tvalid's my_multiclass_logloss: 0.733946\n",
      "[253]\ttrain's my_multiclass_logloss: 0.672147\tvalid's my_multiclass_logloss: 0.73118\n",
      "[254]\ttrain's my_multiclass_logloss: 0.671452\tvalid's my_multiclass_logloss: 0.730518\n",
      "[255]\ttrain's my_multiclass_logloss: 0.670992\tvalid's my_multiclass_logloss: 0.72962\n",
      "[256]\ttrain's my_multiclass_logloss: 0.670537\tvalid's my_multiclass_logloss: 0.725159\n",
      "[257]\ttrain's my_multiclass_logloss: 0.669886\tvalid's my_multiclass_logloss: 0.732033\n",
      "[258]\ttrain's my_multiclass_logloss: 0.669607\tvalid's my_multiclass_logloss: 0.734761\n",
      "[259]\ttrain's my_multiclass_logloss: 0.66924\tvalid's my_multiclass_logloss: 0.741673\n",
      "[260]\ttrain's my_multiclass_logloss: 0.668938\tvalid's my_multiclass_logloss: 0.748895\n",
      "[261]\ttrain's my_multiclass_logloss: 0.668002\tvalid's my_multiclass_logloss: 0.744233\n",
      "[262]\ttrain's my_multiclass_logloss: 0.667367\tvalid's my_multiclass_logloss: 0.742047\n",
      "[263]\ttrain's my_multiclass_logloss: 0.666731\tvalid's my_multiclass_logloss: 0.739923\n",
      "[264]\ttrain's my_multiclass_logloss: 0.666444\tvalid's my_multiclass_logloss: 0.738868\n",
      "[265]\ttrain's my_multiclass_logloss: 0.665929\tvalid's my_multiclass_logloss: 0.73802\n",
      "[266]\ttrain's my_multiclass_logloss: 0.665603\tvalid's my_multiclass_logloss: 0.737759\n",
      "[267]\ttrain's my_multiclass_logloss: 0.665391\tvalid's my_multiclass_logloss: 0.738065\n",
      "[268]\ttrain's my_multiclass_logloss: 0.665341\tvalid's my_multiclass_logloss: 0.738467\n",
      "[269]\ttrain's my_multiclass_logloss: 0.66504\tvalid's my_multiclass_logloss: 0.734843\n",
      "[270]\ttrain's my_multiclass_logloss: 0.66514\tvalid's my_multiclass_logloss: 0.73498\n",
      "[271]\ttrain's my_multiclass_logloss: 0.665376\tvalid's my_multiclass_logloss: 0.732722\n",
      "[272]\ttrain's my_multiclass_logloss: 0.665573\tvalid's my_multiclass_logloss: 0.732638\n",
      "[273]\ttrain's my_multiclass_logloss: 0.664725\tvalid's my_multiclass_logloss: 0.731124\n",
      "[274]\ttrain's my_multiclass_logloss: 0.663907\tvalid's my_multiclass_logloss: 0.727845\n",
      "[275]\ttrain's my_multiclass_logloss: 0.663114\tvalid's my_multiclass_logloss: 0.724968\n",
      "[276]\ttrain's my_multiclass_logloss: 0.662577\tvalid's my_multiclass_logloss: 0.722905\n",
      "[277]\ttrain's my_multiclass_logloss: 0.661527\tvalid's my_multiclass_logloss: 0.72406\n",
      "[278]\ttrain's my_multiclass_logloss: 0.660552\tvalid's my_multiclass_logloss: 0.724044\n",
      "[279]\ttrain's my_multiclass_logloss: 0.659911\tvalid's my_multiclass_logloss: 0.7258\n",
      "[280]\ttrain's my_multiclass_logloss: 0.659399\tvalid's my_multiclass_logloss: 0.727595\n",
      "[281]\ttrain's my_multiclass_logloss: 0.658776\tvalid's my_multiclass_logloss: 0.72434\n",
      "[282]\ttrain's my_multiclass_logloss: 0.658803\tvalid's my_multiclass_logloss: 0.726506\n",
      "[283]\ttrain's my_multiclass_logloss: 0.658621\tvalid's my_multiclass_logloss: 0.728532\n",
      "[284]\ttrain's my_multiclass_logloss: 0.658072\tvalid's my_multiclass_logloss: 0.725234\n",
      "[285]\ttrain's my_multiclass_logloss: 0.657427\tvalid's my_multiclass_logloss: 0.722985\n",
      "[286]\ttrain's my_multiclass_logloss: 0.656925\tvalid's my_multiclass_logloss: 0.721042\n",
      "[287]\ttrain's my_multiclass_logloss: 0.656548\tvalid's my_multiclass_logloss: 0.71937\n",
      "[288]\ttrain's my_multiclass_logloss: 0.656345\tvalid's my_multiclass_logloss: 0.716772\n",
      "[289]\ttrain's my_multiclass_logloss: 0.655964\tvalid's my_multiclass_logloss: 0.715866\n",
      "[290]\ttrain's my_multiclass_logloss: 0.65537\tvalid's my_multiclass_logloss: 0.714643\n",
      "[291]\ttrain's my_multiclass_logloss: 0.654924\tvalid's my_multiclass_logloss: 0.712085\n",
      "[292]\ttrain's my_multiclass_logloss: 0.654384\tvalid's my_multiclass_logloss: 0.712293\n",
      "[293]\ttrain's my_multiclass_logloss: 0.65413\tvalid's my_multiclass_logloss: 0.713902\n",
      "[294]\ttrain's my_multiclass_logloss: 0.653433\tvalid's my_multiclass_logloss: 0.716267\n",
      "[295]\ttrain's my_multiclass_logloss: 0.652895\tvalid's my_multiclass_logloss: 0.717483\n",
      "[296]\ttrain's my_multiclass_logloss: 0.652354\tvalid's my_multiclass_logloss: 0.719282\n",
      "[297]\ttrain's my_multiclass_logloss: 0.651816\tvalid's my_multiclass_logloss: 0.716306\n",
      "[298]\ttrain's my_multiclass_logloss: 0.651\tvalid's my_multiclass_logloss: 0.71353\n",
      "[299]\ttrain's my_multiclass_logloss: 0.650903\tvalid's my_multiclass_logloss: 0.714341\n",
      "[300]\ttrain's my_multiclass_logloss: 0.650454\tvalid's my_multiclass_logloss: 0.711579\n",
      "[301]\ttrain's my_multiclass_logloss: 0.650257\tvalid's my_multiclass_logloss: 0.710663\n",
      "[302]\ttrain's my_multiclass_logloss: 0.649936\tvalid's my_multiclass_logloss: 0.709725\n",
      "[303]\ttrain's my_multiclass_logloss: 0.649779\tvalid's my_multiclass_logloss: 0.707022\n",
      "[304]\ttrain's my_multiclass_logloss: 0.649676\tvalid's my_multiclass_logloss: 0.706332\n",
      "[305]\ttrain's my_multiclass_logloss: 0.648568\tvalid's my_multiclass_logloss: 0.703048\n",
      "[306]\ttrain's my_multiclass_logloss: 0.648228\tvalid's my_multiclass_logloss: 0.699379\n",
      "[307]\ttrain's my_multiclass_logloss: 0.647987\tvalid's my_multiclass_logloss: 0.695917\n",
      "[308]\ttrain's my_multiclass_logloss: 0.64723\tvalid's my_multiclass_logloss: 0.693813\n",
      "[309]\ttrain's my_multiclass_logloss: 0.646995\tvalid's my_multiclass_logloss: 0.697406\n",
      "[310]\ttrain's my_multiclass_logloss: 0.647017\tvalid's my_multiclass_logloss: 0.700112\n",
      "[311]\ttrain's my_multiclass_logloss: 0.646962\tvalid's my_multiclass_logloss: 0.703052\n",
      "[312]\ttrain's my_multiclass_logloss: 0.647115\tvalid's my_multiclass_logloss: 0.706676\n",
      "[313]\ttrain's my_multiclass_logloss: 0.646646\tvalid's my_multiclass_logloss: 0.702894\n",
      "[314]\ttrain's my_multiclass_logloss: 0.646634\tvalid's my_multiclass_logloss: 0.705762\n",
      "[315]\ttrain's my_multiclass_logloss: 0.646448\tvalid's my_multiclass_logloss: 0.704695\n",
      "[316]\ttrain's my_multiclass_logloss: 0.646824\tvalid's my_multiclass_logloss: 0.705679\n",
      "[317]\ttrain's my_multiclass_logloss: 0.645655\tvalid's my_multiclass_logloss: 0.705091\n",
      "[318]\ttrain's my_multiclass_logloss: 0.645408\tvalid's my_multiclass_logloss: 0.704494\n",
      "[319]\ttrain's my_multiclass_logloss: 0.644623\tvalid's my_multiclass_logloss: 0.702663\n",
      "[320]\ttrain's my_multiclass_logloss: 0.643745\tvalid's my_multiclass_logloss: 0.701418\n",
      "[321]\ttrain's my_multiclass_logloss: 0.642932\tvalid's my_multiclass_logloss: 0.700865\n",
      "[322]\ttrain's my_multiclass_logloss: 0.642351\tvalid's my_multiclass_logloss: 0.700474\n",
      "[323]\ttrain's my_multiclass_logloss: 0.641968\tvalid's my_multiclass_logloss: 0.700222\n",
      "[324]\ttrain's my_multiclass_logloss: 0.641686\tvalid's my_multiclass_logloss: 0.701192\n",
      "[325]\ttrain's my_multiclass_logloss: 0.640834\tvalid's my_multiclass_logloss: 0.700222\n",
      "[326]\ttrain's my_multiclass_logloss: 0.640364\tvalid's my_multiclass_logloss: 0.701074\n",
      "[327]\ttrain's my_multiclass_logloss: 0.639957\tvalid's my_multiclass_logloss: 0.697206\n",
      "[328]\ttrain's my_multiclass_logloss: 0.639875\tvalid's my_multiclass_logloss: 0.694418\n",
      "[329]\ttrain's my_multiclass_logloss: 0.639594\tvalid's my_multiclass_logloss: 0.695118\n",
      "[330]\ttrain's my_multiclass_logloss: 0.639572\tvalid's my_multiclass_logloss: 0.692586\n",
      "[331]\ttrain's my_multiclass_logloss: 0.639284\tvalid's my_multiclass_logloss: 0.694372\n",
      "[332]\ttrain's my_multiclass_logloss: 0.63927\tvalid's my_multiclass_logloss: 0.695255\n",
      "[333]\ttrain's my_multiclass_logloss: 0.638502\tvalid's my_multiclass_logloss: 0.699791\n",
      "[334]\ttrain's my_multiclass_logloss: 0.637862\tvalid's my_multiclass_logloss: 0.703934\n",
      "[335]\ttrain's my_multiclass_logloss: 0.637397\tvalid's my_multiclass_logloss: 0.705312\n",
      "[336]\ttrain's my_multiclass_logloss: 0.636865\tvalid's my_multiclass_logloss: 0.708629\n",
      "[337]\ttrain's my_multiclass_logloss: 0.636197\tvalid's my_multiclass_logloss: 0.706722\n",
      "[338]\ttrain's my_multiclass_logloss: 0.63569\tvalid's my_multiclass_logloss: 0.706822\n",
      "[339]\ttrain's my_multiclass_logloss: 0.635015\tvalid's my_multiclass_logloss: 0.704075\n",
      "[340]\ttrain's my_multiclass_logloss: 0.634585\tvalid's my_multiclass_logloss: 0.702545\n",
      "[341]\ttrain's my_multiclass_logloss: 0.633648\tvalid's my_multiclass_logloss: 0.702497\n",
      "[342]\ttrain's my_multiclass_logloss: 0.633082\tvalid's my_multiclass_logloss: 0.701362\n",
      "[343]\ttrain's my_multiclass_logloss: 0.632641\tvalid's my_multiclass_logloss: 0.703278\n",
      "[344]\ttrain's my_multiclass_logloss: 0.631922\tvalid's my_multiclass_logloss: 0.703422\n",
      "[345]\ttrain's my_multiclass_logloss: 0.63172\tvalid's my_multiclass_logloss: 0.699946\n",
      "[346]\ttrain's my_multiclass_logloss: 0.631621\tvalid's my_multiclass_logloss: 0.695783\n",
      "[347]\ttrain's my_multiclass_logloss: 0.631644\tvalid's my_multiclass_logloss: 0.69194\n",
      "[348]\ttrain's my_multiclass_logloss: 0.63147\tvalid's my_multiclass_logloss: 0.687224\n",
      "[349]\ttrain's my_multiclass_logloss: 0.631255\tvalid's my_multiclass_logloss: 0.683796\n",
      "[350]\ttrain's my_multiclass_logloss: 0.631111\tvalid's my_multiclass_logloss: 0.680545\n",
      "[351]\ttrain's my_multiclass_logloss: 0.631265\tvalid's my_multiclass_logloss: 0.677255\n",
      "[352]\ttrain's my_multiclass_logloss: 0.631308\tvalid's my_multiclass_logloss: 0.675798\n",
      "[353]\ttrain's my_multiclass_logloss: 0.630876\tvalid's my_multiclass_logloss: 0.677506\n",
      "[354]\ttrain's my_multiclass_logloss: 0.630778\tvalid's my_multiclass_logloss: 0.677102\n",
      "[355]\ttrain's my_multiclass_logloss: 0.630329\tvalid's my_multiclass_logloss: 0.680107\n",
      "[356]\ttrain's my_multiclass_logloss: 0.629837\tvalid's my_multiclass_logloss: 0.683811\n",
      "[357]\ttrain's my_multiclass_logloss: 0.628762\tvalid's my_multiclass_logloss: 0.679292\n",
      "[358]\ttrain's my_multiclass_logloss: 0.628077\tvalid's my_multiclass_logloss: 0.67828\n",
      "[359]\ttrain's my_multiclass_logloss: 0.628008\tvalid's my_multiclass_logloss: 0.67732\n",
      "[360]\ttrain's my_multiclass_logloss: 0.627547\tvalid's my_multiclass_logloss: 0.674848\n",
      "[361]\ttrain's my_multiclass_logloss: 0.626988\tvalid's my_multiclass_logloss: 0.675171\n",
      "[362]\ttrain's my_multiclass_logloss: 0.626163\tvalid's my_multiclass_logloss: 0.679234\n",
      "[363]\ttrain's my_multiclass_logloss: 0.625477\tvalid's my_multiclass_logloss: 0.681888\n",
      "[364]\ttrain's my_multiclass_logloss: 0.625162\tvalid's my_multiclass_logloss: 0.68355\n",
      "[365]\ttrain's my_multiclass_logloss: 0.624621\tvalid's my_multiclass_logloss: 0.685265\n",
      "[366]\ttrain's my_multiclass_logloss: 0.624194\tvalid's my_multiclass_logloss: 0.687074\n",
      "[367]\ttrain's my_multiclass_logloss: 0.623851\tvalid's my_multiclass_logloss: 0.688514\n",
      "[368]\ttrain's my_multiclass_logloss: 0.623457\tvalid's my_multiclass_logloss: 0.691271\n",
      "[369]\ttrain's my_multiclass_logloss: 0.62335\tvalid's my_multiclass_logloss: 0.688859\n",
      "[370]\ttrain's my_multiclass_logloss: 0.623304\tvalid's my_multiclass_logloss: 0.689926\n",
      "[371]\ttrain's my_multiclass_logloss: 0.623438\tvalid's my_multiclass_logloss: 0.690557\n",
      "[372]\ttrain's my_multiclass_logloss: 0.62372\tvalid's my_multiclass_logloss: 0.691187\n",
      "[373]\ttrain's my_multiclass_logloss: 0.623187\tvalid's my_multiclass_logloss: 0.691035\n",
      "[374]\ttrain's my_multiclass_logloss: 0.622214\tvalid's my_multiclass_logloss: 0.69134\n",
      "[375]\ttrain's my_multiclass_logloss: 0.621621\tvalid's my_multiclass_logloss: 0.692147\n",
      "[376]\ttrain's my_multiclass_logloss: 0.620975\tvalid's my_multiclass_logloss: 0.692853\n",
      "[377]\ttrain's my_multiclass_logloss: 0.620786\tvalid's my_multiclass_logloss: 0.690893\n",
      "[378]\ttrain's my_multiclass_logloss: 0.620536\tvalid's my_multiclass_logloss: 0.690578\n",
      "[379]\ttrain's my_multiclass_logloss: 0.62065\tvalid's my_multiclass_logloss: 0.688783\n",
      "[380]\ttrain's my_multiclass_logloss: 0.620602\tvalid's my_multiclass_logloss: 0.689128\n",
      "[381]\ttrain's my_multiclass_logloss: 0.619571\tvalid's my_multiclass_logloss: 0.689965\n",
      "[382]\ttrain's my_multiclass_logloss: 0.61812\tvalid's my_multiclass_logloss: 0.68861\n",
      "[383]\ttrain's my_multiclass_logloss: 0.617453\tvalid's my_multiclass_logloss: 0.690663\n",
      "[384]\ttrain's my_multiclass_logloss: 0.6169\tvalid's my_multiclass_logloss: 0.692389\n",
      "[385]\ttrain's my_multiclass_logloss: 0.616655\tvalid's my_multiclass_logloss: 0.692776\n",
      "[386]\ttrain's my_multiclass_logloss: 0.616234\tvalid's my_multiclass_logloss: 0.692679\n",
      "[387]\ttrain's my_multiclass_logloss: 0.615893\tvalid's my_multiclass_logloss: 0.691793\n",
      "[388]\ttrain's my_multiclass_logloss: 0.615579\tvalid's my_multiclass_logloss: 0.690633\n",
      "[389]\ttrain's my_multiclass_logloss: 0.614939\tvalid's my_multiclass_logloss: 0.688392\n",
      "[390]\ttrain's my_multiclass_logloss: 0.614481\tvalid's my_multiclass_logloss: 0.68778\n",
      "[391]\ttrain's my_multiclass_logloss: 0.614114\tvalid's my_multiclass_logloss: 0.689049\n",
      "[392]\ttrain's my_multiclass_logloss: 0.613697\tvalid's my_multiclass_logloss: 0.687099\n",
      "[393]\ttrain's my_multiclass_logloss: 0.613017\tvalid's my_multiclass_logloss: 0.686763\n",
      "[394]\ttrain's my_multiclass_logloss: 0.612724\tvalid's my_multiclass_logloss: 0.687986\n",
      "[395]\ttrain's my_multiclass_logloss: 0.612398\tvalid's my_multiclass_logloss: 0.688011\n",
      "[396]\ttrain's my_multiclass_logloss: 0.611983\tvalid's my_multiclass_logloss: 0.687699\n",
      "[397]\ttrain's my_multiclass_logloss: 0.611594\tvalid's my_multiclass_logloss: 0.685451\n",
      "[398]\ttrain's my_multiclass_logloss: 0.611356\tvalid's my_multiclass_logloss: 0.682047\n",
      "[399]\ttrain's my_multiclass_logloss: 0.611232\tvalid's my_multiclass_logloss: 0.67886\n",
      "[400]\ttrain's my_multiclass_logloss: 0.611172\tvalid's my_multiclass_logloss: 0.677687\n",
      "[401]\ttrain's my_multiclass_logloss: 0.610715\tvalid's my_multiclass_logloss: 0.678604\n",
      "[402]\ttrain's my_multiclass_logloss: 0.610625\tvalid's my_multiclass_logloss: 0.680081\n",
      "[403]\ttrain's my_multiclass_logloss: 0.610448\tvalid's my_multiclass_logloss: 0.681031\n",
      "[404]\ttrain's my_multiclass_logloss: 0.61035\tvalid's my_multiclass_logloss: 0.681421\n",
      "[405]\ttrain's my_multiclass_logloss: 0.610135\tvalid's my_multiclass_logloss: 0.68058\n",
      "[406]\ttrain's my_multiclass_logloss: 0.609684\tvalid's my_multiclass_logloss: 0.681554\n",
      "[407]\ttrain's my_multiclass_logloss: 0.609233\tvalid's my_multiclass_logloss: 0.684192\n",
      "[408]\ttrain's my_multiclass_logloss: 0.60918\tvalid's my_multiclass_logloss: 0.685254\n",
      "[409]\ttrain's my_multiclass_logloss: 0.608935\tvalid's my_multiclass_logloss: 0.683191\n",
      "[410]\ttrain's my_multiclass_logloss: 0.608671\tvalid's my_multiclass_logloss: 0.683932\n",
      "[411]\ttrain's my_multiclass_logloss: 0.608318\tvalid's my_multiclass_logloss: 0.682485\n",
      "[412]\ttrain's my_multiclass_logloss: 0.608115\tvalid's my_multiclass_logloss: 0.68048\n",
      "[413]\ttrain's my_multiclass_logloss: 0.608018\tvalid's my_multiclass_logloss: 0.682614\n",
      "[414]\ttrain's my_multiclass_logloss: 0.607904\tvalid's my_multiclass_logloss: 0.683697\n",
      "[415]\ttrain's my_multiclass_logloss: 0.608006\tvalid's my_multiclass_logloss: 0.683444\n",
      "[416]\ttrain's my_multiclass_logloss: 0.608352\tvalid's my_multiclass_logloss: 0.683401\n",
      "[417]\ttrain's my_multiclass_logloss: 0.607876\tvalid's my_multiclass_logloss: 0.68415\n",
      "[418]\ttrain's my_multiclass_logloss: 0.60716\tvalid's my_multiclass_logloss: 0.686696\n",
      "[419]\ttrain's my_multiclass_logloss: 0.606947\tvalid's my_multiclass_logloss: 0.687585\n",
      "[420]\ttrain's my_multiclass_logloss: 0.607107\tvalid's my_multiclass_logloss: 0.687327\n",
      "[421]\ttrain's my_multiclass_logloss: 0.606483\tvalid's my_multiclass_logloss: 0.688806\n",
      "[422]\ttrain's my_multiclass_logloss: 0.606271\tvalid's my_multiclass_logloss: 0.687526\n",
      "[423]\ttrain's my_multiclass_logloss: 0.605905\tvalid's my_multiclass_logloss: 0.690311\n",
      "[424]\ttrain's my_multiclass_logloss: 0.605618\tvalid's my_multiclass_logloss: 0.691855\n",
      "[425]\ttrain's my_multiclass_logloss: 0.604715\tvalid's my_multiclass_logloss: 0.691706\n",
      "[426]\ttrain's my_multiclass_logloss: 0.603758\tvalid's my_multiclass_logloss: 0.69414\n",
      "[427]\ttrain's my_multiclass_logloss: 0.602842\tvalid's my_multiclass_logloss: 0.695661\n",
      "[428]\ttrain's my_multiclass_logloss: 0.602075\tvalid's my_multiclass_logloss: 0.698132\n",
      "[429]\ttrain's my_multiclass_logloss: 0.602218\tvalid's my_multiclass_logloss: 0.697934\n",
      "[430]\ttrain's my_multiclass_logloss: 0.602249\tvalid's my_multiclass_logloss: 0.696481\n",
      "[431]\ttrain's my_multiclass_logloss: 0.602481\tvalid's my_multiclass_logloss: 0.695151\n",
      "[432]\ttrain's my_multiclass_logloss: 0.602734\tvalid's my_multiclass_logloss: 0.693748\n",
      "[433]\ttrain's my_multiclass_logloss: 0.601952\tvalid's my_multiclass_logloss: 0.690732\n",
      "[434]\ttrain's my_multiclass_logloss: 0.601272\tvalid's my_multiclass_logloss: 0.689359\n",
      "[435]\ttrain's my_multiclass_logloss: 0.600712\tvalid's my_multiclass_logloss: 0.688136\n",
      "[436]\ttrain's my_multiclass_logloss: 0.600218\tvalid's my_multiclass_logloss: 0.685986\n",
      "[437]\ttrain's my_multiclass_logloss: 0.600038\tvalid's my_multiclass_logloss: 0.685608\n",
      "[438]\ttrain's my_multiclass_logloss: 0.599575\tvalid's my_multiclass_logloss: 0.684405\n",
      "[439]\ttrain's my_multiclass_logloss: 0.599441\tvalid's my_multiclass_logloss: 0.684073\n",
      "[440]\ttrain's my_multiclass_logloss: 0.599216\tvalid's my_multiclass_logloss: 0.683183\n",
      "[441]\ttrain's my_multiclass_logloss: 0.598657\tvalid's my_multiclass_logloss: 0.681225\n",
      "[442]\ttrain's my_multiclass_logloss: 0.598175\tvalid's my_multiclass_logloss: 0.679747\n",
      "[443]\ttrain's my_multiclass_logloss: 0.597842\tvalid's my_multiclass_logloss: 0.678163\n",
      "[444]\ttrain's my_multiclass_logloss: 0.597614\tvalid's my_multiclass_logloss: 0.67676\n",
      "[445]\ttrain's my_multiclass_logloss: 0.597185\tvalid's my_multiclass_logloss: 0.676619\n",
      "[446]\ttrain's my_multiclass_logloss: 0.5966\tvalid's my_multiclass_logloss: 0.676197\n",
      "[447]\ttrain's my_multiclass_logloss: 0.59614\tvalid's my_multiclass_logloss: 0.675914\n",
      "[448]\ttrain's my_multiclass_logloss: 0.595808\tvalid's my_multiclass_logloss: 0.675919\n",
      "[449]\ttrain's my_multiclass_logloss: 0.595327\tvalid's my_multiclass_logloss: 0.676127\n",
      "[450]\ttrain's my_multiclass_logloss: 0.594841\tvalid's my_multiclass_logloss: 0.674386\n",
      "[451]\ttrain's my_multiclass_logloss: 0.594349\tvalid's my_multiclass_logloss: 0.673753\n",
      "[452]\ttrain's my_multiclass_logloss: 0.594109\tvalid's my_multiclass_logloss: 0.67402\n",
      "[453]\ttrain's my_multiclass_logloss: 0.593797\tvalid's my_multiclass_logloss: 0.675718\n",
      "[454]\ttrain's my_multiclass_logloss: 0.593816\tvalid's my_multiclass_logloss: 0.675274\n",
      "[455]\ttrain's my_multiclass_logloss: 0.593448\tvalid's my_multiclass_logloss: 0.675776\n",
      "[456]\ttrain's my_multiclass_logloss: 0.593265\tvalid's my_multiclass_logloss: 0.678163\n",
      "[457]\ttrain's my_multiclass_logloss: 0.593228\tvalid's my_multiclass_logloss: 0.677952\n",
      "[458]\ttrain's my_multiclass_logloss: 0.593082\tvalid's my_multiclass_logloss: 0.679668\n",
      "[459]\ttrain's my_multiclass_logloss: 0.592644\tvalid's my_multiclass_logloss: 0.682174\n",
      "[460]\ttrain's my_multiclass_logloss: 0.592857\tvalid's my_multiclass_logloss: 0.682236\n",
      "[461]\ttrain's my_multiclass_logloss: 0.591611\tvalid's my_multiclass_logloss: 0.681115\n",
      "[462]\ttrain's my_multiclass_logloss: 0.591062\tvalid's my_multiclass_logloss: 0.680607\n",
      "[463]\ttrain's my_multiclass_logloss: 0.590366\tvalid's my_multiclass_logloss: 0.681501\n",
      "[464]\ttrain's my_multiclass_logloss: 0.590151\tvalid's my_multiclass_logloss: 0.68082\n",
      "[465]\ttrain's my_multiclass_logloss: 0.589493\tvalid's my_multiclass_logloss: 0.683089\n",
      "[466]\ttrain's my_multiclass_logloss: 0.589052\tvalid's my_multiclass_logloss: 0.685647\n",
      "[467]\ttrain's my_multiclass_logloss: 0.588854\tvalid's my_multiclass_logloss: 0.687708\n",
      "[468]\ttrain's my_multiclass_logloss: 0.588626\tvalid's my_multiclass_logloss: 0.687841\n",
      "[469]\ttrain's my_multiclass_logloss: 0.588163\tvalid's my_multiclass_logloss: 0.684921\n",
      "[470]\ttrain's my_multiclass_logloss: 0.5879\tvalid's my_multiclass_logloss: 0.684108\n",
      "[471]\ttrain's my_multiclass_logloss: 0.58777\tvalid's my_multiclass_logloss: 0.682289\n",
      "[472]\ttrain's my_multiclass_logloss: 0.587598\tvalid's my_multiclass_logloss: 0.68129\n",
      "[473]\ttrain's my_multiclass_logloss: 0.587342\tvalid's my_multiclass_logloss: 0.68001\n",
      "[474]\ttrain's my_multiclass_logloss: 0.587174\tvalid's my_multiclass_logloss: 0.678825\n",
      "[475]\ttrain's my_multiclass_logloss: 0.58698\tvalid's my_multiclass_logloss: 0.679238\n",
      "[476]\ttrain's my_multiclass_logloss: 0.586857\tvalid's my_multiclass_logloss: 0.679679\n",
      "[477]\ttrain's my_multiclass_logloss: 0.586666\tvalid's my_multiclass_logloss: 0.6813\n",
      "[478]\ttrain's my_multiclass_logloss: 0.586293\tvalid's my_multiclass_logloss: 0.683233\n",
      "[479]\ttrain's my_multiclass_logloss: 0.585794\tvalid's my_multiclass_logloss: 0.684428\n",
      "[480]\ttrain's my_multiclass_logloss: 0.585288\tvalid's my_multiclass_logloss: 0.684956\n",
      "[481]\ttrain's my_multiclass_logloss: 0.584592\tvalid's my_multiclass_logloss: 0.682741\n",
      "[482]\ttrain's my_multiclass_logloss: 0.584262\tvalid's my_multiclass_logloss: 0.681946\n",
      "[483]\ttrain's my_multiclass_logloss: 0.583877\tvalid's my_multiclass_logloss: 0.680349\n",
      "[484]\ttrain's my_multiclass_logloss: 0.583438\tvalid's my_multiclass_logloss: 0.678784\n",
      "[485]\ttrain's my_multiclass_logloss: 0.583026\tvalid's my_multiclass_logloss: 0.681371\n",
      "[486]\ttrain's my_multiclass_logloss: 0.582691\tvalid's my_multiclass_logloss: 0.683716\n",
      "[487]\ttrain's my_multiclass_logloss: 0.582502\tvalid's my_multiclass_logloss: 0.685526\n",
      "[488]\ttrain's my_multiclass_logloss: 0.582402\tvalid's my_multiclass_logloss: 0.68788\n",
      "[489]\ttrain's my_multiclass_logloss: 0.582278\tvalid's my_multiclass_logloss: 0.690578\n",
      "[490]\ttrain's my_multiclass_logloss: 0.582184\tvalid's my_multiclass_logloss: 0.693246\n",
      "[491]\ttrain's my_multiclass_logloss: 0.582216\tvalid's my_multiclass_logloss: 0.695973\n",
      "[492]\ttrain's my_multiclass_logloss: 0.582359\tvalid's my_multiclass_logloss: 0.698743\n",
      "[493]\ttrain's my_multiclass_logloss: 0.581923\tvalid's my_multiclass_logloss: 0.697875\n",
      "[494]\ttrain's my_multiclass_logloss: 0.581855\tvalid's my_multiclass_logloss: 0.699396\n",
      "[495]\ttrain's my_multiclass_logloss: 0.581565\tvalid's my_multiclass_logloss: 0.698674\n",
      "[496]\ttrain's my_multiclass_logloss: 0.581274\tvalid's my_multiclass_logloss: 0.699516\n",
      "[497]\ttrain's my_multiclass_logloss: 0.580676\tvalid's my_multiclass_logloss: 0.700393\n",
      "[498]\ttrain's my_multiclass_logloss: 0.580228\tvalid's my_multiclass_logloss: 0.701303\n",
      "[499]\ttrain's my_multiclass_logloss: 0.579877\tvalid's my_multiclass_logloss: 0.703731\n",
      "[500]\ttrain's my_multiclass_logloss: 0.579559\tvalid's my_multiclass_logloss: 0.701997\n",
      "[501]\ttrain's my_multiclass_logloss: 0.578904\tvalid's my_multiclass_logloss: 0.704462\n",
      "[502]\ttrain's my_multiclass_logloss: 0.578415\tvalid's my_multiclass_logloss: 0.707066\n",
      "[503]\ttrain's my_multiclass_logloss: 0.57796\tvalid's my_multiclass_logloss: 0.7096\n",
      "[504]\ttrain's my_multiclass_logloss: 0.577735\tvalid's my_multiclass_logloss: 0.711042\n",
      "[505]\ttrain's my_multiclass_logloss: 0.577427\tvalid's my_multiclass_logloss: 0.708282\n",
      "[506]\ttrain's my_multiclass_logloss: 0.57701\tvalid's my_multiclass_logloss: 0.706442\n",
      "[507]\ttrain's my_multiclass_logloss: 0.5768\tvalid's my_multiclass_logloss: 0.707063\n",
      "[508]\ttrain's my_multiclass_logloss: 0.576554\tvalid's my_multiclass_logloss: 0.705448\n",
      "[509]\ttrain's my_multiclass_logloss: 0.575998\tvalid's my_multiclass_logloss: 0.708125\n",
      "[510]\ttrain's my_multiclass_logloss: 0.57556\tvalid's my_multiclass_logloss: 0.710777\n",
      "[511]\ttrain's my_multiclass_logloss: 0.575069\tvalid's my_multiclass_logloss: 0.711193\n",
      "[512]\ttrain's my_multiclass_logloss: 0.574909\tvalid's my_multiclass_logloss: 0.71399\n",
      "[513]\ttrain's my_multiclass_logloss: 0.57438\tvalid's my_multiclass_logloss: 0.712912\n",
      "[514]\ttrain's my_multiclass_logloss: 0.57383\tvalid's my_multiclass_logloss: 0.712697\n",
      "[515]\ttrain's my_multiclass_logloss: 0.5734\tvalid's my_multiclass_logloss: 0.712136\n",
      "[516]\ttrain's my_multiclass_logloss: 0.573131\tvalid's my_multiclass_logloss: 0.711398\n",
      "[517]\ttrain's my_multiclass_logloss: 0.572803\tvalid's my_multiclass_logloss: 0.71207\n",
      "[518]\ttrain's my_multiclass_logloss: 0.572688\tvalid's my_multiclass_logloss: 0.714349\n",
      "[519]\ttrain's my_multiclass_logloss: 0.572713\tvalid's my_multiclass_logloss: 0.716649\n",
      "[520]\ttrain's my_multiclass_logloss: 0.57271\tvalid's my_multiclass_logloss: 0.71793\n",
      "[521]\ttrain's my_multiclass_logloss: 0.572197\tvalid's my_multiclass_logloss: 0.716965\n",
      "[522]\ttrain's my_multiclass_logloss: 0.57199\tvalid's my_multiclass_logloss: 0.714459\n",
      "[523]\ttrain's my_multiclass_logloss: 0.571619\tvalid's my_multiclass_logloss: 0.712986\n",
      "[524]\ttrain's my_multiclass_logloss: 0.571237\tvalid's my_multiclass_logloss: 0.710337\n",
      "[525]\ttrain's my_multiclass_logloss: 0.570948\tvalid's my_multiclass_logloss: 0.709418\n",
      "[526]\ttrain's my_multiclass_logloss: 0.570739\tvalid's my_multiclass_logloss: 0.707797\n",
      "[527]\ttrain's my_multiclass_logloss: 0.570521\tvalid's my_multiclass_logloss: 0.708091\n",
      "[528]\ttrain's my_multiclass_logloss: 0.570433\tvalid's my_multiclass_logloss: 0.706615\n",
      "[529]\ttrain's my_multiclass_logloss: 0.570187\tvalid's my_multiclass_logloss: 0.707218\n",
      "[530]\ttrain's my_multiclass_logloss: 0.570135\tvalid's my_multiclass_logloss: 0.708885\n",
      "[531]\ttrain's my_multiclass_logloss: 0.569638\tvalid's my_multiclass_logloss: 0.709822\n",
      "[532]\ttrain's my_multiclass_logloss: 0.569588\tvalid's my_multiclass_logloss: 0.710495\n",
      "[533]\ttrain's my_multiclass_logloss: 0.569432\tvalid's my_multiclass_logloss: 0.711918\n",
      "[534]\ttrain's my_multiclass_logloss: 0.569496\tvalid's my_multiclass_logloss: 0.714711\n",
      "[535]\ttrain's my_multiclass_logloss: 0.569053\tvalid's my_multiclass_logloss: 0.715281\n",
      "[536]\ttrain's my_multiclass_logloss: 0.569005\tvalid's my_multiclass_logloss: 0.716474\n",
      "[537]\ttrain's my_multiclass_logloss: 0.568319\tvalid's my_multiclass_logloss: 0.716485\n",
      "[538]\ttrain's my_multiclass_logloss: 0.567934\tvalid's my_multiclass_logloss: 0.715472\n",
      "[539]\ttrain's my_multiclass_logloss: 0.567418\tvalid's my_multiclass_logloss: 0.714345\n",
      "[540]\ttrain's my_multiclass_logloss: 0.566911\tvalid's my_multiclass_logloss: 0.715001\n",
      "[541]\ttrain's my_multiclass_logloss: 0.566064\tvalid's my_multiclass_logloss: 0.716297\n",
      "[542]\ttrain's my_multiclass_logloss: 0.564642\tvalid's my_multiclass_logloss: 0.715699\n",
      "[543]\ttrain's my_multiclass_logloss: 0.562835\tvalid's my_multiclass_logloss: 0.711356\n",
      "[544]\ttrain's my_multiclass_logloss: 0.56237\tvalid's my_multiclass_logloss: 0.71016\n",
      "[545]\ttrain's my_multiclass_logloss: 0.561902\tvalid's my_multiclass_logloss: 0.709479\n",
      "[546]\ttrain's my_multiclass_logloss: 0.561715\tvalid's my_multiclass_logloss: 0.71002\n",
      "[547]\ttrain's my_multiclass_logloss: 0.561505\tvalid's my_multiclass_logloss: 0.709666\n",
      "[548]\ttrain's my_multiclass_logloss: 0.56124\tvalid's my_multiclass_logloss: 0.710116\n",
      "[549]\ttrain's my_multiclass_logloss: 0.560996\tvalid's my_multiclass_logloss: 0.712127\n",
      "[550]\ttrain's my_multiclass_logloss: 0.560722\tvalid's my_multiclass_logloss: 0.712044\n",
      "[551]\ttrain's my_multiclass_logloss: 0.560651\tvalid's my_multiclass_logloss: 0.713367\n",
      "Early stopping, best iteration is:\n",
      "[451]\ttrain's my_multiclass_logloss: 0.594349\tvalid's my_multiclass_logloss: 0.673753\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.8\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.102282\n",
      "1   feature2    0.087443\n",
      "2   feature3    0.070223\n",
      "3   feature4    0.091995\n",
      "4   feature5    0.110166\n",
      "5   feature6    0.080610\n",
      "6   feature7    0.091926\n",
      "7   feature8    0.118097\n",
      "8   feature9    0.125351\n",
      "9  feature10    0.121908\n",
      "     feature  importance\n",
      "0   feature1    0.096262\n",
      "1   feature2    0.077880\n",
      "2   feature3    0.085818\n",
      "3   feature4    0.101785\n",
      "4   feature5    0.118991\n",
      "5   feature6    0.072010\n",
      "6   feature7    0.100585\n",
      "7   feature8    0.106165\n",
      "8   feature9    0.125104\n",
      "9  feature10    0.115400\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1   -0.070207\n",
      "1   feature2   -0.111533\n",
      "2   feature3    0.181891\n",
      "3   feature4    0.114189\n",
      "4   feature5    0.102931\n",
      "5   feature6   -0.100304\n",
      "6   feature7    0.100989\n",
      "7   feature8   -0.139167\n",
      "8   feature9   -0.002882\n",
      "9  feature10   -0.075908\n",
      "-------------------- 9 --------------------\n",
      "(98, 10) (98,)\n",
      "(10, 10) (10,)\n",
      "\n",
      "\n",
      "-------------------- GC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's multi_logloss: 1.05601\tvalid's multi_logloss: 1.04531\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's multi_logloss: 1.0459\tvalid's multi_logloss: 1.04676\n",
      "[3]\ttrain's multi_logloss: 1.03702\tvalid's multi_logloss: 1.04287\n",
      "[4]\ttrain's multi_logloss: 1.02865\tvalid's multi_logloss: 1.03941\n",
      "[5]\ttrain's multi_logloss: 1.02024\tvalid's multi_logloss: 1.04126\n",
      "[6]\ttrain's multi_logloss: 1.01309\tvalid's multi_logloss: 1.03965\n",
      "[7]\ttrain's multi_logloss: 1.00588\tvalid's multi_logloss: 1.03978\n",
      "[8]\ttrain's multi_logloss: 0.998328\tvalid's multi_logloss: 1.04012\n",
      "[9]\ttrain's multi_logloss: 0.990741\tvalid's multi_logloss: 1.04177\n",
      "[10]\ttrain's multi_logloss: 0.984733\tvalid's multi_logloss: 1.04337\n",
      "[11]\ttrain's multi_logloss: 0.977897\tvalid's multi_logloss: 1.04538\n",
      "[12]\ttrain's multi_logloss: 0.971371\tvalid's multi_logloss: 1.04871\n",
      "[13]\ttrain's multi_logloss: 0.96475\tvalid's multi_logloss: 1.04486\n",
      "[14]\ttrain's multi_logloss: 0.958889\tvalid's multi_logloss: 1.03969\n",
      "[15]\ttrain's multi_logloss: 0.954643\tvalid's multi_logloss: 1.03819\n",
      "[16]\ttrain's multi_logloss: 0.949441\tvalid's multi_logloss: 1.03363\n",
      "[17]\ttrain's multi_logloss: 0.943691\tvalid's multi_logloss: 1.03658\n",
      "[18]\ttrain's multi_logloss: 0.938806\tvalid's multi_logloss: 1.03449\n",
      "[19]\ttrain's multi_logloss: 0.934073\tvalid's multi_logloss: 1.0377\n",
      "[20]\ttrain's multi_logloss: 0.92914\tvalid's multi_logloss: 1.0392\n",
      "[21]\ttrain's multi_logloss: 0.925291\tvalid's multi_logloss: 1.04171\n",
      "[22]\ttrain's multi_logloss: 0.921375\tvalid's multi_logloss: 1.04306\n",
      "[23]\ttrain's multi_logloss: 0.918067\tvalid's multi_logloss: 1.04098\n",
      "[24]\ttrain's multi_logloss: 0.914037\tvalid's multi_logloss: 1.03866\n",
      "[25]\ttrain's multi_logloss: 0.908423\tvalid's multi_logloss: 1.03508\n",
      "[26]\ttrain's multi_logloss: 0.904045\tvalid's multi_logloss: 1.02392\n",
      "[27]\ttrain's multi_logloss: 0.89991\tvalid's multi_logloss: 1.00901\n",
      "[28]\ttrain's multi_logloss: 0.896178\tvalid's multi_logloss: 0.994846\n",
      "[29]\ttrain's multi_logloss: 0.890461\tvalid's multi_logloss: 0.999517\n",
      "[30]\ttrain's multi_logloss: 0.885691\tvalid's multi_logloss: 1.00239\n",
      "[31]\ttrain's multi_logloss: 0.88051\tvalid's multi_logloss: 1.00619\n",
      "[32]\ttrain's multi_logloss: 0.876094\tvalid's multi_logloss: 1.00722\n",
      "[33]\ttrain's multi_logloss: 0.87342\tvalid's multi_logloss: 1.00577\n",
      "[34]\ttrain's multi_logloss: 0.869016\tvalid's multi_logloss: 0.999179\n",
      "[35]\ttrain's multi_logloss: 0.865712\tvalid's multi_logloss: 1.00123\n",
      "[36]\ttrain's multi_logloss: 0.8626\tvalid's multi_logloss: 0.999933\n",
      "[37]\ttrain's multi_logloss: 0.858957\tvalid's multi_logloss: 1.00198\n",
      "[38]\ttrain's multi_logloss: 0.856718\tvalid's multi_logloss: 1.0008\n",
      "[39]\ttrain's multi_logloss: 0.852274\tvalid's multi_logloss: 1.00358\n",
      "[40]\ttrain's multi_logloss: 0.849456\tvalid's multi_logloss: 1.00874\n",
      "[41]\ttrain's multi_logloss: 0.845749\tvalid's multi_logloss: 1.00514\n",
      "[42]\ttrain's multi_logloss: 0.841828\tvalid's multi_logloss: 1.00959\n",
      "[43]\ttrain's multi_logloss: 0.839718\tvalid's multi_logloss: 1.00776\n",
      "[44]\ttrain's multi_logloss: 0.836747\tvalid's multi_logloss: 1.00482\n",
      "[45]\ttrain's multi_logloss: 0.833103\tvalid's multi_logloss: 1.00257\n",
      "[46]\ttrain's multi_logloss: 0.831968\tvalid's multi_logloss: 0.999829\n",
      "[47]\ttrain's multi_logloss: 0.82863\tvalid's multi_logloss: 0.997944\n",
      "[48]\ttrain's multi_logloss: 0.825723\tvalid's multi_logloss: 0.997824\n",
      "[49]\ttrain's multi_logloss: 0.822325\tvalid's multi_logloss: 0.992774\n",
      "[50]\ttrain's multi_logloss: 0.819872\tvalid's multi_logloss: 0.984938\n",
      "[51]\ttrain's multi_logloss: 0.816901\tvalid's multi_logloss: 0.980354\n",
      "[52]\ttrain's multi_logloss: 0.813687\tvalid's multi_logloss: 0.97874\n",
      "[53]\ttrain's multi_logloss: 0.811657\tvalid's multi_logloss: 0.97273\n",
      "[54]\ttrain's multi_logloss: 0.808765\tvalid's multi_logloss: 0.967379\n",
      "[55]\ttrain's multi_logloss: 0.80614\tvalid's multi_logloss: 0.962315\n",
      "[56]\ttrain's multi_logloss: 0.804321\tvalid's multi_logloss: 0.957352\n",
      "[57]\ttrain's multi_logloss: 0.802779\tvalid's multi_logloss: 0.954203\n",
      "[58]\ttrain's multi_logloss: 0.800153\tvalid's multi_logloss: 0.951398\n",
      "[59]\ttrain's multi_logloss: 0.797421\tvalid's multi_logloss: 0.95304\n",
      "[60]\ttrain's multi_logloss: 0.793877\tvalid's multi_logloss: 0.956236\n",
      "[61]\ttrain's multi_logloss: 0.790893\tvalid's multi_logloss: 0.959939\n",
      "[62]\ttrain's multi_logloss: 0.7874\tvalid's multi_logloss: 0.96052\n",
      "[63]\ttrain's multi_logloss: 0.785164\tvalid's multi_logloss: 0.967277\n",
      "[64]\ttrain's multi_logloss: 0.782072\tvalid's multi_logloss: 0.971359\n",
      "[65]\ttrain's multi_logloss: 0.779831\tvalid's multi_logloss: 0.966431\n",
      "[66]\ttrain's multi_logloss: 0.777304\tvalid's multi_logloss: 0.962934\n",
      "[67]\ttrain's multi_logloss: 0.775002\tvalid's multi_logloss: 0.959678\n",
      "[68]\ttrain's multi_logloss: 0.773536\tvalid's multi_logloss: 0.953062\n",
      "[69]\ttrain's multi_logloss: 0.771913\tvalid's multi_logloss: 0.949952\n",
      "[70]\ttrain's multi_logloss: 0.76925\tvalid's multi_logloss: 0.953146\n",
      "[71]\ttrain's multi_logloss: 0.76713\tvalid's multi_logloss: 0.95599\n",
      "[72]\ttrain's multi_logloss: 0.765374\tvalid's multi_logloss: 0.956477\n",
      "[73]\ttrain's multi_logloss: 0.762249\tvalid's multi_logloss: 0.959294\n",
      "[74]\ttrain's multi_logloss: 0.759401\tvalid's multi_logloss: 0.962177\n",
      "[75]\ttrain's multi_logloss: 0.757617\tvalid's multi_logloss: 0.962515\n",
      "[76]\ttrain's multi_logloss: 0.75545\tvalid's multi_logloss: 0.961595\n",
      "[77]\ttrain's multi_logloss: 0.753616\tvalid's multi_logloss: 0.960619\n",
      "[78]\ttrain's multi_logloss: 0.752019\tvalid's multi_logloss: 0.958687\n",
      "[79]\ttrain's multi_logloss: 0.750296\tvalid's multi_logloss: 0.961944\n",
      "[80]\ttrain's multi_logloss: 0.748601\tvalid's multi_logloss: 0.958688\n",
      "[81]\ttrain's multi_logloss: 0.746212\tvalid's multi_logloss: 0.961109\n",
      "[82]\ttrain's multi_logloss: 0.74431\tvalid's multi_logloss: 0.960408\n",
      "[83]\ttrain's multi_logloss: 0.742266\tvalid's multi_logloss: 0.95952\n",
      "[84]\ttrain's multi_logloss: 0.739784\tvalid's multi_logloss: 0.960337\n",
      "[85]\ttrain's multi_logloss: 0.738369\tvalid's multi_logloss: 0.961671\n",
      "[86]\ttrain's multi_logloss: 0.736324\tvalid's multi_logloss: 0.962641\n",
      "[87]\ttrain's multi_logloss: 0.73557\tvalid's multi_logloss: 0.961396\n",
      "[88]\ttrain's multi_logloss: 0.733592\tvalid's multi_logloss: 0.960054\n",
      "[89]\ttrain's multi_logloss: 0.731929\tvalid's multi_logloss: 0.960647\n",
      "[90]\ttrain's multi_logloss: 0.730607\tvalid's multi_logloss: 0.959595\n",
      "[91]\ttrain's multi_logloss: 0.728525\tvalid's multi_logloss: 0.960745\n",
      "[92]\ttrain's multi_logloss: 0.726908\tvalid's multi_logloss: 0.961771\n",
      "[93]\ttrain's multi_logloss: 0.724684\tvalid's multi_logloss: 0.960668\n",
      "[94]\ttrain's multi_logloss: 0.722988\tvalid's multi_logloss: 0.958666\n",
      "[95]\ttrain's multi_logloss: 0.72147\tvalid's multi_logloss: 0.956881\n",
      "[96]\ttrain's multi_logloss: 0.719509\tvalid's multi_logloss: 0.959003\n",
      "[97]\ttrain's multi_logloss: 0.718092\tvalid's multi_logloss: 0.95643\n",
      "[98]\ttrain's multi_logloss: 0.71653\tvalid's multi_logloss: 0.958406\n",
      "[99]\ttrain's multi_logloss: 0.715336\tvalid's multi_logloss: 0.956007\n",
      "[100]\ttrain's multi_logloss: 0.714546\tvalid's multi_logloss: 0.956677\n",
      "[101]\ttrain's multi_logloss: 0.712424\tvalid's multi_logloss: 0.960087\n",
      "[102]\ttrain's multi_logloss: 0.711036\tvalid's multi_logloss: 0.964195\n",
      "[103]\ttrain's multi_logloss: 0.708442\tvalid's multi_logloss: 0.968481\n",
      "[104]\ttrain's multi_logloss: 0.707344\tvalid's multi_logloss: 0.972638\n",
      "[105]\ttrain's multi_logloss: 0.705005\tvalid's multi_logloss: 0.975515\n",
      "[106]\ttrain's multi_logloss: 0.702862\tvalid's multi_logloss: 0.979719\n",
      "[107]\ttrain's multi_logloss: 0.701259\tvalid's multi_logloss: 0.981354\n",
      "[108]\ttrain's multi_logloss: 0.699438\tvalid's multi_logloss: 0.98553\n",
      "[109]\ttrain's multi_logloss: 0.697874\tvalid's multi_logloss: 0.987994\n",
      "[110]\ttrain's multi_logloss: 0.696404\tvalid's multi_logloss: 0.989335\n",
      "[111]\ttrain's multi_logloss: 0.695048\tvalid's multi_logloss: 0.990732\n",
      "[112]\ttrain's multi_logloss: 0.693759\tvalid's multi_logloss: 0.993372\n",
      "[113]\ttrain's multi_logloss: 0.691699\tvalid's multi_logloss: 0.990391\n",
      "[114]\ttrain's multi_logloss: 0.689814\tvalid's multi_logloss: 0.988892\n",
      "[115]\ttrain's multi_logloss: 0.688298\tvalid's multi_logloss: 0.98988\n",
      "[116]\ttrain's multi_logloss: 0.686814\tvalid's multi_logloss: 0.990883\n",
      "[117]\ttrain's multi_logloss: 0.685878\tvalid's multi_logloss: 0.992194\n",
      "[118]\ttrain's multi_logloss: 0.684692\tvalid's multi_logloss: 0.989321\n",
      "[119]\ttrain's multi_logloss: 0.683617\tvalid's multi_logloss: 0.986584\n",
      "[120]\ttrain's multi_logloss: 0.682512\tvalid's multi_logloss: 0.987784\n",
      "[121]\ttrain's multi_logloss: 0.681109\tvalid's multi_logloss: 0.987373\n",
      "[122]\ttrain's multi_logloss: 0.679848\tvalid's multi_logloss: 0.987074\n",
      "[123]\ttrain's multi_logloss: 0.678699\tvalid's multi_logloss: 0.985188\n",
      "[124]\ttrain's multi_logloss: 0.677652\tvalid's multi_logloss: 0.985031\n",
      "[125]\ttrain's multi_logloss: 0.677229\tvalid's multi_logloss: 0.986057\n",
      "[126]\ttrain's multi_logloss: 0.676967\tvalid's multi_logloss: 0.988382\n",
      "[127]\ttrain's multi_logloss: 0.676292\tvalid's multi_logloss: 0.990939\n",
      "[128]\ttrain's multi_logloss: 0.674933\tvalid's multi_logloss: 0.995188\n",
      "[129]\ttrain's multi_logloss: 0.673772\tvalid's multi_logloss: 0.997139\n",
      "[130]\ttrain's multi_logloss: 0.673008\tvalid's multi_logloss: 0.996032\n",
      "[131]\ttrain's multi_logloss: 0.672382\tvalid's multi_logloss: 0.995026\n",
      "[132]\ttrain's multi_logloss: 0.672475\tvalid's multi_logloss: 0.991526\n",
      "[133]\ttrain's multi_logloss: 0.670134\tvalid's multi_logloss: 0.993204\n",
      "[134]\ttrain's multi_logloss: 0.667981\tvalid's multi_logloss: 0.995002\n",
      "[135]\ttrain's multi_logloss: 0.66689\tvalid's multi_logloss: 0.998019\n",
      "[136]\ttrain's multi_logloss: 0.664968\tvalid's multi_logloss: 0.997811\n",
      "[137]\ttrain's multi_logloss: 0.663742\tvalid's multi_logloss: 0.997803\n",
      "[138]\ttrain's multi_logloss: 0.662562\tvalid's multi_logloss: 0.998804\n",
      "[139]\ttrain's multi_logloss: 0.661417\tvalid's multi_logloss: 1.00207\n",
      "[140]\ttrain's multi_logloss: 0.660615\tvalid's multi_logloss: 1.00087\n",
      "[141]\ttrain's multi_logloss: 0.659236\tvalid's multi_logloss: 1.00097\n",
      "[142]\ttrain's multi_logloss: 0.657783\tvalid's multi_logloss: 1.00083\n",
      "[143]\ttrain's multi_logloss: 0.656405\tvalid's multi_logloss: 1.0026\n",
      "[144]\ttrain's multi_logloss: 0.655116\tvalid's multi_logloss: 1.00449\n",
      "[145]\ttrain's multi_logloss: 0.653763\tvalid's multi_logloss: 1.00914\n",
      "[146]\ttrain's multi_logloss: 0.652503\tvalid's multi_logloss: 1.01373\n",
      "[147]\ttrain's multi_logloss: 0.651042\tvalid's multi_logloss: 1.01519\n",
      "[148]\ttrain's multi_logloss: 0.649915\tvalid's multi_logloss: 1.01971\n",
      "[149]\ttrain's multi_logloss: 0.649174\tvalid's multi_logloss: 1.02328\n",
      "[150]\ttrain's multi_logloss: 0.648093\tvalid's multi_logloss: 1.02373\n",
      "[151]\ttrain's multi_logloss: 0.646859\tvalid's multi_logloss: 1.0258\n",
      "[152]\ttrain's multi_logloss: 0.645756\tvalid's multi_logloss: 1.02791\n",
      "[153]\ttrain's multi_logloss: 0.644779\tvalid's multi_logloss: 1.02148\n",
      "[154]\ttrain's multi_logloss: 0.643896\tvalid's multi_logloss: 1.0181\n",
      "[155]\ttrain's multi_logloss: 0.643141\tvalid's multi_logloss: 1.0149\n",
      "[156]\ttrain's multi_logloss: 0.642506\tvalid's multi_logloss: 1.01188\n",
      "[157]\ttrain's multi_logloss: 0.64063\tvalid's multi_logloss: 1.01069\n",
      "[158]\ttrain's multi_logloss: 0.639029\tvalid's multi_logloss: 1.01322\n",
      "[159]\ttrain's multi_logloss: 0.637392\tvalid's multi_logloss: 1.01712\n",
      "[160]\ttrain's multi_logloss: 0.635499\tvalid's multi_logloss: 1.02243\n",
      "[161]\ttrain's multi_logloss: 0.633942\tvalid's multi_logloss: 1.0292\n",
      "[162]\ttrain's multi_logloss: 0.633257\tvalid's multi_logloss: 1.03192\n",
      "[163]\ttrain's multi_logloss: 0.632257\tvalid's multi_logloss: 1.03755\n",
      "[164]\ttrain's multi_logloss: 0.63103\tvalid's multi_logloss: 1.04325\n",
      "[165]\ttrain's multi_logloss: 0.629924\tvalid's multi_logloss: 1.04621\n",
      "[166]\ttrain's multi_logloss: 0.629075\tvalid's multi_logloss: 1.04991\n",
      "[167]\ttrain's multi_logloss: 0.62808\tvalid's multi_logloss: 1.05014\n",
      "[168]\ttrain's multi_logloss: 0.627259\tvalid's multi_logloss: 1.05296\n",
      "[169]\ttrain's multi_logloss: 0.6275\tvalid's multi_logloss: 1.05349\n",
      "Early stopping, best iteration is:\n",
      "[69]\ttrain's multi_logloss: 0.771913\tvalid's multi_logloss: 0.949952\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.5\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.078159\n",
      "1   feature2    0.118260\n",
      "2   feature3    0.026172\n",
      "3   feature4    0.024904\n",
      "4   feature5    0.131761\n",
      "5   feature6    0.049043\n",
      "6   feature7    0.084389\n",
      "7   feature8    0.178924\n",
      "8   feature9    0.210713\n",
      "9  feature10    0.097676\n",
      "\n",
      "\n",
      "-------------------- SFC model learning -------------------- \n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[1]\ttrain's my_multiclass_logloss: 1.08714\tvalid's my_multiclass_logloss: 1.08592\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttrain's my_multiclass_logloss: 1.07414\tvalid's my_multiclass_logloss: 1.07994\n",
      "[3]\ttrain's my_multiclass_logloss: 1.05896\tvalid's my_multiclass_logloss: 1.07641\n",
      "[4]\ttrain's my_multiclass_logloss: 1.0487\tvalid's my_multiclass_logloss: 1.07237\n",
      "[5]\ttrain's my_multiclass_logloss: 1.03845\tvalid's my_multiclass_logloss: 1.06505\n",
      "[6]\ttrain's my_multiclass_logloss: 1.0282\tvalid's my_multiclass_logloss: 1.06202\n",
      "[7]\ttrain's my_multiclass_logloss: 1.02054\tvalid's my_multiclass_logloss: 1.05733\n",
      "[8]\ttrain's my_multiclass_logloss: 1.0124\tvalid's my_multiclass_logloss: 1.05938\n",
      "[9]\ttrain's my_multiclass_logloss: 1.00442\tvalid's my_multiclass_logloss: 1.06295\n",
      "[10]\ttrain's my_multiclass_logloss: 0.996009\tvalid's my_multiclass_logloss: 1.06508\n",
      "[11]\ttrain's my_multiclass_logloss: 0.988424\tvalid's my_multiclass_logloss: 1.06774\n",
      "[12]\ttrain's my_multiclass_logloss: 0.981943\tvalid's my_multiclass_logloss: 1.06759\n",
      "[13]\ttrain's my_multiclass_logloss: 0.976761\tvalid's my_multiclass_logloss: 1.0641\n",
      "[14]\ttrain's my_multiclass_logloss: 0.971777\tvalid's my_multiclass_logloss: 1.0627\n",
      "[15]\ttrain's my_multiclass_logloss: 0.96523\tvalid's my_multiclass_logloss: 1.05818\n",
      "[16]\ttrain's my_multiclass_logloss: 0.959842\tvalid's my_multiclass_logloss: 1.05555\n",
      "[17]\ttrain's my_multiclass_logloss: 0.953103\tvalid's my_multiclass_logloss: 1.05645\n",
      "[18]\ttrain's my_multiclass_logloss: 0.94749\tvalid's my_multiclass_logloss: 1.05391\n",
      "[19]\ttrain's my_multiclass_logloss: 0.942153\tvalid's my_multiclass_logloss: 1.05321\n",
      "[20]\ttrain's my_multiclass_logloss: 0.936795\tvalid's my_multiclass_logloss: 1.05514\n",
      "[21]\ttrain's my_multiclass_logloss: 0.933585\tvalid's my_multiclass_logloss: 1.05366\n",
      "[22]\ttrain's my_multiclass_logloss: 0.931386\tvalid's my_multiclass_logloss: 1.05312\n",
      "[23]\ttrain's my_multiclass_logloss: 0.927267\tvalid's my_multiclass_logloss: 1.05474\n",
      "[24]\ttrain's my_multiclass_logloss: 0.925454\tvalid's my_multiclass_logloss: 1.05446\n",
      "[25]\ttrain's my_multiclass_logloss: 0.921486\tvalid's my_multiclass_logloss: 1.04238\n",
      "[26]\ttrain's my_multiclass_logloss: 0.916445\tvalid's my_multiclass_logloss: 1.025\n",
      "[27]\ttrain's my_multiclass_logloss: 0.911215\tvalid's my_multiclass_logloss: 1.01693\n",
      "[28]\ttrain's my_multiclass_logloss: 0.908325\tvalid's my_multiclass_logloss: 1.00866\n",
      "[29]\ttrain's my_multiclass_logloss: 0.904238\tvalid's my_multiclass_logloss: 1.01102\n",
      "[30]\ttrain's my_multiclass_logloss: 0.899724\tvalid's my_multiclass_logloss: 1.01771\n",
      "[31]\ttrain's my_multiclass_logloss: 0.895515\tvalid's my_multiclass_logloss: 1.02311\n",
      "[32]\ttrain's my_multiclass_logloss: 0.891902\tvalid's my_multiclass_logloss: 1.03012\n",
      "[33]\ttrain's my_multiclass_logloss: 0.887895\tvalid's my_multiclass_logloss: 1.02325\n",
      "[34]\ttrain's my_multiclass_logloss: 0.884433\tvalid's my_multiclass_logloss: 1.017\n",
      "[35]\ttrain's my_multiclass_logloss: 0.881008\tvalid's my_multiclass_logloss: 1.01902\n",
      "[36]\ttrain's my_multiclass_logloss: 0.879096\tvalid's my_multiclass_logloss: 1.01919\n",
      "[37]\ttrain's my_multiclass_logloss: 0.874537\tvalid's my_multiclass_logloss: 1.02\n",
      "[38]\ttrain's my_multiclass_logloss: 0.869892\tvalid's my_multiclass_logloss: 1.02275\n",
      "[39]\ttrain's my_multiclass_logloss: 0.865804\tvalid's my_multiclass_logloss: 1.02134\n",
      "[40]\ttrain's my_multiclass_logloss: 0.863412\tvalid's my_multiclass_logloss: 1.02316\n",
      "[41]\ttrain's my_multiclass_logloss: 0.860065\tvalid's my_multiclass_logloss: 1.02061\n",
      "[42]\ttrain's my_multiclass_logloss: 0.858846\tvalid's my_multiclass_logloss: 1.02927\n",
      "[43]\ttrain's my_multiclass_logloss: 0.856105\tvalid's my_multiclass_logloss: 1.02727\n",
      "[44]\ttrain's my_multiclass_logloss: 0.854056\tvalid's my_multiclass_logloss: 1.02673\n",
      "[45]\ttrain's my_multiclass_logloss: 0.851231\tvalid's my_multiclass_logloss: 1.02307\n",
      "[46]\ttrain's my_multiclass_logloss: 0.849031\tvalid's my_multiclass_logloss: 1.02087\n",
      "[47]\ttrain's my_multiclass_logloss: 0.845687\tvalid's my_multiclass_logloss: 1.01884\n",
      "[48]\ttrain's my_multiclass_logloss: 0.843107\tvalid's my_multiclass_logloss: 1.01357\n",
      "[49]\ttrain's my_multiclass_logloss: 0.839625\tvalid's my_multiclass_logloss: 1.01522\n",
      "[50]\ttrain's my_multiclass_logloss: 0.836527\tvalid's my_multiclass_logloss: 1.01372\n",
      "[51]\ttrain's my_multiclass_logloss: 0.834459\tvalid's my_multiclass_logloss: 1.0122\n",
      "[52]\ttrain's my_multiclass_logloss: 0.832132\tvalid's my_multiclass_logloss: 1.00777\n",
      "[53]\ttrain's my_multiclass_logloss: 0.828778\tvalid's my_multiclass_logloss: 1.00189\n",
      "[54]\ttrain's my_multiclass_logloss: 0.82617\tvalid's my_multiclass_logloss: 0.996063\n",
      "[55]\ttrain's my_multiclass_logloss: 0.824441\tvalid's my_multiclass_logloss: 0.990241\n",
      "[56]\ttrain's my_multiclass_logloss: 0.822299\tvalid's my_multiclass_logloss: 0.984989\n",
      "[57]\ttrain's my_multiclass_logloss: 0.820093\tvalid's my_multiclass_logloss: 0.989976\n",
      "[58]\ttrain's my_multiclass_logloss: 0.818272\tvalid's my_multiclass_logloss: 0.990404\n",
      "[59]\ttrain's my_multiclass_logloss: 0.817154\tvalid's my_multiclass_logloss: 0.99115\n",
      "[60]\ttrain's my_multiclass_logloss: 0.814676\tvalid's my_multiclass_logloss: 0.992955\n",
      "[61]\ttrain's my_multiclass_logloss: 0.811508\tvalid's my_multiclass_logloss: 0.993362\n",
      "[62]\ttrain's my_multiclass_logloss: 0.809656\tvalid's my_multiclass_logloss: 0.996546\n",
      "[63]\ttrain's my_multiclass_logloss: 0.807788\tvalid's my_multiclass_logloss: 0.996464\n",
      "[64]\ttrain's my_multiclass_logloss: 0.8059\tvalid's my_multiclass_logloss: 0.997208\n",
      "[65]\ttrain's my_multiclass_logloss: 0.804476\tvalid's my_multiclass_logloss: 0.992736\n",
      "[66]\ttrain's my_multiclass_logloss: 0.802254\tvalid's my_multiclass_logloss: 0.99319\n",
      "[67]\ttrain's my_multiclass_logloss: 0.801054\tvalid's my_multiclass_logloss: 0.99277\n",
      "[68]\ttrain's my_multiclass_logloss: 0.799299\tvalid's my_multiclass_logloss: 0.993558\n",
      "[69]\ttrain's my_multiclass_logloss: 0.797344\tvalid's my_multiclass_logloss: 0.994685\n",
      "[70]\ttrain's my_multiclass_logloss: 0.795717\tvalid's my_multiclass_logloss: 0.995959\n",
      "[71]\ttrain's my_multiclass_logloss: 0.793479\tvalid's my_multiclass_logloss: 0.994008\n",
      "[72]\ttrain's my_multiclass_logloss: 0.791958\tvalid's my_multiclass_logloss: 0.997123\n",
      "[73]\ttrain's my_multiclass_logloss: 0.789806\tvalid's my_multiclass_logloss: 0.996539\n",
      "[74]\ttrain's my_multiclass_logloss: 0.787613\tvalid's my_multiclass_logloss: 1.00007\n",
      "[75]\ttrain's my_multiclass_logloss: 0.785903\tvalid's my_multiclass_logloss: 1.00034\n",
      "[76]\ttrain's my_multiclass_logloss: 0.784205\tvalid's my_multiclass_logloss: 1.00386\n",
      "[77]\ttrain's my_multiclass_logloss: 0.784428\tvalid's my_multiclass_logloss: 1.005\n",
      "[78]\ttrain's my_multiclass_logloss: 0.782965\tvalid's my_multiclass_logloss: 1.0039\n",
      "[79]\ttrain's my_multiclass_logloss: 0.782161\tvalid's my_multiclass_logloss: 1.00618\n",
      "[80]\ttrain's my_multiclass_logloss: 0.780929\tvalid's my_multiclass_logloss: 1.00514\n",
      "[81]\ttrain's my_multiclass_logloss: 0.779512\tvalid's my_multiclass_logloss: 1.00482\n",
      "[82]\ttrain's my_multiclass_logloss: 0.778231\tvalid's my_multiclass_logloss: 1.00459\n",
      "[83]\ttrain's my_multiclass_logloss: 0.777418\tvalid's my_multiclass_logloss: 0.999713\n",
      "[84]\ttrain's my_multiclass_logloss: 0.776553\tvalid's my_multiclass_logloss: 0.996195\n",
      "[85]\ttrain's my_multiclass_logloss: 0.775649\tvalid's my_multiclass_logloss: 0.993261\n",
      "[86]\ttrain's my_multiclass_logloss: 0.774318\tvalid's my_multiclass_logloss: 0.994703\n",
      "[87]\ttrain's my_multiclass_logloss: 0.773446\tvalid's my_multiclass_logloss: 0.995689\n",
      "[88]\ttrain's my_multiclass_logloss: 0.772859\tvalid's my_multiclass_logloss: 0.995529\n",
      "[89]\ttrain's my_multiclass_logloss: 0.771307\tvalid's my_multiclass_logloss: 0.996501\n",
      "[90]\ttrain's my_multiclass_logloss: 0.769926\tvalid's my_multiclass_logloss: 0.995644\n",
      "[91]\ttrain's my_multiclass_logloss: 0.769011\tvalid's my_multiclass_logloss: 0.996903\n",
      "[92]\ttrain's my_multiclass_logloss: 0.768401\tvalid's my_multiclass_logloss: 0.99826\n",
      "[93]\ttrain's my_multiclass_logloss: 0.766076\tvalid's my_multiclass_logloss: 0.997313\n",
      "[94]\ttrain's my_multiclass_logloss: 0.763993\tvalid's my_multiclass_logloss: 0.996765\n",
      "[95]\ttrain's my_multiclass_logloss: 0.762156\tvalid's my_multiclass_logloss: 0.996183\n",
      "[96]\ttrain's my_multiclass_logloss: 0.760489\tvalid's my_multiclass_logloss: 0.995955\n",
      "[97]\ttrain's my_multiclass_logloss: 0.759624\tvalid's my_multiclass_logloss: 0.994834\n",
      "[98]\ttrain's my_multiclass_logloss: 0.758595\tvalid's my_multiclass_logloss: 0.999333\n",
      "[99]\ttrain's my_multiclass_logloss: 0.757819\tvalid's my_multiclass_logloss: 0.998305\n",
      "[100]\ttrain's my_multiclass_logloss: 0.75689\tvalid's my_multiclass_logloss: 1.00262\n",
      "[101]\ttrain's my_multiclass_logloss: 0.754334\tvalid's my_multiclass_logloss: 1.00761\n",
      "[102]\ttrain's my_multiclass_logloss: 0.752322\tvalid's my_multiclass_logloss: 1.00927\n",
      "[103]\ttrain's my_multiclass_logloss: 0.750615\tvalid's my_multiclass_logloss: 1.01282\n",
      "[104]\ttrain's my_multiclass_logloss: 0.749691\tvalid's my_multiclass_logloss: 1.01617\n",
      "[105]\ttrain's my_multiclass_logloss: 0.748272\tvalid's my_multiclass_logloss: 1.0167\n",
      "[106]\ttrain's my_multiclass_logloss: 0.746779\tvalid's my_multiclass_logloss: 1.01497\n",
      "[107]\ttrain's my_multiclass_logloss: 0.745467\tvalid's my_multiclass_logloss: 1.0165\n",
      "[108]\ttrain's my_multiclass_logloss: 0.744527\tvalid's my_multiclass_logloss: 1.01778\n",
      "[109]\ttrain's my_multiclass_logloss: 0.744723\tvalid's my_multiclass_logloss: 1.02092\n",
      "[110]\ttrain's my_multiclass_logloss: 0.743931\tvalid's my_multiclass_logloss: 1.02549\n",
      "[111]\ttrain's my_multiclass_logloss: 0.743286\tvalid's my_multiclass_logloss: 1.03008\n",
      "[112]\ttrain's my_multiclass_logloss: 0.743127\tvalid's my_multiclass_logloss: 1.03231\n",
      "[113]\ttrain's my_multiclass_logloss: 0.741422\tvalid's my_multiclass_logloss: 1.03081\n",
      "[114]\ttrain's my_multiclass_logloss: 0.739958\tvalid's my_multiclass_logloss: 1.02486\n",
      "[115]\ttrain's my_multiclass_logloss: 0.739068\tvalid's my_multiclass_logloss: 1.01986\n",
      "[116]\ttrain's my_multiclass_logloss: 0.738401\tvalid's my_multiclass_logloss: 1.01776\n",
      "[117]\ttrain's my_multiclass_logloss: 0.737796\tvalid's my_multiclass_logloss: 1.01389\n",
      "[118]\ttrain's my_multiclass_logloss: 0.736902\tvalid's my_multiclass_logloss: 1.01228\n",
      "[119]\ttrain's my_multiclass_logloss: 0.73641\tvalid's my_multiclass_logloss: 1.00735\n",
      "[120]\ttrain's my_multiclass_logloss: 0.735757\tvalid's my_multiclass_logloss: 1.0061\n",
      "[121]\ttrain's my_multiclass_logloss: 0.734512\tvalid's my_multiclass_logloss: 1.00672\n",
      "[122]\ttrain's my_multiclass_logloss: 0.732746\tvalid's my_multiclass_logloss: 1.00801\n",
      "[123]\ttrain's my_multiclass_logloss: 0.732036\tvalid's my_multiclass_logloss: 1.0101\n",
      "[124]\ttrain's my_multiclass_logloss: 0.731189\tvalid's my_multiclass_logloss: 1.01265\n",
      "[125]\ttrain's my_multiclass_logloss: 0.731075\tvalid's my_multiclass_logloss: 1.01549\n",
      "[126]\ttrain's my_multiclass_logloss: 0.730483\tvalid's my_multiclass_logloss: 1.01993\n",
      "[127]\ttrain's my_multiclass_logloss: 0.730518\tvalid's my_multiclass_logloss: 1.02265\n",
      "[128]\ttrain's my_multiclass_logloss: 0.729623\tvalid's my_multiclass_logloss: 1.02651\n",
      "[129]\ttrain's my_multiclass_logloss: 0.728995\tvalid's my_multiclass_logloss: 1.02505\n",
      "[130]\ttrain's my_multiclass_logloss: 0.728221\tvalid's my_multiclass_logloss: 1.02406\n",
      "[131]\ttrain's my_multiclass_logloss: 0.727971\tvalid's my_multiclass_logloss: 1.0237\n",
      "[132]\ttrain's my_multiclass_logloss: 0.727461\tvalid's my_multiclass_logloss: 1.02287\n",
      "[133]\ttrain's my_multiclass_logloss: 0.726044\tvalid's my_multiclass_logloss: 1.02667\n",
      "[134]\ttrain's my_multiclass_logloss: 0.724853\tvalid's my_multiclass_logloss: 1.03064\n",
      "[135]\ttrain's my_multiclass_logloss: 0.724087\tvalid's my_multiclass_logloss: 1.03244\n",
      "[136]\ttrain's my_multiclass_logloss: 0.723476\tvalid's my_multiclass_logloss: 1.03427\n",
      "[137]\ttrain's my_multiclass_logloss: 0.72239\tvalid's my_multiclass_logloss: 1.03503\n",
      "[138]\ttrain's my_multiclass_logloss: 0.722061\tvalid's my_multiclass_logloss: 1.03387\n",
      "[139]\ttrain's my_multiclass_logloss: 0.722047\tvalid's my_multiclass_logloss: 1.03063\n",
      "[140]\ttrain's my_multiclass_logloss: 0.721509\tvalid's my_multiclass_logloss: 1.03127\n",
      "[141]\ttrain's my_multiclass_logloss: 0.72048\tvalid's my_multiclass_logloss: 1.03671\n",
      "[142]\ttrain's my_multiclass_logloss: 0.719285\tvalid's my_multiclass_logloss: 1.03628\n",
      "[143]\ttrain's my_multiclass_logloss: 0.718203\tvalid's my_multiclass_logloss: 1.03588\n",
      "[144]\ttrain's my_multiclass_logloss: 0.717281\tvalid's my_multiclass_logloss: 1.03578\n",
      "[145]\ttrain's my_multiclass_logloss: 0.716255\tvalid's my_multiclass_logloss: 1.03671\n",
      "[146]\ttrain's my_multiclass_logloss: 0.715903\tvalid's my_multiclass_logloss: 1.03412\n",
      "[147]\ttrain's my_multiclass_logloss: 0.715105\tvalid's my_multiclass_logloss: 1.03338\n",
      "[148]\ttrain's my_multiclass_logloss: 0.714257\tvalid's my_multiclass_logloss: 1.03303\n",
      "[149]\ttrain's my_multiclass_logloss: 0.713048\tvalid's my_multiclass_logloss: 1.03611\n",
      "[150]\ttrain's my_multiclass_logloss: 0.71199\tvalid's my_multiclass_logloss: 1.03922\n",
      "[151]\ttrain's my_multiclass_logloss: 0.711145\tvalid's my_multiclass_logloss: 1.0397\n",
      "[152]\ttrain's my_multiclass_logloss: 0.710145\tvalid's my_multiclass_logloss: 1.03906\n",
      "[153]\ttrain's my_multiclass_logloss: 0.71\tvalid's my_multiclass_logloss: 1.03635\n",
      "[154]\ttrain's my_multiclass_logloss: 0.7093\tvalid's my_multiclass_logloss: 1.03377\n",
      "[155]\ttrain's my_multiclass_logloss: 0.709134\tvalid's my_multiclass_logloss: 1.03534\n",
      "[156]\ttrain's my_multiclass_logloss: 0.709239\tvalid's my_multiclass_logloss: 1.03313\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttrain's my_multiclass_logloss: 0.822299\tvalid's my_multiclass_logloss: 0.984989\n",
      "-------------------- Test data accuracy -------------------- \n",
      "\n",
      "0.5\n",
      "-------------------- gain importance in GC -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.078159\n",
      "1   feature2    0.118260\n",
      "2   feature3    0.026172\n",
      "3   feature4    0.024904\n",
      "4   feature5    0.131761\n",
      "5   feature6    0.049043\n",
      "6   feature7    0.084389\n",
      "7   feature8    0.178924\n",
      "8   feature9    0.210713\n",
      "9  feature10    0.097676\n",
      "     feature  importance\n",
      "0   feature1    0.087520\n",
      "1   feature2    0.113259\n",
      "2   feature3    0.042944\n",
      "3   feature4    0.043572\n",
      "4   feature5    0.133659\n",
      "5   feature6    0.035983\n",
      "6   feature7    0.090686\n",
      "7   feature8    0.154137\n",
      "8   feature9    0.195677\n",
      "9  feature10    0.102563\n",
      "None\n",
      "-------------------- Difference of importance -------------------- \n",
      "\n",
      "     feature  importance\n",
      "0   feature1    0.080862\n",
      "1   feature2   -0.043193\n",
      "2   feature3    0.144875\n",
      "3   feature4    0.161257\n",
      "4   feature5    0.016398\n",
      "5   feature6   -0.112815\n",
      "6   feature7    0.054392\n",
      "7   feature8   -0.214108\n",
      "8   feature9   -0.129884\n",
      "9  feature10    0.042215\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cross validation \n",
    "best_params = trial.params\n",
    "best_params.update({\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 3,\n",
    "    'num_iteration': 3000,\n",
    "    'verbosity': -1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'importance_type': 'gain',\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "# params in modified model\n",
    "best_params_mm = trial.params.copy()\n",
    "best_params_mm.update({\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'none',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_class': 3,\n",
    "    'num_iteration': 3000,\n",
    "    'verbosity': -1,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'seed': 42\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# divide dataset\n",
    "n_splits = 10\n",
    "cv = list(StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = 2022).split(x_gbm, y_gbm))\n",
    "\n",
    "feature_names = [f\"feature{i+1}\" for i in range(x_gbm.shape[1])]\n",
    "CV_names = [f\"CV{i}\" for i in range(n_splits)]\n",
    "df_importance_GC = pd.DataFrame(columns=CV_names, index=feature_names)\n",
    "df_acc = pd.DataFrame(columns=CV_names, index=['GC','SFC'])\n",
    "df_importance_SFC = pd.DataFrame(columns=CV_names, index=feature_names)\n",
    "df_importance_Diff = pd.DataFrame(columns=CV_names, index=feature_names)\n",
    "\n",
    "# CV loop\n",
    "for nfold, (train_index, valid_index) in enumerate(cv):\n",
    "    print(\"-\"*20, nfold, \"-\"*20)\n",
    "    x_tr, y_tr = x_gbm[train_index], y_gbm[train_index]\n",
    "    x_va, y_va = x_gbm[valid_index], y_gbm[valid_index]\n",
    "    print(x_tr.shape, y_tr.shape)\n",
    "    print(x_va.shape, y_va.shape)\n",
    "    \n",
    "    # define lgb dataset\n",
    "    lgb_train = lgb.Dataset(x_tr, label = y_tr)\n",
    "    lgb_eval = lgb.Dataset(x_va, label = y_va, reference = lgb_train)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"-\"*20, \"GC model learning\", \"-\"*20, \"\\n\")\n",
    "    \n",
    "    \n",
    "    # model training\n",
    "    evaluation_results={}\n",
    "    model_GC = lgb.train(best_params,\n",
    "                      train_set=lgb_train,\n",
    "                      valid_names=['train', 'valid'],\n",
    "                      valid_sets=[lgb_train, lgb_eval],\n",
    "                      evals_result=evaluation_results,\n",
    "                      early_stopping_rounds = 100\n",
    "                     )\n",
    "    \n",
    "    # calculate accuracy of test data\n",
    "    y_pred_prob = model_GC.predict(x_va, num_iteration = model_GC.best_iteration)\n",
    "    y_pred_GC = np.argmax(y_pred_prob,axis=1)\n",
    "    acc_GC = accuracy_score(y_va, y_pred_GC)\n",
    "    print(\"-\"*20, \"Test data accuracy\", \"-\"*20, \"\\n\")\n",
    "    print(acc_GC)\n",
    "    \n",
    "    # gain features in normal LightGBM\n",
    "    cols = list(df_10dims.drop('patient number', axis = 1).columns)\n",
    "    f_importance = np.array(model_GC.feature_importance(importance_type='gain'))\n",
    "    f_importance = f_importance / np.sum(f_importance)\n",
    "    df_importance = pd.DataFrame({'feature':feature_names , 'importance':f_importance})\n",
    "    # df_importance = df_importance.sort_values('importance', ascending=False)\n",
    "    print(\"-\"*20, \"gain importance in GC\", \"-\"*20, \"\\n\")\n",
    "    print(df_importance)\n",
    "    \n",
    "    # model training\n",
    "    print(\"\\n\")\n",
    "    print(\"-\"*20, \"SFC model learning\", \"-\"*20, \"\\n\")\n",
    "    evaluation_results2={}\n",
    "    my_mlnloss = MultiLoglossForLGBM(n_class = 3, use_softmax = True)\n",
    "    model_SFC = lgb.train(best_params_mm,\n",
    "                                train_set=lgb_train,\n",
    "                                valid_names=['train', 'valid'],\n",
    "                                valid_sets=[lgb_train, lgb_eval],\n",
    "                                evals_result=evaluation_results2,\n",
    "                                fobj=my_mlnloss.return_grad_and_hess,\n",
    "                                feval=lambda preds, data: my_mlnloss.return_loss(preds, data),\n",
    "                                early_stopping_rounds = 100\n",
    "                               )\n",
    "    \n",
    "    # calculate accuracy of test data\n",
    "    y_pred_prob_SFC = model_SFC.predict(x_va, num_iteration = model_SFC.best_iteration)\n",
    "    y_pred_SFC = np.argmax(y_pred_prob_SFC,axis=1)\n",
    "    acc_SFC = accuracy_score(y_va, y_pred_SFC)\n",
    "    print(\"-\"*20, \"Test data accuracy\", \"-\"*20, \"\\n\")\n",
    "    print(acc_SFC)\n",
    "    \n",
    "    print(\"-\"*20, \"gain importance in GC\", \"-\"*20, \"\\n\")\n",
    "    print(df_importance)\n",
    "    \n",
    "    # gain features\n",
    "    cols = list(df_10dims.drop('patient number', axis = 1).columns)\n",
    "    f_importance_SFC = np.array(model_SFC.feature_importance(importance_type='gain'))\n",
    "    f_importance_SFC = f_importance_SFC / np.sum(f_importance_SFC)\n",
    "    df_importance_SFC = pd.DataFrame({'feature':feature_names, 'importance':f_importance_SFC})\n",
    "    # df_importance_SFC = df_importance_SFC.sort_values('importance', ascending=False)\n",
    "    print(df_importance_SFC)\n",
    "    print(plot_feature_importance(df_importance_SFC))\n",
    "    \n",
    "    # calculate differences between GC and SFC\n",
    "    f_importance_GC = np.array(model_GC.feature_importance(importance_type='gain'))\n",
    "    f_importance_GC = f_importance_GC / np.sum(f_importance_GC)\n",
    "    f_importance_SFC = np.array(model_SFC.feature_importance(importance_type='gain'))\n",
    "    f_importance_SFC = f_importance_SFC / np.sum(f_importance_SFC)\n",
    "    # after - before gain\n",
    "    df_importance_subtracted = f_importance_SFC - f_importance_GC\n",
    "    df_importance_subtracted = df_importance_subtracted / np.sum(np.abs(df_importance_subtracted))\n",
    "    df_importance_subtracted = pd.DataFrame({'feature':feature_names , 'importance':df_importance_subtracted})\n",
    "    # df_importance_subtracted = df_importance_subtracted.sort_values('importance', ascending=False)\n",
    "    print(\"-\"*20, \"Difference of importance\", \"-\"*20, \"\\n\")\n",
    "    \n",
    "    print(df_importance_subtracted)\n",
    "    df_acc.at['GC',CV_names[nfold]] = acc_GC\n",
    "    df_acc.at['SFC',CV_names[nfold]] = acc_SFC\n",
    "    df_importance_GC[CV_names[nfold]] = f_importance_SFC\n",
    "    df_importance_SFC[CV_names[nfold]] = f_importance_SFC\n",
    "    df_importance_Diff[CV_names[nfold]] = f_importance_SFC - f_importance_GC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a604310f-8ec4-4b50-9c06-cebf8a26c54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV0</th>\n",
       "      <th>CV1</th>\n",
       "      <th>CV2</th>\n",
       "      <th>CV3</th>\n",
       "      <th>CV4</th>\n",
       "      <th>CV5</th>\n",
       "      <th>CV6</th>\n",
       "      <th>CV7</th>\n",
       "      <th>CV8</th>\n",
       "      <th>CV9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GC</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SFC</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CV0       CV1       CV2       CV3       CV4       CV5       CV6  \\\n",
       "GC   0.545455  0.636364  0.636364  0.545455  0.727273  0.818182  0.454545   \n",
       "SFC  0.545455  0.636364  0.636364  0.454545  0.727273  0.818182  0.545455   \n",
       "\n",
       "          CV7  CV8  CV9  \n",
       "GC   0.454545  0.8  0.5  \n",
       "SFC  0.545455  0.8  0.5  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1808e45-2b04-4d09-8b69-503c540ef3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV0</th>\n",
       "      <th>CV1</th>\n",
       "      <th>CV2</th>\n",
       "      <th>CV3</th>\n",
       "      <th>CV4</th>\n",
       "      <th>CV5</th>\n",
       "      <th>CV6</th>\n",
       "      <th>CV7</th>\n",
       "      <th>CV8</th>\n",
       "      <th>CV9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>-0.030294</td>\n",
       "      <td>-0.023715</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.011150</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009046</td>\n",
       "      <td>-0.006019</td>\n",
       "      <td>0.009361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>0.008635</td>\n",
       "      <td>-0.006082</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>-0.008316</td>\n",
       "      <td>0.037187</td>\n",
       "      <td>0.065672</td>\n",
       "      <td>-0.005279</td>\n",
       "      <td>-0.009563</td>\n",
       "      <td>-0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>-0.003892</td>\n",
       "      <td>-0.011512</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>-0.007882</td>\n",
       "      <td>-0.011141</td>\n",
       "      <td>-0.027868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015595</td>\n",
       "      <td>0.016772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>0.000565</td>\n",
       "      <td>-0.043410</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.003911</td>\n",
       "      <td>-0.024643</td>\n",
       "      <td>-0.060729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>0.018668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>0.016674</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>-0.004001</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>-0.333555</td>\n",
       "      <td>-0.052529</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.001898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>0.012402</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.004754</td>\n",
       "      <td>-0.028035</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>-0.017682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011188</td>\n",
       "      <td>-0.008600</td>\n",
       "      <td>-0.013060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>-0.003724</td>\n",
       "      <td>0.008545</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.023905</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026780</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.006297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.045029</td>\n",
       "      <td>-0.014173</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.025057</td>\n",
       "      <td>0.228017</td>\n",
       "      <td>0.058484</td>\n",
       "      <td>-0.011932</td>\n",
       "      <td>-0.024787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>-0.021506</td>\n",
       "      <td>0.010672</td>\n",
       "      <td>-0.013364</td>\n",
       "      <td>-0.012757</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>-0.050662</td>\n",
       "      <td>-0.023656</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>-0.015036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>0.019297</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.090529</td>\n",
       "      <td>-0.005943</td>\n",
       "      <td>-0.006508</td>\n",
       "      <td>0.004887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                CV0       CV1       CV2       CV3       CV4       CV5  \\\n",
       "feature1  -0.030294 -0.023715  0.001977  0.011150  0.002458  0.010100   \n",
       "feature2   0.008635 -0.006082  0.002538  0.007143 -0.008316  0.037187   \n",
       "feature3  -0.003892 -0.011512  0.005160 -0.007882 -0.011141 -0.027868   \n",
       "feature4   0.000565 -0.043410  0.000111 -0.003911 -0.024643 -0.060729   \n",
       "feature5   0.016674  0.015039  0.001555  0.023989 -0.004001  0.014320   \n",
       "feature6   0.012402  0.001738  0.004754 -0.028035  0.002856 -0.017682   \n",
       "feature7  -0.003724  0.008545  0.010192  0.003734  0.023905  0.000620   \n",
       "feature8   0.001843  0.045029 -0.014173  0.005334  0.009634  0.025057   \n",
       "feature9  -0.021506  0.010672 -0.013364 -0.012757  0.003845  0.017700   \n",
       "feature10  0.019297  0.003696  0.001250  0.001235  0.005403  0.001296   \n",
       "\n",
       "                CV6       CV7       CV8       CV9  \n",
       "feature1   0.000000 -0.009046 -0.006019  0.009361  \n",
       "feature2   0.065672 -0.005279 -0.009563 -0.005000  \n",
       "feature3   0.000000  0.000000  0.015595  0.016772  \n",
       "feature4   0.000000  0.000000  0.009791  0.018668  \n",
       "feature5  -0.333555 -0.052529  0.008825  0.001898  \n",
       "feature6   0.000000  0.011188 -0.008600 -0.013060  \n",
       "feature7   0.000000  0.026780  0.008659  0.006297  \n",
       "feature8   0.228017  0.058484 -0.011932 -0.024787  \n",
       "feature9  -0.050662 -0.023656 -0.000247 -0.015036  \n",
       "feature10  0.090529 -0.005943 -0.006508  0.004887  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_importance_Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd3239e6-b6b6-4686-ab07-28ae1cf13939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV4</th>\n",
       "      <th>CV5</th>\n",
       "      <th>CV8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CV4   CV5   CV8\n",
       "feature1    6.0   5.0   6.0\n",
       "feature2    8.0   1.0   9.0\n",
       "feature3    9.0   9.0   1.0\n",
       "feature4   10.0  10.0   2.0\n",
       "feature5    7.0   4.0   3.0\n",
       "feature6    5.0   8.0   8.0\n",
       "feature7    1.0   7.0   4.0\n",
       "feature8    2.0   2.0  10.0\n",
       "feature9    4.0   3.0   5.0\n",
       "feature10   3.0   6.0   7.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance_Diff_rank3 = pd.DataFrame(index = df_importance_Diff.index)\n",
    "\n",
    "for col in df_acc.loc[:,(df_acc>=acc_th).sum(axis=0)==2].columns:\n",
    "    rank = df_importance_Diff[col].rank(method=\"min\" , ascending=False)\n",
    "    rank3 = (rank).rank(method=\"min\" , ascending=True)\n",
    "    df_importance_Diff_rank3[col] = rank3\n",
    "df_importance_Diff_rank3.loc[:,df_acc.loc[:,(df_acc>=acc_th).sum(axis=0)==2].columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8bf7507f-3c8a-43d5-9aaf-e61192a42741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnN0lEQVR4nO3de1hVZf7//9fi6JaTghCgJCBCOipSmkEZdjBNy/FnhZWGmKOZZYUyzseywnQkNZycmTxlA2aTxjiaqQX1iUiNxkGNsjQ1CsiGxqx0ewTarN8fXuxPO/GUe8lXfT6ui2tc97r3vd5rwZ7da9/rYJimaQoAAAAAALidR3MXAAAAAADAxYrQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAHC7/Px8GYbR5E9WVpYl29y+fbuys7NVWVlpyfjnorKyUoZh6LnnnmvuUn610tJSZWdna//+/c1dykXpYvgbAQA0zau5CwAAXLzy8vJ0xRVXuLRFRkZasq3t27dr6tSp6tOnj6Kjoy3ZxqWstLRUU6dOVUZGhlq1atXc5QAAcMEgdAMALNOlSxf16NGjucs4J/X19TIMQ15el+ZH5tGjR9WiRYvmLuP/GUeOHFHLli2buwwAwAWE08sBAM3mtddeU3Jysvz8/OTv769+/frpo48+cumzefNm3X333YqOjpbNZlN0dLTuueceVVVVOfvk5+frrrvukiTdcMMNzlPZ8/PzJUnR0dHKyMg4Yft9+vRRnz59nMslJSUyDENLly7VxIkT1bZtW/n6+uqLL76QJP3v//6vbrrpJgUGBqply5a69tpr9e677/6qfW88Bb+4uFijR49WSEiIAgMDlZ6ersOHD+vbb79VWlqaWrVqpYiICGVlZam+vt75+sbTkWfNmqU//vGPuvzyy9WiRQv16NGjyZo2btyom266SQEBAWrZsqVSUlK0bt26Jmt6++23df/99ys0NFQtW7bU5MmT9fvf/16SFBMT4zy+JSUlko7/Hm+55RZFRETIZrOpU6dO+p//+R8dPnzYZfyMjAz5+/vriy++0IABA+Tv76+oqChNnDhRtbW1Ln1ra2v1zDPPqFOnTmrRooVCQkJ0ww03qLS01NnHNE3NmzdP3bt3l81mU+vWrXXnnXfqyy+/dBnro48+0m233aawsDD5+voqMjJSAwcO1J49e075O+rTp4+6dOmi9evXKyUlRS1bttT9999v2T7/Un19vUaMGCF/f3+tXbv2lH3nz5+vxMRE+fv7KyAgQFdccYUef/xx5/rs7GwZhnHC6xp/5z+/LCM6Olq33Xab1q5dq6SkJOf+NdaQn5+vTp06yc/PT1dffbU2b958ytoA4FJH6AYAWMbhcOinn35y+Wk0Y8YM3XPPPercubMKCgq0dOlSHTx4UL1799b27dud/SorK5WQkKDnn39eRUVFmjlzpmpqatSzZ0/t27dPkjRw4EDNmDFDkvTCCy/oww8/1IcffqiBAwf+qronT56s6upqLViwQGvWrFFYWJheeeUV3XLLLQoMDNSSJUtUUFCg4OBg9evX71cHb0n63e9+p6CgIC1fvlxTpkzRq6++qtGjR2vgwIFKTEzUihUrNGLECOXm5uovf/nLCa//61//qsLCQj3//PN65ZVX5OHhoVtvvVUffvihs8/777+vG2+8UQcOHNBLL72kZcuWKSAgQLfffrtee+21E8a8//775e3traVLl2rFihV68MEHNX78eEnSypUrncf3yiuvlCTt3r1bAwYM0EsvvaTCwkI99thjKigo0O23337C2PX19Ro0aJBuuukmrV69Wvfff7/+9Kc/aebMmc4+P/30k2699VZNmzZNt912m1atWqX8/HylpKSourra2e+BBx7QY489pptvvlmvv/665s2bp88++0wpKSn673//K0k6fPiw+vbtq//+97964YUX9M477+j555/X5ZdfroMHD57291NTU6Phw4fr3nvv1Ztvvqlx48ZZss+/tH//fvXr109vv/223n//fd12220n7bt8+XKNGzdOqampWrVqlV5//XVlZmae8AXA2fj44481efJk/eEPf9DKlSsVFBSkIUOG6Omnn9bixYs1Y8YM/f3vf9eBAwd022236ejRo796WwBw0TMBAHCzvLw8U1KTP/X19WZ1dbXp5eVljh8/3uV1Bw8eNMPDw820tLSTjv3TTz+Zhw4dMv38/My5c+c62//xj3+Yksz33nvvhNe0b9/eHDFixAntqampZmpqqnP5vffeMyWZ119/vUu/w4cPm8HBwebtt9/u0u5wOMzExETz6quvPsXRMM2vvvrKlGTOnj3b2dZ4jH55DAYPHmxKMufMmePS3r17d/PKK688YczIyEjz6NGjzna73W4GBwebN998s7PtmmuuMcPCwsyDBw8623766SezS5cuZrt27cyGhgaXmtLT00/Yh9mzZ5uSzK+++uqU+9rQ0GDW19eb77//vinJ/Pjjj53rRowYYUoyCwoKXF4zYMAAMyEhwbn88ssvm5LMF1988aTb+fDDD01JZm5urkv7119/bdpsNnPSpEmmaZrm5s2bTUnm66+/fsq6m5KammpKMt99991T9nPHPv/8b+Srr74yO3fubHbu3NmsrKw8bZ0PP/yw2apVq1P2efrpp82m/rOv8Xf+899r+/btTZvNZu7Zs8fZVl5ebkoyIyIizMOHDzvbX3/9dVOS+cYbb5y2TgC4VDHTDQCwzMsvv6yysjKXHy8vLxUVFemnn35Senq6yyx4ixYtlJqa6jxtWZIOHTqkP/zhD4qLi5OXl5e8vLzk7++vw4cPa8eOHZbUfccdd7gsl5aW6ocfftCIESNc6m1oaFD//v1VVlb2q2cVfzmD2alTJ0k6YZa+U6dOLqfUNxoyZIjLNdeNM9jr16+Xw+HQ4cOHtWnTJt15553y9/d39vP09NR9992nPXv2aOfOnafc/9P58ssvde+99yo8PFyenp7y9vZWamqqJJ3wOzIM44TZ4G7durns21tvvaUWLVo4T+Vuytq1a2UYhoYPH+7yOwkPD1diYqLzbyguLk6tW7fWH/7wBy1YsMDlLIoz0bp1a914442W73OjrVu36pprrtFll12mDz74QO3btz9tjVdffbX279+ve+65R6tXr3aeAXIuunfvrrZt2zqXG/8u+/Tp43JNe2N7U/sCADju0rwrDADgvOjUqVOTN1JrPPW3Z8+eTb7Ow+P/vhO+99579e677+rJJ59Uz549FRgYKMMwNGDAAMtOaY2IiGiy3jvvvPOkr/nhhx/k5+d31tsKDg52Wfbx8Tlp+7Fjx054fXh4eJNtdXV1OnTokA4ePCjTNE/YJ+n/7iT//fffu7Q31fdkDh06pN69e6tFixaaPn264uPj1bJlS3399dcaMmTICb+jli1bnnBjNl9fX5d9++677xQZGenyd/BL//3vf2Wapi677LIm18fGxkqSgoKC9P777+uPf/yjHn/8cf3444+KiIjQ6NGjNWXKFHl7e59y/5o6Flbsc6N33nlH+/bt05w5c874LvH33XeffvrpJ7344ou644471NDQoJ49e2r69Onq27fvGY3xS2fzdympyX0BABxH6AYAnHdt2rSRJK1YseKUM3kHDhzQ2rVr9fTTT+t//ud/nO21tbX64Ycfznh7LVq0aPKmVfv27XPW8nO/vOFUY5+//OUvuuaaa5rcxsnCn9W+/fbbJtt8fHzk7+8vLy8veXh4qKam5oR+//nPfyTphGPQ1A23Tqa4uFj/+c9/VFJS4pzplXROz/MODQ3Vxo0b1dDQcNLg3aZNGxmGoQ0bNsjX1/eE9T9v69q1q5YvXy7TNPXJJ58oPz9fzzzzjGw2m8vfVVOaOhZW7HOj3//+96qoqHCeBZKenn5Grxs5cqRGjhypw4cPa/369Xr66ad12223adeuXWrfvr0z9NfW1rocG3fMigMATo3QDQA47/r16ycvLy9VVFSc8lRmwzBkmuYJoWrx4sVyOBwubY19mpr9jo6O1ieffOLStmvXLu3cubPJ0P1L1157rVq1aqXt27fr4YcfPm3/82nlypWaPXu2M1QdPHhQa9asUe/eveXp6Sk/Pz/16tVLK1eu1HPPPSebzSZJamho0CuvvKJ27dopPj7+tNs52fFtDKW//B0tXLjwV+/TrbfeqmXLlik/P/+kp5jfdtttevbZZ/XNN98oLS3tjMY1DEOJiYn605/+pPz8fG3duvVX1WfFPjfy8PDQwoUL5e/vr4yMDB0+fFgPPvjgGb/ez89Pt956q+rq6jR48GB99tlnat++vfPZ9Z988onLGSZr1qw555oBAKdG6AYAnHfR0dF65pln9MQTT+jLL79U//791bp1a/33v//Vv//9b/n5+Wnq1KkKDAzU9ddfr9mzZ6tNmzaKjo7W+++/r5deeumEU2+7dOkiSVq0aJECAgLUokULxcTEKCQkRPfdd5+GDx+ucePG6Y477lBVVZVmzZql0NDQM6rX399ff/nLXzRixAj98MMPuvPOOxUWFqbvvvtOH3/8sb777jvNnz/f3YfpjHh6eqpv376aMGGCGhoaNHPmTNntdk2dOtXZJycnR3379tUNN9ygrKws+fj4aN68efr000+1bNmyM5rZ7tq1qyRp7ty5GjFihLy9vZWQkKCUlBS1bt1aY8eO1dNPPy1vb2/9/e9/18cff/yr9+mee+5RXl6exo4dq507d+qGG25QQ0ODNm3apE6dOunuu+/WtddeqzFjxmjkyJHavHmzrr/+evn5+ammpkYbN25U165d9eCDD2rt2rWaN2+eBg8erNjYWJmmqZUrV2r//v2/+tRrK/b5l3JzcxUQEKBx48bp0KFDzke2NWX06NGy2Wy69tprFRERoW+//VY5OTkKCgpyBuwBAwYoODhYo0aN0jPPPCMvLy/l5+fr66+/dlvNAICmcSM1AECzmDx5slasWKFdu3ZpxIgR6tevnyZNmqSqqipdf/31zn6vvvqqbrjhBk2aNElDhgzR5s2b9c477ygoKMhlvJiYGD3//PP6+OOP1adPH/Xs2dM5i3fvvfdq1qxZKioq0m233ab58+dr/vz5ZzTD22j48OF67733dOjQIT3wwAO6+eab9eijj2rr1q266aab3HNQfoWHH35Yffv21SOPPKJ7771XP/30k9atW6drr73W2Sc1NVXFxcXy8/NTRkaG7r77bh04cEBvvPGGhg4dekbb6dOnjyZPnqw1a9bouuuuU8+ePbVlyxaFhIRo3bp1atmypYYPH677779f/v7+TT6K7Ex5eXnpzTff1OTJk7Vq1Sr99re/VXp6ujZu3OhyOcLChQv117/+VevXr9fdd9+tgQMH6qmnntLhw4d19dVXS5I6duyoVq1aadasWRo0aJDuuusubd26Vfn5+Ro9evSvqs+KfW5Kdna2Zs+erUmTJunpp58+ab/evXvr008/1aOPPqq+ffsqMzNT8fHx2rBhg/OLpcDAQBUWFiogIEDDhw/X2LFj1aVLFz3xxBNurRkAcCLDNE2zuYsAAABnp7KyUjExMZo9e7aysrKauxwAAHASzHQDAAAAAGARQjcAAAAAABbh9HIAAAAAACzCTDcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFvFq7gIuZg0NDfrPf/6jgIAAGYbR3OUAAAAAANzENE0dPHhQkZGR8vA4+Xw2odtC//nPfxQVFdXcZQAAAAAALPL111+rXbt2J11P6LZQQECApOO/hMDAwGauBgAAAADgLna7XVFRUc7cdzKEbgs1nlIeGBhI6AYAAACAi9DpLiXmRmoAAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARbyau4BLQZeni+Th27K5ywAAAMAFoPLZgc1dAgA3YqYbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALCI20O3aZoaM2aMgoODZRiGysvL3b0JAAAAAAAuCG4P3YWFhcrPz9fatWtVU1OjLl26nPOYGRkZGjx48LkXd4aOHTumjIwMde3aVV5eXud12wAAAACAi4eXuwesqKhQRESEUlJS3D30OXM4HDIMQx4ep/6uweFwyGaz6ZFHHtE///nP81QdAAAAAOBi49aZ7oyMDI0fP17V1dUyDEPR0dEyTVOzZs1SbGysbDabEhMTtWLFCudrHA6HRo0apZiYGNlsNiUkJGju3LnO9dnZ2VqyZIlWr14twzBkGIZKSkpUUlIiwzC0f/9+Z9/y8nIZhqHKykpJUn5+vlq1aqW1a9eqc+fO8vX1VVVVlerq6jRp0iS1bdtWfn5+6tWrl0pKSpzj+Pn5af78+Ro9erTCw8PdeYgAAAAAAJcQt850z507Vx06dNCiRYtUVlYmT09PTZkyRStXrtT8+fPVsWNHrV+/XsOHD1doaKhSU1PV0NCgdu3aqaCgQG3atFFpaanGjBmjiIgIpaWlKSsrSzt27JDdbldeXp4kKTg4WKWlpWdU05EjR5STk6PFixcrJCREYWFhGjlypCorK7V8+XJFRkZq1apV6t+/v7Zt26aOHTu685AAAAAAAC5hbg3dQUFBCggIkKenp8LDw3X48GHNmTNHxcXFSk5OliTFxsZq48aNWrhwoVJTU+Xt7a2pU6c6x4iJiVFpaakKCgqUlpYmf39/2Ww21dbW/qpZ5/r6es2bN0+JiYmSjp/+vmzZMu3Zs0eRkZGSpKysLBUWFiovL08zZsz41ftfW1ur2tpa57Ldbv/VYwEAAAAALnxuv6b757Zv365jx46pb9++Lu11dXVKSkpyLi9YsECLFy9WVVWVjh49qrq6OnXv3t0tNfj4+Khbt27O5a1bt8o0TcXHx7v0q62tVUhIyDltKycnx+ULBAAAAADApc3S0N3Q0CBJWrdundq2beuyztfXV5JUUFCgzMxM5ebmKjk5WQEBAZo9e7Y2bdp0yrEbb4Zmmqazrb6+/oR+NptNhmG41OTp6aktW7bI09PTpa+/v/9Z7N2JJk+erAkTJjiX7Xa7oqKizmlMAAAAAMCFy9LQ3XjzsurqaqWmpjbZZ8OGDUpJSdG4ceOcbRUVFS59fHx85HA4XNpCQ0MlSTU1NWrdurUkndEzwZOSkuRwOLR371717t37bHbntHx9fZ1fJgAAAAAAYGnoDggIUFZWljIzM9XQ0KDrrrtOdrtdpaWl8vf314gRIxQXF6eXX35ZRUVFiomJ0dKlS1VWVqaYmBjnONHR0SoqKtLOnTsVEhKioKAgxcXFKSoqStnZ2Zo+fbp2796t3Nzc09YUHx+vYcOGKT09Xbm5uUpKStK+fftUXFysrl27asCAAZKOnxpfV1enH374QQcPHnQGened9g4AAAAAuPhZGroladq0aQoLC1NOTo6+/PJLtWrVSldeeaUef/xxSdLYsWNVXl6uoUOHyjAM3XPPPRo3bpzeeust5xijR49WSUmJevTooUOHDum9995Tnz59tGzZMj344INKTExUz549NX36dN11112nrSkvL0/Tp0/XxIkT9c033ygkJETJycnOwC1JAwYMUFVVlXO58Rr0n5/ODgAAAADAqRgmKdIydrtdQUFBinqsQB6+LZu7HAAAAFwAKp8d2NwlADgDjXnvwIEDCgwMPGk/j/NYEwAAAAAAlxRCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEa/mLuBS8OnUfgoMDGzuMgAAAAAA5xkz3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEV4ZNh50OXpInn4tmzuMgAAAIBzUvnswOYuAbjgMNMNAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEXcHrpN09SYMWMUHBwswzBUXl7u7k0AAAAAAHBBcHvoLiwsVH5+vtauXauamhp16dLlnMfMyMjQ4MGDz724M1RSUqLf/va3ioiIkJ+fn7p3766///3v5237AAAAAICLg5e7B6yoqFBERIRSUlLcPfQ5czgcMgxDHh6n/q6htLRU3bp10x/+8AdddtllWrdundLT0xUYGKjbb7/9PFULAAAAALjQuXWmOyMjQ+PHj1d1dbUMw1B0dLRM09SsWbMUGxsrm82mxMRErVixwvkah8OhUaNGKSYmRjabTQkJCZo7d65zfXZ2tpYsWaLVq1fLMAwZhqGSkhKVlJTIMAzt37/f2be8vFyGYaiyslKSlJ+fr1atWmnt2rXq3LmzfH19VVVVpbq6Ok2aNElt27aVn5+fevXqpZKSEuc4jz/+uKZNm6aUlBR16NBBjzzyiPr3769Vq1a583ABAAAAAC5ybp3pnjt3rjp06KBFixaprKxMnp6emjJlilauXKn58+erY8eOWr9+vYYPH67Q0FClpqaqoaFB7dq1U0FBgdq0aaPS0lKNGTNGERERSktLU1ZWlnbs2CG73a68vDxJUnBwsEpLS8+opiNHjignJ0eLFy9WSEiIwsLCNHLkSFVWVmr58uWKjIzUqlWr1L9/f23btk0dO3ZscpwDBw6oU6dObjtWAAAAAICLn1tDd1BQkAICAuTp6anw8HAdPnxYc+bMUXFxsZKTkyVJsbGx2rhxoxYuXKjU1FR5e3tr6tSpzjFiYmJUWlqqgoICpaWlyd/fXzabTbW1tQoPDz/rmurr6zVv3jwlJiZKOn76+7Jly7Rnzx5FRkZKkrKyslRYWKi8vDzNmDHjhDFWrFihsrIyLVy48JTbqq2tVW1trXPZbrefdb0AAAAAgIuH26/p/rnt27fr2LFj6tu3r0t7XV2dkpKSnMsLFizQ4sWLVVVVpaNHj6qurk7du3d3Sw0+Pj7q1q2bc3nr1q0yTVPx8fEu/WpraxUSEnLC60tKSpSRkaEXX3xRv/nNb065rZycHJcvEAAAAAAAlzZLQ3dDQ4Mkad26dWrbtq3LOl9fX0lSQUGBMjMzlZubq+TkZAUEBGj27NnatGnTKcduvBmaaZrOtvr6+hP62Ww2GYbhUpOnp6e2bNkiT09Pl77+/v4uy++//75uv/12zZkzR+np6afbXU2ePFkTJkxwLtvtdkVFRZ32dQAAAACAi5Olobvx5mXV1dVKTU1tss+GDRuUkpKicePGOdsqKipc+vj4+MjhcLi0hYaGSpJqamrUunVrSTqjZ4InJSXJ4XBo79696t2790n7lZSU6LbbbtPMmTM1ZsyY044rHf8iofHLBAAAAAAALA3dAQEBysrKUmZmphoaGnTdddfJbrertLRU/v7+GjFihOLi4vTyyy+rqKhIMTExWrp0qcrKyhQTE+McJzo6WkVFRdq5c6dCQkIUFBSkuLg4RUVFKTs7W9OnT9fu3buVm5t72pri4+M1bNgwpaenKzc3V0lJSdq3b5+Ki4vVtWtXDRgwQCUlJRo4cKAeffRR3XHHHfr2228lHQ//wcHBlh0vAAAAAMDFxa2PDGvKtGnT9NRTTyknJ0edOnVSv379tGbNGmeoHjt2rIYMGaKhQ4eqV69e+v77711mvSVp9OjRSkhIUI8ePRQaGqoPPvhA3t7eWrZsmT7//HMlJiZq5syZmj59+hnVlJeXp/T0dE2cOFEJCQkaNGiQNm3a5DwVPD8/33nX84iICOfPkCFD3HtwAAAAAAAXNcP8+UXRcCu73a6goCBFPVYgD9+WzV0OAAAAcE4qnx3Y3CUA/89ozHsHDhxQYGDgSftZPtMNAAAAAMClitANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFjEq7kLuBR8OrWfAgMDm7sMAAAAAMB5xkw3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgER4Zdh50ebpIHr4tm7sMAAAA4KJV+ezA5i4BaBIz3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYxO2h2zRNjRkzRsHBwTIMQ+Xl5e7eBAAAAAAAFwS3h+7CwkLl5+dr7dq1qqmpUZcuXc55zIyMDA0ePPjciztDO3fu1A033KDLLrtMLVq0UGxsrKZMmaL6+vrzVgMAAAAA4MLn5e4BKyoqFBERoZSUFHcPfc4cDocMw5CHx6m/a/D29lZ6erquvPJKtWrVSh9//LFGjx6thoYGzZgx4zxVCwAAAAC40Ll1pjsjI0Pjx49XdXW1DMNQdHS0TNPUrFmzFBsbK5vNpsTERK1YscL5GofDoVGjRikmJkY2m00JCQmaO3euc312draWLFmi1atXyzAMGYahkpISlZSUyDAM7d+/39m3vLxchmGosrJSkpSfn69WrVpp7dq16ty5s3x9fVVVVaW6ujpNmjRJbdu2lZ+fn3r16qWSkhLnOLGxsRo5cqQSExPVvn17DRo0SMOGDdOGDRvcebgAAAAAABc5t850z507Vx06dNCiRYtUVlYmT09PTZkyRStXrtT8+fPVsWNHrV+/XsOHD1doaKhSU1PV0NCgdu3aqaCgQG3atFFpaanGjBmjiIgIpaWlKSsrSzt27JDdbldeXp4kKTg4WKWlpWdU05EjR5STk6PFixcrJCREYWFhGjlypCorK7V8+XJFRkZq1apV6t+/v7Zt26aOHTueMMYXX3yhwsJCDRky5JTbqq2tVW1trXPZbrefxdEDAAAAAFxs3Bq6g4KCFBAQIE9PT4WHh+vw4cOaM2eOiouLlZycLOn4LPLGjRu1cOFCpaamytvbW1OnTnWOERMTo9LSUhUUFCgtLU3+/v6y2Wyqra1VeHj4WddUX1+vefPmKTExUdLx09+XLVumPXv2KDIyUpKUlZWlwsJC5eXluZw+npKSoq1bt6q2tlZjxozRM888c8pt5eTkuOwLAAAAAODS5vZrun9u+/btOnbsmPr27evSXldXp6SkJOfyggULtHjxYlVVVeno0aOqq6tT9+7d3VKDj4+PunXr5lzeunWrTNNUfHy8S7/a2lqFhIS4tL322ms6ePCgPv74Y/3+97/Xc889p0mTJp10W5MnT9aECROcy3a7XVFRUW7ZDwAAAADAhcfS0N3Q0CBJWrdundq2beuyztfXV5JUUFCgzMxM5ebmKjk5WQEBAZo9e7Y2bdp0yrEbb4Zmmqazram7i9tsNhmG4VKTp6entmzZIk9PT5e+/v7+LsuNgblz585yOBwaM2aMJk6ceMLrfr5PjfsFAAAAAIClobvx5mXV1dVKTU1tss+GDRuUkpKicePGOdsqKipc+vj4+MjhcLi0hYaGSpJqamrUunVrSTqjZ4InJSXJ4XBo79696t279xnvi2maqq+vdwn5AAAAAACciqWhOyAgQFlZWcrMzFRDQ4Ouu+462e12lZaWyt/fXyNGjFBcXJxefvllFRUVKSYmRkuXLlVZWZliYmKc40RHR6uoqEg7d+5USEiIgoKCFBcXp6ioKGVnZ2v69OnavXu3cnNzT1tTfHy8hg0bpvT0dOXm5iopKUn79u1TcXGxunbtqgEDBujvf/+7vL291bVrV/n6+mrLli2aPHmyhg4dKi8vSw8ZAAAAAOAiYnmCnDZtmsLCwpSTk6Mvv/xSrVq10pVXXqnHH39ckjR27FiVl5dr6NChMgxD99xzj8aNG6e33nrLOcbo0aNVUlKiHj166NChQ3rvvffUp08fLVu2TA8++KASExPVs2dPTZ8+XXfddddpa8rLy9P06dM1ceJEffPNNwoJCVFycrIGDBggSfLy8tLMmTO1a9cumaap9u3b66GHHlJmZqY1BwkAAAAAcFEyTM6XtozdbldQUJCiHiuQh2/L5i4HAAAAuGhVPjuwuUvAJaYx7x04cECBgYEn7edxHmsCAAAAAOCSQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAiXs1dwKXg06n9TvmwdAAAAADAxYmZbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALMJzus+DLk8XycO3ZXOXAQAAAAAXhMpnBzZ3CW7DTDcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBG3h27TNDVmzBgFBwfLMAyVl5e7exMAAAAAAFwQ3B66CwsLlZ+fr7Vr16qmpkZdunQ55zEzMjI0ePDgcy/uV/jiiy8UEBCgVq1aNcv2AQAAAAAXLreH7oqKCkVERCglJUXh4eHy8vJy9yZ+NYfDoYaGhjPuX19fr3vuuUe9e/e2sCoAAAAAwMXKraE7IyND48ePV3V1tQzDUHR0tEzT1KxZsxQbGyubzabExEStWLHC+RqHw6FRo0YpJiZGNptNCQkJmjt3rnN9dna2lixZotWrV8swDBmGoZKSEpWUlMgwDO3fv9/Zt7y8XIZhqLKyUpKUn5+vVq1aae3atercubN8fX1VVVWluro6TZo0SW3btpWfn5969eqlkpKSE/ZnypQpuuKKK5SWlubOwwQAAAAAuES4dRp67ty56tChgxYtWqSysjJ5enpqypQpWrlypebPn6+OHTtq/fr1Gj58uEJDQ5WamqqGhga1a9dOBQUFatOmjUpLSzVmzBhFREQoLS1NWVlZ2rFjh+x2u/Ly8iRJwcHBKi0tPaOajhw5opycHC1evFghISEKCwvTyJEjVVlZqeXLlysyMlKrVq1S//79tW3bNnXs2FGSVFxcrH/84x8qLy/XypUr3XmYAAAAAACXCLeG7qCgIAUEBMjT01Ph4eE6fPiw5syZo+LiYiUnJ0uSYmNjtXHjRi1cuFCpqany9vbW1KlTnWPExMSotLRUBQUFSktLk7+/v2w2m2praxUeHn7WNdXX12vevHlKTEyUdPz092XLlmnPnj2KjIyUJGVlZamwsFB5eXmaMWOGvv/+e2VkZOiVV15RYGDgGW+rtrZWtbW1zmW73X7W9QIAAAAALh6WXnC9fft2HTt2TH379nVpr6urU1JSknN5wYIFWrx4saqqqnT06FHV1dWpe/fubqnBx8dH3bp1cy5v3bpVpmkqPj7epV9tba1CQkIkSaNHj9a9996r66+//qy2lZOT4/IFAgAAAADg0mZp6G68adm6devUtm1bl3W+vr6SpIKCAmVmZio3N1fJyckKCAjQ7NmztWnTplOO7eFx/HJ00zSdbfX19Sf0s9lsMgzDpSZPT09t2bJFnp6eLn39/f0lHT+1/I033tBzzz3n3EZDQ4O8vLy0aNEi3X///U3WNHnyZE2YMMG5bLfbFRUVdcr9AAAAAABcvCwN3Y03L6uurlZqamqTfTZs2KCUlBSNGzfO2VZRUeHSx8fHRw6Hw6UtNDRUklRTU6PWrVtL0hk9EzwpKUkOh0N79+496V3JP/zwQ5ftrV69WjNnzlRpaekJXx78nK+vr/PLBAAAAAAALA3dAQEBysrKUmZmphoaGnTdddfJbrertLRU/v7+GjFihOLi4vTyyy+rqKhIMTExWrp0qcrKyhQTE+McJzo6WkVFRdq5c6dCQkIUFBSkuLg4RUVFKTs7W9OnT9fu3buVm5t72pri4+M1bNgwpaenKzc3V0lJSdq3b5+Ki4vVtWtXDRgwQJ06dXJ5zebNm+Xh4eGWZ44DAAAAAC4dbn9O9y9NmzZNTz31lHJyctSpUyf169dPa9ascYbqsWPHasiQIRo6dKh69eql77//3mXWWzp+jXVCQoJ69Oih0NBQffDBB/L29tayZcv0+eefKzExUTNnztT06dPPqKa8vDylp6dr4sSJSkhI0KBBg7Rp0yZOBQcAAAAAuJVh/vyiaLiV3W5XUFCQoh4rkIdvy+YuBwAAAAAuCJXPDmzuEk6rMe8dOHDglE+9snymGwAAAACASxWhGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwiFdzF3Ap+HRqPwUGBjZ3GQAAAACA84yZbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALMJzus+DLk8XycO3ZXOXAQAALmKVzw5s7hIAAE1gphsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsIjbQ7dpmhozZoyCg4NlGIbKy8vdvQkAAAAAAC4Ibg/dhYWFys/P19q1a1VTU6MuXbqc85gZGRkaPHjwuRd3hiorK2UYxgk/hYWF560GAAAAAMCFz8vdA1ZUVCgiIkIpKSnuHvqcORwOGYYhD48z+67hf//3f/Wb3/zGuRwcHGxVaQAAAACAi5BbZ7ozMjI0fvx4VVdXyzAMRUdHyzRNzZo1S7GxsbLZbEpMTNSKFSucr3E4HBo1apRiYmJks9mUkJCguXPnOtdnZ2dryZIlWr16tXPGuaSkRCUlJTIMQ/v373f2LS8vl2EYqqyslCTl5+erVatWWrt2rTp37ixfX19VVVWprq5OkyZNUtu2beXn56devXqppKTkhP0JCQlReHi488fHx8edhwsAAAAAcJFz60z33Llz1aFDBy1atEhlZWXy9PTUlClTtHLlSs2fP18dO3bU+vXrNXz4cIWGhio1NVUNDQ1q166dCgoK1KZNG5WWlmrMmDGKiIhQWlqasrKytGPHDtntduXl5Uk6PuNcWlp6RjUdOXJEOTk5Wrx4sUJCQhQWFqaRI0eqsrJSy5cvV2RkpFatWqX+/ftr27Zt6tixo/O1gwYN0rFjx9SxY0dlZmbqzjvvdOfhAgAAAABc5NwauoOCghQQECBPT0+Fh4fr8OHDmjNnjoqLi5WcnCxJio2N1caNG7Vw4UKlpqbK29tbU6dOdY4RExOj0tJSFRQUKC0tTf7+/rLZbKqtrVV4ePhZ11RfX6958+YpMTFR0vHT35ctW6Y9e/YoMjJSkpSVlaXCwkLl5eVpxowZ8vf315w5c3TttdfKw8NDb7zxhoYOHaolS5Zo+PDhJ91WbW2tamtrnct2u/2s6wUAAAAAXDzcfk33z23fvl3Hjh1T3759Xdrr6uqUlJTkXF6wYIEWL16sqqoqHT16VHV1derevbtbavDx8VG3bt2cy1u3bpVpmoqPj3fpV1tbq5CQEElSmzZtlJmZ6VzXo0cP/fjjj5o1a9YpQ3dOTo7LFwgAAAAAgEubpaG7oaFBkrRu3Tq1bdvWZZ2vr68kqaCgQJmZmcrNzVVycrICAgI0e/Zsbdq06ZRjN94MzTRNZ1t9ff0J/Ww2mwzDcKnJ09NTW7Zskaenp0tff3//k27vmmuu0eLFi09Z0+TJkzVhwgTnst1uV1RU1ClfAwAAAAC4eFkauhtvXlZdXa3U1NQm+2zYsEEpKSkaN26cs62iosKlj4+PjxwOh0tbaGioJKmmpkatW7eWpDN6JnhSUpIcDof27t2r3r17n/G+fPTRR4qIiDhlH19fX+eXCQAAAAAAWBq6AwIClJWVpczMTDU0NOi6666T3W5XaWmp/P39NWLECMXFxenll19WUVGRYmJitHTpUpWVlSkmJsY5TnR0tIqKirRz506FhIQoKChIcXFxioqKUnZ2tqZPn67du3crNzf3tDXFx8dr2LBhSk9PV25urpKSkrRv3z4VFxera9euGjBggJYsWSJvb28lJSXJw8NDa9as0Z///GfNnDnTysMFAAAAALjIWBq6JWnatGkKCwtTTk6OvvzyS7Vq1UpXXnmlHn/8cUnS2LFjVV5erqFDh8owDN1zzz0aN26c3nrrLecYo0ePVklJiXr06KFDhw7pvffeU58+fbRs2TI9+OCDSkxMVM+ePTV9+nTdddddp60pLy9P06dP18SJE/XNN98oJCREycnJGjBggLPP9OnTVVVVJU9PT8XHx+tvf/vbKa/nBgAAAADglwzz5xdFw63sdruCgoIU9ViBPHxbNnc5AADgIlb57MDmLgEALimNee/AgQMKDAw8aT+P81gTAAAAAACXFEI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARr+Yu4FLw6dR+CgwMbO4yAAAAAADnGTPdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARXhk2HnQ5ekiefi2bO4yAAAAAMCp8tmBzV3CJYGZbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAs4vbQbZqmxowZo+DgYBmGofLycndvAgAAAACAC4LbQ3dhYaHy8/O1du1a1dTUqEuXLuc8ZkZGhgYPHnzuxZ0F0zT13HPPKT4+Xr6+voqKitKMGTPOaw0AAAAAgAubl7sHrKioUEREhFJSUtw99DlzOBwyDEMeHqf/ruHRRx/V22+/reeee05du3bVgQMHtG/fvvNQJQAAAADgYuHWme6MjAyNHz9e1dXVMgxD0dHRMk1Ts2bNUmxsrGw2mxITE7VixQrnaxwOh0aNGqWYmBjZbDYlJCRo7ty5zvXZ2dlasmSJVq9eLcMwZBiGSkpKVFJSIsMwtH//fmff8vJyGYahyspKSVJ+fr5atWqltWvXqnPnzvL19VVVVZXq6uo0adIktW3bVn5+furVq5dKSkqc4+zYsUPz58/X6tWrNWjQIMXExKh79+66+eab3Xm4AAAAAAAXObfOdM+dO1cdOnTQokWLVFZWJk9PT02ZMkUrV67U/Pnz1bFjR61fv17Dhw9XaGioUlNT1dDQoHbt2qmgoEBt2rRRaWmpxowZo4iICKWlpSkrK0s7duyQ3W5XXl6eJCk4OFilpaVnVNORI0eUk5OjxYsXKyQkRGFhYRo5cqQqKyu1fPlyRUZGatWqVerfv7+2bdumjh07as2aNYqNjdXatWvVv39/maapm2++WbNmzVJwcPBJt1VbW6va2lrnst1uP7cDCgAAAAC4oLk1dAcFBSkgIECenp4KDw/X4cOHNWfOHBUXFys5OVmSFBsbq40bN2rhwoVKTU2Vt7e3pk6d6hwjJiZGpaWlKigoUFpamvz9/WWz2VRbW6vw8PCzrqm+vl7z5s1TYmKipOOnvy9btkx79uxRZGSkJCkrK0uFhYXKy8vTjBkz9OWXX6qqqkr/+Mc/9PLLL8vhcCgzM1N33nmniouLT7qtnJwcl30BAAAAAFza3H5N989t375dx44dU9++fV3a6+rqlJSU5FxesGCBFi9erKqqKh09elR1dXXq3r27W2rw8fFRt27dnMtbt26VaZqKj4936VdbW6uQkBBJUkNDg2pra/Xyyy87+7300ku66qqrtHPnTiUkJDS5rcmTJ2vChAnOZbvdrqioKLfsBwAAAADgwmNp6G5oaJAkrVu3Tm3btnVZ5+vrK0kqKChQZmamcnNzlZycrICAAM2ePVubNm065diNN0MzTdPZVl9ff0I/m80mwzBcavL09NSWLVvk6enp0tff31+SFBERIS8vL5dg3qlTJ0lSdXX1SUO3r6+vc78AAAAAALA0dDfevKy6ulqpqalN9tmwYYNSUlI0btw4Z1tFRYVLHx8fHzkcDpe20NBQSVJNTY1at24tSWf0TPCkpCQ5HA7t3btXvXv3brLPtddeq59++kkVFRXq0KGDJGnXrl2SpPbt2592GwAAAAAASBY8p/vnAgIClJWVpczMTC1ZskQVFRX66KOP9MILL2jJkiWSpLi4OG3evFlFRUXatWuXnnzySZWVlbmMEx0drU8++UQ7d+7Uvn37VF9fr7i4OEVFRSk7O1u7du3SunXrlJube9qa4uPjNWzYMKWnp2vlypX66quvVFZWppkzZ+rNN9+UJN1888268sordf/99+ujjz7Sli1b9MADD6hv374nnJYOAAAAAMDJWBq6JWnatGl66qmnlJOTo06dOqlfv35as2aNYmJiJEljx47VkCFDNHToUPXq1Uvff/+9y6y3JI0ePVoJCQnq0aOHQkND9cEHH8jb21vLli3T559/rsTERM2cOVPTp08/o5ry8vKUnp6uiRMnKiEhQYMGDdKmTZuc1197eHhozZo1atOmja6//noNHDhQnTp10vLly917cAAAAAAAFzXD/PlF0XAru92uoKAgRT1WIA/fls1dDgAAAAA4VT47sLlLuKA15r0DBw4oMDDwpP0sn+kGAAAAAOBSRegGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAiXs1dwKXg06n9TvmwdAAAAADAxYmZbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALMJzus+DLk8XycO3ZXOXAQDABaHy2YHNXQIAAG7DTDcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBG3h27TNDVmzBgFBwfLMAyVl5e7exMAAAAAAFwQ3B66CwsLlZ+fr7Vr16qmpkZdunQ55zEzMjI0ePDgcy/uDGVnZ8swjBN+/Pz8zlsNAAAAAIALn5e7B6yoqFBERIRSUlLcPfQ5czgcMgxDHh6n/q4hKytLY8eOdWm76aab1LNnTyvLAwAAAABcZNw6052RkaHx48erurpahmEoOjpapmlq1qxZio2Nlc1mU2JiolasWOF8jcPh0KhRoxQTEyObzaaEhATNnTvXuT47O1tLlizR6tWrnTPOJSUlKikpkWEY2r9/v7NveXm5DMNQZWWlJCk/P1+tWrXS2rVr1blzZ/n6+qqqqkp1dXWaNGmS2rZtKz8/P/Xq1UslJSXOcfz9/RUeHu78+e9//6vt27dr1KhR7jxcAAAAAICLnFtnuufOnasOHTpo0aJFKisrk6enp6ZMmaKVK1dq/vz56tixo9avX6/hw4crNDRUqampamhoULt27VRQUKA2bdqotLRUY8aMUUREhNLS0pSVlaUdO3bIbrcrLy9PkhQcHKzS0tIzqunIkSPKycnR4sWLFRISorCwMI0cOVKVlZVavny5IiMjtWrVKvXv31/btm1Tx44dTxhj8eLFio+PV+/evd15uAAAAAAAFzm3hu6goCAFBATI09NT4eHhOnz4sObMmaPi4mIlJydLkmJjY7Vx40YtXLhQqamp8vb21tSpU51jxMTEqLS0VAUFBUpLS5O/v79sNptqa2sVHh5+1jXV19dr3rx5SkxMlHT89Pdly5Zpz549ioyMlHT8dPLCwkLl5eVpxowZLq+vra3V3//+d/3P//zPabdVW1ur2tpa57Ldbj/regEAAAAAFw+3X9P9c9u3b9exY8fUt29fl/a6ujolJSU5lxcsWKDFixerqqpKR48eVV1dnbp37+6WGnx8fNStWzfn8tatW2WapuLj41361dbWKiQk5ITXr1y5UgcPHlR6evppt5WTk+PyBQIAAAAA4NJmaehuaGiQJK1bt05t27Z1Wefr6ytJKigoUGZmpnJzc5WcnKyAgADNnj1bmzZtOuXYjTdDM03T2VZfX39CP5vNJsMwXGry9PTUli1b5Onp6dLX39//hNcvXrxYt9122xnNsk+ePFkTJkxwLtvtdkVFRZ32dQAAAACAi5Olobvx5mXV1dVKTU1tss+GDRuUkpKicePGOdsqKipc+vj4+MjhcLi0hYaGSpJqamrUunVrSTqjZ4InJSXJ4XBo7969p71G+6uvvtJ7772nN95447TjSse/SGj8MgEAAAAAAEtDd0BAgLKyspSZmamGhgZdd911stvtKi0tlb+/v0aMGKG4uDi9/PLLKioqUkxMjJYuXaqysjLFxMQ4x4mOjlZRUZF27typkJAQBQUFKS4uTlFRUcrOztb06dO1e/du5ebmnram+Ph4DRs2TOnp6crNzVVSUpL27dun4uJide3aVQMGDHD2/dvf/qaIiAjdeuutlhwfAAAAAMDFza2PDGvKtGnT9NRTTyknJ0edOnVSv379tGbNGmeoHjt2rIYMGaKhQ4eqV69e+v77711mvSVp9OjRSkhIUI8ePRQaGqoPPvhA3t7eWrZsmT7//HMlJiZq5syZmj59+hnVlJeXp/T0dE2cOFEJCQkaNGiQNm3a5HIqeENDg/Lz85WRkXHCaegAAAAAAJwJw/z5RdFwK7vdrqCgIEU9ViAP35bNXQ4AABeEymcHNncJAACcVmPeO3DggAIDA0/az/KZbgAAAAAALlWEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIl7NXcCl4NOp/RQYGNjcZQAAAAAAzjNmugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAjP6T4PujxdJA/fls1dBgAAuIhVPjuwuUsAADSBmW4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCJuD92maWrMmDEKDg6WYRgqLy939yYAAAAAALgguD10FxYWKj8/X2vXrlVNTY26dOlyzmNmZGRo8ODB517cWSgqKtI111yjgIAAhYaG6o477tBXX311XmsAAAAAAFzY3B66KyoqFBERoZSUFIWHh8vLy8vdm/jVHA6HGhoaTtvvyy+/1G9/+1vdeOONKi8vV1FRkfbt26chQ4achyoBAAAAABcLt4bujIwMjR8/XtXV1TIMQ9HR0TJNU7NmzVJsbKxsNpsSExO1YsUK52scDodGjRqlmJgY2Ww2JSQkaO7cuc712dnZWrJkiVavXi3DMGQYhkpKSlRSUiLDMLR//35n3/LychmGocrKSklSfn6+WrVqpbVr16pz587y9fVVVVWV6urqNGnSJLVt21Z+fn7q1auXSkpKnONs3bpVDodD06dPV4cOHXTllVcqKytLH3/8serr6915yAAAAAAAFzG3TkPPnTtXHTp00KJFi1RWViZPT09NmTJFK1eu1Pz589WxY0etX79ew4cPV2hoqFJTU9XQ0KB27dqpoKBAbdq0UWlpqcaMGaOIiAilpaUpKytLO3bskN1uV15eniQpODhYpaWlZ1TTkSNHlJOTo8WLFyskJERhYWEaOXKkKisrtXz5ckVGRmrVqlXq37+/tm3bpo4dO6pHjx7y9PRUXl6eMjIydOjQIS1dulS33HKLvL293XnIAAAAAAAXMbeG7qCgIAUEBMjT01Ph4eE6fPiw5syZo+LiYiUnJ0uSYmNjtXHjRi1cuFCpqany9vbW1KlTnWPExMSotLRUBQUFSktLk7+/v2w2m2praxUeHn7WNdXX12vevHlKTEyUdPz092XLlmnPnj2KjIyUJGVlZamwsFB5eXmaMWOGoqOj9fbbb+uuu+7SAw88IIfDoeTkZL355pun3FZtba1qa2udy3a7/azrBQAAAABcPCy94Hr79u06duyY+vbt69JeV1enpKQk5/KCBQu0ePFiVVVV6ejRo6qrq1P37t3dUoOPj4+6devmXN66datM01R8fLxLv9raWoWEhEiSvv32W/3ud7/TiBEjdM899+jgwYN66qmndOedd+qdd96RYRhNbisnJ8flCwQAAAAAwKXN0tDdeNOydevWqW3bti7rfH19JUkFBQXKzMxUbm6ukpOTFRAQoNmzZ2vTpk2nHNvD4/jl6KZpOtuaut7aZrO5hOSGhgZ5enpqy5Yt8vT0dOnr7+8vSXrhhRcUGBioWbNmOde98sorioqK0qZNm3TNNdc0WdPkyZM1YcIE57LdbldUVNQp9wMAAAAAcPGyNHQ33rysurpaqampTfbZsGGDUlJSNG7cOGdbRUWFSx8fHx85HA6XttDQUElSTU2NWrduLUln9EzwpKQkORwO7d27V717926yz5EjR04I5I3Lp7r7ua+vr/PLBAAAAAAA3P7IsJ8LCAhQVlaWMjMztWTJElVUVOijjz7SCy+8oCVLlkiS4uLitHnzZhUVFWnXrl168sknVVZW5jJOdHS0PvnkE+3cuVP79u1TfX294uLiFBUVpezsbO3atUvr1q1Tbm7uaWuKj4/XsGHDlJ6erpUrV+qrr75SWVmZZs6c6bxme+DAgSorK9Mzzzyj3bt3a+vWrRo5cqTat2/vclo8AAAAAACnYmnolqRp06bpqaeeUk5Ojjp16qR+/fppzZo1iomJkSSNHTtWQ4YM0dChQ9WrVy99//33LrPekjR69GglJCSoR48eCg0N1QcffCBvb28tW7ZMn3/+uRITEzVz5kxNnz79jGrKy8tTenq6Jk6cqISEBA0aNEibNm1yngp+44036tVXX9Xrr7+upKQk9e/fX76+viosLJTNZnPvAQIAAAAAXLQM8+cXRcOt7Ha7goKCFPVYgTx8WzZ3OQAA4CJW+ezA5i4BAC4pjXnvwIEDCgwMPGk/y2e6AQAAAAC4VBG6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLeDV3AZeCT6f2U2BgYHOXAQAAAAA4z5jpBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALMIjw86DLk8XycO3ZXOXAQDABaHy2YHNXQIAAG7DTDcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFnF76DZNU2PGjFFwcLAMw1B5ebm7NwEAAAAAwAXB7aG7sLBQ+fn5Wrt2rWpqatSlS5dzHjMjI0ODBw8+9+LOQkFBgbp3766WLVuqffv2mj179nndPgAAAADgwufl7gErKioUERGhlJQUdw99zhwOhwzDkIfHqb9reOuttzRs2DD95S9/0S233KIdO3bod7/7nWw2mx5++OHzVC0AAAAA4ELn1pnujIwMjR8/XtXV1TIMQ9HR0TJNU7NmzVJsbKxsNpsSExO1YsUK52scDodGjRqlmJgY2Ww2JSQkaO7cuc712dnZWrJkiVavXi3DMGQYhkpKSlRSUiLDMLR//35n3/LychmGocrKSklSfn6+WrVqpbVr16pz587y9fVVVVWV6urqNGnSJLVt21Z+fn7q1auXSkpKnOMsXbpUgwcP1tixYxUbG6uBAwfqD3/4g2bOnCnTNN15yAAAAAAAFzG3znTPnTtXHTp00KJFi1RWViZPT09NmTJFK1eu1Pz589WxY0etX79ew4cPV2hoqFJTU9XQ0KB27dqpoKBAbdq0UWlpqcaMGaOIiAilpaUpKytLO3bskN1uV15eniQpODhYpaWlZ1TTkSNHlJOTo8WLFyskJERhYWEaOXKkKisrtXz5ckVGRmrVqlXq37+/tm3bpo4dO6q2tlYtW7Z0Gcdms2nPnj2qqqpSdHR0k9uqra1VbW2tc9lut/+6AwkAAAAAuCi4NXQHBQUpICBAnp6eCg8P1+HDhzVnzhwVFxcrOTlZkhQbG6uNGzdq4cKFSk1Nlbe3t6ZOneocIyYmRqWlpSooKFBaWpr8/f1ls9lUW1ur8PDws66pvr5e8+bNU2JioqTjp78vW7ZMe/bsUWRkpCQpKytLhYWFysvL04wZM9SvXz9lZmYqIyNDN9xwg7744gs9//zzkqSampqThu6cnByXfQEAAAAAXNrcfk33z23fvl3Hjh1T3759Xdrr6uqUlJTkXF6wYIEWL16sqqoqHT16VHV1derevbtbavDx8VG3bt2cy1u3bpVpmoqPj3fpV1tbq5CQEEnS6NGjVVFRodtuu0319fUKDAzUo48+quzsbHl6ep50W5MnT9aECROcy3a7XVFRUW7ZDwAAAADAhcfS0N3Q0CBJWrdundq2beuyztfXV9Lxu4RnZmYqNzdXycnJCggI0OzZs7Vp06ZTjt14M7SfX2NdX19/Qj+bzSbDMFxq8vT01JYtW04I0P7+/pIkwzA0c+ZMzZgxQ99++61CQ0P17rvvStJJZ7kb96lxvwAAAAAAsDR0N968rLq6WqmpqU322bBhg1JSUjRu3DhnW0VFhUsfHx8fORwOl7bQ0FBJx0/3bt26tSSd0TPBk5KS5HA4tHfvXvXu3fuUfT09PZ1fFixbtkzJyckKCws77TYAAAAAAJAsDt0BAQHKyspSZmamGhoadN1118lut6u0tFT+/v4aMWKE4uLi9PLLL6uoqEgxMTFaunSpysrKFBMT4xwnOjpaRUVF2rlzp0JCQhQUFKS4uDhFRUUpOztb06dP1+7du5Wbm3vamuLj4zVs2DClp6crNzdXSUlJ2rdvn4qLi9W1a1cNGDBA+/bt04oVK9SnTx8dO3ZMeXl5+sc//qH333/fysMFAAAAALjIuPWRYU2ZNm2annrqKeXk5KhTp07q16+f1qxZ4wzVY8eO1ZAhQzR06FD16tVL33//vcust3T8GuuEhAT16NFDoaGh+uCDD+Tt7a1ly5bp888/V2JiombOnKnp06efUU15eXlKT0/XxIkTlZCQoEGDBmnTpk0u118vWbJEPXr00LXXXqvPPvtMJSUluvrqq913YAAAAAAAFz3D5MHTlrHb7QoKClLUYwXy8G15+hcAAABVPjuwuUsAAOC0GvPegQMHFBgYeNJ+ls90AwAAAABwqSJ0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEa/mLuBS8OnUfqd8WDoAAAAA4OLETDcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABbhOd3nQZeni+Th27K5ywAAAMDPVD47sLlLAHAJYKYbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIucVeg2TVNjxoxRcHCwDMNQeXm5RWUBAAAAAHDhO6vQXVhYqPz8fK1du1Y1NTXq0qXLOReQkZGhwYMHn/M4Z+rYsWPKyMhQ165d5eXlddJtv//++7rqqqvUokULxcbGasGCBeetRgAAAADAxeGsQndFRYUiIiKUkpKi8PBweXl5WVXXWXM4HGpoaDijfjabTY888ohuvvnmJvt89dVXGjBggHr37q2PPvpIjz/+uB555BH985//dHfZAAAAAICL2BmH7oyMDI0fP17V1dUyDEPR0dEyTVOzZs1SbGysbDabEhMTtWLFCudrHA6HRo0apZiYGNlsNiUkJGju3LnO9dnZ2VqyZIlWr14twzBkGIZKSkpUUlIiwzC0f/9+Z9/y8nIZhqHKykpJUn5+vlq1aqW1a9eqc+fO8vX1VVVVlerq6jRp0iS1bdtWfn5+6tWrl0pKSpzj+Pn5af78+Ro9erTCw8Ob3NcFCxbo8ssv1/PPP69OnTrpd7/7ne6//34999xzZ3q4AAAAAADQGU9Vz507Vx06dNCiRYtUVlYmT09PTZkyRStXrtT8+fPVsWNHrV+/XsOHD1doaKhSU1PV0NCgdu3aqaCgQG3atFFpaanGjBmjiIgIpaWlKSsrSzt27JDdbldeXp4kKTg4WKWlpWdU05EjR5STk6PFixcrJCREYWFhGjlypCorK7V8+XJFRkZq1apV6t+/v7Zt26aOHTue0bgffvihbrnlFpe2fv366aWXXlJ9fb28vb2bfF1tba1qa2udy3a7/Yy2BwAAAAC4OJ1x6A4KClJAQIA8PT0VHh6uw4cPa86cOSouLlZycrIkKTY2Vhs3btTChQuVmpoqb29vTZ061TlGTEyMSktLVVBQoLS0NPn7+8tms6m2tvaks86nUl9fr3nz5ikxMVHS8dPfly1bpj179igyMlKSlJWVpcLCQuXl5WnGjBlnNO63336ryy67zKXtsssu008//aR9+/YpIiKiydfl5OS47C8AAAAA4NL2qy/K3r59u44dO6a+ffu6tNfV1SkpKcm5vGDBAi1evFhVVVU6evSo6urq1L17919d8M/5+PioW7duzuWtW7fKNE3Fx8e79KutrVVISMhZjW0YhsuyaZpNtv/c5MmTNWHCBOey3W5XVFTUWW0XAAAAAHDx+NWhu/GmZevWrVPbtm1d1vn6+kqSCgoKlJmZqdzcXCUnJysgIECzZ8/Wpk2bTjm2h8fxS80bg650fFb7l2w2m0sIbmhokKenp7Zs2SJPT0+Xvv7+/me8b+Hh4fr2229d2vbu3SsvL69ThndfX1/nvgMAAAAA8KtDd+PNy6qrq5Wamtpknw0bNiglJUXjxo1ztlVUVLj08fHxkcPhcGkLDQ2VJNXU1Kh169aSdEbPBE9KSpLD4dDevXvVu3fvs9kdF8nJyVqzZo1L29tvv60ePXqc9HpuAAAAAAB+6aweGfZzAQEBysrKUmZmppYsWaKKigp99NFHeuGFF7RkyRJJUlxcnDZv3qyioiLt2rVLTz75pMrKylzGiY6O1ieffKKdO3dq3759qq+vV1xcnKKiopSdna1du3Zp3bp1ys3NPW1N8fHxGjZsmNLT07Vy5Up99dVXKisr08yZM/Xmm286+23fvl3l5eX64YcfdODAAZWXl7uE+rFjx6qqqkoTJkzQjh079Le//U0vvfSSsrKyfu3hAgAAAABcgs7pQdvTpk1TWFiYcnJy9OWXX6pVq1a68sor9fjjj0s6Hl7Ly8s1dOhQGYahe+65R+PGjdNbb73lHGP06NEqKSlRjx49dOjQIb333nvq06ePli1bpgcffFCJiYnq2bOnpk+frrvuuuu0NeXl5Wn69OmaOHGivvnmG4WEhCg5OVkDBgxw9hkwYICqqqqcy43XoDeezh4TE6M333xTmZmZeuGFFxQZGak///nPuuOOO87lcAEAAAAALjGG+fMLp+FWdrtdQUFBinqsQB6+LZu7HAAAAPxM5bMDm7sEABewxrx34MABBQYGnrTfrz69HAAAAAAAnBqhGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAi3g1dwGXgk+n9jvlw9IBAAAAABcnZroBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwiFdzF3AxM01TkmS325u5EgAAAACAOzXmvMbcdzKEbgt9//33kqSoqKhmrgQAAAAAYIWDBw8qKCjopOsJ3RYKDg6WJFVXV5/ylwBcrOx2u6KiovT1118rMDCwucsBzjveA7jU8R7ApY73wMXNNE0dPHhQkZGRp+xH6LaQh8fxS+aDgoJ4k+GSFhgYyHsAlzTeA7jU8R7ApY73wMXrTCZXuZEaAAAAAAAWIXQDAAAAAGARQreFfH199fTTT8vX17e5SwGaBe8BXOp4D+BSx3sAlzreA5Akwzzd/c0BAAAAAMCvwkw3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdFtk3rx5iomJUYsWLXTVVVdpw4YNzV0ScN5kZ2fLMAyXn/Dw8OYuC7DM+vXrdfvttysyMlKGYej11193WW+aprKzsxUZGSmbzaY+ffros88+a55iAQuc7j2QkZFxwufCNddc0zzFAm6Wk5Ojnj17KiAgQGFhYRo8eLB27tzp0ofPgUsbodsCr732mh577DE98cQT+uijj9S7d2/deuutqq6ubu7SgPPmN7/5jWpqapw/27Zta+6SAMscPnxYiYmJ+utf/9rk+lmzZmnOnDn661//qrKyMoWHh6tv3746ePDgea4UsMbp3gOS1L9/f5fPhTfffPM8VghY5/3339dDDz2kf/3rX3rnnXf0008/6ZZbbtHhw4edffgcuLRx93IL9OrVS1deeaXmz5/vbOvUqZMGDx6snJycZqwMOD+ys7P1+uuvq7y8vLlLAc47wzC0atUqDR48WNLx2Y3IyEg99thj+sMf/iBJqq2t1WWXXaaZM2fqgQceaMZqAff75XtAOj7TvX///hNmwIGL0XfffaewsDC9//77uv766/kcADPd7lZXV6ctW7bolltucWm/5ZZbVFpa2kxVAeff7t27FRkZqZiYGN1999368ssvm7skoFl89dVX+vbbb10+F3x9fZWamsrnAi4pJSUlCgsLU3x8vEaPHq29e/c2d0mAJQ4cOCBJCg4OlsTnAAjdbrdv3z45HA5ddtllLu2XXXaZvv3222aqCji/evXqpZdffllFRUV68cUX9e233yolJUXff/99c5cGnHeN/9/P5wIuZbfeeqv+/ve/q7i4WLm5uSorK9ONN96o2tra5i4NcCvTNDVhwgRdd9116tKliyQ+ByB5NXcBFyvDMFyWTdM8oQ24WN16663Of3ft2lXJycnq0KGDlixZogkTJjRjZUDz4XMBl7KhQ4c6/92lSxf16NFD7du317p16zRkyJBmrAxwr4cffliffPKJNm7ceMI6PgcuXcx0u1mbNm3k6el5wrdWe/fuPeHbLeBS4efnp65du2r37t3NXQpw3jXeuZ/PBeD/REREqH379nwu4KIyfvx4vfHGG3rvvffUrl07ZzufAyB0u5mPj4+uuuoqvfPOOy7t77zzjlJSUpqpKqB51dbWaseOHYqIiGjuUoDzLiYmRuHh4S6fC3V1dXr//ff5XMAl6/vvv9fXX3/N5wIuCqZp6uGHH9bKlStVXFysmJgYl/V8DoDTyy0wYcIE3XffferRo4eSk5O1aNEiVVdXa+zYsc1dGnBeZGVl6fbbb9fll1+uvXv3avr06bLb7RoxYkRzlwZY4tChQ/riiy+cy1999ZXKy8sVHBysyy+/XI899phmzJihjh07qmPHjpoxY4Zatmype++9txmrBtznVO+B4OBgZWdn64477lBERIQqKyv1+OOPq02bNvr//r//rxmrBtzjoYce0quvvqrVq1crICDAOaMdFBQkm80mwzD4HLjUmbDECy+8YLZv39708fExr7zySvP9999v7pKA82bo0KFmRESE6e3tbUZGRppDhgwxP/vss+YuC7DMe++9Z0o64WfEiBGmaZpmQ0OD+fTTT5vh4eGmr6+vef3115vbtm1r3qIBNzrVe+DIkSPmLbfcYoaGhpre3t7m5Zdfbo4YMcKsrq5u7rIBt2jqb1+SmZeX5+zD58Cljed0AwAAAABgEa7pBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAOAiU1lZKcMwVF5eLkkqKSmRYRjav3+/s8/rr7+uuLg4eXp66rHHHjtpW1OKi4t1xRVXqKGhwbqdaEZ79+5VaGiovvnmm2aroanfGQDgwkToBgCcFxkZGRo8eHBzl3FSvwyqF5OUlBTV1NQoKCjI2fbAAw/ozjvv1Ndff61p06adtK0pkyZN0hNPPCEPj+P/GeFwOJSTk6MrrrhCNptNwcHBuuaaa5SXl+d8TUZGhgzDOOHniy++cPb59ttvNX78eMXGxsrX11dRUVG6/fbb9e677552H+12u5588kn95je/kc1mU0hIiHr27KlZs2bpxx9/dOn7xRdfaOTIkWrXrp18fX0VExOje+65R5s3b5YkhYWF6b777tPTTz99BkcXAIBT82ruAgAAaG51dXXNXYKlfHx8FB4e7lw+dOiQ9u7dq379+ikyMvKkbU0pLS3V7t27dddddznbsrOztWjRIv31r39Vjx49ZLfbtXnz5hPCbv/+/V2CuCSFhoZKOv6lx7XXXqtWrVpp1qxZ6tatm+rr61VUVKSHHnpIn3/++Ulr+uGHH3TdddfJbrdr2rRpuuqqq+Tj46MvvvhCr776ql599VU99NBDkqTNmzfrpptuUpcuXbRw4UJdccUVOnjwoFavXq2JEyfq/ffflySNHDlSV199tWbPnq3WrVufyWGWw+GQYRjOLyMAAJAkmQAAnAcjRowwf/vb3zqXU1NTzYcffth89NFHzVatWplhYWHmwoULzUOHDpkZGRmmv7+/GRsba7755pvO17z33numJHPt2rVmt27dTF9fX/Pqq682P/nkE5dtrVixwuzcubPp4+Njtm/f3nzuuedc1rdv396cNm2aOWLECDMwMNBMT083Jbn8pKammqZpmv/+97/Nm2++2QwJCTEDAwPN66+/3tyyZYvLeJLMF1980Rw8eLBps9nMuLg4c/Xq1S59Pv30U3PAgAFmQECA6e/vb1533XXmF1984Vz/t7/9zbziiitMX19fMyEhwXzhhRfO+Nhu2rTJ7N69u+nr62teddVV5sqVK01J5kcffeRy3H788Ufnv3/+c7K2powfP9688847XdoSExPN7OzsU9b4y9//L916661m27ZtzUOHDp2w7scffzzl2A888IDp5+dn7tmzp8n1DQ0Nzv/9zW9+Y1511VWmw+E47Xaio6PNl1566aTbzcvLM4OCgsw1a9aYnTp1Mj09Pc0vv/zSLX8zP/+dmaZpHjlyxBwwYIDZq1cv8/vvv2+ynn/84x9mly5dzBYtWpjBwcHmTTfd5Dyeqamp5qOPPurS/7e//a05YsQI53Lj++K+++4z/fz8zMsvv9x8/fXXzb1795qDBg0y/fz8zC5duphlZWUnPSYAgBPxVSwAoNksWbJEbdq00b///W+NHz9eDz74oO666y6lpKRo69at6tevn+677z4dOXLE5XW///3v9dxzz6msrExhYWEaNGiQ6uvrJUlbtmxRWlqa7r77bm3btk3Z2dl68sknlZ+f7zLG7Nmz1aVLF23ZskVPPvmk/v3vf0uS/vd//1c1NTVauXKlJOngwYMaMWKENmzYoH/961/q2LGjBgwYoIMHD7qMN3XqVKWlpemTTz7RgAEDNGzYMP3www+SpG+++UbXX3+9WrRooeLiYm3ZskX333+/fvrpJ0nSiy++qCeeeEJ//OMftWPHDs2YMUNPPvmklixZctpjePjwYd12221KSEjQli1blJ2draysrJP2T0lJ0c6dOyVJ//znP1VTU3PStqasX79ePXr0cGkLDw9XcXGxvvvuu9PW25QffvhBhYWFeuihh+Tn53fC+latWp30tQ0NDXrttdc0fPhwtW3btsk+hmFIksrLy/XZZ59p4sSJTc5G/3I7V199tTZs2HDK2o8cOaKcnBwtXrxYn332mcLCwtzyN/NzBw4c0C233KK6ujq9++67Cg4OPqFPTU2N7rnnHt1///3asWOHSkpKNGTIEJmmecr6f+lPf/qTrr32Wn300UcaOHCg7rvvPqWnp2v48OHaunWr4uLilJ6eftbjAsAlrblTPwDg0tDUTPd1113nXP7pp59MPz8/87777nO21dTUmJLMDz/80DTN/5v9W758ubPP999/b9psNvO1114zTdM07733XrNv374u2/79739vdu7c2bncvn17c/DgwS59vvrqK5fZ4ZP56aefzICAAHPNmjXONknmlClTnMuHDh0yDcMw33rrLdM0TXPy5MlmTEyMWVdX1+SYUVFR5quvvurSNm3aNDM5OfmUtZimaS5cuNAMDg42Dx8+7GybP3/+SWe6TfP4jK5+MZvdVFtTgoKCzJdfftml7bPPPjM7depkenh4mF27djUfeOABlzMUTPP479/T09P08/Nz/jTOmG/atMmUZK5cufK0+/tL3377rSnJnDNnjkv7lVde6dzO3XffbZqmab722mumJHPr1q1nNHZmZqbZp0+fk67Py8szJZnl5eWnHOfX/M00/s4+//xzMzEx0RwyZIhZW1t70m1s2bLFlGRWVlY2uf5MZ7qHDx/uXG58/z355JPOtg8//NCUZNbU1JxynwEA/4eZbgBAs+nWrZvz356engoJCVHXrl2dbZdddpmk43eT/rnk5GTnv4ODg5WQkKAdO3ZIknbs2KFrr73Wpf+1116r3bt3y+FwONt+OVt7Mnv37tXYsWMVHx+voKAgBQUF6dChQ6qurj7pvvj5+SkgIMBZd3l5uXr37i1vb+8Txv/uu+/09ddfa9SoUfL393f+TJ8+XRUVFaetb8eOHUpMTFTLli2dbT8/Pu529OhRtWjRwqWtc+fO+vTTT/Wvf/1LI0eO1H//+1/dfvvt+t3vfufS74YbblB5ebnz589//rMkOWdNG2ekT2bs2LEux+jnfvnaVatWqby8XP369dPRo0fPajuNbDbbCWdZ/JKPj4/L715yz99Mo5tvvlmxsbEqKCiQj4/PSetITEzUTTfdpK5du+quu+7Siy++eMI19Wfi5zU1vv/O5D0JADg5bqQGAGg2vwyhhmG4tDWGozN5NFVjX9M0TwhVZhOnwjZ1GnNTMjIy9N133+n5559X+/bt5evrq+Tk5BNuvtbUvjTWbbPZTjp+Y58XX3xRvXr1clnn6el52vqa2jcrtWnTpskw5+HhoZ49e6pnz57KzMzUK6+8ovvuu09PPPGEYmJiJB0/5nFxcSe8tmPHjjIMQzt27DjlHe6feeaZE06dDw0NVatWrU640drll18uSQoICHA+dis+Pl7S8S8qunfvftp9/eGHH5w3ejsZm812wt+bO/5mGg0cOFD//Oc/tX37dpfw+0uenp565513VFpaqrffflt/+ctf9MQTT2jTpk2KiYmRh4fHCX8rjZdknKymxv36te9JAMBxzHQDAC44//rXv5z//vHHH7Vr1y5dccUVko7Pum7cuNGlf2lpqeLj408ZYhtnEX8+Gy5JGzZs0COPPKIBAwboN7/5jXx9fbVv376zqrdbt27asGFDkyHnsssuU9u2bfXll18qLi7O5acxrJ5K586d9fHHHztncyXX4+NuSUlJ2r59+xnVJR2/5vx0goOD1a9fP73wwgtN9m8MzWFhYS7HRzoe9tPS0vTKK6+c9rna3bt3V+fOnZWbm9tkaPzlM7E//fRTJSUlnbb+X3LH30yjZ599ViNGjNBNN9102uNuGIauvfZaTZ06VR999JF8fHy0atUqSce/nKipqXH2dTgc+vTTT39VTQCAs0PoBgBccJ555hm9++67+vTTT5WRkaE2bdo4Z0gnTpyod999V9OmTdOuXbu0ZMkS/fWvfz3lzcWk44HOZrOpsLBQ//3vf3XgwAFJUlxcnJYuXaodO3Zo06ZNGjZs2Clnrpvy8MMPy2636+6779bmzZu1e/duLV261HnzsuzsbOXk5Gju3LnatWuXtm3bpry8PM2ZM+e0Y997773y8PDQqFGjtH37dr355pt67rnnzqq+s9GvX78TvtS488479ac//UmbNm1SVVWVSkpK9NBDDyk+Pt75ZcjpzJs3Tw6HQ1dffbX++c9/avfu3dqxY4f+/Oc/n/Z0+RkzZqht27bq1auX/va3v+mTTz5RRUWFVq1apQ8//ND5ZYthGMrLy9OuXbt0/fXX680339SXX36pTz75RH/84x/129/+1jnmkSNHtGXLFt1yyy1neYTc8zfzc88995yGDRumG2+88aSPTtu0aZNmzJihzZs3q7q6WitXrtR3332nTp06SZJuvPFGrVu3TuvWrdPnn3+ucePGnfAlAwDAGoRuAMAF59lnn9Wjjz6qq666SjU1NXrjjTecM9VXXnmlCgoKtHz5cnXp0kVPPfWUnnnmGWVkZJxyTC8vL/35z3/WwoULFRkZ6Qxgf/vb3/Tjjz8qKSlJ9913nx555BGFhYWdVb0hISEqLi7WoUOHlJqaqquuukovvvii87Td3/3ud1q8eLHy8/PVtWtXpaamKj8//4xmuv39/bVmzRpt375dSUlJeuKJJzRz5syzqu9sDB8+XNu3b3d+YSAdD+Jr1qzR7bffrvj4eI0YMUJXXHGF3n77bXl5ndmVbDExMdq6datuuOEGTZw4UV26dFHfvn317rvvav78+ad8bUhIiP79738rPT1ds2fP1tVXX62uXbsqOztbQ4cO1Ysvvujse/XVV2vz5s3q0KGDRo8erU6dOmnQoEH67LPP9Pzzzzv7rV69Wpdffrl69+59dgdI7vmb+aU//elPSktL04033qhdu3adsD4wMFDr16/XgAEDFB8frylTpig3N1e33nqrJOn+++/XiBEjlJ6ertTUVMXExOiGG244p5oAAGfGMM/3xWAAAPxKJSUluuGGG/Tjjz+e8jFSsNakSZN04MABLVy4sLlLsczVV1+txx57TPfee29zlwIAuMAx0w0AAM7KE088ofbt259w/fvFYu/evbrzzjt1zz33NHcpAICLADPdAIALxqU40z1jxgzNmDGjyXW9e/fWW2+9dZ4rAgAAZ4PQDQDA/8N++OEH/fDDD02us9lsatu27XmuCAAAnA1CNwAAAAAAFuGabgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIv8/t6pWL78E9UoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_importance_Diff_rank3['rank_sum']   =df_importance_Diff_rank3.loc[:,:].sum(axis=1)\n",
    "df_importance_Diff_rank3['rank_median']=df_importance_Diff_rank3.iloc[:,:-1].median(axis=1)\n",
    "\n",
    "\n",
    "# high acc CV only\n",
    "df_acc.loc[:,(df_acc>=acc_th).sum(axis=0)==2]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_names, df_importance_Diff_rank3['rank_sum'])\n",
    "plt.xlabel(\"Importance_diff (SFC-GC) rank sum\")\n",
    "plt.title(\"Feature Importances rank sum\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# high acc CV only median\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_names, df_importance_Diff_rank3['rank_median'])\n",
    "plt.xlabel(\"Importance_diff (SFC-GC) rank median\")\n",
    "plt.title(\"Feature Importances rank median\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02cca43b-6704-45d2-ba01-b46a59e2a696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CV4</th>\n",
       "      <th>CV5</th>\n",
       "      <th>CV8</th>\n",
       "      <th>rank_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feature7</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature9</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature5</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature10</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature1</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature2</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature3</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature6</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CV4  CV5  CV8  rank_sum\n",
       "feature7     1    7    4        12\n",
       "feature9     4    3    5        12\n",
       "feature5     7    4    3        14\n",
       "feature8     2    2   10        14\n",
       "feature10    3    6    7        16\n",
       "feature1     6    5    6        17\n",
       "feature2     8    1    9        18\n",
       "feature3     9    9    1        19\n",
       "feature6     5    8    8        21\n",
       "feature4    10   10    2        22"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance_Diff_rank3.drop('rank_median',axis=1).sort_values('rank_sum').astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3268f-ba03-4197-aa4e-504af262551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
